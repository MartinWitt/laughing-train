# hudi 
 
# Bad smells
I found 2929 bad smells with 107 repairable:
| ruleID | number | fixable |
| --- | --- | --- |
| UNCHECKED_WARNING | 721 | false |
| JavadocDeclaration | 462 | false |
| Deprecation | 396 | false |
| FieldMayBeFinal | 255 | false |
| DuplicatedCode | 125 | false |
| DefaultAnnotationParam | 68 | false |
| FieldCanBeLocal | 66 | false |
| DataFlowIssue | 58 | false |
| AutoCloseableResource | 50 | false |
| UnnecessaryToStringCall | 43 | true |
| UnusedAssignment | 42 | false |
| ConstantValue | 41 | false |
| UnnecessaryLocalVariable | 40 | true |
| NullableProblems | 37 | false |
| JavadocLinkAsPlainText | 37 | false |
| DuplicateBranchesInSwitch | 34 | false |
| SimplifyStreamApiCallChains | 31 | false |
| StringBufferReplaceableByString | 27 | false |
| IgnoreResultOfCall | 25 | false |
| OptionalGetWithoutIsPresent | 25 | false |
| JavadocReference | 24 | false |
| RedundantCast | 24 | false |
| StringConcatenationInsideStringBufferAppend | 20 | false |
| DeprecatedIsStillUsed | 20 | false |
| DanglingJavadoc | 19 | false |
| RedundantMethodOverride | 16 | false |
| WrapperTypeMayBePrimitive | 15 | false |
| SwitchStatementWithTooFewBranches | 14 | false |
| ToArrayCallWithZeroLengthArrayArgument | 11 | true |
| TrivialStringConcatenation | 10 | false |
| SuspiciousMethodCalls | 10 | false |
| BusyWait | 8 | false |
| CommentedOutCode | 7 | false |
| NonFinalFieldInEnum | 7 | false |
| DoubleCheckedLocking | 6 | false |
| UnnecessarySemicolon | 6 | false |
| RedundantStreamOptionalCall | 5 | false |
| InfiniteLoopStatement | 5 | false |
| ArraysAsListWithZeroOrOneArgument | 5 | false |
| UnnecessaryModifier | 4 | true |
| RegExpRedundantEscape | 4 | false |
| SimplifiableConditionalExpression | 4 | false |
| ReplaceInefficientStreamCount | 4 | false |
| TypeParameterHidesVisibleType | 4 | false |
| SuspiciousToArrayCall | 4 | false |
| UnnecessaryUnicodeEscape | 4 | false |
| IntegerDivisionInFloatingPointContext | 4 | false |
| EmptyStatementBody | 3 | false |
| RedundantArrayCreation | 3 | true |
| ThrowFromFinallyBlock | 3 | false |
| IntegerMultiplicationImplicitCastToLong | 3 | false |
| FuseStreamOperations | 3 | false |
| MathRoundingWithIntArgument | 3 | false |
| IOStreamConstructor | 3 | false |
| TrivialIf | 3 | false |
| SynchronizeOnNonFinalField | 3 | false |
| SynchronizationOnLocalVariableOrMethodParameter | 3 | false |
| BigDecimalMethodWithoutRoundingCalled | 3 | false |
| UnnecessaryReturn | 2 | true |
| FinalStaticMethod | 2 | false |
| RedundantTypeArguments | 2 | false |
| SlowAbstractSetRemoveAll | 2 | false |
| DuplicateThrows | 2 | false |
| StringOperationCanBeSimplified | 2 | false |
| JavaReflectionMemberAccess | 2 | false |
| CollectionAddAllCanBeReplacedWithConstructor | 2 | false |
| InnerClassMayBeStatic | 2 | true |
| SwitchStatementWithConfusingDeclaration | 2 | false |
| CaughtExceptionImmediatelyRethrown | 2 | false |
| PointlessArithmeticExpression | 1 | false |
| FinalMethodInFinalClass | 1 | false |
| ForLoopReplaceableByWhile | 1 | false |
| FinalPrivateMethod | 1 | false |
| MismatchedStringBuilderQueryUpdate | 1 | false |
| NegativeIntConstantInLongContext | 1 | false |
| EqualsBetweenInconvertibleTypes | 1 | false |
| MalformedFormatString | 1 | false |
| ReturnFromFinallyBlock | 1 | false |
| MismatchedCollectionQueryUpdate | 1 | false |
| RedundantStringFormatCall | 1 | false |
| RedundantUnmodifiable | 1 | false |
| NullArgumentToVariableArgMethod | 1 | false |
| NonAtomicOperationOnVolatileField | 1 | false |
| RedundantOperationOnEmptyContainer | 1 | false |
| UnaryPlus | 1 | false |
| FinallyBlockCannotCompleteNormally | 1 | false |
| MismatchedJavadocCode | 1 | false |
| ProtectedMemberInFinalClass | 1 | true |
| NonStrictComparisonCanBeEquality | 1 | true |
| OptionalUsedAsFieldOrParameterType | 1 | false |
| CharsetObjectCanBeUsed | 1 | false |
| ImplicitArrayToString | 1 | false |
| ConditionCoveredByFurtherCondition | 1 | false |
| PrimitiveArrayArgumentToVariableArgMethod | 1 | false |
| EqualsWhichDoesntCheckParameterClass | 1 | false |
| UseBulkOperation | 1 | false |
## RuleId[id=ToArrayCallWithZeroLengthArrayArgument]
### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[l.size()\]'
in `hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java`
#### Snippet
```java

    String[][] rows =
        buffer.getRenderRows().stream().map(l -> l.toArray(new String[l.size()])).toArray(String[][]::new);
    return printTextTable(header, rows);
  }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Comparable\[row.size()\]'
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java
        structType = structType.add(DataTypes.createStructField(headersNoSpaces.get(i), headerDataType, true));
      }
      List<Row> records = rows.stream().map(row -> RowFactory.create(row.toArray(new Comparable[row.size()])))
              .collect(Collectors.toList());
      Dataset<Row> dataset = this.sqlContext.createDataFrame(records, structType);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Comparable\[commitDetails.size()\]'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ArchivedCommitsCommand.java`
#### Snippet
```java
      commitDetails.add(Option.ofNullable(record.get(metadataName)).orElse("{}").toString());
    }
    return commitDetails.toArray(new Comparable[commitDetails.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[uniquePaths.size()\]'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
    // read files based on the file extension name
    if (paths.size() == 0 || paths.get(0).endsWith(HoodieFileFormat.PARQUET.getFileExtension())) {
      originalDF = sqlContextOpt.get().read().parquet(uniquePaths.toArray(new String[uniquePaths.size()]));
    } else if (paths.get(0).endsWith(HoodieFileFormat.ORC.getFileExtension())) {
      originalDF = sqlContextOpt.get().read().orc(uniquePaths.toArray(new String[uniquePaths.size()]));
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[uniquePaths.size()\]'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
      originalDF = sqlContextOpt.get().read().parquet(uniquePaths.toArray(new String[uniquePaths.size()]));
    } else if (paths.get(0).endsWith(HoodieFileFormat.ORC.getFileExtension())) {
      originalDF = sqlContextOpt.get().read().orc(uniquePaths.toArray(new String[uniquePaths.size()]));
    }
    StructType schema = originalDF.schema();
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Path\[nonHoodiePaths.size()\]'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieCopyOnWriteTableInputFormat.java`
#### Snippet
```java
    List<Path> nonHoodiePaths = inputPathHandler.getNonHoodieInputPaths();
    if (nonHoodiePaths.size() > 0) {
      setInputPaths(job, nonHoodiePaths.toArray(new Path[nonHoodiePaths.size()]));
      FileStatus[] fileStatuses = doListStatus(job);
      returns.addAll(Arrays.asList(fileStatuses));
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CombineFileSplit\[combineFileSplits.size()\]'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
          }
        }
        return combineFileSplits.toArray(new CombineFileSplit[combineFileSplits.size()]);
      } else {
        InputSplit[] splits = super.getSplits(job, numSplits);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new HadoopShimsSecure.InputSplitShim\[inputSplitShims.size()\]'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
        }
        return (CombineFileSplit[]) inputSplitShims
            .toArray(new HadoopShimsSecure.InputSplitShim[inputSplitShims.size()]);
      }
    }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CombineHiveInputSplit\[result.size()\]'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java

    LOG.info("number of splits " + result.size());
    return result.toArray(new CombineHiveInputSplit[result.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Path\[paths.size()\]'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
        try {
          List<Path> paths = Utilities.getInputPathsTez(job, mrwork);
          dirs = paths.toArray(new Path[paths.size()]);
        } catch (Exception e) {
          throw new IOException("Could not create input files", e);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new InputSplit\[result.size()\]'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
    LOG.info("Number of all splits " + result.size());
    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);
    return result.toArray(new InputSplit[result.size()]);
  }

```

## RuleId[id=WrapperTypeMayBePrimitive]
### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
    try (HTable hTable = (HTable) hbaseConnection.getTable(TableName.valueOf(tableName));
         BufferedMutator mutator = hbaseConnection.getBufferedMutator(TableName.valueOf(tableName))) {
      Long rollbackTime = HoodieActiveTimeline.parseDateFromInstantTime(instantTime).getTime();
      Long currentTime = new Date().getTime();
      Scan scan = new Scan();
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
         BufferedMutator mutator = hbaseConnection.getBufferedMutator(TableName.valueOf(tableName))) {
      Long rollbackTime = HoodieActiveTimeline.parseDateFromInstantTime(instantTime).getTime();
      Long currentTime = new Date().getTime();
      Scan scan = new Scan();
      scan.addFamily(SYSTEM_COLUMN_FAMILY);
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
    // `multiGetBatchSize` is intended to be a batch per 100ms. To create a rate limiter that measures
    // operations per second, we need to multiply `multiGetBatchSize` by 10.
    Integer multiGetBatchSize = config.getHbaseIndexGetBatchSize();
    return (partitionNum, hoodieRecordIterator) -> {
      boolean updatePartitionPath = config.getHbaseIndexUpdatePartitionPath();
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
      byte[] val = isCompressionEnabled ? DISK_COMPRESSION_REF.get().compressBytes(SerializationUtils.serialize(value)) :
          SerializationUtils.serialize(value);
      Integer valueSize = val.length;
      Long timestamp = System.currentTimeMillis();
      this.valueMetadataMap.put(key,
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
          SerializationUtils.serialize(value);
      Integer valueSize = val.length;
      Long timestamp = System.currentTimeMillis();
      this.valueMetadataMap.put(key,
          new BitCaskDiskMap.ValueMetadata(this.filePath, valueSize, filePosition.get(), timestamp));
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalUpsertTime() {
    Long totalUpsertTime = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalCompactedRecordsUpdated() {
    Long totalUpdateRecords = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalLogRecordsCompacted() {
    Long totalLogRecords = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalScanTime() {
    Long totalScanTime = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalLogFilesCompacted() {
    Long totalLogFiles = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalLogFilesSize() {
    Long totalLogFilesSize = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java

  public Long getTotalCreateTime() {
    Long totalCreateTime = 0L;
    for (Map.Entry<String, List<HoodieWriteStat>> entry : partitionToWriteStats.entrySet()) {
      for (HoodieWriteStat writeStat : entry.getValue()) {
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java`
#### Snippet
```java
  private Pair<HoodieSeekingFileReader<?>, Long> getBaseFileReader(FileSlice slice, HoodieTimer timer) throws IOException {
    HoodieSeekingFileReader<?> baseFileReader;
    Long baseFileOpenMs;
    // If the base file is present then create a reader
    Option<HoodieBaseFile> basefile = slice.getBaseFile();
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
        prunedSchema = InternalSchemaUtils.pruneInternalSchema(internalSchemaOption.get(), requiredColumns);
        InternalSchema querySchema = prunedSchema;
        Long commitTime = Long.valueOf(FSUtils.getCommitTime(finalPath.getName()));
        InternalSchema fileSchema = InternalSchemaCache.searchSchemaAndCache(commitTime, metaClient, false);
        InternalSchema mergedInternalSchema = new InternalSchemaMerger(fileSchema, querySchema, true,
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java

  private Long delayOffsetCalculation(Option<String> lastCheckpointStr, Set<TopicPartition> topicPartitions, KafkaConsumer consumer) {
    Long delayCount = 0L;
    Map<TopicPartition, Long> checkpointOffsets = CheckpointUtils.strToOffsets(lastCheckpointStr.get());
    Map<TopicPartition, Long> lastOffsets = consumer.endOffsets(topicPartitions);
```

## RuleId[id=UnnecessaryModifier]
### UnnecessaryModifier
Modifier `transient` is redundant for a 'static' field
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
  private long totalNumInserts;
  private int numWriteStatusWithInserts;
  private static transient Thread shutdownThread;

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/InstantRange.java`
#### Snippet
```java
   * Represents a range type.
   */
  public static enum RangeType {
    OPEN_CLOSE, CLOSE_CLOSE
  }
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChange.java`
#### Snippet
```java
    private String name;

    private ColumnChangeID() {
      this.name = this.name().toLowerCase(Locale.ROOT);
    }
```

### UnnecessaryModifier
Modifier `transient` is redundant for a 'static' field
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieCombineRealtimeRecordReader.java`
#### Snippet
```java
public class HoodieCombineRealtimeRecordReader implements RecordReader<NullWritable, ArrayWritable> {

  private static final transient Logger LOG = LoggerFactory.getLogger(HoodieCombineRealtimeRecordReader.class);
  // RecordReaders for each split
  List<HoodieRealtimeRecordReader> recordReaders = new LinkedList<>();
```

## RuleId[id=PointlessArithmeticExpression]
### PointlessArithmeticExpression
`1 * 1024 * 1024` can be replaced with '1024 \* 1024'
in `hudi-common/src/main/java/org/apache/hudi/common/config/HoodieStorageConfig.java`
#### Snippet
```java
  public static final ConfigProperty<String> PARQUET_PAGE_SIZE = ConfigProperty
      .key("hoodie.parquet.page.size")
      .defaultValue(String.valueOf(1 * 1024 * 1024))
      .markAdvanced()
      .withDocumentation("Parquet page size in bytes. Page is the unit of read within a parquet file. "
```

## RuleId[id=DoubleCheckedLocking]
### DoubleCheckedLocking
Double-checked locking
in `hudi-common/src/main/java/org/apache/hudi/util/Transient.java`
#### Snippet
```java

  public T get() {
    if (!initialized) {
      synchronized (this) {
        if (!initialized) {
```

### DoubleCheckedLocking
Double-checked locking
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDbDiskMap.java`
#### Snippet
```java

  private RocksDBDAO getRocksDb() {
    if (null == rocksDb) {
      synchronized (this) {
        if (null == rocksDb) {
```

### DoubleCheckedLocking
Double-checked locking
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java

    private HFile.Reader partitionIndexReader() {
      if (null == indexByPartitionReader) {
        synchronized (this) {
          if (null == indexByPartitionReader) {
```

### DoubleCheckedLocking
Double-checked locking
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java

    private HFile.Reader fileIdIndexReader() {
      if (null == indexByFileIdReader) {
        synchronized (this) {
          if (null == indexByFileIdReader) {
```

### DoubleCheckedLocking
Double-checked locking
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/kafka/KafkaConnectControlAgent.java`
#### Snippet
```java
  public static KafkaConnectControlAgent createKafkaControlManager(String bootstrapServers,
                                                                   String controlTopicName) {
    if (agent == null) {
      synchronized (LOCK) {
        if (agent == null) {
```

### DoubleCheckedLocking
Double-checked locking
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/MarkerHandler.java`
#### Snippet
```java
    MarkerDirState markerDirState = getMarkerDirState(markerDir);
    markerDirState.addMarkerCreationFuture(future);
    if (!firstCreationRequestSeen) {
      synchronized (firstCreationRequestSeenLock) {
        if (!firstCreationRequestSeen) {
```

## RuleId[id=EmptyStatementBody]
### EmptyStatementBody
`else` statement has empty body
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
    } else if (sparkType instanceof UserDefinedType) {
      throw new UnsupportedOperationException("User-defined types are not supported");
    } else {
      // do nothings
    }
```

### EmptyStatementBody
`else` statement has empty body
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java
        scanner.close();
        return new ArrayList<>();
      } else {
        // TODO: we may directly rename original parquet file if there is not evolution/devolution of schema
        /*
```

### EmptyStatementBody
`if` statement has empty body
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
    Heartbeat heartbeat = instantToHeartbeatMap.get(instantTime);
    ValidationUtils.checkArgument(heartbeat == null || !heartbeat.isHeartbeatStopped(), "Cannot restart a stopped heartbeat for " + instantTime);
    if (heartbeat != null && heartbeat.isHeartbeatStarted()) {
      // heartbeat already started, NO_OP
    } else {
```

## RuleId[id=CommentedOutCode]
### CommentedOutCode
Commented out code (7 lines)
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndexHelper.java`
#### Snippet
```java
      String fileGroupId = partitionFileGroupId.getFileId();

      /*
      // TODO(HUDI-5619) uncomment when addressed
      String baseFileName =
```

### CommentedOutCode
Commented out code (7 lines)
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java
      } else {
        // TODO: we may directly rename original parquet file if there is not evolution/devolution of schema
        /*
        TaskContextSupplier taskContextSupplier = hoodieCopyOnWriteTable.getTaskContextSupplier();
        String newFileName = FSUtils.makeDataFileName(instantTime,
```

### CommentedOutCode
Commented out code (2 lines)
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java
    // TODO - compute this compression ratio dynamically by looking at the bytes written to the
    // stream and the actual file size reported by HDFS
    // this.maxFileSize = hfileConfig.getMaxFileSize()
    //    + Math.round(hfileConfig.getMaxFileSize() * hfileConfig.getCompressionRatio());
    this.maxFileSize = hfileConfig.getMaxFileSize();
```

### CommentedOutCode
Commented out code (4 lines)
in `hudi-common/src/main/java/org/apache/hudi/avro/ConvertingGenericData.java`
#### Snippet
```java
  // NOTE: Those are not supported in Avro 1.8.2
  // TODO re-enable upon upgrading to 1.10
  // private static final TimeConversions.TimestampMillisConversion TIMESTAMP_MILLIS_CONVERSION = new TimeConversions.TimestampMillisConversion();
  // private static final TimeConversions.TimeMillisConversion TIME_MILLIS_CONVERSION = new TimeConversions.TimeMillisConversion();
  // private static final TimeConversions.LocalTimestampMillisConversion LOCAL_TIMESTAMP_MILLIS_CONVERSION = new TimeConversions.LocalTimestampMillisConversion();
```

### CommentedOutCode
Commented out code (4 lines)
in `hudi-common/src/main/java/org/apache/hudi/avro/ConvertingGenericData.java`
#### Snippet
```java
    // NOTE: Those are not supported in Avro 1.8.2
    // TODO re-enable upon upgrading to 1.10
    // addLogicalTypeConversion(TIME_MILLIS_CONVERSION);
    // addLogicalTypeConversion(TIMESTAMP_MILLIS_CONVERSION);
    // addLogicalTypeConversion(LOCAL_TIMESTAMP_MILLIS_CONVERSION);
```

### CommentedOutCode
Commented out code (2 lines)
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
    // Do not need to explicitly initialize the default filesystem, its done already in the above
    // FileSystem.get
    // fileSystem.initialize(FileSystem.getDefaultUri(conf), conf);
    // fileSystem.setConf(conf);
  }
```

### CommentedOutCode
Commented out code (10 lines)
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java`
#### Snippet
```java
                                                                   final Reporter reporter) throws IOException {
    // TODO enable automatic predicate pushdown after fixing issues
    // FileSplit fileSplit = (FileSplit) split;
    // HoodieTableMetadata metadata = getTableMetadata(fileSplit.getPath().getParent());
    // String tableName = metadata.getTableName();
```

## RuleId[id=RegExpRedundantEscape]
### RegExpRedundantEscape
Redundant character escape `\\]` in RegExp
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/SerDeHelper.java`
#### Snippet
```java
  private static final String VALUE_OPTIONAL = "value_optional";

  private static final Pattern FIXED = Pattern.compile("fixed\\[(\\d+)\\]");
  private static final Pattern DECIMAL = Pattern.compile("decimal\\((\\d+),\\s+(\\d+)\\)");

```

### RegExpRedundantEscape
Redundant character escape `\\+` in RegExp
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
 */
public class KafkaConnectHdfsProvider extends InitialCheckPointProvider {
  private static String FILENAME_SEPARATOR = "[\\+\\.]";

  public KafkaConnectHdfsProvider(TypedProperties props) {
```

### RegExpRedundantEscape
Redundant character escape `\\.` in RegExp
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
 */
public class KafkaConnectHdfsProvider extends InitialCheckPointProvider {
  private static String FILENAME_SEPARATOR = "[\\+\\.]";

  public KafkaConnectHdfsProvider(TypedProperties props) {
```

### RegExpRedundantEscape
Redundant character escape `\\.` in RegExp
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
    private static final Pattern DIRECTORY_PATTERN = Pattern.compile(".*=.*");
    private static final Pattern PATTERN =
        Pattern.compile("[a-zA-Z0-9\\._\\-]+\\+\\d+\\+\\d+\\+\\d+(.\\w+)?");

    @Override
```

## RuleId[id=RedundantArrayCreation]
### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
        return typeInfo;
      case TIME:
        throw new UnsupportedOperationException(String.format("cannot convert %s type to hive", new Object[] { type }));
      default:
        LOG.error(String.format("cannot convert unknown type: %s to Hive", new Object[] { type }));
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
        throw new UnsupportedOperationException(String.format("cannot convert %s type to hive", new Object[] { type }));
      default:
        LOG.error(String.format("cannot convert unknown type: %s to Hive", new Object[] { type }));
        throw new UnsupportedOperationException(String.format("cannot convert unknown type: %s to Hive", new Object[] { type }));
    }
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
      default:
        LOG.error(String.format("cannot convert unknown type: %s to Hive", new Object[] { type }));
        throw new UnsupportedOperationException(String.format("cannot convert unknown type: %s to Hive", new Object[] { type }));
    }
  }
```

## RuleId[id=FinalMethodInFinalClass]
### FinalMethodInFinalClass
Method declared `final` in 'final' class
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordGlobalLocation.java`
#### Snippet
```java

  @Override
  public final void write(Kryo kryo, Output output) {
    super.write(kryo, output);

```

## RuleId[id=DuplicateBranchesInSwitch]
### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrompt.java`
#### Snippet
```java
          return new AttributedString("hudi->");
        case TABLE:
          return new AttributedString("hudi:" + tableName + "->");
        case SYNC:
          return new AttributedString("hudi:" + tableName + " <==> " + HoodieCLI.syncTableMetadata.getTableConfig().getTableName() + "->");
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                break;
              case INT_32:
                typeInfo = BasicTypeInfo.INT_TYPE_INFO;
                break;
              default:
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndexFactory.java`
#### Snippet
```java
        return true;
      case INMEMORY:
        return true;
      case BLOOM:
        return false;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndexFactory.java`
#### Snippet
```java
        return false;
      case GLOBAL_BLOOM:
        return true;
      case SIMPLE:
        return false;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndexFactory.java`
#### Snippet
```java
        return true;
      case SIMPLE:
        return false;
      case GLOBAL_SIMPLE:
        return true;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndexFactory.java`
#### Snippet
```java
        return false;
      case GLOBAL_SIMPLE:
        return true;
      case BUCKET:
        return false;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndexFactory.java`
#### Snippet
```java
        return true;
      case BUCKET:
        return false;
      default:
        return createIndex(config).isGlobal();
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
        return StringType$.MODULE$;
      case UUID:
        return StringType$.MODULE$;
      case FIXED:
        return BinaryType$.MODULE$;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
        return BinaryType$.MODULE$;
      case BINARY:
        return BinaryType$.MODULE$;
      case DECIMAL:
        Types.DecimalType decimal = (Types.DecimalType) type;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case BOOLEAN:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case INT:
            return result.mergedWith(typeMismatch(reader, writer, locations));
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case INT:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case LONG: {
            return (writer.getType() == Type.INT) ? result : result.mergedWith(typeMismatch(reader, writer, locations));
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java

          case ARRAY:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case MAP:
            return result.mergedWith(typeMismatch(reader, writer, locations));
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case MAP:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case FIXED:
            return result.mergedWith(typeMismatch(reader, writer, locations));
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case FIXED:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case ENUM:
            return result.mergedWith(typeMismatch(reader, writer, locations));
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case ENUM:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case RECORD:
            return result.mergedWith(typeMismatch(reader, writer, locations));
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case RECORD:
            return result.mergedWith(typeMismatch(reader, writer, locations));
          case UNION: {
            for (final Schema readerBranch : reader.getTypes()) {
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case SHORT:
        // smallint (16 bit), use int to hold it
        return Schema.create(Schema.Type.INT);
      case INT:
        // the Avro logical type could be AvroTypeUtil.LOGICAL_TYPE_TIME_MILLIS, but there is no way to distinguish
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case INT:
        // the Avro logical type could be AvroTypeUtil.LOGICAL_TYPE_TIME_MILLIS, but there is no way to distinguish
        return Schema.create(Schema.Type.INT);
      case LONG:
        // the Avro logical type could be AvroTypeUtil.LOGICAL_TYPE_TIME_MICROS, but there is no way to distinguish
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case DATE:
        // convert to daysSinceEpoch for LogicalType.Date
        return (int) ((LongColumnVector) colVector).vector[vectorPos];
      case TIMESTAMP:
        // The unit of time in ORC is millis. Convert (time,nanos) to the desired unit per logicalType
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case SHORT:
        // smallint (16 bit), use int to hold it
        return Schema.create(Schema.Type.INT);
      case INT:
        // the Avro logical type could be AvroTypeUtil.LOGICAL_TYPE_TIME_MILLIS, but there is no way to distinguish
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case INT:
        // the Avro logical type could be AvroTypeUtil.LOGICAL_TYPE_TIME_MILLIS, but there is no way to distinguish
        return Schema.create(Schema.Type.INT);
      case LONG:
        // the Avro logical type could be AvroTypeUtil.LOGICAL_TYPE_TIME_MICROS, but there is no way to distinguish
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case ENUM:
        // represent as String for now
        return TypeDescription.createString();
      case FIXED:
        return TypeDescription.createBinary();
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        return TypeDescription.createString();
      case FIXED:
        return TypeDescription.createBinary();
      default:
        throw new IllegalStateException(String.format("Unrecognized Avro type: %s", type.getName()));
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return false;
      case 1:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
      default:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
      default:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return false;
      case 1:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
      default:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
      default:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlockVersion.java`
#### Snippet
```java
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
      default:
        return true;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
        return new BooleanWritable((Boolean) value);
      case NULL:
        return null;
      case RECORD:
        GenericRecord record = (GenericRecord) value;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
        return new ArrayWritable(Writable.class, recordValues);
      case ENUM:
        return new Text(value.toString());
      case ARRAY:
        GenericArray arrayValue = (GenericArray) value;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
        return MapredParquetOutputFormat.class.getName();
      case HFILE:
        return MapredParquetOutputFormat.class.getName();
      case ORC:
        return OrcOutputFormat.class.getName();
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
        return ParquetHiveSerDe.class.getName();
      case HFILE:
        return ParquetHiveSerDe.class.getName();
      case ORC:
        return OrcSerde.class.getName();
```

## RuleId[id=ForLoopReplaceableByWhile]
### ForLoopReplaceableByWhile
`for` loop statement may be replace by 'while' loop
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDPartitionSortPartitioner.java`
#### Snippet
```java
          // Sort locally in partition
          List<Tuple2<String, HoodieRecord<T>>> recordList = new ArrayList<>();
          for (; partition.hasNext(); ) {
            recordList.add(partition.next());
          }
```

## RuleId[id=NonFinalFieldInEnum]
### NonFinalFieldInEnum
Non-final field `supportAtomicCreation` in enum 'StorageSchemes'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java
  private Boolean isWriteTransactional;
  // null for uncertain if dfs support atomic create&delete, please update this for each FS
  private Boolean supportAtomicCreation;

  StorageSchemes(String scheme, boolean supportsAppend, Boolean isWriteTransactional, Boolean supportAtomicCreation) {
```

### NonFinalFieldInEnum
Non-final field `isWriteTransactional` in enum 'StorageSchemes'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java
  private boolean supportsAppend;
  // null for uncertain if write is transactional, please update this for each FS
  private Boolean isWriteTransactional;
  // null for uncertain if dfs support atomic create&delete, please update this for each FS
  private Boolean supportAtomicCreation;
```

### NonFinalFieldInEnum
Non-final field `supportsAppend` in enum 'StorageSchemes'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java

  private String scheme;
  private boolean supportsAppend;
  // null for uncertain if write is transactional, please update this for each FS
  private Boolean isWriteTransactional;
```

### NonFinalFieldInEnum
Non-final field `scheme` in enum 'StorageSchemes'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java
  CFS("cfs", true, null, null);

  private String scheme;
  private boolean supportsAppend;
  // null for uncertain if write is transactional, please update this for each FS
```

### NonFinalFieldInEnum
Non-final field `name` in enum 'TypeID'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/Type.java`
#### Snippet
```java
    RECORD, ARRAY, MAP, FIXED, STRING, BINARY,
    INT, LONG, FLOAT, DOUBLE, DATE, BOOLEAN, TIME, TIMESTAMP, DECIMAL, UUID;
    private String name;

    TypeID() {
```

### NonFinalFieldInEnum
Non-final field `name` in enum 'ColumnChangeID'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChange.java`
#### Snippet
```java
  enum ColumnChangeID {
    ADD, UPDATE, DELETE, PROPERTY_CHANGE, REPLACE;
    private String name;

    private ColumnChangeID() {
```

### NonFinalFieldInEnum
Non-final field `value` in enum 'InsertMode'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/sql/InsertMode.java`
#### Snippet
```java
  NON_STRICT("non-strict");

  private String value;

  InsertMode(String value) {
```

## RuleId[id=FinalPrivateMethod]
### FinalPrivateMethod
'private' method declared `final`
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java`
#### Snippet
```java
  }

  private static final Map<String, String> createComparableActionsMap() {
    Map<String, String> comparableMap = new HashMap<>();
    comparableMap.put(HoodieTimeline.COMPACTION_ACTION, HoodieTimeline.COMMIT_ACTION);
```

## RuleId[id=StringBufferReplaceableByString]
### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java
    for (String partitionKey : config.getSplitStrings(META_SYNC_PARTITION_FIELDS)) {
      String partitionKeyWithTicks = tickSurround(partitionKey);
      partitionFields.add(new StringBuilder().append(partitionKeyWithTicks).append(" ")
          .append(getPartitionKeyType(hiveSchema, partitionKeyWithTicks)).toString());
    }
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/bulkinsert/JavaGlobalSortPartitioner.java`
#### Snippet
```java
        HoodieRecord o11 = (HoodieRecord) o1;
        HoodieRecord o22 = (HoodieRecord) o2;
        String left = new StringBuilder()
            .append(o11.getPartitionPath())
            .append("+")
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/bulkinsert/JavaGlobalSortPartitioner.java`
#### Snippet
```java
            .append(o11.getRecordKey())
            .toString();
        String right = new StringBuilder()
            .append(o22.getPartitionPath())
            .append("+")
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/GlobalSortPartitioner.java`
#### Snippet
```java
      // the records split evenly across RDD partitions, such that small partitions fit
      // into 1 RDD partition, while big ones spread evenly across multiple RDD partitions
      return new StringBuilder()
          .append(record.getPartitionPath())
          .append("+")
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDPartitionSortPartitioner.java`
#### Snippet
```java
        .mapToPair(record ->
            new Tuple2<>(
                new StringBuilder()
                    .append(record.getPartitionPath())
                    .append("+")
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/BloomIndexFileInfo.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("BloomIndexFileInfo {");
    sb.append(" fileId=").append(fileId);
    sb.append(" minRecordKey=").append(minRecordKey);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/WorkloadStat.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("WorkloadStat {");
    sb.append("numInserts=").append(numInserts).append(", ");
    sb.append("numUpdates=").append(numUpdates);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/WorkloadProfile.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("WorkloadProfile {");
    sb.append("globalStat=").append(globalStat).append(", ");
    sb.append("InputPartitionStat=").append(inputPartitionPathStatMap).append(", ");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/SmallFile.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("SmallFile {");
    sb.append("location=").append(location).append(", ");
    sb.append("sizeBytes=").append(sizeBytes);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/InsertBucket.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("InsertBucket {");
    sb.append("bucketNumber=").append(bucketNumber).append(", ");
    sb.append("weight=").append(weight);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BucketInfo.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("BucketInfo {");
    sb.append("bucketType=").append(bucketType).append(", ");
    sb.append("fileIdPrefix=").append(fileIdPrefix).append(", ");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/WriteStatus.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("WriteStatus {");
    sb.append("fileId=").append(fileId);
    sb.append(", writeStat=").append(stat);
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Pair.java`
#### Snippet
```java
  @Override
  public String toString() {
    return new StringBuilder().append('(').append(getLeft()).append(',').append(getRight()).append(')').toString();
  }

```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Triple.java`
#### Snippet
```java
  @Override
  public String toString() {
    return new StringBuilder().append('(').append(getLeft()).append(',').append(getMiddle()).append(',')
        .append(getRight()).append(')').toString();
  }
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/ConsistentHashingNode.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("ConsistentHashingNode{");
    sb.append("value=").append(value);
    sb.append(", fileIdPfx='").append(fileIdPrefix).append('\'');
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/FileSlice.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("FileSlice {");
    sb.append("fileGroupId=").append(fileGroupId);
    sb.append(", baseCommitTime=").append(baseInstantTime);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordLocation.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("HoodieRecordLocation {");
    sb.append("instantTime=").append(instantTime).append(", ");
    sb.append("fileId=").append(fileId);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordGlobalLocation.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("HoodieGlobalRecordLocation {");
    sb.append("partitionPath=").append(partitionPath).append(", ");
    sb.append("instantTime=").append(instantTime).append(", ");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieFileGroup.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("HoodieFileGroup {");
    sb.append("id=").append(fileGroupId);
    sb.append(", fileSlices='").append(fileSlices).append('\'');
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("HoodieKey {");
    sb.append(" recordKey=").append(recordKey);
    sb.append(" partitionPath=").append(partitionPath);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("HoodieRecord{");
    sb.append("key=").append(key);
    sb.append(", currentLocation='").append(currentLocation).append('\'');
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("HoodieTableMetaClient{");
    sb.append("basePath='").append(basePath).append('\'');
    sb.append(", metaPath='").append(metaPath).append('\'');
```

### StringBufferReplaceableByString
`StringBuilder builder` can be replaced with 'String'
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/utils/QuickstartConfigurations.java`
#### Snippet
```java

  public static String getCreateHudiCatalogDDL(final String catalogName, final String catalogPath) {
    StringBuilder builder = new StringBuilder();
    builder.append("create catalog ").append(catalogName).append(" with (\n");
    builder.append("  'type' = 'hudi',\n"
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append(inputSplitShim.toString());
      sb.append("InputFormatClass: " + inputFormatClassName);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
      // merge the ranges by partition to maintain one offset range map to one topic partition.
      ranges = mergeRangesByTopicPartition(ranges);
      StringBuilder sb = new StringBuilder();
      // at least 1 partition will be present.
      sb.append(ranges[0].topic() + ",");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java`
#### Snippet
```java
        .mapToObj(i -> schemaFieldNames[i])
        .collect(Collectors.toList());
    StringBuilder sb = new StringBuilder();
    sb.append(operatorName)
        .append("(")
```

### StringBufferReplaceableByString
`StringBuffer stringBuffer` can be replaced with 'String'
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java

  private static Option<String> convertToString(String uuid, String partitionPath, Long ts) {
    StringBuffer stringBuffer = new StringBuffer();
    stringBuffer.append("{");
    stringBuffer.append("\"ts\": \"" + (ts == null ? "0.0" : ts) + "\",");
```

## RuleId[id=Deprecation]
### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkMain.SparkCommand.ROLLBACK_TO_SAVEPOINT.toString(), master, sparkMemory,
        instantTime, metaClient.getBasePath(), lazyFailedWritesCleanPolicy);
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DELETE_SAVEPOINT.toString(), master, sparkMemory, instantTime,
        metaClient.getBasePath());
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SavepointsCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkMain.SparkCommand.SAVEPOINT.toString(), master, sparkMemory, commitTime,
        user, comments, metaClient.getBasePath());
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java
    TableHeader header = new TableHeader().addTableHeaderField("Property").addTableHeaderField("Value");
    List<Comparable[]> rows = new ArrayList<>();
    rows.add(new Comparable[] {"basePath", client.getBasePath()});
    rows.add(new Comparable[] {"metaPath", client.getMetaPath()});
    rows.add(new Comparable[] {"fileSystem", client.getFs().getScheme()});
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java

    Set<String> deleteConfigs = Arrays.stream(csConfigs.split(",")).collect(Collectors.toSet());
    Path metaPathDir = new Path(client.getBasePath(), METAFOLDER_NAME);
    HoodieTableConfig.delete(client.getFs(), metaPathDir, deleteConfigs);

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java
      updatedProps.load(fileInputStream);
    }
    Path metaPathDir = new Path(client.getBasePath(), METAFOLDER_NAME);
    HoodieTableConfig.update(client.getFs(), metaPathDir, updatedProps);

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java
    HoodieCLI.refreshTableMetadata();
    HoodieTableMetaClient client = HoodieCLI.getTableMetaClient();
    Path metaPathDir = new Path(client.getBasePath(), METAFOLDER_NAME);
    HoodieTableConfig.recover(client.getFs(), metaPathDir);
    return descTable();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ClusteringCommand.java`
#### Snippet
```java

    sparkLauncher.addAppArgs(SparkCommand.CLUSTERING_SCHEDULE.toString(), master, sparkMemory,
        client.getBasePath(), client.getTableConfig().getTableName(), clusteringInstantTime, propsFilePath);
    UtilHelpers.validateAndAddProperties(configs, sparkLauncher);
    Process process = sparkLauncher.launch();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ClusteringCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkCommand.CLUSTERING_RUN.toString(), master, sparkMemory,
        client.getBasePath(), client.getTableConfig().getTableName(), clusteringInstantTime,
        parallelism, retry, propsFilePath);
    UtilHelpers.validateAndAddProperties(configs, sparkLauncher);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ClusteringCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkCommand.CLUSTERING_SCHEDULE_AND_EXECUTE.toString(), master, sparkMemory,
        client.getBasePath(), client.getTableConfig().getTableName(), parallelism, retry, propsFilePath);
    UtilHelpers.validateAndAddProperties(configs, sparkLauncher);
    Process process = sparkLauncher.launch();
```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java

      jsc = new JavaSparkContext(sparkConf);
      sqlContext = new SQLContext(jsc);
    } catch (Throwable ex) {
      // log full stack trace and rethrow. Without this its difficult to debug failures, if any
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RollbacksCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkMain.SparkCommand.ROLLBACK.toString(), master, sparkMemory, instantTime,
        HoodieCLI.getTableMetaClient().getBasePath(), rollbackUsingMarkers);
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MarkersCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DELETE_MARKER.toString(), master, sparkMemory, instantTime,
        metaClient.getBasePath());
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/StatsCommand.java`
#### Snippet
```java

    FileSystem fs = HoodieCLI.fs;
    String globPath = String.format("%s/%s/*", HoodieCLI.getTableMetaClient().getBasePath(), globRegex);
    List<FileStatus> statuses = FSUtils.getGlobStatusExcludingMetaFolder(fs, new Path(globPath));

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java`
#### Snippet
```java
    HoodieTableMetaClient client = HoodieCLI.getTableMetaClient();
    HoodieTableMetaClient metaClient =
        HoodieTableMetaClient.builder().setConf(client.getHadoopConf()).setBasePath(client.getBasePath()).setLoadActiveTimelineOnLoad(true).build();
    FileSystem fs = HoodieCLI.fs;
    String globPath = String.format("%s/%s/*", client.getBasePath(), globRegex);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java`
#### Snippet
```java
        HoodieTableMetaClient.builder().setConf(client.getHadoopConf()).setBasePath(client.getBasePath()).setLoadActiveTimelineOnLoad(true).build();
    FileSystem fs = HoodieCLI.fs;
    String globPath = String.format("%s/%s/*", client.getBasePath(), globRegex);
    List<FileStatus> statuses = FSUtils.getGlobStatusExcludingMetaFolder(fs, new Path(globPath));
    Stream<HoodieInstant> instantsStream;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CleansCommand.java`
#### Snippet
```java

    String cmd = SparkMain.SparkCommand.CLEAN.toString();
    sparkLauncher.addAppArgs(cmd, master, sparkMemory, metaClient.getBasePath(), propsFilePath);
    UtilHelpers.validateAndAddProperties(configs, sparkLauncher);
    Process process = sparkLauncher.launch();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java`
#### Snippet
```java
  private int copyArchivedInstants(List<FileStatus> statuses, Set<String> actionSet, int limit, String localFolder) throws Exception {
    int copyCount = 0;
    FileSystem fileSystem = FSUtils.getFs(HoodieCLI.getTableMetaClient().getBasePath(), HoodieCLI.conf);

    for (FileStatus fs : statuses) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java`
#### Snippet
```java
      throws Exception {

    final String basePath = HoodieCLI.getTableMetaClient().getBasePath();
    final Path archivePath = new Path(HoodieCLI.getTableMetaClient().getArchivePath());
    final Set<String> actionSet = new HashSet<String>(Arrays.asList(filter.split(",")));
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/UpgradeOrDowngradeCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    String toVersionName = getHoodieTableVersionName(toVersion, true);
    sparkLauncher.addAppArgs(SparkCommand.UPGRADE.toString(), master, sparkMemory, metaClient.getBasePath(), toVersionName);
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/UpgradeOrDowngradeCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    String toVersionName = getHoodieTableVersionName(toVersion, false);
    sparkLauncher.addAppArgs(SparkCommand.DOWNGRADE.toString(), master, sparkMemory, metaClient.getBasePath(), toVersionName);
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ArchivedCommitsCommand.java`
#### Snippet
```java
      throws IOException {
    System.out.println("===============> Showing only " + limit + " archived commits <===============");
    String basePath = HoodieCLI.getTableMetaClient().getBasePath();
    Path archivePath = new Path(HoodieCLI.getTableMetaClient().getArchivePath() + "/.commits_.archive*");
    if (folder != null && !folder.isEmpty()) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ArchivedCommitsCommand.java`
#### Snippet
```java
    System.out.println("===============> Showing only " + limit + " archived commits <===============");
    HoodieTableMetaClient metaClient = HoodieCLI.getTableMetaClient();
    String basePath = metaClient.getBasePath();
    Path archivePath = new Path(metaClient.getArchivePath() + "/.commits_.archive*");
    FileStatus[] fsStatuses =
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    HoodieLocalEngineContext engineContext = new HoodieLocalEngineContext(HoodieCLI.conf);
    HoodieTableMetaClient client = HoodieCLI.getTableMetaClient();
    List<String> partitionPaths = FSUtils.getAllPartitionPaths(engineContext, client.getBasePath(), false, false);
    Path basePath = new Path(client.getBasePath());

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    HoodieTableMetaClient client = HoodieCLI.getTableMetaClient();
    List<String> partitionPaths = FSUtils.getAllPartitionPaths(engineContext, client.getBasePath(), false, false);
    Path basePath = new Path(client.getBasePath());

    String[][] rows = new String[partitionPaths.size()][];
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    int ind = 0;
    for (String partitionPath : partitionPaths) {
      Path partition = FSUtils.getPartitionPath(client.getBasePath(), partitionPath);
      Option<Path> textFormatFile = HoodiePartitionMetadata.textFormatMetaPathIfExists(HoodieCLI.fs, partition);
      Option<Path> baseFormatFile = HoodiePartitionMetadata.baseFormatMetaPathIfExists(HoodieCLI.fs, partition);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
        client.getActiveTimeline().getCommitTimeline().lastInstant().get().getTimestamp();
    List<String> partitionPaths =
        FSUtils.getAllPartitionFoldersThreeLevelsDown(HoodieCLI.fs, client.getBasePath());
    Path basePath = new Path(client.getBasePath());
    String[][] rows = new String[partitionPaths.size()][];
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    List<String> partitionPaths =
        FSUtils.getAllPartitionFoldersThreeLevelsDown(HoodieCLI.fs, client.getBasePath());
    Path basePath = new Path(client.getBasePath());
    String[][] rows = new String[partitionPaths.size()][];

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,
        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(),
        String.valueOf(dryRun), dedupeType);
    Process process = sparkLauncher.launch();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    }
    Map<String, String> oldProps = client.getTableConfig().propsMap();
    Path metaPathDir = new Path(client.getBasePath(), METAFOLDER_NAME);
    HoodieTableConfig.create(client.getFs(), metaPathDir, newProps);
    // reload new props as checksum would have been added
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java`
#### Snippet
```java
          HoodieMergedLogRecordScanner.newBuilder()
              .withFileSystem(fs)
              .withBasePath(client.getBasePath())
              .withLogFilePaths(logFilePaths)
              .withReaderSchema(readerSchema)
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
          .getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));
      SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
      sparkLauncher.addAppArgs(SparkCommand.COMPACT_VALIDATE.toString(), master, sparkMemory, client.getBasePath(),
          compactionInstant, outputPathStr, parallelism);
      Process process = sparkLauncher.launch();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
        Utils.getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkCommand.COMPACT_SCHEDULE_AND_EXECUTE.toString(), master, sparkMemory, client.getBasePath(),
        client.getTableConfig().getTableName(), parallelism, schemaFilePath,
        retry, propsFilePath);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
          .getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));
      SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
      sparkLauncher.addAppArgs(SparkCommand.COMPACT_UNSCHEDULE_FILE.toString(), master, sparkMemory, client.getBasePath(),
          fileId, partitionPath, outputPathStr, "1", Boolean.valueOf(skipV).toString(),
          Boolean.valueOf(dryRun).toString());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
          .getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));
      SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
      sparkLauncher.addAppArgs(SparkCommand.COMPACT_UNSCHEDULE_PLAN.toString(), master, sparkMemory, client.getBasePath(),
          compactionInstant, outputPathStr, parallelism, Boolean.valueOf(skipV).toString(),
          Boolean.valueOf(dryRun).toString());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    String cmd = SparkCommand.COMPACT_SCHEDULE.toString();
    sparkLauncher.addAppArgs(cmd, master, sparkMemory, client.getBasePath(),
        client.getTableConfig().getTableName(), compactionInstantTime, propsFilePath);
    UtilHelpers.validateAndAddProperties(configs, sparkLauncher);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
        Utils.getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));
    SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
    sparkLauncher.addAppArgs(SparkCommand.COMPACT_RUN.toString(), master, sparkMemory, client.getBasePath(),
        client.getTableConfig().getTableName(), compactionInstantTime, parallelism, schemaFilePath,
        retry, propsFilePath);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
          .getDefaultPropertiesFile(scala.collection.JavaConversions.propertiesAsScalaMap(System.getProperties()));
      SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);
      sparkLauncher.addAppArgs(SparkCommand.COMPACT_REPAIR.toString(), master, sparkMemory, client.getBasePath(),
          compactionInstant, outputPathStr, parallelism, Boolean.valueOf(dryRun).toString());
      Process process = sparkLauncher.launch();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TimelineCommand.java`
#### Snippet
```java
  private HoodieTableMetaClient getMetadataTableMetaClient(HoodieTableMetaClient metaClient) {
    return HoodieTableMetaClient.builder().setConf(HoodieCLI.conf)
        .setBasePath(HoodieTableMetadata.getMetadataTableBasePath(metaClient.getBasePath()))
        .setLoadActiveTimelineOnLoad(false)
        .setConsistencyGuardConfig(HoodieCLI.consistencyGuardConfig)
```

### Deprecation
'HIVE_USE_JDBC' is deprecated
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveSyncClient.java`
#### Snippet
```java
import static org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.getSerDeClassName;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_SYNC_MODE;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_USE_JDBC;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_USE_PRE_APACHE_INPUT_FORMAT;
import static org.apache.hudi.sync.common.HoodieSyncConfig.META_SYNC_BASE_FILE_FORMAT;
```

### Deprecation
'HIVE_USE_JDBC' is deprecated
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveSyncClient.java`
#### Snippet
```java
        }
      } else {
        ddlExecutor = config.getBoolean(HIVE_USE_JDBC) ? new JDBCExecutor(config) : new HiveQueryDDLExecutor(config, this.client);
      }
    } catch (Exception e) {
```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java

  public static int renamePartition(JavaSparkContext jsc, String basePath, String oldPartition, String newPartition) {
    SQLContext sqlContext = new SQLContext(jsc);
    Dataset<Row> recordsToRewrite = getRecordsToRewrite(basePath, oldPartition, sqlContext);

```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
  private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,
                                              String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {
    DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),
        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), DeDupeType.withName(dedupeType));
    job.fixDuplicates(dryRun);
```

### Deprecation
'rollback(java.lang.String)' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
  private static int rollback(JavaSparkContext jsc, String instantTime, String basePath, Boolean rollbackUsingMarkers) throws Exception {
    SparkRDDWriteClient client = createHoodieClient(jsc, basePath, rollbackUsingMarkers, false);
    if (client.rollback(instantTime)) {
      LOG.info(String.format("The commit \"%s\" rolled back.", instantTime));
      return 0;
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java

  private static void rewriteRecordsToNewPartition(String basePath, String newPartition, Dataset<Row> recordsToRewrite, HoodieTableMetaClient metaClient, Map<String, String> propsMap) {
    String partitionFieldProp = metaClient.getTableConfig().getPartitionFieldProp();
    StructType structType = recordsToRewrite.schema();
    int partitionIndex = structType.fieldIndex(partitionFieldProp);
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
    int partitionIndex = structType.fieldIndex(partitionFieldProp);

    recordsToRewrite.withColumn(metaClient.getTableConfig().getPartitionFieldProp(), functions.lit(null).cast(structType.apply(partitionIndex).dataType()))
        .write()
        .options(propsMap)
```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java

  public static int repairDeprecatedPartition(JavaSparkContext jsc, String basePath) {
    SQLContext sqlContext = new SQLContext(jsc);
    Dataset<Row> recordsToRewrite = getRecordsToRewrite(basePath, PartitionPathEncodeUtils.DEPRECATED_DEFAULT_PARTITION_PATH, sqlContext);

```

### Deprecation
'org.apache.hudi.utilities.HDFSParquetImporter' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
    cfg.propsFilePath = propsFilePath;
    cfg.configs = configs;
    return new HDFSParquetImporter(cfg).dataImport(jsc, retry);
  }

```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
    propsMap.put(HoodieWriteConfig.SKIP_DEFAULT_PARTITION_VALIDATION.key(), "true");
    propsMap.put(DataSourceWriteOptions.RECORDKEY_FIELD().key(), metaClient.getTableConfig().getRecordKeyFieldProp());
    propsMap.put(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), metaClient.getTableConfig().getPartitionFieldProp());
    propsMap.put(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(), metaClient.getTableConfig().getKeyGeneratorClassName());
    return propsMap;
```

### Deprecation
'LockRequestBuilder()' is deprecated
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/transaction/lock/HiveMetastoreBasedLockProvider.java`
#### Snippet
```java
    try {
      // TODO : FIX:Using the parameterized constructor throws MethodNotFound
      final LockRequestBuilder builder = new LockRequestBuilder();
      lockRequest = builder.addLockComponent(lockComponent).setUser(System.getProperty("user.name")).build();
      lockRequest.setUserIsSet(true);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/ManifestFileWriter.java`
#### Snippet
```java
    try {
      List<String> partitions = FSUtils.getAllPartitionPaths(new HoodieLocalEngineContext(metaClient.getHadoopConf()),
          metaClient.getBasePath(), useFileListingFromMetadata, assumeDatePartitioning);
      LOG.info("Retrieve all partitions: " + partitions.size());
      return partitions.parallelStream().flatMap(p -> {
```

### Deprecation
Overrides deprecated method in 'org.apache.hudi.index.HoodieIndex'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,
                                                    HoodieEngineContext context,
                                                    HoodieTable hoodieTable) throws HoodieIndexException;
```

### Deprecation
Overrides deprecated method in 'org.apache.hudi.index.HoodieIndex'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<WriteStatus> updateLocation(List<WriteStatus> writeStatuses,
                                                   HoodieEngineContext context,
                                                   HoodieTable hoodieTable) throws HoodieIndexException;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java`
#### Snippet
```java
    Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();
    List<String> partitionPaths = FSUtils.getAllPartitionPaths(context,
        table.getMetaClient().getBasePath(), config.isMetadataTableEnabled(), config.shouldAssumeDatePartitioning());

    if (partitionPaths != null && partitionPaths.size() > 0) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
        scanner = HoodieMergedLogRecordScanner.newBuilder()
            .withFileSystem(table.getMetaClient().getFs())
            .withBasePath(table.getMetaClient().getBasePath())
            .withLogFilePaths(clusteringOp.getDeltaFilePaths())
            .withReaderSchema(readerSchema)
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
            tableConfig.getProps(),
            tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
                tableConfig.getPartitionFieldProp())));
        fileSliceReader.forEachRemaining(records::add);
      } catch (IOException e) {
```

### Deprecation
'BaseHoodieWriteClient(org.apache.hudi.common.engine.HoodieEngineContext, org.apache.hudi.config.HoodieWriteConfig, org.apache.hudi.common.util.Option, org.apache.hudi.table.upgrade.SupportsUpgradeDowngrade)' is deprecated
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
                               boolean rollbackPending,
                               Option<EmbeddedTimelineService> timelineService) {
    super(context, writeConfig, timelineService, JavaUpgradeDowngradeHelper.getInstance());
    this.tableServiceClient = new HoodieJavaTableServiceClient(context, writeConfig, getTimelineServer());
  }
```

### Deprecation
'BaseHoodieWriteClient(org.apache.hudi.common.engine.HoodieEngineContext, org.apache.hudi.config.HoodieWriteConfig, org.apache.hudi.table.upgrade.SupportsUpgradeDowngrade)' is deprecated
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java

  public HoodieJavaWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig) {
    super(context, writeConfig, JavaUpgradeDowngradeHelper.getInstance());
    this.tableServiceClient = new HoodieJavaTableServiceClient(context, writeConfig, getTimelineServer());
  }
```

### Deprecation
'ParquetWriter(org.apache.hadoop.fs.Path, org.apache.parquet.hadoop.ParquetFileWriter.Mode, org.apache.parquet.hadoop.api.WriteSupport, org.apache.parquet.hadoop.metadata.CompressionCodecName, int, int, int, ...)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataParquetWriter.java`
#### Snippet
```java
  public HoodieRowDataParquetWriter(Path file, HoodieParquetConfig<HoodieRowDataParquetWriteSupport> parquetConfig)
      throws IOException {
    super(HoodieWrapperFileSystem.convertToHoodiePath(file, parquetConfig.getHadoopConf()),
        ParquetFileWriter.Mode.CREATE, parquetConfig.getWriteSupport(), parquetConfig.getCompressionCodecName(),
        parquetConfig.getBlockSize(), parquetConfig.getPageSize(), parquetConfig.getPageSize(),
```

### Deprecation
Overrides deprecated method in 'org.apache.hudi.index.HoodieIndex'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,
                                                    HoodieEngineContext context,
                                                    HoodieTable hoodieTable) throws HoodieIndexException;
```

### Deprecation
Overrides deprecated method in 'org.apache.hudi.index.HoodieIndex'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<WriteStatus> updateLocation(List<WriteStatus> writeStatuses,
                                                   HoodieEngineContext context,
                                                   HoodieTable hoodieTable) throws HoodieIndexException;
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
                .as(OriginalType.DECIMAL)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.INT_TYPE_INFO)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
                .as(OriginalType.DECIMAL)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.INT_TYPE_INFO)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.INT_32)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.DOUBLE_TYPE_INFO)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.INT_32)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.DOUBLE_TYPE_INFO)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT64, repetition)
                .as(OriginalType.INT_64)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.SHORT_TYPE_INFO)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT64, repetition)
                .as(OriginalType.INT_64)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.SHORT_TYPE_INFO)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.INT_16)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.BYTE_TYPE_INFO)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.INT_16)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.BYTE_TYPE_INFO)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.INT_8)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.CHAR_TYPE_INFO)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.INT_8)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.CHAR_TYPE_INFO)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
                .as(OriginalType.UTF8)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.BOOLEAN_TYPE_INFO)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
                .as(OriginalType.UTF8)
                .named(fieldName);
      } else if (basicTypeInfo.equals(BasicTypeInfo.BOOLEAN_TYPE_INFO)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
                .as(OriginalType.UTF8)
                .named(fieldName);
      }
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
                .as(OriginalType.UTF8)
                .named(fieldName);
      }
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
          Types.buildGroup(repetition)
              .addField(elementGroup)
              .as(OriginalType.LIST)
              .named(fieldName);
    } else if (typeInfo instanceof BasicArrayTypeInfo) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
          Types.buildGroup(repetition)
              .addField(elementGroup)
              .as(OriginalType.LIST)
              .named(fieldName);
    } else if (typeInfo instanceof BasicArrayTypeInfo) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
            Types.buildGroup(repetition)
                .addField(listGroup)
                .as(OriginalType.LIST)
                .named(fieldName);
      } else {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
            Types.buildGroup(repetition)
                .addField(listGroup)
                .as(OriginalType.LIST)
                .named(fieldName);
      } else {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
            Types.buildGroup(repetition)
                .repeated(primitiveTyp.getPrimitiveTypeName())
                .as(primitiveTyp.getOriginalType())
                .named(LIST_ARRAY_TYPE)
                .as(OriginalType.LIST)
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                .as(primitiveTyp.getOriginalType())
                .named(LIST_ARRAY_TYPE)
                .as(OriginalType.LIST)
                .named(fieldName);
      }
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                .as(primitiveTyp.getOriginalType())
                .named(LIST_ARRAY_TYPE)
                .as(OriginalType.LIST)
                .named(fieldName);
      }
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.DATE)
                .named(fieldName);
      } else if (typeInfo.equals(SqlTimeTypeInfo.TIME)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.DATE)
                .named(fieldName);
      } else if (typeInfo.equals(SqlTimeTypeInfo.TIME)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.TIME_MILLIS)
                .named(fieldName);
      } else if (typeInfo.equals(SqlTimeTypeInfo.TIMESTAMP)) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT32, repetition)
                .as(OriginalType.TIME_MILLIS)
                .named(fieldName);
      } else if (typeInfo.equals(SqlTimeTypeInfo.TIMESTAMP)) {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT64, repetition)
                .as(OriginalType.TIMESTAMP_MILLIS)
                .named(fieldName);
      } else {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        fieldType =
            Types.primitive(PrimitiveType.PrimitiveTypeName.INT64, repetition)
                .as(OriginalType.TIMESTAMP_MILLIS)
                .named(fieldName);
      } else {
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
      case VARCHAR:
        return Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
            .as(OriginalType.UTF8)
            .named(name);
      case BOOLEAN:
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
      case VARCHAR:
        return Types.primitive(PrimitiveType.PrimitiveTypeName.BINARY, repetition)
            .as(OriginalType.UTF8)
            .named(name);
      case BOOLEAN:
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        LogicalType elementType = arrayType.getElementType();
        return Types
            .buildGroup(repetition).as(OriginalType.LIST)
            .addField(
                Types.repeatedGroup()
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        LogicalType elementType = arrayType.getElementType();
        return Types
            .buildGroup(repetition).as(OriginalType.LIST)
            .addField(
                Types.repeatedGroup()
```

### Deprecation
'as(org.apache.parquet.schema.OriginalType)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        LogicalType valueType = mapType.getValueType();
        return Types
            .buildGroup(repetition).as(OriginalType.MAP)
            .addField(
                Types
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        LogicalType valueType = mapType.getValueType();
        return Types
            .buildGroup(repetition).as(OriginalType.MAP)
            .addField(
                Types
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
    TypeInformation<?> typeInfo;
    if (fieldType.isPrimitive()) {
      OriginalType originalType = fieldType.getOriginalType();
      PrimitiveType primitiveType = fieldType.asPrimitiveType();
      switch (primitiveType.getPrimitiveTypeName()) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
    } else {
      GroupType parquetGroupType = fieldType.asGroupType();
      OriginalType originalType = parquetGroupType.getOriginalType();
      if (originalType != null) {
        switch (originalType) {
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                .getPrimitiveTypeName()
                .equals(PrimitiveType.PrimitiveTypeName.BINARY)
                || !keyType.getOriginalType().equals(OriginalType.UTF8)) {
              throw new IllegalArgumentException(
                  "Map key type must be required binary (UTF8): " + keyType);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkTableServiceClient.java`
#### Snippet
```java
    } catch (IOException e) {
      throw new HoodieClusteringException(
          "Failed to commit " + table.getMetaClient().getBasePath() + " at time " + clusteringCommitTime, e);
    } finally {
      this.txnManager.endTransaction(Option.of(clusteringInstant));
```

### Deprecation
Overrides deprecated method in 'org.apache.hudi.index.HoodieIndex'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> records,
                                                       HoodieEngineContext context,
                                                       HoodieTable hoodieTable) throws HoodieIndexException;
```

### Deprecation
Overrides deprecated method in 'org.apache.hudi.index.HoodieIndex'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD,
                                                      HoodieEngineContext context,
                                                      HoodieTable hoodieTable) throws HoodieIndexException;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkTable.java`
#### Snippet
```java
      try {
        if (isMetadataTableExists || metaClient.getFs().exists(new Path(
            HoodieTableMetadata.getMetadataTableBasePath(metaClient.getBasePath())))) {
          isMetadataTableExists = true;
          return Option.of(metadataWriter);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
        Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();
        List<String> partitionPaths =
            FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), table.getMetaClient().getBasePath());
        if (partitionPaths != null && partitionPaths.size() > 0) {
          context.setJobStatus(this.getClass().getSimpleName(), "Getting ExistingFileIds of all partitions: " + config.getTableName());
```

### Deprecation
'BaseHoodieWriteClient(org.apache.hudi.common.engine.HoodieEngineContext, org.apache.hudi.config.HoodieWriteConfig, org.apache.hudi.table.upgrade.SupportsUpgradeDowngrade)' is deprecated
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java

  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig) {
    super(context, writeConfig, FlinkUpgradeDowngradeHelper.getInstance());
    this.bucketToHandles = new HashMap<>();
    this.tableServiceClient = new HoodieFlinkTableServiceClient<>(context, writeConfig, getTimelineServer());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java`
#### Snippet
```java
  @Override
  protected Map<String, List<String>> getPartitionToReplacedFileIds(HoodieWriteMetadata<HoodieData<WriteStatus>> writeMetadata) {
    List<String> partitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), table.getMetaClient().getBasePath());
    if (partitionPaths == null || partitionPaths.isEmpty()) {
      return Collections.emptyMap();
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/ParquetBootstrapMetadataHandler.java`
#### Snippet
```java
  @Override
  Schema getAvroSchema(Path sourceFilePath) throws IOException {
    ParquetMetadata readFooter = ParquetFileReader.readFooter(table.getHadoopConf(), sourceFilePath,
        ParquetMetadataConverter.NO_FILTER);
    MessageType parquetSchema = readFooter.getFileMetaData().getSchema();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
  public List<Pair<String, HoodieCompactionPlan>> getPendingCompactions() {
    HoodieTableMetaClient metaClient =
        HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(hoodieTable.getMetaClient().getBasePath()).setLoadActiveTimelineOnLoad(true).build();
    return CompactionUtils.getAllPendingCompactionPlans(metaClient).stream()
        .map(
```

### Deprecation
'org.apache.hudi.client.HoodieReadClient' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java

  /**
   * Initializes the {@link HoodieReadClient} with engine context, base path, SQL context and index type.
   *
   * @param context    Hudi Spark engine context
```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkValidatorUtils.java`
#### Snippet
```java
      }
      Set<String> partitionsModified = writeMetadata.getWriteStats().get().stream().map(HoodieWriteStat::getPartitionPath).collect(Collectors.toSet());
      SQLContext sqlContext = new SQLContext(HoodieSparkEngineContext.getSparkContext(context));
      // Refresh timeline to ensure validator sees the any other operations done on timeline (async operations such as other clustering/compaction/rollback)
      table.getMetaClient().reloadActiveTimeline();
```

### Deprecation
'registerTempTable(java.lang.String)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryPreCommitValidator.java`
#### Snippet
```java
    String hoodieTableBeforeCurrentCommit = hoodieTableName + "_before";
    String hoodieTableWithInflightCommit = hoodieTableName + "_after";
    before.registerTempTable(hoodieTableBeforeCurrentCommit);
    after.registerTempTable(hoodieTableWithInflightCommit);
    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(getEngineContext());
```

### Deprecation
'registerTempTable(java.lang.String)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryPreCommitValidator.java`
#### Snippet
```java
    String hoodieTableWithInflightCommit = hoodieTableName + "_after";
    before.registerTempTable(hoodieTableBeforeCurrentCommit);
    after.registerTempTable(hoodieTableWithInflightCommit);
    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(getEngineContext());
    SQLContext sqlContext = new SQLContext(jsc);
```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryPreCommitValidator.java`
#### Snippet
```java
    after.registerTempTable(hoodieTableWithInflightCommit);
    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(getEngineContext());
    SQLContext sqlContext = new SQLContext(jsc);

    String[] queries = getQueriesToRun();
```

### Deprecation
'BaseHoodieWriteClient(org.apache.hudi.common.engine.HoodieEngineContext, org.apache.hudi.config.HoodieWriteConfig, org.apache.hudi.common.util.Option, org.apache.hudi.table.upgrade.SupportsUpgradeDowngrade)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public SparkRDDWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig,
                             Option<EmbeddedTimelineService> timelineService) {
    super(context, writeConfig, timelineService, SparkUpgradeDowngradeHelper.getInstance());
    this.tableServiceClient = new SparkRDDTableServiceClient<>(context, writeConfig, getTimelineServer());
  }
```

### Deprecation
'org.apache.hudi.keygen.CustomKeyGenerator' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/factory/HoodieSparkKeyGeneratorFactory.java`
#### Snippet
```java
        return TimestampBasedKeyGenerator.class.getName();
      case CUSTOM:
        return CustomKeyGenerator.class.getName();
      case NON_PARTITION:
        return NonpartitionedKeyGenerator.class.getName();
```

### Deprecation
'SparkRDDWriteClient(org.apache.hudi.common.engine.HoodieEngineContext, org.apache.hudi.config.HoodieWriteConfig, boolean)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
    LOG.info("Deleting Metadata Table partitions: " + partitionsToDrop);

    try (SparkRDDWriteClient writeClient = new SparkRDDWriteClient(engineContext, metadataWriteConfig, true)) {
      String actionType = CommitUtils.getCommitActionType(WriteOperationType.DELETE_PARTITION, HoodieTableType.MERGE_ON_READ);
      writeClient.startCommitWithTime(instantTime, actionType);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
          HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()
              .withFileSystem(table.getMetaClient().getFs())
              .withBasePath(table.getMetaClient().getBasePath())
              .withLogFilePaths(clusteringOp.getDeltaFilePaths())
              .withReaderSchema(readerSchema)
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
              tableConfig.getProps(),
              tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
                  tableConfig.getPartitionFieldProp()))));
        } catch (IOException e) {
          throw new HoodieClusteringException("Error reading input data for " + clusteringOp.getDataFilePath()
```

### Deprecation
'SQLContext(org.apache.spark.SparkContext)' is deprecated
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
        .map(ClusteringOperation::create).collect(Collectors.toList());
    boolean hasLogFiles = clusteringOps.stream().anyMatch(op -> op.getDeltaFilePaths().size() > 0);
    SQLContext sqlContext = new SQLContext(jsc.sc());

    Path[] baseFilePaths = clusteringOps
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java`
#### Snippet
```java
      HoodieTableConfig tableConfig = hoodieTable.getMetaClient().getTableConfig();
      Option<Pair<String, String>> simpleKeyGenFieldsOpt =
          tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(), tableConfig.getPartitionFieldProp()));
      while (baseFileItr.hasNext()) {
        HoodieRecord<T> record = baseFileItr.next().wrapIntoHoodieRecordPayloadWithParams(readerSchema,
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java`
#### Snippet
```java

    return HoodieLogFormat.newWriterBuilder()
        .onParentPath(FSUtils.getPartitionPath(hoodieTable.getMetaClient().getBasePath(), partitionPath))
        .withFileId(fileId)
        .overBaseCommit(baseCommitTime)
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
        writeStatus.setGlobalError(e);
        throw new HoodieUpsertException("Failed to initialize HoodieAppendHandle for FileId: " + fileId + " on commit "
            + instantTime + " on HDFS path " + hoodieTable.getMetaClient().getBasePath() + "/" + partitionPath, e);
      }
      doInit = false;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
      writeStatus.setGlobalError(io);
      throw new HoodieUpsertException("Failed to initialize HoodieUpdateHandle for FileId: " + fileId + " on commit "
          + instantTime + " on path " + hoodieTable.getMetaClient().getBasePath(), io);
    }
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java`
#### Snippet
```java
                                                                   final HoodieTable hoodieTable) {
    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
    List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), metaClient.getBasePath());
    return super.loadColumnRangesFromFiles(allPartitionPaths, context, hoodieTable);
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java`
#### Snippet
```java
      final HoodieEngineContext context, final HoodieTable hoodieTable) {
    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
    List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), metaClient.getBasePath());
    // Obtain the latest data files from all the partitions.
    return getLatestBaseFilesForAllPartitions(allPartitionPaths, context, hoodieTable);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/ScheduleIndexActionExecutor.java`
#### Snippet
```java
    // delete metadata partition
    partitionIndexTypes.forEach(partitionType -> {
      if (metadataPartitionExists(table.getMetaClient().getBasePath(), context, partitionType)) {
        deleteMetadataPartition(table.getMetaClient().getBasePath(), context, partitionType);
      }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/ScheduleIndexActionExecutor.java`
#### Snippet
```java
    partitionIndexTypes.forEach(partitionType -> {
      if (metadataPartitionExists(table.getMetaClient().getBasePath(), context, partitionType)) {
        deleteMetadataPartition(table.getMetaClient().getBasePath(), context, partitionType);
      }
    });
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
    // does not exist, or the table config is non-empty indicating that metadata table
    // partitions are ready to use
    return !HoodieTableMetadata.isMetadataTable(metaClient.getBasePath())
        && !config.isMetadataTableEnabled()
        && !metaClient.getTableConfig().getMetadataPartitions().isEmpty();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
      // Reconcile marker and data files with WriteStats so that partially written data-files due to failed
      // (but succeeded on retry) tasks are removed.
      String basePath = getMetaClient().getBasePath();
      WriteMarkers markers = WriteMarkersFactory.get(config.getMarkersType(), this, instantTs);

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
      AvroSchemaUtils.checkSchemaCompatible(tableSchema, writerSchema, shouldValidate, allowProjection, getDropPartitionColNames());
    } catch (Exception e) {
      throw new HoodieException("Failed to read schema/check compatibility for base path " + metaClient.getBasePath(), e);
    }
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
        try {
          LOG.info("Deleting metadata partition because it is disabled in writer: " + partitionType.name());
          if (metadataPartitionExists(metaClient.getBasePath(), context, partitionType)) {
            deleteMetadataPartition(metaClient.getBasePath(), context, partitionType);
          }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
          LOG.info("Deleting metadata partition because it is disabled in writer: " + partitionType.name());
          if (metadataPartitionExists(metaClient.getBasePath(), context, partitionType)) {
            deleteMetadataPartition(metaClient.getBasePath(), context, partitionType);
          }
          clearMetadataTablePartitionsConfig(Option.of(partitionType), false);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java`
#### Snippet
```java
    partitionsToBeDeleted.forEach(entry -> {
      try {
        deleteFileAndGetResult(table.getMetaClient().getFs(), table.getMetaClient().getBasePath() + "/" + entry);
      } catch (IOException e) {
        LOG.warn("Partition deletion failed " + entry);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java`
#### Snippet
```java

    HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();
    LOG.info("Scheduling clustering for " + metaClient.getBasePath());
    HoodieWriteConfig config = getWriteConfig();

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java`
#### Snippet
```java
    if (StringUtils.isNullOrEmpty(partitionSelected)) {
      // get matched partitions if set
      partitionPaths = getRegexPatternMatchedPartitions(config, FSUtils.getAllPartitionPaths(getEngineContext(), config.getMetadataConfig(), metaClient.getBasePath()));
      // filter the partition paths if needed to reduce list status
    } else {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java`
#### Snippet
```java

        // reconcile with metadata table timeline
        String metadataBasePath = getMetadataTableBasePath(table.getMetaClient().getBasePath());
        HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataBasePath).build();
        Set<String> metadataCompletedTimestamps = getCompletedArchivedAndActiveInstantsAfter(indexUptoInstant, metadataMetaClient).stream()
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java`
#### Snippet
```java
    requestedPartitions.forEach(partition -> {
      MetadataPartitionType partitionType = MetadataPartitionType.valueOf(partition.toUpperCase(Locale.ROOT));
      if (metadataPartitionExists(table.getMetaClient().getBasePath(), context, partitionType)) {
        deleteMetadataPartition(table.getMetaClient().getBasePath(), context, partitionType);
      }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java`
#### Snippet
```java
      MetadataPartitionType partitionType = MetadataPartitionType.valueOf(partition.toUpperCase(Locale.ROOT));
      if (metadataPartitionExists(table.getMetaClient().getBasePath(), context, partitionType)) {
        deleteMetadataPartition(table.getMetaClient().getBasePath(), context, partitionType);
      }
    });
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactHelpers.java`
#### Snippet
```java
    } catch (IOException e) {
      throw new HoodieCompactionException(
          "Failed to commit " + table.getMetaClient().getBasePath() + " at time " + compactionCommitTime, e);
    }
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactHelpers.java`
#### Snippet
```java
    } catch (IOException e) {
      throw new HoodieCompactionException(
          "Failed to commit " + table.getMetaClient().getBasePath() + " at time " + logCompactionCommitTime, e);
    }
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/HoodieLogCompactionPlanGenerator.java`
#### Snippet
```java
    HoodieUnMergedLogRecordScanner scanner = HoodieUnMergedLogRecordScanner.newBuilder()
        .withFileSystem(metaClient.getFs())
        .withBasePath(hoodieTable.getMetaClient().getBasePath())
        .withLogFilePaths(fileSlice.getLogFiles()
            .sorted(HoodieLogFile.getLogFileComparator())
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java

    List<String> logFiles = operation.getDeltaFileNames().stream().map(
        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())
        .collect(toList());
    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java
    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()
        .withFileSystem(fs)
        .withBasePath(metaClient.getBasePath())
        .withLogFilePaths(logFiles)
        .withReaderSchema(readerSchema)
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java

    Option<HoodieBaseFile> oldDataFileOpt =
        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());

    // Considering following scenario: if all log blocks in this fileSlice is rollback, it returns an empty scanner.
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/BaseHoodieCompactionPlanGenerator.java`
#### Snippet
```java
    // TODO - rollback any compactions in flight
    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
    List<String> partitionPaths = FSUtils.getAllPartitionPaths(engineContext, writeConfig.getMetadataConfig(), metaClient.getBasePath());

    // filter the partition paths if needed to reduce list status
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/BaseHoodieCompactionPlanGenerator.java`
#### Snippet
```java

    if (operations.isEmpty()) {
      LOG.warn("No operations are retrieved for " + metaClient.getBasePath());
      return null;
    }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/BaseHoodieCompactionPlanGenerator.java`
#### Snippet
```java
            + ", Selected workload :" + compactionPlan);
    if (compactionPlan.getOperations().isEmpty()) {
      LOG.warn("After filtering, Nothing to compact for " + metaClient.getBasePath());
    }
    return compactionPlan;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java`
#### Snippet
```java
    this.table = table;
    this.context = context;
    this.basePath = table.getMetaClient().getBasePath();
    this.config = config;
    this.instantTime = instantTime;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java
  protected List<HoodieRollbackStat> deleteFiles(HoodieTableMetaClient metaClient, List<String> filesToBeDeleted, boolean doDelete) throws IOException {
    return filesToBeDeleted.stream().map(fileToDelete -> {
      String basePath = metaClient.getBasePath();
      try {
        Path fullDeletePath = new Path(fileToDelete);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java

          writer = HoodieLogFormat.newWriterBuilder()
              .onParentPath(FSUtils.getPartitionPath(metaClient.getBasePath(), rollbackRequest.getPartitionPath()))
              .withFileId(fileId)
              .overBaseCommit(latestBaseInstant)
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkersFactory.java`
#### Snippet
```java
          return new DirectWriteMarkers(table, instantTime);
        }
        String basePath = table.getMetaClient().getBasePath();
        if (StorageSchemes.HDFS.getScheme().equals(
            FSUtils.getFs(basePath, table.getContext().getHadoopConf().newCopy()).getScheme())) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/DirectWriteMarkers.java`
#### Snippet
```java
  public DirectWriteMarkers(HoodieTable table, String instantTime) {
    this(table.getMetaClient().getFs(),
        table.getMetaClient().getBasePath(),
        table.getMetaClient().getMarkerFolderPath(instantTime),
        instantTime);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackStrategy.java`
#### Snippet
```java
      HoodieTableMetaClient metaClient = table.getMetaClient();
      List<String> partitionPaths =
          FSUtils.getAllPartitionPaths(context, table.getMetaClient().getBasePath(), false, false);
      int numPartitions = Math.max(Math.min(partitionPaths.size(), config.getRollbackParallelism()), 1);

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackStrategy.java`
#### Snippet
```java
        List<HoodieRollbackRequest> hoodieRollbackRequests = new ArrayList<>(partitionPaths.size());
        FileStatus[] filesToDelete =
            fetchFilesFromInstant(instantToRollback, partitionPath, metaClient.getBasePath(), baseFileExtension,
                metaClient.getFs(), commitMetadataOptional, isCommitMetadataCompleted);

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java

  public TimelineServerBasedWriteMarkers(HoodieTable table, String instantTime) {
    this(table.getMetaClient().getBasePath(),
        table.getMetaClient().getMarkerFolderPath(instantTime), instantTime,
        table.getConfig().getViewStorageConfig().getRemoteViewServerHost(),
```

### Deprecation
'HoodieTimer()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
  public Option<Path> createWithEarlyConflictDetection(String partitionPath, String dataFileName, IOType type, boolean checkIfExists,
                                                       HoodieWriteConfig config, String fileId, HoodieActiveTimeline activeTimeline) {
    HoodieTimer timer = new HoodieTimer().startTimer();
    String markerFileName = getMarkerFileName(dataFileName, type);
    Map<String, String> paramsMap = getConfigMap(partitionPath, markerFileName, true);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/savepoint/SavepointActionExecutor.java`
#### Snippet
```java
                }));
      } else {
        List<String> partitions = FSUtils.getAllPartitionPaths(context, config.getMetadataConfig(), table.getMetaClient().getBasePath());
        latestFilesMap = context.mapToPair(partitions, partitionPath -> {
          // Scan all partitions files with this commit time
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java`
#### Snippet
```java
   */
  private static String getFileNameForMarkerFromLogFile(String logFilePath, HoodieTable<?, ?, ?, ?> table) {
    Path logPath = new Path(table.getMetaClient().getBasePath(), logFilePath);
    String fileId = FSUtils.getFileIdFromLogPath(logPath);
    String baseInstant = FSUtils.getBaseCommitTimeFromLogPath(logPath);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java`
#### Snippet
```java
            String expPath = metaClient.getFs()
                .getFileStatus(
                    new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()),
                        new Path(operation.getDataFileName().get())))
                .getPath().toString();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java`
#### Snippet
```java
            try {
              FileStatus[] fileStatuses = metaClient.getFs().listStatus(new Path(
                  FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), new Path(dp)));
              ValidationUtils.checkArgument(fileStatuses.length == 1, "Expect only 1 file-status");
              return new HoodieLogFile(fileStatuses[0]);
```

### Deprecation
'DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/InProcessLockProvider.java`
#### Snippet
```java
    lock = LOCK_INSTANCE_PER_BASEPATH.computeIfAbsent(basePath, (ignore) -> new ReentrantReadWriteLock());
    maxWaitTimeMillis = typedProperties.getLong(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY,
        LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS);
  }

```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/HoodieDateTimeParser.java`
#### Snippet
```java
  public DateTimeZone getInputDateTimeZone() {
    String inputTimeZone;
    if (config.containsKey(TIMESTAMP_TIMEZONE_FORMAT.key())) {
      inputTimeZone = config.getString(TIMESTAMP_TIMEZONE_FORMAT.key(), "GMT");
    } else {
```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/HoodieDateTimeParser.java`
#### Snippet
```java
    String inputTimeZone;
    if (config.containsKey(TIMESTAMP_TIMEZONE_FORMAT.key())) {
      inputTimeZone = config.getString(TIMESTAMP_TIMEZONE_FORMAT.key(), "GMT");
    } else {
      inputTimeZone = config.getString(TIMESTAMP_INPUT_TIMEZONE_FORMAT.key(), "");
```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/HoodieDateTimeParser.java`
#### Snippet
```java
  public DateTimeZone getOutputDateTimeZone() {
    String outputTimeZone;
    if (config.containsKey(TIMESTAMP_TIMEZONE_FORMAT.key())) {
      outputTimeZone = config.getString(TIMESTAMP_TIMEZONE_FORMAT.key(), "GMT");
    } else {
```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/HoodieDateTimeParser.java`
#### Snippet
```java
    String outputTimeZone;
    if (config.containsKey(TIMESTAMP_TIMEZONE_FORMAT.key())) {
      outputTimeZone = config.getString(TIMESTAMP_TIMEZONE_FORMAT.key(), "GMT");
    } else {
      outputTimeZone = config.getString(TIMESTAMP_OUTPUT_TIMEZONE_FORMAT.key(), "");
```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/HoodieDateTimeParser.java`
#### Snippet
```java
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_OUTPUT_DATE_FORMAT;
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_OUTPUT_TIMEZONE_FORMAT;
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_TIMEZONE_FORMAT;
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_TYPE_FIELD;

```

### Deprecation
'HoodieAvroParquetReaderBuilder(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroParquetReader.java`
#### Snippet
```java
      AvroReadSupport.setRequestedProjection(conf, requestedSchema.get());
    }
    ParquetReader<IndexedRecord> reader = new HoodieAvroParquetReaderBuilder<IndexedRecord>(path).withConf(conf).build();
    ParquetReaderIterator<IndexedRecord> parquetReaderIterator = new ParquetReaderIterator<>(reader);
    readerIterators.add(parquetReaderIterator);
```

### Deprecation
'ParquetWriter(org.apache.hadoop.fs.Path, org.apache.parquet.hadoop.ParquetFileWriter.Mode, org.apache.parquet.hadoop.api.WriteSupport, org.apache.parquet.hadoop.metadata.CompressionCodecName, int, int, int, ...)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieBaseParquetWriter.java`
#### Snippet
```java
  public HoodieBaseParquetWriter(Path file,
                                 HoodieParquetConfig<? extends WriteSupport<R>> parquetConfig) throws IOException {
    super(HoodieWrapperFileSystem.convertToHoodiePath(file, parquetConfig.getHadoopConf()),
        ParquetFileWriter.Mode.CREATE,
        parquetConfig.getWriteSupport(),
```

### Deprecation
'org.apache.hadoop.hbase.HColumnDescriptor' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java

    conf.set(CacheConfig.PREFETCH_BLOCKS_ON_OPEN_KEY, String.valueOf(hfileConfig.shouldPrefetchBlocksOnOpen()));
    conf.set(HColumnDescriptor.CACHE_DATA_IN_L1, String.valueOf(hfileConfig.shouldCacheDataInL1()));
    conf.set(DROP_BEHIND_CACHE_COMPACTION_KEY, String.valueOf(hfileConfig.shouldDropBehindCacheCompaction()));
    CacheConfig cacheConfig = new CacheConfig(conf);
```

### Deprecation
'org.apache.hadoop.hbase.HColumnDescriptor' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileConfig.java`
#### Snippet
```java
  public static final CellComparator HFILE_COMPARATOR = new HoodieHBaseKVComparator();
  public static final boolean PREFETCH_ON_OPEN = CacheConfig.DEFAULT_PREFETCH_ON_OPEN;
  public static final boolean CACHE_DATA_IN_L1 = HColumnDescriptor.DEFAULT_CACHE_DATA_IN_L1;
  // This is private in CacheConfig so have been copied here.
  public static final boolean DROP_BEHIND_CACHE_COMPACTION = true;
```

### Deprecation
'CLEANER_POLICY' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java
      }

      HoodieCleaningPolicy cleaningPolicy = HoodieCleaningPolicy.valueOf(writeConfig.getString(CLEANER_POLICY));
      if (cleaningPolicy == HoodieCleaningPolicy.KEEP_LATEST_COMMITS) {
        // Ensure minInstantsToKeep > cleanerCommitsRetained, otherwise we will archive some
```

### Deprecation
'CLEANER_POLICY' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java

  public HoodieCleaningPolicy getCleanerPolicy() {
    return HoodieCleaningPolicy.valueOf(getString(CLEANER_POLICY));
  }

```

### Deprecation
'CLEANER_POLICY' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java

import static org.apache.hudi.common.util.ValidationUtils.checkArgument;
import static org.apache.hudi.config.HoodieCleanConfig.CLEANER_POLICY;
import static org.apache.hudi.config.HoodieCompactionConfig.COPY_ON_WRITE_RECORD_SIZE_ESTIMATE;
import static org.apache.hudi.table.marker.ConflictDetectionUtils.getDefaultEarlyConflictDetectionStrategy;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
    SerializableConfiguration conf = new SerializableConfiguration(datasetMetaClient.getHadoopConf());
    final String dirFilterRegex = dataWriteConfig.getMetadataConfig().getDirectoryFilterRegex();
    final String datasetBasePath = datasetMetaClient.getBasePath();
    SerializablePath serializableBasePath = new SerializablePath(new CachingPath(datasetBasePath));

```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
  }

  private MetadataRecordsGenerationParams getRecordsGenerationParams() {
    return new MetadataRecordsGenerationParams(
        dataMetaClient,
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java

  private MetadataRecordsGenerationParams getRecordsGenerationParams() {
    return new MetadataRecordsGenerationParams(
        dataMetaClient,
        enabledPartitionTypes,
```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public short getDefaultReplication() {
    return fileSystem.getDefaultReplication();
  }
```

### Deprecation
'getDefaultReplication()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public short getDefaultReplication() {
    return fileSystem.getDefaultReplication();
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public boolean delete(Path f) throws IOException {
    return new RetryHelper<Boolean, IOException>(maxRetryIntervalMs, maxRetryNumbers, initialRetryIntervalMs, retryExceptionsList).tryWith(() -> fileSystem.delete(f, true)).start().booleanValue();
  }
```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public long getDefaultBlockSize() {
    return fileSystem.getDefaultBlockSize();
  }
```

### Deprecation
'getDefaultBlockSize()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public long getDefaultBlockSize() {
    return fileSystem.getDefaultBlockSize();
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public long getBlockSize(Path f) throws IOException {
    return fileSystem.getBlockSize(convertToDefaultPath(f));
  }
```

### Deprecation
'getBlockSize(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public long getBlockSize(Path f) throws IOException {
    return fileSystem.getBlockSize(convertToDefaultPath(f));
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public short getReplication(Path src) throws IOException {
    return fileSystem.getReplication(convertToDefaultPath(src));
  }
```

### Deprecation
'getReplication(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public short getReplication(Path src) throws IOException {
    return fileSystem.getReplication(convertToDefaultPath(src));
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public String getName() {
    return fileSystem.getName();
  }
```

### Deprecation
'getName()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public String getName() {
    return fileSystem.getName();
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public short getDefaultReplication() {
    return fileSystem.getDefaultReplication();
  }
```

### Deprecation
'getDefaultReplication()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public short getDefaultReplication() {
    return fileSystem.getDefaultReplication();
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public boolean delete(Path f) throws IOException {
    return executeFuncWithTimeMetrics(MetricName.delete.name(), f, () -> {
      return delete(f, true);
```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public long getLength(Path f) throws IOException {
    return fileSystem.getLength(convertToDefaultPath(f));
  }
```

### Deprecation
'getLength(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public long getLength(Path f) throws IOException {
    return fileSystem.getLength(convertToDefaultPath(f));
  }

```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.fs.FileSystem'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public FsServerDefaults getServerDefaults() throws IOException {
    return fileSystem.getServerDefaults();
  }
```

### Deprecation
'getServerDefaults()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
  @Override
  public FsServerDefaults getServerDefaults() throws IOException {
    return fileSystem.getServerDefaults();
  }

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java

  public RocksDBSchemaHelper(HoodieTableMetaClient metaClient) {
    this.colFamilyForBootstrapBaseFile = "hudi_bootstrap_basefile_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingCompaction = "hudi_pending_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingLogCompaction = "hudi_pending_log_compaction_" + metaClient.getBasePath().replace("/", "_");
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java
  public RocksDBSchemaHelper(HoodieTableMetaClient metaClient) {
    this.colFamilyForBootstrapBaseFile = "hudi_bootstrap_basefile_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingCompaction = "hudi_pending_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingLogCompaction = "hudi_pending_log_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForStoredPartitions = "hudi_partitions_" + metaClient.getBasePath().replace("/", "_");
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java
    this.colFamilyForBootstrapBaseFile = "hudi_bootstrap_basefile_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingCompaction = "hudi_pending_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingLogCompaction = "hudi_pending_log_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForStoredPartitions = "hudi_partitions_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForView = "hudi_view_" + metaClient.getBasePath().replace("/", "_");
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java
    this.colFamilyForPendingCompaction = "hudi_pending_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingLogCompaction = "hudi_pending_log_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForStoredPartitions = "hudi_partitions_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForView = "hudi_view_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForReplacedFileGroups = "hudi_replaced_fg" + metaClient.getBasePath().replace("/", "_");
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java
    this.colFamilyForPendingLogCompaction = "hudi_pending_log_compaction_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForStoredPartitions = "hudi_partitions_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForView = "hudi_view_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForReplacedFileGroups = "hudi_replaced_fg" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingClusteringFileGroups = "hudi_pending_clustering_fg" + metaClient.getBasePath().replace("/", "_");
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java
    this.colFamilyForStoredPartitions = "hudi_partitions_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForView = "hudi_view_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForReplacedFileGroups = "hudi_replaced_fg" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingClusteringFileGroups = "hudi_pending_clustering_fg" + metaClient.getBasePath().replace("/", "_");
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBSchemaHelper.java`
#### Snippet
```java
    this.colFamilyForView = "hudi_view_" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForReplacedFileGroups = "hudi_replaced_fg" + metaClient.getBasePath().replace("/", "_");
    this.colFamilyForPendingClusteringFileGroups = "hudi_pending_clustering_fg" + metaClient.getBasePath().replace("/", "_");
  }

```

### Deprecation
'Field(java.lang.String, org.apache.avro.Schema, java.lang.String, org.codehaus.jackson.JsonNode)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
          TypeDescription childType = orcSchema.getChildren().get(i);
          String childName = orcSchema.getFieldNames().get(i);
          childFields.add(new Field(childName, createAvroSchema(childType), "", null));
        }
        return Schema.createRecord(childFields);
```

### Deprecation
'Field(java.lang.String, org.apache.avro.Schema, java.lang.String, org.codehaus.jackson.JsonNode)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
          TypeDescription childType = orcSchema.getChildren().get(i);
          String childName = orcSchema.getFieldNames().get(i);
          childFields.add(new Field(childName, createAvroSchemaWithNamespace(childType, childName, ""), null, null));
        }
        return Schema.createRecord(recordName, null, namespace, false, childFields);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java`
#### Snippet
```java
      return InternalSchemaUtils.searchSchema(versionID, getHistoricalSchemas(metaClient));
    }
    String tablePath = metaClient.getBasePath();
    // use segment lock to reduce competition.
    synchronized (lockList[tablePath.hashCode() & (lockList.length - 1)]) {
```

### Deprecation
'builder(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    AvroReadSupport.setRequestedProjection(conf, readSchema);
    Set<String> rowKeys = new HashSet<>();
    try (ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build()) {
      Object obj = reader.read();
      while (obj != null) {
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    try {
      // TODO(vc): Should we use the parallel reading version here?
      footer = ParquetFileReader.readFooter(FSUtils.getFs(parquetFilePath.toString(), conf).getConf(), parquetFilePath);
    } catch (IOException e) {
      throw new HoodieIOException("Failed to read footer for parquet " + parquetFilePath, e);
```

### Deprecation
'builder(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
      AvroReadSupport.setAvroReadSchema(conf, readSchema);
      AvroReadSupport.setRequestedProjection(conf, readSchema);
      ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(filePath).withConf(conf).build();
      return HoodieKeyIterator.getInstance(new ParquetReaderIterator<>(reader), keyGeneratorOpt);
    } catch (IOException e) {
```

### Deprecation
'getPath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
          .flatMap(blockMetaData ->
              blockMetaData.getColumns().stream()
                .filter(f -> cols.contains(f.getPath().toDotString()))
                .map(columnChunkMetaData -> {
                  Statistics stats = columnChunkMetaData.getStatistics();
```

### Deprecation
'getPath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
                  return HoodieColumnRangeMetadata.<Comparable>create(
                      parquetFilePath.getName(),
                      columnChunkMetaData.getPath().toDotString(),
                      convertToNativeJavaType(
                          columnChunkMetaData.getPrimitiveType(),
```

### Deprecation
'builder(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
  public List<GenericRecord> readAvroRecords(Configuration configuration, Path filePath) {
    List<GenericRecord> records = new ArrayList<>();
    try (ParquetReader reader = AvroParquetReader.builder(filePath).withConf(configuration).build()) {
      Object obj = reader.read();
      while (obj != null) {
```

### Deprecation
'ParquetWriter(org.apache.hadoop.fs.Path, org.apache.parquet.hadoop.api.WriteSupport, org.apache.parquet.hadoop.metadata.CompressionCodecName, int, int)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodiePartitionMetadata.java`
#### Snippet
```java
          MessageType type = Types.buildMessage().optional(PrimitiveTypeName.INT64).named("dummyint").named("dummy");
          HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(type, schema, Option.empty());
          try (ParquetWriter writer = new ParquetWriter(filePath, writeSupport, CompressionCodecName.UNCOMPRESSED, 1024, 1024)) {
            for (String key : props.stringPropertyNames()) {
              writeSupport.addFooterMetadata(key, props.getProperty(key));
```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java
      TIMESTAMP_OUTPUT_DATE_FORMAT.key(),
      TIMESTAMP_OUTPUT_TIMEZONE_FORMAT.key(),
      TIMESTAMP_TIMEZONE_FORMAT.key(),
      DATE_TIME_PARSER.key()
  );
```

### Deprecation
'TIMESTAMP_TIMEZONE_FORMAT' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_OUTPUT_DATE_FORMAT;
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_OUTPUT_TIMEZONE_FORMAT;
import static org.apache.hudi.common.config.TimestampKeyGeneratorConfig.TIMESTAMP_TIMEZONE_FORMAT;

/**
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
          } else {
            throw new IllegalArgumentException("Could not find any data file written for commit, "
                + "so could not get schema for table " + metaClient.getBasePath());
          }
        default:
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
        default:
          LOG.error("Unknown table type " + metaClient.getTableType());
          throw new InvalidTableException(metaClient.getBasePath());
      }
    } catch (IOException e) {
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
    FileSystem fs = metaClient.getRawFs();
    ParquetMetadata fileFooter =
        ParquetFileReader.readFooter(fs.getConf(), parquetFilePath, ParquetMetadataConverter.NO_FILTER);
    return fileFooter.getFileMetaData().getSchema();
  }
```

### Deprecation
'getBlock(byte\[\], org.apache.avro.Schema, org.apache.hudi.internal.schema.InternalSchema)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java`
#### Snippet
```java
      case AVRO_DATA_BLOCK:
        if (nextBlockVersion.getVersion() == HoodieLogFormatVersion.DEFAULT_VERSION) {
          return HoodieAvroDataBlock.getBlock(content.get(), readerSchema, internalSchema);
        } else {
          return new HoodieAvroDataBlock(inputStream, content, readBlockLazily, logBlockContentLoc,
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
                                                                                   HoodieMetadataConfig metadataConfig,
                                                                                   HoodieTimeline timeline) {
    LOG.info("Creating InMemory based view for basePath " + metaClient.getBasePath());
    if (metadataConfig.enabled()) {
      return new HoodieMetadataFileSystemView(engineContext, metaClient, timeline, metadataConfig);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   */
  public SyncableFileSystemView getFileSystemView(HoodieTableMetaClient metaClient) {
    return globalViewMap.computeIfAbsent(metaClient.getBasePath(),
        (path) -> viewCreator.apply(metaClient, viewStorageConfig));
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
  private static SpillableMapBasedFileSystemView createSpillableMapBasedFileSystemView(SerializableConfiguration conf,
      FileSystemViewStorageConfig viewConf, HoodieTableMetaClient metaClient, HoodieCommonConfig commonConfig) {
    LOG.info("Creating SpillableMap based view for basePath " + metaClient.getBasePath());
    HoodieTimeline timeline = metaClient.getActiveTimeline().filterCompletedAndCompactionInstants();
    return new SpillableMapBasedFileSystemView(metaClient, timeline, viewConf, commonConfig);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
  private static RemoteHoodieTableFileSystemView createRemoteFileSystemView(SerializableConfiguration conf,
      FileSystemViewStorageConfig viewConf, HoodieTableMetaClient metaClient) {
    LOG.info("Creating remote view for basePath " + metaClient.getBasePath() + ". Server="
        + viewConf.getRemoteViewServerHost() + ":" + viewConf.getRemoteViewServerPort() + ", Timeout="
        + viewConf.getRemoteTimelineClientTimeoutSecs());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
        .filter(file -> partitionStr.equals(file.getPartitionPath()))
        .collect(Collectors.toMap(HoodieWriteStat::getFileId, writeStat -> 
            new HoodieBaseFile(new Path(tableMetaClient.getBasePath(), writeStat.getPath()).toString())));

    Stream<HoodieBaseFile> committedBaseFiles = this.completedCommitsFileSystemView.getLatestBaseFiles(partitionStr);
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
      this.populateMetaFields = false;
      this.recordKeyField = tableConfig.getRecordKeyFieldProp();
      this.partitionPathFieldOpt = Option.of(tableConfig.getPartitionFieldProp());
    }

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineUtils.java`
#### Snippet
```java
      return false;
    } catch (IOException e) {
      throw new HoodieIOException("Unable to read instant information: " + instant + " for " + metaClient.getBasePath(), e);
    }
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java`
#### Snippet
```java
    this.config = config;
    this.schemaHelper = new RocksDBSchemaHelper(metaClient);
    this.rocksDB = new RocksDBDAO(metaClient.getBasePath(), config.getRocksdbBasePath());
    init(metaClient, visibleActiveTimeline);
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java`
#### Snippet
```java
    LOG.info("Deleting all rocksdb data associated with table filesystem view");
    rocksDB.close();
    rocksDB = new RocksDBDAO(metaClient.getBasePath(), config.getRocksdbBasePath());
    schemaHelper.getAllColumnFamilies().forEach(rocksDB::addColumnFamily);
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
        FileStatus[] statuses = entry.getValue().stream().map(p -> {
          FileStatus status = new FileStatus(p.getFileSizeInBytes(), false, 0, 0, 0, 0, null, null, null,
              new Path(String.format("%s/%s", metaClient.getBasePath(), p.getPath())));
          return status;
        }).toArray(FileStatus[]::new);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
    HoodieCleanMetadata cleanMetadata = CleanerUtils.getCleanerMetadata(metaClient, instant);
    cleanMetadata.getPartitionMetadata().entrySet().stream().forEach(entry -> {
      final String basePath = metaClient.getBasePath();
      final String partitionPath = entry.getValue().getPartitionPath();
      List<String> fullPathList = entry.getValue().getSuccessDeleteFiles()
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java

  public RemoteHoodieTableFileSystemView(HoodieTableMetaClient metaClient, FileSystemViewStorageConfig viewConf) {
    this.basePath = metaClient.getBasePath();
    this.mapper = new ObjectMapper();
    this.metaClient = metaClient;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java`
#### Snippet
```java
        plan.getFilesToBeDeletedPerPartition().entrySet().stream().map(e -> Pair.of(e.getKey(), e.getValue().stream()
            .map(v -> new HoodieCleanFileInfo(
                new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), e.getKey()), v).toString(), false))
            .collect(Collectors.toList()))).collect(Collectors.toMap(Pair::getKey, Pair::getValue));
    return new HoodieCleanerPlan(plan.getEarliestInstantToRetain(), plan.getLastCompletedCommitTimestamp(),
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/compaction/CompactionV1MigrationHandler.java`
#### Snippet
```java
    ValidationUtils.checkArgument(input.getVersion() == 2, "Input version is " + input.getVersion() + ". Must be 2");
    HoodieCompactionPlan compactionPlan = new HoodieCompactionPlan();
    final Path basePath = new Path(metaClient.getBasePath());
    List<HoodieCompactionOperation> v1CompactionOperationList = new ArrayList<>();
    if (null != input.getOperations()) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanMetadataV1MigrationHandler.java`
#### Snippet
```java
    ValidationUtils.checkArgument(input.getVersion() == 2,
        "Input version is " + input.getVersion() + ". Must be 2");
    final Path basePath = new Path(metaClient.getBasePath());

    final Map<String, HoodieCleanPartitionMetadata> partitionMetadataMap = input
```

### Deprecation
'HoodieDefaultTimeline()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java`
#### Snippet
```java
  }

  protected HoodieActiveTimeline(HoodieTableMetaClient metaClient, Set<String> includedExtensions,
      boolean applyLayoutFilters) {
    // Filter all the filter in the metapath and include only the extensions passed and
```

### Deprecation
'HoodieDefaultTimeline()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieArchivedTimeline.java`
#### Snippet
```java
   * Note that there is no lazy loading, so this may not work if really early startTs is specified.
   */
  public HoodieArchivedTimeline(HoodieTableMetaClient metaClient, String startTs) {
    this.metaClient = metaClient;
    setInstants(loadInstants(new StartTsFilter(startTs), true,
```

### Deprecation
'HoodieDefaultTimeline()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieArchivedTimeline.java`
#### Snippet
```java
   * TBD: Should we enforce maximum time range?
   */
  public HoodieArchivedTimeline(HoodieTableMetaClient metaClient) {
    this.metaClient = metaClient;
    setInstants(this.loadInstants(false));
```

### Deprecation
'delete(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/io/FileBasedInternalSchemaStorageManager.java`
#### Snippet
```java
        List<String> validateSchemaFiles = candidateSchemaFiles.stream().filter(f -> validateCommits.contains(f.split("\\.")[0])).collect(Collectors.toList());
        for (int i = 0; i < validateSchemaFiles.size(); i++) {
          fs.delete(new Path(validateSchemaFiles.get(i)));
        }
      }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/io/FileBasedInternalSchemaStorageManager.java`
#### Snippet
```java

  public FileBasedInternalSchemaStorageManager(HoodieTableMetaClient metaClient) {
    Path metaPath = new Path(metaClient.getBasePath(), ".hoodie");
    this.baseSchemaPath = new Path(metaPath, SCHEMA_NAME);
    this.conf = metaClient.getHadoopConf();
```

### Deprecation
'delete(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/io/FileBasedInternalSchemaStorageManager.java`
#### Snippet
```java
        residualSchemaFiles.forEach(f -> {
          try {
            fs.delete(new Path(getMetaClient().getSchemaFolderName(), f));
          } catch (IOException o) {
            throw new HoodieException(o);
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataFileSystemView.java`
#### Snippet
```java
                                      HoodieMetadataConfig metadataConfig) {
    this(metaClient, visibleActiveTimeline, HoodieTableMetadata.create(engineContext, metadataConfig,
        metaClient.getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue(), true));
  }

```

### Deprecation
'GroupType(org.apache.parquet.schema.Type.Repetition, java.lang.String, org.apache.parquet.schema.OriginalType, java.util.List)' is deprecated
in `hudi-common/src/main/java/org/apache/parquet/avro/HoodieAvroReadSupport.java`
#### Snippet
```java
          newTypes.add(new GroupType(parent.getRepetition(), "key_value", types));
        } else {
          newTypes.add(new GroupType(parent.getRepetition(), parent.getName(), parent.getOriginalType(), types));
        }
      } else {
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.table.connector.source.DataStreamScanProvider'
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/source/ContinuousFileSource.java`
#### Snippet
```java

      @Override
      public DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {
        final RowType rowType = (RowType) tableSchema.toSourceRowDataType().getLogicalType();
        return execEnv.addSource(new BoundedSourceFunction(path, conf.getInteger(CHECKPOINTS)))
```

### Deprecation
'fromDataTypeToLegacyInfo(org.apache.flink.table.types.DataType)' is deprecated
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/factory/CollectSinkTableFactory.java`
#### Snippet
```java
    public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
      final DataType rowType = schema.toPhysicalRowDataType();
      final RowTypeInfo rowTypeInfo = (RowTypeInfo) TypeConversions.fromDataTypeToLegacyInfo(rowType);
      DataStructureConverter converter = context.createDataStructureConverter(schema.toPhysicalRowDataType());
      return SinkFunctionProvider.of(new CollectSinkFunction(converter, rowTypeInfo));
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/InputPathHandler.java`
#### Snippet
```java
      boolean basePathKnown = false;
      for (HoodieTableMetaClient metaClient : tableMetaClientMap.values()) {
        if (inputPath.toString().contains(metaClient.getBasePath())) {
          // We already know the base path for this inputPath.
          basePathKnown = true;
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java`
#### Snippet
```java
    return SpillableMapUtils.convertToHoodieRecordPayload(avroRecord,
        metadataTableConfig.getPayloadClass(), metadataTableConfig.getPreCombineField(),
        Pair.of(metadataTableConfig.getRecordKeyFieldProp(), metadataTableConfig.getPartitionFieldProp()),
        false, Option.of(partitionName), Option.empty());
  }
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java
    String commitTime = Long.toString(System.currentTimeMillis());
    List<String> updates = dataGen.convertToStringList(dataGen.generateUniqueUpdates(commitTime));
    Dataset<Row> df = spark.read().json(jsc.parallelize(updates, 1));
    df.write().format("hudi")
        .options(QuickstartUtils.getQuickstartWriteConfigs())
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java
    String commitTime = Long.toString(System.currentTimeMillis());
    List<String> inserts = dataGen.convertToStringList(dataGen.generateInsertsOnPartition(commitTime, 20, HoodieExampleDataGenerator.DEFAULT_THIRD_PARTITION_PATH));
    Dataset<Row> df = spark.read().json(jsc.parallelize(inserts, 1));

    df.write().format("hudi")
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java
    String commitTime = Long.toString(System.currentTimeMillis());
    List<String> inserts = dataGen.convertToStringList(dataGen.generateInserts(commitTime, 20));
    Dataset<Row> df = spark.read().json(jsc.parallelize(inserts, 1));

    df.write().format("hudi")
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java`
#### Snippet
```java
                metaClient, HoodieInputFormatUtils.buildMetadataConfig(getConf()));
          }
          String partition = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), folder);
          List<HoodieBaseFile> latestFiles = fsView.getLatestBaseFiles(partition).collect(Collectors.toList());
          // populate the cache
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
    for (Path path : snapshotPaths) {
      // Find meta client associated with the input path
      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))
              .forEach(metaClient -> grouped.get(metaClient).add(path));
    }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
        continue;
      }
      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {
        for (HoodieTableMetaClient metaClient : metaClientList) {
          if (inputPath.toString().contains(metaClient.getBasePath())) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {
        for (HoodieTableMetaClient metaClient : metaClientList) {
          if (inputPath.toString().contains(metaClient.getBasePath())) {
            metadata = metaClient;
            if (!grouped.containsKey(metadata)) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
    }
    String incrementalInputPaths = partitionsToList.stream()
        .map(s -> StringUtils.isNullOrEmpty(s) ? tableMetaClient.getBasePath() : tableMetaClient.getBasePath() + Path.SEPARATOR + s)
        .filter(s -> {
          /*
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
    }
    String incrementalInputPaths = partitionsToList.stream()
        .map(s -> StringUtils.isNullOrEmpty(s) ? tableMetaClient.getBasePath() : tableMetaClient.getBasePath() + Path.SEPARATOR + s)
        .filter(s -> {
          /*
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
    // build fileGroup from fsView
    List<FileStatus> affectedFileStatus = Arrays.asList(HoodieInputFormatUtils
        .listAffectedFilesForCommits(job, new Path(tableMetaClient.getBasePath()), metadataList));
    // step3
    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(tableMetaClient, commitsTimelineToReturn, affectedFileStatus.toArray(new FileStatus[0]));
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(tableMetaClient, commitsTimelineToReturn, affectedFileStatus.toArray(new FileStatus[0]));
    // build fileGroup from fsView
    Path basePath = new Path(tableMetaClient.getBasePath());
    // filter affectedPartition by inputPaths
    List<String> affectedPartition = HoodieInputFormatUtils.getWritePartitionPaths(metadataList).stream()
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
    try {
      Schema schema = tableSchemaResolver.getTableAvroSchema();
      boolean isNonPartitionedKeyGen = StringUtils.isNullOrEmpty(tableConfig.getPartitionFieldProp());
      return Option.of(
          new HoodieVirtualKeyInfo(
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
          new HoodieVirtualKeyInfo(
              tableConfig.getRecordKeyFieldProp(),
              isNonPartitionedKeyGen ? Option.empty() : Option.of(tableConfig.getPartitionFieldProp()),
              schema.getField(tableConfig.getRecordKeyFieldProp()).pos(),
              isNonPartitionedKeyGen ? Option.empty() : Option.of(schema.getField(tableConfig.getPartitionFieldProp()).pos())));
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
              isNonPartitionedKeyGen ? Option.empty() : Option.of(tableConfig.getPartitionFieldProp()),
              schema.getField(tableConfig.getRecordKeyFieldProp()).pos(),
              isNonPartitionedKeyGen ? Option.empty() : Option.of(schema.getField(tableConfig.getPartitionFieldProp()).pos())));
    } catch (Exception exception) {
      throw new HoodieException("Fetching table schema failed with exception ", exception);
```

### Deprecation
'SQLContext(org.apache.spark.api.java.JavaSparkContext)' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java`
#### Snippet
```java
        .toLocalIterator();

    Dataset<Row> sourceDataset = new SQLContext(jsc).read().parquet(JavaConversions.asScalaIterator(exportingFilePaths).toSeq());
    partitioner.partition(sourceDataset)
        .format(cfg.outputFormat)
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
    boolean finalResult = true;
    metaClient.reloadActiveTimeline();
    String basePath = metaClient.getBasePath();
    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);
    try {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
        // commit metadata.
        Map<String, List<String>> instantToFilesMap = RepairUtils.tagInstantsOfBaseAndLogFiles(
            metaClient.getBasePath(), allDataFilePaths);
        HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();
        List<HoodieInstant> hoodieInstants = activeTimeline.filterCompletedInstants().getInstants();
```

### Deprecation
Overrides deprecated method in 'org.apache.hadoop.mapred.lib.CombineFileInputFormat'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java

    @Override
    public void createPool(JobConf conf, PathFilter... filters) {
      super.createPool(conf, filters);
    }
```

### Deprecation
'createPool(org.apache.hadoop.mapred.JobConf, org.apache.hadoop.fs.PathFilter...)' is deprecated
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
    @Override
    public void createPool(JobConf conf, PathFilter... filters) {
      super.createPool(conf, filters);
    }

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    // instant time -> relative paths of base and log files to base path
    Map<String, List<String>> instantToFilesMap = RepairUtils.tagInstantsOfBaseAndLogFiles(
        metaClient.getBasePath(), allFilesInPartitions);
    List<String> instantTimesToRepair = instantToFilesMap.keySet().stream()
        .filter(instant -> (!startingInstantOption.isPresent()
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
                                                                          Map<String, List<String>> partitionToDeletedFiles,
                                                                          Map<String, Map<String, Long>> partitionToAppendedFiles,
                                                                          MetadataRecordsGenerationParams recordsGenerationParams,
                                                                          String instantTime) {
    HoodieData<HoodieRecord> allRecordsRDD = engineContext.emptyHoodieData();
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
        }
        final String pathWithPartition = partitionName + "/" + appendedFile;
        final Path appendedFilePath = new Path(recordsGenerationParams.getDataMetaClient().getBasePath(), pathWithPartition);
        try (HoodieFileReader fileReader =
                 HoodieFileReaderFactory.getReaderFactory(HoodieRecordType.AVRO).getFileReader(recordsGenerationParams.getDataMetaClient().getHadoopConf(), appendedFilePath)) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
      return Option.of(schemaResolver.getTableAvroSchema());
    } catch (Exception e) {
      throw new HoodieException("Failed to get latest columns for " + dataTableMetaClient.getBasePath(), e);
    }
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
    try {
      if (filePath.endsWith(HoodieFileFormat.PARQUET.getFileExtension())) {
        Path fullFilePath = new Path(datasetMetaClient.getBasePath(), filePath);
        List<HoodieColumnRangeMetadata<Comparable>> columnRangeMetadataList =
            new ParquetUtils().readRangeFromParquetMetadata(datasetMetaClient.getHadoopConf(), fullFilePath, columnsToIndex);
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static HoodieData<HoodieRecord> convertMetadataToColumnStatsRecords(HoodieCleanMetadata cleanMetadata,
                                                                             HoodieEngineContext engineContext,
                                                                             MetadataRecordsGenerationParams recordsGenerationParams) {
    List<Pair<String, String>> deleteFileList = new ArrayList<>();
    cleanMetadata.getPartitionMetadata().forEach((partition, partitionMetadata) -> {
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static HoodieData<HoodieRecord> convertMetadataToColumnStatsRecords(HoodieCommitMetadata commitMetadata,
                                                                             HoodieEngineContext engineContext,
                                                                             MetadataRecordsGenerationParams recordsGenerationParams) {
    List<HoodieWriteStat> allWriteStats = commitMetadata.getPartitionToWriteStats().values().stream()
        .flatMap(Collection::stream).collect(Collectors.toList());
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
                                                                          Map<String, List<String>> partitionToDeletedFiles,
                                                                          Map<String, Map<String, Long>> partitionToAppendedFiles,
                                                                          MetadataRecordsGenerationParams recordsGenerationParams) {
    HoodieData<HoodieRecord> allRecordsRDD = engineContext.emptyHoodieData();
    HoodieTableMetaClient dataTableMetaClient = recordsGenerationParams.getDataMetaClient();
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static HoodieData<HoodieRecord> convertMetadataToBloomFilterRecords(
      HoodieEngineContext context, HoodieCommitMetadata commitMetadata,
      String instantTime, MetadataRecordsGenerationParams recordsGenerationParams) {
    final List<HoodieWriteStat> allWriteStats = commitMetadata.getPartitionToWriteStats().values().stream()
        .flatMap(entry -> entry.stream()).collect(Collectors.toList());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
      }

      final Path writeFilePath = new Path(recordsGenerationParams.getDataMetaClient().getBasePath(), pathWithPartition);
      try (HoodieFileReader fileReader =
               HoodieFileReaderFactory.getReaderFactory(HoodieRecordType.AVRO).getFileReader(recordsGenerationParams.getDataMetaClient().getHadoopConf(), writeFilePath)) {
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static Map<MetadataPartitionType, HoodieData<HoodieRecord>> convertMetadataToRecords(HoodieEngineContext engineContext,
                                                                                              HoodieCleanMetadata cleanMetadata,
                                                                                              MetadataRecordsGenerationParams recordsGenerationParams,
                                                                                              String instantTime) {
    final Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionToRecordsMap = new HashMap<>();
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static Map<MetadataPartitionType, HoodieData<HoodieRecord>> convertMetadataToRecords(
      HoodieEngineContext context, HoodieCommitMetadata commitMetadata, String instantTime,
      MetadataRecordsGenerationParams recordsGenerationParams) {
    final Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionToRecordsMap = new HashMap<>();
    final HoodieData<HoodieRecord> filesPartitionRecordsRDD = context.parallelize(
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static Map<MetadataPartitionType, HoodieData<HoodieRecord>> convertMetadataToRecords(
      HoodieEngineContext engineContext, HoodieActiveTimeline metadataTableTimeline,
      HoodieRollbackMetadata rollbackMetadata, MetadataRecordsGenerationParams recordsGenerationParams,
      String instantTime, Option<String> lastSyncTs, boolean wasSynced) {
    final Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionToRecordsMap = new HashMap<>();
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
  public static Map<MetadataPartitionType, HoodieData<HoodieRecord>> convertMetadataToRecords(
      HoodieEngineContext engineContext, HoodieActiveTimeline metadataTableTimeline, HoodieRestoreMetadata restoreMetadata,
      MetadataRecordsGenerationParams recordsGenerationParams, String instantTime, Option<String> lastSyncTs) {
    final Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionToRecordsMap = new HashMap<>();
    final Map<String, Map<String, Long>> partitionToAppendedFiles = new HashMap<>();
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
                                                                             HoodieEngineContext engineContext,
                                                                             String instantTime,
                                                                             MetadataRecordsGenerationParams recordsGenerationParams) {
    List<Pair<String, String>> deleteFileList = new ArrayList<>();
    cleanMetadata.getPartitionMetadata().forEach((partition, partitionMetadata) -> {
```

### Deprecation
'org.apache.hudi.metadata.MetadataRecordsGenerationParams' is deprecated
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
   * Get the list of columns for the table for column stats indexing
   */
  private static List<String> getColumnsToIndex(MetadataRecordsGenerationParams recordsGenParams,
                                                Lazy<Option<Schema>> lazyWriterSchemaOpt) {
    checkState(recordsGenParams.isColumnStatsIndexEnabled());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
                new ParquetUtils().readRangeFromParquetMetadata(
                    metaClient.getHadoopConf(),
                    new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), partitionPath), filename),
                    allColumnNameList).stream())
            .sorted(new HoodieColumnRangeMetadataComparator())
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
      this.fileSystemView = FileSystemViewManager.createInMemoryFileSystemView(engineContext,
          metaClient, metadataConfig);
      this.tableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, metaClient.getBasePath(),
          FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue());
      if (metaClient.getCommitsTimeline().filterCompletedInstants().countInstants() > 0) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
            .map(Schema.Field::name).collect(Collectors.toList());
      } catch (Exception e) {
        throw new HoodieException("Failed to get all column names for " + metaClient.getBasePath());
      }
    }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    boolean finalResult = true;
    metaClient.reloadActiveTimeline();
    String basePath = metaClient.getBasePath();
    Set<String> baseFilesForCleaning = Collections.emptySet();

```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java
    int numInstantsPerFetch = props.getInteger(HoodieIncrSourceConfig.NUM_INSTANTS_PER_FETCH.key(),
        HoodieIncrSourceConfig.NUM_INSTANTS_PER_FETCH.defaultValue());
    boolean readLatestOnMissingCkpt = props.getBoolean(HoodieIncrSourceConfig.READ_LATEST_INSTANT_ON_MISSING_CKPT.key(),
        HoodieIncrSourceConfig.READ_LATEST_INSTANT_ON_MISSING_CKPT.defaultValue());
    IncrSourceHelper.MissingCheckpointStrategy missingCheckpointStrategy =
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java
        HoodieIncrSourceConfig.NUM_INSTANTS_PER_FETCH.defaultValue());
    boolean readLatestOnMissingCkpt = props.getBoolean(HoodieIncrSourceConfig.READ_LATEST_INSTANT_ON_MISSING_CKPT.key(),
        HoodieIncrSourceConfig.READ_LATEST_INSTANT_ON_MISSING_CKPT.defaultValue());
    IncrSourceHelper.MissingCheckpointStrategy missingCheckpointStrategy =
        (props.containsKey(HoodieIncrSourceConfig.MISSING_CHECKPOINT_STRATEGY.key()))
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java`
#### Snippet
```java
import static org.apache.hudi.utilities.config.HoodieIncrSourceConfig.HOODIE_SRC_BASE_PATH;
import static org.apache.hudi.utilities.config.HoodieIncrSourceConfig.NUM_INSTANTS_PER_FETCH;
import static org.apache.hudi.utilities.config.HoodieIncrSourceConfig.READ_LATEST_INSTANT_ON_MISSING_CKPT;
import static org.apache.hudi.utilities.config.HoodieIncrSourceConfig.SOURCE_FILE_FORMAT;
import static org.apache.hudi.utilities.sources.helpers.CloudObjectsSelectorCommon.getCloudObjectMetadataPerPartition;
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java`
#### Snippet
```java
    int numInstantsPerFetch = props.getInteger(NUM_INSTANTS_PER_FETCH.key(), NUM_INSTANTS_PER_FETCH.defaultValue());
    boolean readLatestOnMissingCkpt = props.getBoolean(
        READ_LATEST_INSTANT_ON_MISSING_CKPT.key(), READ_LATEST_INSTANT_ON_MISSING_CKPT.defaultValue());
    IncrSourceHelper.MissingCheckpointStrategy missingCheckpointStrategy = (props.containsKey(HoodieIncrSourceConfig.MISSING_CHECKPOINT_STRATEGY.key()))
        ? IncrSourceHelper.MissingCheckpointStrategy.valueOf(props.getString(HoodieIncrSourceConfig.MISSING_CHECKPOINT_STRATEGY.key())) : null;
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java`
#### Snippet
```java
    int numInstantsPerFetch = props.getInteger(NUM_INSTANTS_PER_FETCH.key(), NUM_INSTANTS_PER_FETCH.defaultValue());
    boolean readLatestOnMissingCkpt = props.getBoolean(
        READ_LATEST_INSTANT_ON_MISSING_CKPT.key(), READ_LATEST_INSTANT_ON_MISSING_CKPT.defaultValue());
    IncrSourceHelper.MissingCheckpointStrategy missingCheckpointStrategy = (props.containsKey(HoodieIncrSourceConfig.MISSING_CHECKPOINT_STRATEGY.key()))
        ? IncrSourceHelper.MissingCheckpointStrategy.valueOf(props.getString(HoodieIncrSourceConfig.MISSING_CHECKPOINT_STRATEGY.key())) : null;
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java`
#### Snippet
```java
  public static MissingCheckpointStrategy getMissingCheckpointStrategy(TypedProperties props) {
    boolean readLatestOnMissingCkpt = props.getBoolean(
            READ_LATEST_INSTANT_ON_MISSING_CKPT.key(), READ_LATEST_INSTANT_ON_MISSING_CKPT.defaultValue());

    if (readLatestOnMissingCkpt) {
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java`
#### Snippet
```java
  public static MissingCheckpointStrategy getMissingCheckpointStrategy(TypedProperties props) {
    boolean readLatestOnMissingCkpt = props.getBoolean(
            READ_LATEST_INSTANT_ON_MISSING_CKPT.key(), READ_LATEST_INSTANT_ON_MISSING_CKPT.defaultValue());

    if (readLatestOnMissingCkpt) {
```

### Deprecation
'READ_LATEST_INSTANT_ON_MISSING_CKPT' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java`
#### Snippet
```java

import static org.apache.hudi.utilities.config.HoodieIncrSourceConfig.MISSING_CHECKPOINT_STRATEGY;
import static org.apache.hudi.utilities.config.HoodieIncrSourceConfig.READ_LATEST_INSTANT_ON_MISSING_CKPT;

public class IncrSourceHelper {
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
          Option<Dataset<Row>> dataset = r.getBatch().map(rdd -> source.getSparkSession().read()
              .option("columnNameOfCorruptRecord", ERROR_TABLE_CURRUPT_RECORD_COL_NAME).schema(dataType.asNullable())
              .json(rdd));
          Option<Dataset<Row>> eventsDataset = processErrorEvents(dataset,
              ErrorEvent.ErrorReason.JSON_ROW_DESERIALIZATION_FAILURE);
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
          return new InputBatch<>(
              Option.ofNullable(
                  r.getBatch().map(rdd -> source.getSparkSession().read().schema(dataType).json(rdd)).orElse(null)),
              r.getCheckpointForNextBatch(), r.getSchemaProvider());
        }
```

### Deprecation
'getTypeSerializerIn1(java.lang.ClassLoader)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/sort/SortOperator.java`
#### Snippet
```java
    AbstractRowDataSerializer inputSerializer =
        (AbstractRowDataSerializer)
            getOperatorConfig().getTypeSerializerIn1(getUserCodeClassloader());
    this.binarySerializer = new BinaryRowDataSerializer(inputSerializer.getArity());

```

### Deprecation
'rollback(java.lang.String)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteOperatorCoordinator.java`
#### Snippet
```java
      });
      // Rolls back instant
      writeClient.rollback(instant);
      throw new HoodieException(String.format("Commit instant [%s] failed and rolled back !", instant));
    }
```

### Deprecation
'HIVE_USE_JDBC' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/utils/HiveSyncContext.java`
#### Snippet
```java
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_URL;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_USER;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_USE_JDBC;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_USE_PRE_APACHE_INPUT_FORMAT;
import static org.apache.hudi.hive.HiveSyncConfigHolder.METASTORE_URIS;
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/meta/CkpMetadata.java`
#### Snippet
```java

  public static CkpMetadata getInstance(HoodieTableMetaClient metaClient, String uniqueId) {
    return new CkpMetadata(metaClient.getFs(), metaClient.getBasePath(), uniqueId);
  }

```

### Deprecation
'HIVE_USE_JDBC' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/utils/HiveSyncContext.java`
#### Snippet
```java
    props.setPropertyIfNonNull(META_SYNC_PARTITION_FIELDS.key(), String.join(",", FilePathUtils.extractHivePartitionFields(conf)));
    props.setPropertyIfNonNull(META_SYNC_PARTITION_EXTRACTOR_CLASS.key(), conf.getString(FlinkOptions.HIVE_SYNC_PARTITION_EXTRACTOR_CLASS_NAME));
    props.setPropertyIfNonNull(HIVE_USE_JDBC.key(), String.valueOf(conf.getBoolean(FlinkOptions.HIVE_SYNC_USE_JDBC)));
    props.setPropertyIfNonNull(META_SYNC_USE_FILE_LISTING_FROM_METADATA.key(), String.valueOf(conf.getBoolean(FlinkOptions.METADATA_ENABLED)));
    props.setPropertyIfNonNull(HIVE_IGNORE_EXCEPTIONS.key(), String.valueOf(conf.getBoolean(FlinkOptions.HIVE_SYNC_IGNORE_EXCEPTIONS)));
```

### Deprecation
'setProcessingTimeService(org.apache.flink.streaming.runtime.tasks.ProcessingTimeService)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/common/WriteOperatorFactory.java`
#### Snippet
```java
    this.operator.setOperatorEventGateway(eventDispatcher.getOperatorEventGateway(operatorID));
    this.operator.setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());
    this.operator.setProcessingTimeService(this.processingTimeService);
    eventDispatcher.registerEventHandler(operatorID, operator);
    return (T) operator;
```

### Deprecation
'delete(org.apache.hadoop.fs.Path)' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
            if (metaClientToValidate.reloadActiveTimeline().countInstants() > 0) {
              // Deleting the recreated hoodie.properties and throwing exception.
              fs.delete(new Path(String.format("%s%s/%s", basePathWithForwardSlash, HoodieTableMetaClient.METAFOLDER_NAME, HoodieTableConfig.HOODIE_PROPERTIES_FILE)));
              throw new HoodieIOException("hoodie.properties is missing. Likely due to some external entity. Please populate the hoodie.properties and restart the pipeline. ",
                  e.getIOException());
```

### Deprecation
'rollback(java.lang.String)' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
            case ROLLBACK_COMMIT:
              LOG.info("Commit " + instantTime + " failed!");
              writeClient.rollback(instantTime);
              throw new HoodieException("Error Table Commit failed!");
            case LOG_ERROR:
```

### Deprecation
'rollback(java.lang.String)' is deprecated
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
      });
      // Rolling back instant
      writeClient.rollback(instantTime);
      throw new HoodieException("Commit " + instantTime + " failed and rolled-back !");
    }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bucket/BucketStreamWriteFunction.java`
#### Snippet
```java
    }
    LOG.info(String.format("Loading Hoodie Table %s, with path %s", this.metaClient.getTableConfig().getTableName(),
        this.metaClient.getBasePath() + "/" + partition));

    // Load existing fileID belongs to this task
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bootstrap/BootstrapOperator.java`
#### Snippet
```java
   */
  protected void preLoadIndexRecords() throws Exception {
    String basePath = hoodieTable.getMetaClient().getBasePath();
    int taskID = getRuntimeContext().getIndexOfThisSubtask();
    LOG.info("Start loading records in table {} into the index state, taskId = {}", basePath, taskID);
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java`
#### Snippet
```java

      // set partition field
      conf.setString(FlinkOptions.PARTITION_PATH_FIELD, metaClient.getTableConfig().getPartitionFieldProp());

      // set table schema
```

### Deprecation
'produceDataStream(org.apache.flink.streaming.api.environment.StreamExecutionEnvironment)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/HoodiePipeline.java`
#### Snippet
```java
        .createDynamicTableSource(context))
        .getScanRuntimeProvider(new ScanRuntimeProviderContext());
    return dataStreamScanProvider.produceDataStream(execEnv);
  }

```

### Deprecation
'consumeDataStream(org.apache.flink.streaming.api.datastream.DataStream)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/HoodiePipeline.java`
#### Snippet
```java
    return ((DataStreamSinkProvider) hoodieTableFactory.createDynamicTableSink(context)
        .getSinkRuntimeProvider(new SinkRuntimeProviderContext(isBounded)))
        .consumeDataStream(input);
  }

```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringOperator.java`
#### Snippet
```java
        HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()
            .withFileSystem(table.getMetaClient().getFs())
            .withBasePath(table.getMetaClient().getBasePath())
            .withLogFilePaths(clusteringOp.getDeltaFilePaths())
            .withReaderSchema(readerSchema)
```

### Deprecation
'getPartitionFieldProp()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringOperator.java`
#### Snippet
```java
            tableConfig.getProps(),
            tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
                tableConfig.getPartitionFieldProp())));

        recordIterators.add(StreamSupport.stream(Spliterators.spliteratorUnknownSize(hoodieFileSliceReader, Spliterator.NONNULL), false).map(hoodieRecord -> {
```

### Deprecation
'org.apache.flink.table.types.logical.TypeInformationRawType' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/AvroSchemaConverter.java`
#### Snippet
```java
          // use Kryo for serialization
          DataType rawDataType = new AtomicDataType(
              new TypeInformationRawType<>(false, Types.GENERIC(Object.class)))
              .notNull();

```

### Deprecation
'getSchema()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableFactory.java`
#### Snippet
```java
   */
  private static void setupHoodieKeyOptions(Configuration conf, CatalogTable table) {
    List<String> pkColumns = table.getSchema().getPrimaryKey()
        .map(UniqueConstraint::getColumns).orElse(Collections.emptyList());
    if (pkColumns.size() > 0) {
```

### Deprecation
'org.apache.flink.table.api.constraints.UniqueConstraint' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableFactory.java`
#### Snippet
```java
  private static void setupHoodieKeyOptions(Configuration conf, CatalogTable table) {
    List<String> pkColumns = table.getSchema().getPrimaryKey()
        .map(UniqueConstraint::getColumns).orElse(Collections.emptyList());
    if (pkColumns.size() > 0) {
      // the PRIMARY KEY syntax always has higher priority than option FlinkOptions#RECORD_KEY_FIELD
```

### Deprecation
'getSchema()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableFactory.java`
#### Snippet
```java
        return;
      }
      DataType partitionFieldType = table.getSchema().getFieldDataType(partitionField)
          .orElseThrow(() -> new HoodieValidationException("Field " + partitionField + " does not exist"));
      if (pks.length <= 1 && DataTypeUtils.isDatetimeType(partitionFieldType)) {
```

### Deprecation
'getString(java.lang.String, java.lang.String)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/FormatUtils.java`
#### Snippet
```java
          .withReadBlocksLazily(
              string2Boolean(
                  flinkConf.getString(HoodieRealtimeConfig.COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP,
                      HoodieRealtimeConfig.DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)))
          .withReverseReader(false)
```

### Deprecation
'getInteger(java.lang.String, int)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/FormatUtils.java`
#### Snippet
```java
          .withReverseReader(false)
          .withBufferSize(
              flinkConf.getInteger(HoodieRealtimeConfig.MAX_DFS_STREAM_BUFFER_SIZE_PROP,
                  HoodieRealtimeConfig.DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE))
          .withInstantRange(split.getInstantRange())
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.api.common.io.FileInputFormat'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/cow/CopyOnWriteInputFormat.java`
#### Snippet
```java

  @Override
  public boolean supportsMultiPaths() {
    return true;
  }
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.table.connector.source.abilities.SupportsProjectionPushDown'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java`
#### Snippet
```java

  @Override
  public void applyProjection(int[][] projections) {
    // nested projection is not supported.
    this.requiredPos = Arrays.stream(projections).mapToInt(array -> array[0]).toArray();
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.table.connector.source.DataStreamScanProvider'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java`
#### Snippet
```java

      @Override
      public DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {
        @SuppressWarnings("unchecked")
        TypeInformation<RowData> typeInfo =
```

### Deprecation
'org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java`
#### Snippet
```java
        @SuppressWarnings("unchecked")
        TypeInformation<RowData> typeInfo =
            (TypeInformation<RowData>) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());
        OptionsInference.setupSourceTasks(conf, execEnv.getParallelism());
        if (conf.getBoolean(FlinkOptions.READ_AS_STREAMING)) {
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java`
#### Snippet
```java
                  .collect(Collectors.toList()));
              return new MergeOnReadInputSplit(cnt.getAndAdd(1), basePath, logPaths, latestCommit,
                  metaClient.getBasePath(), maxCompactionMemoryInBytes, mergeType, null, fileSlice.getFileId());
            }).collect(Collectors.toList()))
        .flatMap(Collection::stream)
```

### Deprecation
'org.apache.flink.table.api.TableSchema' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HiveSchemaUtils.java`
#### Snippet
```java
   * Create Hive field schemas from Flink table schema including the hoodie metadata fields.
   */
  public static List<FieldSchema> toHiveFieldSchema(TableSchema schema, boolean withOperationField) {
    List<FieldSchema> columns = new ArrayList<>();
    Collection<String> metaFields = new ArrayList<>(HoodieRecord.HOODIE_META_COLUMNS);
```

### Deprecation
'org.apache.flink.table.api.TableSchema' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HiveSchemaUtils.java`
#### Snippet
```java
   * Create Hive columns from Flink table schema.
   */
  private static List<FieldSchema> createHiveColumns(TableSchema schema) {
    final DataType dataType = schema.toPersistedRowDataType();
    final RowType rowType = (RowType) dataType.getLogicalType();
```

### Deprecation
'getSchema()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/TableOptionProperties.java`
#### Snippet
```java
      List<String> partitionKeys,
      boolean withOperationField) {
    RowType rowType = supplementMetaFields((RowType) catalogTable.getSchema().toPhysicalRowDataType().getLogicalType(), withOperationField);
    Schema schema = AvroSchemaConverter.convertToSchema(rowType);
    MessageType messageType = TableSchemaResolver.convertAvroSchemaToParquet(schema, hadoopConf);
```

### Deprecation
'getInsertValue(org.apache.avro.Schema)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
      }
      try {
        return record.getData().getInsertValue(tableSchema);
      } catch (IOException e) {
        throw new HoodieIOException("Get insert value from payload exception", e);
```

### Deprecation
'getInsertValue(org.apache.avro.Schema)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + curAvroKey, e);
```

### Deprecation
'getInsertValue(org.apache.avro.Schema)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) recordsIterator.next();
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + hoodieRecord.getRecordKey(), e);
```

### Deprecation
'getInsertValue(org.apache.avro.Schema)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + curAvroKey, e);
```

### Deprecation
'org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/streamer/HoodieFlinkStreamer.java`
#### Snippet
```java
    conf.setLong(FlinkOptions.WRITE_COMMIT_ACK_TIMEOUT, ckpTimeout);

    DataStream<RowData> dataStream = env.addSource(new FlinkKafkaConsumer<>(
            cfg.kafkaTopic,
            new JsonRowDataDeserializationSchema(
```

### Deprecation
'getBoolean(java.lang.String, boolean)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java`
#### Snippet
```java
   */
  public static boolean allowCommitOnEmptyBatch(Configuration conf) {
    return conf.getBoolean(HoodieWriteConfig.ALLOW_EMPTY_COMMIT.key(), false);
  }

```

### Deprecation
'getString(java.lang.String, java.lang.String)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java`
#### Snippet
```java
   */
  public static boolean isOptimisticConcurrencyControl(Configuration conf) {
    return conf.getString(HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(), HoodieWriteConfig.WRITE_CONCURRENCY_MODE.defaultValue())
        .equalsIgnoreCase(WriteConcurrencyMode.OPTIMISTIC_CONCURRENCY_CONTROL.name());
  }
```

### Deprecation
'getString(java.lang.String, java.lang.String)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java`
#### Snippet
```java
  public static boolean isLockRequired(Configuration conf) {
    return conf.getBoolean(FlinkOptions.METADATA_ENABLED)
        || ConfigUtils.resolveEnum(WriteConcurrencyMode.class, conf.getString(
        HoodieWriteConfig.WRITE_CONCURRENCY_MODE.key(),
        HoodieWriteConfig.WRITE_CONCURRENCY_MODE.defaultValue()))
```

### Deprecation
'getBoolean(java.lang.String, boolean)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java`
#### Snippet
```java
   */
  public static boolean isReadByTxnCompletionTime(Configuration conf) {
    return conf.getBoolean(HoodieCommonConfig.READ_BY_STATE_TRANSITION_TIME.key(), HoodieCommonConfig.READ_BY_STATE_TRANSITION_TIME.defaultValue());
  }

```

### Deprecation
'getBoolean(java.lang.String, boolean)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java`
#### Snippet
```java
   */
  public static boolean isSchemaEvolutionEnabled(Configuration conf) {
    return conf.getBoolean(HoodieCommonConfig.SCHEMA_EVOLUTION_ENABLE.key(), HoodieCommonConfig.SCHEMA_EVOLUTION_ENABLE.defaultValue());
  }

```

### Deprecation
'getBoolean(java.lang.String, boolean)' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/configuration/OptionsResolver.java`
#### Snippet
```java
   */
  public static boolean isConsistentLogicalTimestampEnabled(Configuration conf) {
    return conf.getBoolean(KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED.key(),
        Boolean.parseBoolean(KeyGeneratorOptions.KEYGENERATOR_CONSISTENT_LOGICAL_TIMESTAMP_ENABLED.defaultValue()));
  }
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java`
#### Snippet
```java
    return fileSplits.entrySet().stream()
        .map(splits ->
            new CdcInputSplit(cnt.getAndAdd(1), metaClient.getBasePath(), maxCompactionMemoryInBytes,
                splits.getKey().getFileId(), splits.getValue().stream().sorted().toArray(HoodieCDCFileSplit[]::new)))
        .collect(Collectors.toList());
```

### Deprecation
'getBasePath()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java`
#### Snippet
```java
              return new MergeOnReadInputSplit(cnt.getAndAdd(1),
                  basePath, logPaths, endInstant,
                  metaClient.getBasePath(), maxCompactionMemoryInBytes, mergeType, instantRange, fileSlice.getFileId());
            }).collect(Collectors.toList()))
        .flatMap(Collection::stream)
```

### Deprecation
'getSchema()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java`
#### Snippet
```java
    Configuration flinkConf = Configuration.fromMap(catalogTable.getOptions());
    final String avroSchema = AvroSchemaConverter.convertToSchema(
        catalogTable.getSchema().toPersistedRowDataType().getLogicalType(),
        AvroSchemaUtils.getAvroRecordQualifiedName(tablePath.getObjectName())).toString();
    flinkConf.setString(FlinkOptions.SOURCE_AVRO_SCHEMA, avroSchema);
```

### Deprecation
'getSchema()' is deprecated
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/catalog/HoodieHiveCatalog.java`
#### Snippet
```java
    // a compatibility issue would be reported.
    boolean withOperationField = Boolean.parseBoolean(table.getOptions().getOrDefault(FlinkOptions.CHANGELOG_ENABLED.key(), "false"));
    List<FieldSchema> allColumns = HiveSchemaUtils.toHiveFieldSchema(table.getSchema(), withOperationField);

    // Table columns and partition keys
```

### Deprecation
'org.apache.flink.streaming.api.TimeCharacteristic' is deprecated
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/adapter/Utils.java`
#### Snippet
```java
public class Utils {
  public static <O> SourceFunction.SourceContext<O> getSourceContext(
      TimeCharacteristic timeCharacteristic,
      ProcessingTimeService processingTimeService,
      StreamTask<?, ?> streamTask,
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        checkArgument(primitiveType.getOriginalType() != OriginalType.TIME_MICROS,
            "TIME_MICROS original type is not ");
        return new HeapTimestampVector(batchSize);
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
            (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                || typeName == PrimitiveType.PrimitiveTypeName.BINARY)
                && primitiveType.getOriginalType() == OriginalType.DECIMAL,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
```

### Deprecation
'org.apache.flink.streaming.api.TimeCharacteristic' is deprecated
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/adapter/Utils.java`
#### Snippet
```java
public class Utils {
  public static <O> SourceFunction.SourceContext<O> getSourceContext(
      TimeCharacteristic timeCharacteristic,
      ProcessingTimeService processingTimeService,
      StreamTask<?, ?> streamTask,
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.batchSize = batchSize;
    // then we need to apply the predicate push down filter
    ParquetMetadata footer = readFooter(conf, path, range(splitStart, splitStart + splitLength));
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
```

### Deprecation
'filterRowGroups(org.apache.parquet.filter2.compat.FilterCompat.Filter, java.util.List, org.apache.parquet.schema.MessageType)' is deprecated
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
    List<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);

    this.fileSchema = footer.getFileMetaData().getSchema();
```

### Deprecation
'ParquetFileReader(org.apache.hadoop.conf.Configuration, org.apache.parquet.hadoop.metadata.FileMetaData, org.apache.hadoop.fs.Path, java.util.List, java.util.List)' is deprecated
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.requestedTypes = Arrays.stream(requestedIndices).mapToObj(i -> selectedTypes[i]).toArray(LogicalType[]::new);
    this.requestedSchema = Types.buildMessage().addFields(readTypes).named("flink-parquet");
    this.reader = new ParquetFileReader(
        conf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());

```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        checkArgument(primitiveType.getOriginalType() != OriginalType.TIME_MICROS,
            "TIME_MICROS original type is not ");
        return new HeapTimestampVector(batchSize);
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
            (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                || typeName == PrimitiveType.PrimitiveTypeName.BINARY)
                && primitiveType.getOriginalType() == OriginalType.DECIMAL,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.batchSize = batchSize;
    // then we need to apply the predicate push down filter
    ParquetMetadata footer = readFooter(conf, path, range(splitStart, splitStart + splitLength));
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
```

### Deprecation
'filterRowGroups(org.apache.parquet.filter2.compat.FilterCompat.Filter, java.util.List, org.apache.parquet.schema.MessageType)' is deprecated
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
    List<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);

    this.fileSchema = footer.getFileMetaData().getSchema();
```

### Deprecation
'ParquetFileReader(org.apache.hadoop.conf.Configuration, org.apache.parquet.hadoop.metadata.FileMetaData, org.apache.hadoop.fs.Path, java.util.List, java.util.List)' is deprecated
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.requestedTypes = Arrays.stream(requestedIndices).mapToObj(i -> selectedTypes[i]).toArray(LogicalType[]::new);
    this.requestedSchema = Types.buildMessage().addFields(readTypes).named("flink-parquet");
    this.reader = new ParquetFileReader(
        conf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());

```

### Deprecation
'produceDataStream(org.apache.flink.streaming.api.environment.StreamExecutionEnvironment)' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/adapter/DataStreamScanProviderAdapter.java`
#### Snippet
```java
public interface DataStreamScanProviderAdapter extends DataStreamScanProvider {
  default DataStream<RowData> produceDataStream(ProviderContext providerContext, StreamExecutionEnvironment streamExecutionEnvironment) {
    return produceDataStream(streamExecutionEnvironment);
  }
}
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.table.connector.sink.DataStreamSinkProvider'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/adapter/DataStreamSinkProviderAdapter.java`
#### Snippet
```java
 */
public interface DataStreamSinkProviderAdapter extends DataStreamSinkProvider {
  DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream);

  @Override
```

### Deprecation
'org.apache.flink.streaming.api.TimeCharacteristic' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/adapter/Utils.java`
#### Snippet
```java
public class Utils {
  public static <O> SourceFunction.SourceContext<O> getSourceContext(
      TimeCharacteristic timeCharacteristic,
      ProcessingTimeService processingTimeService,
      StreamTask<?, ?> streamTask,
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        checkArgument(primitiveType.getOriginalType() != OriginalType.TIME_MICROS,
            "TIME_MICROS original type is not ");
        return new HeapTimestampVector(batchSize);
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
            (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                || typeName == PrimitiveType.PrimitiveTypeName.BINARY)
                && primitiveType.getOriginalType() == OriginalType.DECIMAL,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.batchSize = batchSize;
    // then we need to apply the predicate push down filter
    ParquetMetadata footer = readFooter(conf, path, range(splitStart, splitStart + splitLength));
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
```

### Deprecation
'filterRowGroups(org.apache.parquet.filter2.compat.FilterCompat.Filter, java.util.List, org.apache.parquet.schema.MessageType)' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
    List<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);

    this.fileSchema = footer.getFileMetaData().getSchema();
```

### Deprecation
'ParquetFileReader(org.apache.hadoop.conf.Configuration, org.apache.parquet.hadoop.metadata.FileMetaData, org.apache.hadoop.fs.Path, java.util.List, java.util.List)' is deprecated
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.requestedTypes = Arrays.stream(requestedIndices).mapToObj(i -> selectedTypes[i]).toArray(LogicalType[]::new);
    this.requestedSchema = Types.buildMessage().addFields(readTypes).named("flink-parquet");
    this.reader = new ParquetFileReader(
        conf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());

```

### Deprecation
'produceDataStream(org.apache.flink.streaming.api.environment.StreamExecutionEnvironment)' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/adapter/DataStreamScanProviderAdapter.java`
#### Snippet
```java
public interface DataStreamScanProviderAdapter extends DataStreamScanProvider {
  default DataStream<RowData> produceDataStream(ProviderContext providerContext, StreamExecutionEnvironment streamExecutionEnvironment) {
    return produceDataStream(streamExecutionEnvironment);
  }
}
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.table.connector.sink.DataStreamSinkProvider'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/adapter/DataStreamSinkProviderAdapter.java`
#### Snippet
```java
 */
public interface DataStreamSinkProviderAdapter extends DataStreamSinkProvider {
  DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream);

  @Override
```

### Deprecation
'org.apache.flink.streaming.api.TimeCharacteristic' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/adapter/Utils.java`
#### Snippet
```java
public class Utils {
  public static <O> SourceFunction.SourceContext<O> getSourceContext(
      TimeCharacteristic timeCharacteristic,
      ProcessingTimeService processingTimeService,
      StreamTask<?, ?> streamTask,
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.batchSize = batchSize;
    // then we need to apply the predicate push down filter
    ParquetMetadata footer = readFooter(conf, path, range(splitStart, splitStart + splitLength));
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
```

### Deprecation
'filterRowGroups(org.apache.parquet.filter2.compat.FilterCompat.Filter, java.util.List, org.apache.parquet.schema.MessageType)' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
    List<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);

    this.fileSchema = footer.getFileMetaData().getSchema();
```

### Deprecation
'ParquetFileReader(org.apache.hadoop.conf.Configuration, org.apache.parquet.hadoop.metadata.FileMetaData, org.apache.hadoop.fs.Path, java.util.List, java.util.List)' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.requestedTypes = Arrays.stream(requestedIndices).mapToObj(i -> selectedTypes[i]).toArray(LogicalType[]::new);
    this.requestedSchema = Types.buildMessage().addFields(readTypes).named("flink-parquet");
    this.reader = new ParquetFileReader(
        conf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());

```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        checkArgument(primitiveType.getOriginalType() != OriginalType.TIME_MICROS,
            "TIME_MICROS original type is not ");
        return new HeapTimestampVector(batchSize);
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
            (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                || typeName == PrimitiveType.PrimitiveTypeName.BINARY)
                && primitiveType.getOriginalType() == OriginalType.DECIMAL,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
```

### Deprecation
'produceDataStream(org.apache.flink.streaming.api.environment.StreamExecutionEnvironment)' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/adapter/DataStreamScanProviderAdapter.java`
#### Snippet
```java
public interface DataStreamScanProviderAdapter extends DataStreamScanProvider {
  default DataStream<RowData> produceDataStream(ProviderContext providerContext, StreamExecutionEnvironment streamExecutionEnvironment) {
    return produceDataStream(streamExecutionEnvironment);
  }
}
```

### Deprecation
Overrides deprecated method in 'org.apache.flink.table.connector.sink.DataStreamSinkProvider'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/adapter/DataStreamSinkProviderAdapter.java`
#### Snippet
```java
 */
public interface DataStreamSinkProviderAdapter extends DataStreamSinkProvider {
  DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream);

  @Override
```

### Deprecation
'org.apache.flink.streaming.api.TimeCharacteristic' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/adapter/Utils.java`
#### Snippet
```java
public class Utils {
  public static <O> SourceFunction.SourceContext<O> getSourceContext(
      TimeCharacteristic timeCharacteristic,
      ProcessingTimeService processingTimeService,
      StreamTask<?, ?> streamTask,
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        checkArgument(primitiveType.getOriginalType() != OriginalType.TIME_MICROS,
            "TIME_MICROS original type is not ");
        return new HeapTimestampVector(batchSize);
```

### Deprecation
'org.apache.parquet.schema.OriginalType' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
            (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                || typeName == PrimitiveType.PrimitiveTypeName.BINARY)
                && primitiveType.getOriginalType() == OriginalType.DECIMAL,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
```

### Deprecation
'readFooter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter)' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.batchSize = batchSize;
    // then we need to apply the predicate push down filter
    ParquetMetadata footer = readFooter(conf, path, range(splitStart, splitStart + splitLength));
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
```

### Deprecation
'filterRowGroups(org.apache.parquet.filter2.compat.FilterCompat.Filter, java.util.List, org.apache.parquet.schema.MessageType)' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    MessageType fileSchema = footer.getFileMetaData().getSchema();
    FilterCompat.Filter filter = getFilter(conf);
    List<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);

    this.fileSchema = footer.getFileMetaData().getSchema();
```

### Deprecation
'ParquetFileReader(org.apache.hadoop.conf.Configuration, org.apache.parquet.hadoop.metadata.FileMetaData, org.apache.hadoop.fs.Path, java.util.List, java.util.List)' is deprecated
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    this.requestedTypes = Arrays.stream(requestedIndices).mapToObj(i -> selectedTypes[i]).toArray(LogicalType[]::new);
    this.requestedSchema = Types.buildMessage().addFields(readTypes).named("flink-parquet");
    this.reader = new ParquetFileReader(
        conf, footer.getFileMetaData(), path, blocks, requestedSchema.getColumns());

```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
import java.util.List;

import static org.apache.hudi.common.testutils.RawTripTestPayload.recordsToStrings;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_PASS;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_SYNC_ENABLED;
```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
    String instantTime = HoodieActiveTimeline.createNewInstantTime();
    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime/* ignore */, 100));
    List<String> records1 = recordsToStrings(recordsSoFar);
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime/* ignore */, 100));
    List<String> records1 = recordsToStrings(recordsSoFar);
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

    // Save as hoodie dataset (copy on write)
```

### Deprecation
'HoodieTestDataGenerator(java.lang.String\[\])' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
    if (nonPartitionedTable) {
      // All data goes to base-path
      return new HoodieTestDataGenerator(new String[]{""});
    } else {
      return new HoodieTestDataGenerator();
```

### Deprecation
'HoodieTestDataGenerator()' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
      return new HoodieTestDataGenerator(new String[]{""});
    } else {
      return new HoodieTestDataGenerator();
    }
  }
```

### Deprecation
'HoodieTestDataGenerator(java.lang.String\[\])' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    if (nonPartitionedTable) {
      // All data goes to base-path
      dataGen = new HoodieTestDataGenerator(new String[]{""});
    } else {
      dataGen = new HoodieTestDataGenerator();
```

### Deprecation
'HoodieTestDataGenerator()' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
      dataGen = new HoodieTestDataGenerator(new String[]{""});
    } else {
      dataGen = new HoodieTestDataGenerator();
    }

```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    // Generate some input..
    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts("001"/* ignore */, 100));
    List<String> records1 = recordsToStrings(recordsSoFar);
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts("001"/* ignore */, 100));
    List<String> records1 = recordsToStrings(recordsSoFar);
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

    // Save as hoodie dataset (copy on write)
```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    List<HoodieRecord> recordsToBeUpdated = dataGen.generateUpdates("002"/* ignore */, 100);
    recordsSoFar.addAll(recordsToBeUpdated);
    List<String> records2 = recordsToStrings(recordsToBeUpdated);
    Dataset<Row> inputDF2 = spark.read().json(jssc.parallelize(records2, 2));
    writer = inputDF2.write().format("org.apache.hudi").option("hoodie.insert.shuffle.parallelism", "2")
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    recordsSoFar.addAll(recordsToBeUpdated);
    List<String> records2 = recordsToStrings(recordsToBeUpdated);
    Dataset<Row> inputDF2 = spark.read().json(jssc.parallelize(records2, 2));
    writer = inputDF2.write().format("org.apache.hudi").option("hoodie.insert.shuffle.parallelism", "2")
        .option("hoodie.upsert.shuffle.parallelism", "2")
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
        .map(hr -> "{\"_row_key\":\"" + hr.getRecordKey() + "\",\"partition\":\"" + hr.getPartitionPath() + "\"}")
        .collect(Collectors.toList());
    Dataset<Row> inputDF3 = spark.read().json(jssc.parallelize(deletes, 2));
    writer = inputDF3.write().format("org.apache.hudi").option("hoodie.insert.shuffle.parallelism", "2")
        .option("hoodie.upsert.shuffle.parallelism", "2")
```

### Deprecation
'registerTempTable(java.lang.String)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
        // datasets
        .load(tablePath + (nonPartitionedTable ? "/*" : "/*/*/*/*"));
    snapshotQueryDF.registerTempTable("hoodie_ro");
    spark.sql("describe hoodie_ro").show();
    // all trips whose fare amount was greater than 2.
```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
import java.util.stream.Collectors;

import static org.apache.hudi.common.testutils.RawTripTestPayload.recordsToStrings;
import static org.apache.hudi.common.testutils.Transformations.randomSelectAsHoodieKeys;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_PASS;
```

### Deprecation
'HoodieTestDataGenerator()' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

    // Generator of some records to be loaded in.
    HoodieTestDataGenerator dataGen = new HoodieTestDataGenerator();

    List<String> records1 = recordsToStrings(dataGen.generateInserts("001", 100));
```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    HoodieTestDataGenerator dataGen = new HoodieTestDataGenerator();

    List<String> records1 = recordsToStrings(dataGen.generateInserts("001", 100));
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

    List<String> records1 = recordsToStrings(dataGen.generateInserts("001", 100));
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

    List<String> records2 = recordsToStrings(dataGen.generateUpdatesForAllRecords("002"));
```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));

    List<String> records2 = recordsToStrings(dataGen.generateUpdatesForAllRecords("002"));
    Dataset<Row> inputDF2 = spark.read().json(jssc.parallelize(records2, 2));

```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

    List<String> records2 = recordsToStrings(dataGen.generateUpdatesForAllRecords("002"));
    Dataset<Row> inputDF2 = spark.read().json(jssc.parallelize(records2, 2));


```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    fs.mkdirs(new Path(srcPath2));
    Dataset<Row> delStreamingInput = newSpark.readStream().schema(inputDF1.schema()).json(srcPath2 + "/*");
    List<String> deletes = recordsToStrings(dataGen.generateUniqueUpdates("002", 20));
    Dataset<Row> inputDF3 = newSpark.read().json(jssc.parallelize(deletes, 2));
    executor = Executors.newFixedThreadPool(2);
```

### Deprecation
'json(org.apache.spark.api.java.JavaRDD)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    Dataset<Row> delStreamingInput = newSpark.readStream().schema(inputDF1.schema()).json(srcPath2 + "/*");
    List<String> deletes = recordsToStrings(dataGen.generateUniqueUpdates("002", 20));
    Dataset<Row> inputDF3 = newSpark.read().json(jssc.parallelize(deletes, 2));
    executor = Executors.newFixedThreadPool(2);

```

### Deprecation
'recordsToStrings(java.util.List)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
import java.util.concurrent.Future;

import static org.apache.hudi.common.testutils.RawTripTestPayload.recordsToStrings;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_PASS;
import static org.apache.hudi.hive.HiveSyncConfigHolder.HIVE_SYNC_ENABLED;
```

### Deprecation
'registerTempTable(java.lang.String)' is deprecated
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
        // datasets
        .load(tablePath + "/*/*/*/*");
    hoodieROViewDF.registerTempTable("hoodie_ro");
    spark.sql("describe hoodie_ro").show();
    // all trips whose fare amount was greater than 2.
```

### Deprecation
'rollback(java.lang.String)' is deprecated
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java`
#### Snippet
```java
  public void abort() {
    LOG.error("Commit " + instantTime + " aborted ");
    writeClient.rollback(instantTime);
    writeClient.close();
  }
```

## RuleId[id=UnnecessaryReturn]
### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/InternalSchemaBuilder.java`
#### Snippet
```java
        return;
      default:
        return;
    }
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hudi-common/src/main/java/org/apache/hudi/secondary/index/HoodieSecondaryIndex.java`
#### Snippet
```java
        break;
      default:
        return;
    }
  }
```

## RuleId[id=SimplifiableConditionalExpression]
### SimplifiableConditionalExpression
`row.isNullAt(index) ? false : row.getBoolean(index)` can be simplified to '!row.isNullAt(index) \&\& row.getBoolean(index)'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/sort/SpaceCurveSortingHelper.java`
#### Snippet
```java
      return row.isNullAt(index) ? Long.MAX_VALUE : row.getDecimal(index).longValue();
    } else if (dataType instanceof BooleanType) {
      boolean value = row.isNullAt(index) ? false : row.getBoolean(index);
      return value ? Long.MAX_VALUE : 0;
    } else if (dataType instanceof BinaryType) {
```

### SimplifiableConditionalExpression
`row.isNullAt(index) ? false : row.getBoolean(index)` can be simplified to '!row.isNullAt(index) \&\& row.getBoolean(index)'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/sort/SpaceCurveSortingHelper.java`
#### Snippet
```java
      return BinaryUtil.longTo8Byte(row.isNullAt(index) ? Long.MAX_VALUE : row.getDecimal(index).longValue());
    } else if (dataType instanceof BooleanType) {
      boolean value = row.isNullAt(index) ? false : row.getBoolean(index);
      return BinaryUtil.intTo8Byte(value ? 1 : 0);
    } else if (dataType instanceof BinaryType) {
```

### SimplifiableConditionalExpression
`oldValue.orderingVal.compareTo(orderingVal) > 0 ? true : false` can be simplified to 'oldValue.orderingVal.compareTo(orderingVal) \> 0'
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
    }
    // pick the payload with greater ordering value as insert record
    final boolean shouldPickOldRecord = oldValue.orderingVal.compareTo(orderingVal) > 0 ? true : false;
    try {
      GenericRecord oldRecord = HoodieAvroUtils.bytesToAvro(oldValue.recordBytes, schema);
```

### SimplifiableConditionalExpression
`hoodieVirtualKeyInfo.get().getPartitionPathField().isPresent() ? readColNames.contains(hoodieVirtualKeyInfo.get().getPartitionPathField().get())
: true` can be simplified to '!hoodieVirtualKeyInfo.get().getPartitionPathField().isPresent() \|\| readColNames.contains(hoodieVirtualKeyInfo.get().getPartitionPathField().get())'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java`
#### Snippet
```java
    } else {
      return readColNames.contains(hoodieVirtualKeyInfo.get().getRecordKeyField())
          && (hoodieVirtualKeyInfo.get().getPartitionPathField().isPresent() ? readColNames.contains(hoodieVirtualKeyInfo.get().getPartitionPathField().get())
          : true);
    }
```

## RuleId[id=FinalStaticMethod]
### FinalStaticMethod
'static' method declared `final`
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java`
#### Snippet
```java
  }

  private static final Map<String, String> createComparableActionsMap() {
    Map<String, String> comparableMap = new HashMap<>();
    comparableMap.put(HoodieTimeline.COMPACTION_ACTION, HoodieTimeline.COMMIT_ACTION);
```

### FinalStaticMethod
'static' method declared `final`
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
  }

  public static final Config getConfig(String[] args) {
    Config cfg = new Config();
    JCommander cmd = new JCommander(cfg, null, args);
```

## RuleId[id=TrivialStringConcatenation]
### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
          newV.putDecimal(i, oldDecimal, ((DecimalType) newType).precision());
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, (oldV.getDouble(i) + "").getBytes(StandardCharsets.UTF_8));
        }
      }
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
          newV.putDouble(i, isInt ? oldV.getInt(i) : oldV.getLong(i));
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, ((isInt ? oldV.getInt(i) : oldV.getLong(i)) + "").getBytes(StandardCharsets.UTF_8));
        } else if (newType instanceof DecimalType) {
          Decimal oldDecimal = Decimal.apply(isInt ? oldV.getInt(i) : oldV.getLong(i));
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
        // float -> double/string/decimal
        if (newType instanceof DoubleType) {
          newV.putDouble(i, Double.valueOf(oldV.getFloat(i) + ""));
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, (oldV.getFloat(i) + "").getBytes(StandardCharsets.UTF_8));
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
          newV.putDouble(i, Double.valueOf(oldV.getFloat(i) + ""));
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, (oldV.getFloat(i) + "").getBytes(StandardCharsets.UTF_8));
        } else if (newType instanceof DecimalType) {
          Decimal oldDecimal = Decimal.apply(oldV.getFloat(i));
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
        if (oldSchema.getType() == Schema.Type.FLOAT) {
          // java float cannot convert to double directly, deal with float precision change
          return Double.valueOf(oldValue + "");
        } else if (oldSchema.getType() == Schema.Type.INT) {
          return ((Integer) oldValue).doubleValue();
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-common/src/main/java/org/apache/hudi/common/util/NumericUtils.java`
#### Snippet
```java
    }
    int exp = (int) (Math.log(bytes) / Math.log(1024));
    String pre = "KMGTPE".charAt(exp - 1) + "";
    return String.format("%.1f %sB", bytes / Math.pow(1024, exp), pre);
  }
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java`
#### Snippet
```java
    Path hoodieMetaPath = new Path(tablePath, HoodieTableMetaClient.METAFOLDER_NAME);
    //step1:
    Path candidateCommitFile = commitSet.stream().filter(fileName -> HoodieInstant.extractTimestamp(fileName).equals(versionId + ""))
        .findFirst().map(f -> new Path(hoodieMetaPath, f)).orElse(null);
    if (candidateCommitFile != null) {
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/utils/QuickstartConfigurations.java`
#### Snippet
```java
      builder.append("\n");
    }
    final String withProps = ""
        + ") with (\n"
        + "  'connector' = '" + CollectSinkTableFactory.FACTORY_ID + "'\n"
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java`
#### Snippet
```java
    ValidationUtils.checkArgument(ts > 0, "Timestamp must be positive");
    long lower = ts - 1;
    return "" + lower;
  }

```

### TrivialStringConcatenation
Empty string used in concatenation
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClientIds.java`
#### Snippet
```java
      // 2. else returns an auto inc id
      String largestClientId = getClientId(sortedPaths.get(sortedPaths.size() - 1));
      return INIT_CLIENT_ID.equals(largestClientId) ? "1" : (Integer.parseInt(largestClientId) + 1) + "";
    } catch (IOException e) {
      throw new RuntimeException("Generate next client id error", e);
```

## RuleId[id=RedundantTypeArguments]
### RedundantTypeArguments
Explicit type arguments can be inferred
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDeleteBlock.java`
#### Snippet
```java
    if (version == 1) {
      // legacy version
      HoodieKey[] keys = SerializationUtils.<HoodieKey[]>deserialize(data);
      return Arrays.stream(keys).map(DeleteRecord::create).toArray(DeleteRecord[]::new);
    } else {
```

### RedundantTypeArguments
Explicit type arguments can be inferred
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDeleteBlock.java`
#### Snippet
```java
      return Arrays.stream(keys).map(DeleteRecord::create).toArray(DeleteRecord[]::new);
    } else {
      return SerializationUtils.<DeleteRecord[]>deserialize(data);
    }
  }
```

## RuleId[id=MismatchedStringBuilderQueryUpdate]
### MismatchedStringBuilderQueryUpdate
Contents of `StringBuilder str` are updated, but never queried
in `hudi-common/src/main/java/org/apache/hudi/common/config/SerializableConfiguration.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder str = new StringBuilder();
    configuration.iterator().forEachRemaining(e -> str.append(String.format("%s => %s \n", e.getKey(), e.getValue())));
    return configuration.toString();
```

## RuleId[id=NullableProblems]
### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-aws/src/main/java/org/apache/hudi/aws/transaction/lock/DynamoDBBasedLockProvider.java`
#### Snippet
```java

  @Override
  public boolean tryLock(long time, TimeUnit unit) {
    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));
    try {
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/transaction/lock/HiveMetastoreBasedLockProvider.java`
#### Snippet
```java

  @Override
  public boolean tryLock(long time, TimeUnit unit) {
    LOG.info(generateLogStatement(ACQUIRING, generateLogSuffixString()));
    try {
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/FileSystemBasedLockProvider.java`
#### Snippet
```java

  @Override
  public boolean tryLock(long time, TimeUnit unit) {
    try {
      synchronized (LOCK_FILE_NAME) {
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java`
#### Snippet
```java

  @Override
  public boolean tryLock(long time, TimeUnit unit) {
    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));
    try {
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java`
#### Snippet
```java

  @Override
  public void putAll(Map<? extends K, ? extends R> m) {
    getRocksDBDAO().writeBatch(batch -> m.forEach((key, value) -> getRocksDBDAO().putInBatch(batch, columnFamilyName, key, value)));
  }
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Pair.java`
#### Snippet
```java
   */
  @Override
  public int compareTo(final Pair<L, R> other) {

    checkComparable(this);
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java
    }

    public int compareTo(List o) {
      return compare(list, o);
    }
```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Triple.java`
#### Snippet
```java
   */
  @Override
  public int compareTo(final Triple<L, M, R> other) {
    checkComparable(this);
    checkComparable(other);
```

### NullableProblems
Not annotated method overrides method annotated with @NotNull
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDbDiskMap.java`
#### Snippet
```java

  @Override
  public Set<Entry<T, R>> entrySet() {
    Set<Entry<T, R>> entrySet = new HashSet<>();
    for (T key : keySet) {
```

### NullableProblems
Not annotated method overrides method annotated with @NotNull
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java

  @Override
  public Set<T> keySet() {
    return valueMetadataMap.keySet();
  }
```

### NullableProblems
Not annotated method overrides method annotated with @NotNull
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
   */
  @Override
  public Iterator<R> iterator() {
    ClosableIterator<R> iterator = new LazyFileIterable(filePath, valueMetadataMap, isCompressionEnabled).iterator();
    this.iterators.add(iterator);
```

### NullableProblems
Not annotated method overrides method annotated with @NotNull
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java

  @Override
  public Set<Entry<T, R>> entrySet() {
    Set<Entry<T, R>> entrySet = new HashSet<>();
    for (T key : valueMetadataMap.keySet()) {
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRollingStat.java`
#### Snippet
```java
  @Nullable
  private long totalInputWriteBytesToDisk;
  @Nullable
  private long totalInputWriteBytesOnDisk;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRollingStat.java`
#### Snippet
```java
  private long deletes;
  // TODO
  @Nullable
  private long totalInputWriteBytesToDisk;
  @Nullable
```

### NullableProblems
Setter parameter for @Nullable field might be annotated @Nullable itself
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  }

  public void setPartitionPath(String partitionPath) {
    this.partitionPath = partitionPath;
  }
```

### NullableProblems
Getter for @Nullable field might be annotated @Nullable itself
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  }

  public Long getMinEventTime() {
    return minEventTime;
  }
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total number of corrupt blocks seen in a compaction operation.
   */
  @Nullable
  private long totalCorruptLogBlock;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
    }

    public void setTotalScanTime(@Nullable long totalScanTime) {
      this.totalScanTime = totalScanTime;
    }
```

### NullableProblems
Setter parameter for @Nullable field might be annotated @Nullable itself
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  }

  public void setTempPath(String tempPath) {
    this.tempPath = tempPath;
  }
```

### NullableProblems
Getter for @Nullable field might be annotated @Nullable itself
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  }

  public String getTempPath() {
    return this.tempPath;
  }
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total number of log blocks seen in a compaction operation.
   */
  @Nullable
  private long totalLogBlocks;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total number of log records that were compacted by a compaction operation.
   */
  @Nullable
  private long totalLogRecords;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
     * Total time taken to read and merge logblocks in a log file.
     */
    @Nullable
    private long totalScanTime;

```

### NullableProblems
Getter for @Nullable field might be annotated @Nullable itself
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  }

  public Long getMaxEventTime() {
    return maxEventTime;
  }
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total number of records updated by a compaction operation.
   */
  @Nullable
  private long totalUpdatedRecordsCompacted;

```

### NullableProblems
Getter for @Nullable field might be annotated @Nullable itself
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  }

  public String getPartitionPath() {
    return partitionPath;
  }
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
    }

    public void setTotalUpsertTime(@Nullable long totalUpsertTime) {
      this.totalUpsertTime = totalUpsertTime;
    }
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
    }

    @Nullable
    public long getTotalUpsertTime() {
      return totalUpsertTime;
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
    private long totalCreateTime;

    @Nullable
    public long getTotalScanTime() {
      return totalScanTime;
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
     * Total time taken by a Hoodie Merge for an existing file.
     */
    @Nullable
    private long totalUpsertTime;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
    }

    @Nullable
    public long getTotalCreateTime() {
      return totalCreateTime;
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total number of log files compacted for a file slice with this base fileid.
   */
  @Nullable
  private long totalLogFilesCompacted;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
    }

    public void setTotalCreateTime(@Nullable long totalCreateTime) {
      this.totalCreateTime = totalCreateTime;
    }
```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
     * Total time taken by a Hoodie Insert to a file.
     */
    @Nullable
    private long totalCreateTime;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total number of rollback blocks seen in a compaction operation.
   */
  @Nullable
  private long totalRollbackBlocks;

```

### NullableProblems
Primitive type members cannot be annotated
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
   * Total size of all log files for a file slice with this base fileid.
   */
  @Nullable
  private long totalLogSizeCompacted;

```

### NullableProblems
Not annotated parameter overrides @NotNull parameter
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java`
#### Snippet
```java

  @Override
  public int compareTo(HoodieInstant o) {
    return COMPARATOR.compare(this, o);
  }
```

## RuleId[id=NegativeIntConstantInLongContext]
### NegativeIntConstantInLongContext
Negative int hexadecimal constant in long context
in `hudi-common/src/main/java/org/apache/hudi/common/util/hash/HashID.java`
#### Snippet
```java
      case BITS_64:
        XXHash64 hash64 = factory.hash64();
        return Bytes.toBytes(hash64.hash(message, 0, message.length, HASH_SEED));
      default:
        throw new HoodieIOException("XX" + bits + " hash is unsupported!");
```

## RuleId[id=JavadocLinkAsPlainText]
### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-aws/src/main/java/org/apache/hudi/aws/sync/AwsGlueCatalogSyncTool.java`
#### Snippet
```java
/**
 * Currently Experimental. Utility class that implements syncing a Hudi Table with the
 * AWS Glue Data Catalog (https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html)
 * to enable querying via Glue ETLs, Athena etc.
 * <p>
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java`
#### Snippet
```java
/**
 * This class implements all the AWS APIs to enable syncing of a Hudi Table with the
 * AWS Glue Data Catalog (https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html).
 *
 * @Experimental
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetRowDataWriter.java`
#### Snippet
```java
  /**
   * Timestamp of INT96 bytes, julianDay(4) + nanosOfDay(8). See
   * https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
   * <p>
   * TODO: Leaving this here as there might be a requirement to support TIMESTAMP(9) in the future
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetRowDataWriter.java`
#### Snippet
```java
   * TIMESTAMP_MILLIS and TIMESTAMP_MICROS is the deprecated ConvertedType of TIMESTAMP with the MILLIS and MICROS
   * precision respectively. See
   * https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
   */
  private class Timestamp64Writer implements FieldWriter {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/datadog/DatadogReporter.java`
#### Snippet
```java
   * Build payload that contains metrics data.
   * <p>
   * Refer to Datadog API reference https://docs.datadoghq.com/api/?lang=bash#post-timeseries-points
   */
  static class PayloadBuilder {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
  /**
   * Sanitizes Name according to Avro rule for names.
   * Removes characters other than the ones mentioned in https://avro.apache.org/docs/current/spec.html#names .
   *
   * @param name input name
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
  /**
   * Sanitizes Name according to Avro rule for names.
   * Removes characters other than the ones mentioned in https://avro.apache.org/docs/current/spec.html#names .
   *
   * @param name input name
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
 *         - adopted from org.apache.cassandra.io
 *           Copyright: 2015-2019 The Apache Software Foundation
 *           Home page: http://cassandra.apache.org/
 *           License: http://www.apache.org/licenses/LICENSE-2.0
 */
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
 *           Copyright: 2015-2019 The Apache Software Foundation
 *           Home page: http://cassandra.apache.org/
 *           License: http://www.apache.org/licenses/LICENSE-2.0
 */
public final class BufferedRandomAccessFile extends RandomAccessFile {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
/**
 * Methods including addToVector, addUnionValue, createOrcSchema are originally from
 * https://github.com/streamsets/datacollector.
 * Source classes:
 * - com.streamsets.pipeline.lib.util.avroorc.AvroToOrcRecordConverter
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
 * 2) Current position in the file NOTE : Only String.class type supported for Key
 * <p>
 * Inspired by https://github.com/basho/bitcask
 */
public final class BitCaskDiskMap<T extends Serializable, R extends Serializable> extends DiskMap<T, R> {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
 * This is just a copy of the org.apache.hadoop.hive.ql.io.CombineHiveInputFormat from Hive 2.x Search for **MOD** to
 * see minor modifications to support custom inputformat in CombineHiveInputFormat. See
 * https://issues.apache.org/jira/browse/HIVE-9771
 * <p>
 * <p>
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java`
#### Snippet
```java
  /**
   * The method takes the provided url {@code registryUrl} and gets the schema from the schema registry using that url.
   * If the caller provides userInfo credentials in the url (e.g "https://foo:bar@schemaregistry.org") then the credentials
   * are extracted the url using the Matcher and the extracted credentials are set on the request as an Authorization
   * header.
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java`
#### Snippet
```java
 * Obtains latest schema from the Confluent/Kafka schema-registry.
 * <p>
 * https://github.com/confluentinc/schema-registry
 */
public class SchemaRegistryProvider extends SchemaProvider {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/CsvDFSSource.java`
#### Snippet
```java
 *
 * Detailed information of these CSV options can be found at:
 * https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#csv-scala.collection.Seq-
 *
 * If the source Avro schema is provided through the {@link org.apache.hudi.utilities.schema.FilebasedSchemaProvider}
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudStoreIngestionConfig.java`
#### Snippet
```java
   * a) it will keep delivering the same message since it wasn't acknowledged in time.
   * b) The size of the request that acknowledges outstanding messages may exceed the limit,
   * which is 512KB as per Google's docs. See: https://cloud.google.com/pubsub/quotas#resource_limits
   */
  @Deprecated
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java
  /**
   * Create partitions of list using specific batch size. we can't use third party API for this
   * functionality, due to https://github.com/apache/hudi/blob/master/style/checkstyle.xml#L270
   */
  protected List<List<Message>> createListPartitions(List<Message> singleList, int eachBatchSize) {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/SanitizationUtils.java`
#### Snippet
```java

/**
 * Avro schemas have restrictions (https://avro.apache.org/docs/current/spec.html#names) on field names
 * that other schema formats do not have. This class provides utilities to help with sanitizing
 */
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/MetadataMessage.java`
#### Snippet
```java
  /**
   * Whether a message is valid to be ingested and stored by this Metadata puller.
   * Ref: https://cloud.google.com/storage/docs/pubsub-notifications#events
   */
  public MessageValidity shouldBeProcessed() {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/MetadataMessage.java`
#### Snippet
```java
  /**
   * Returns true if message corresponds to new file creation, false if not.
   * Ref: https://cloud.google.com/storage/docs/pubsub-notifications#events
   */
  private boolean isNewFileCreation() {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/MetadataMessage.java`
#### Snippet
```java
 * Wraps a PubsubMessage assuming it's from Cloud Storage Pubsub Notifications (CSPN), and
 * adds relevant helper methods.
 * For details of CSPN messages see: https://cloud.google.com/storage/docs/pubsub-notifications
 */
public class MetadataMessage {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/MetadataMessage.java`
#### Snippet
```java
  /**
   * Whether message represents an overwrite of an existing file.
   * Ref: https://cloud.google.com/storage/docs/pubsub-notifications#replacing_objects
   */
  private boolean isOverwriteOfExistingFile() {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/PostgresDebeziumSource.java`
#### Snippet
```java

  /**
   * Debezium Kafka Payload has a nested structure (see https://debezium.io/documentation/reference/1.4/connectors/postgresql.html#postgresql-create-events).
   * This function flattens this nested structure for the Postgres data, and also extracts a subset of Debezium metadata fields.
   *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/MysqlDebeziumSource.java`
#### Snippet
```java

  /**
   * Debezium Kafka Payload has a nested structure (see https://debezium.io/documentation/reference/1.4/connectors/mysql.html).
   * This function flattens this nested structure for the Mysql data, and also extracts a subset of Debezium metadata fields.
   *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
/**
 * Generate checkpoint from Kafka-Connect-HDFS managed data set.
 * Documentation: https://docs.confluent.io/current/connect/kafka-connect-hdfs/index.html
 */
public class KafkaConnectHdfsProvider extends InitialCheckPointProvider {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/schema/SchemaRegistryProvider.java`
#### Snippet
```java
  /**
   * The method takes the provided url {@code registryUrl} and gets the schema from the schema registry using that url.
   * If the caller provides userInfo credentials in the url (e.g "https://foo:bar@schemaregistry.org") then the credentials
   * are extracted the url using the Matcher and the extracted credentials are set on the request as an Authorization
   * header.
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/schema/SchemaRegistryProvider.java`
#### Snippet
```java
 * Obtains latest schema from the Confluent/Kafka schema-registry.
 * <p>
 * https://github.com/confluentinc/schema-registry
 */
public class SchemaRegistryProvider extends SchemaProvider {
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/Int64TimestampColumnReader.java`
#### Snippet
```java
 * that is UTC normalized and has MILLIS precision.
 *
 * <p>See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
 * TIMESTAMP_MILLIS and TIMESTAMP_MICROS are the deprecated ConvertedType.
 */
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
/**
 * Run length decoder for data and dictionary ids.
 * See https://github.com/apache/parquet-format/blob/master/Encodings.md
 * See {@link RunLengthBitPackingHybridDecoder}.
 *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/Int64TimestampColumnReader.java`
#### Snippet
```java
 * that is UTC normalized and has MILLIS precision.
 *
 * <p>See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
 * TIMESTAMP_MILLIS and TIMESTAMP_MICROS are the deprecated ConvertedType.
 */
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
/**
 * Run length decoder for data and dictionary ids.
 * See https://github.com/apache/parquet-format/blob/master/Encodings.md
 * See {@link RunLengthBitPackingHybridDecoder}.
 *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
/**
 * Run length decoder for data and dictionary ids.
 * See https://github.com/apache/parquet-format/blob/master/Encodings.md
 * See {@link RunLengthBitPackingHybridDecoder}.
 *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/Int64TimestampColumnReader.java`
#### Snippet
```java
 * that is UTC normalized and has MILLIS precision.
 *
 * <p>See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
 * TIMESTAMP_MILLIS and TIMESTAMP_MICROS are the deprecated ConvertedType.
 */
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/Int64TimestampColumnReader.java`
#### Snippet
```java
 * that is UTC normalized and has MILLIS precision.
 *
 * <p>See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
 * TIMESTAMP_MILLIS and TIMESTAMP_MICROS are the deprecated ConvertedType.
 */
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
/**
 * Run length decoder for data and dictionary ids.
 * See https://github.com/apache/parquet-format/blob/master/Encodings.md
 * See {@link RunLengthBitPackingHybridDecoder}.
 *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
/**
 * Run length decoder for data and dictionary ids.
 * See https://github.com/apache/parquet-format/blob/master/Encodings.md
 * See {@link RunLengthBitPackingHybridDecoder}.
 *
```

### JavadocLinkAsPlainText
Link specified as plain text
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/Int64TimestampColumnReader.java`
#### Snippet
```java
 * that is UTC normalized and has MILLIS precision.
 *
 * <p>See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp
 * TIMESTAMP_MILLIS and TIMESTAMP_MICROS are the deprecated ConvertedType.
 */
```

## RuleId[id=EqualsBetweenInconvertibleTypes]
### EqualsBetweenInconvertibleTypes
`equals` between objects of inconvertible types 'String' and 'StringType'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case STRING:
        String stringType = avroSchema.getProp(GenericData.STRING_PROP);
        if (stringType == null || !stringType.equals(StringType.String)) {
          int stringLength = ((BytesColumnVector) colVector).length[vectorPos];
          int stringOffset = ((BytesColumnVector) colVector).start[vectorPos];
```

## RuleId[id=FieldCanBeLocal]
### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/expression/Expression.java`
#### Snippet
```java
  }

  private final List<Expression> children;

  public Expression(List<Expression> children) {
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkCompactor.java`
#### Snippet
```java
    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {
  private static final Logger LOG = LoggerFactory.getLogger(HoodieSparkCompactor.class);
  private transient HoodieEngineContext context;

  public HoodieSparkCompactor(BaseHoodieWriteClient<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> compactionClient,
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanActionExecutor.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(CleanPlanner.class);

  private final Option<Map<String, String>> extraMetadata;

  public CleanPlanActionExecutor(HoodieEngineContext context,
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
  private transient FileSystem fs;
  private Path updatedPropsFilePath;
  private Path propsFilePath;

  public UpgradeDowngrade(
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
  protected HoodieWriteConfig config;
  protected HoodieEngineContext context;
  private transient FileSystem fs;
  private Path updatedPropsFilePath;
  private Path propsFilePath;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
  protected HoodieEngineContext context;
  private transient FileSystem fs;
  private Path updatedPropsFilePath;
  private Path propsFilePath;

```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  // heartbeat interval in millis
  private final Long heartbeatIntervalInMs;
  private Integer numTolerableHeartbeatMisses;
  private final Long maxAllowableHeartbeatIntervalInMs;
  private Map<String, Heartbeat> instantToHeartbeatMap;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/metrics/HoodieLockMetrics.java`
#### Snippet
```java
  private final HoodieWriteConfig writeConfig;
  private final boolean isMetricsEnabled;
  private final int keepLastNtimes = 100;
  private final transient HoodieTimer lockDurationTimer = HoodieTimer.create();
  private final transient HoodieTimer lockApiRequestDurationTimer = HoodieTimer.create();
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/metrics/HoodieLockMetrics.java`
#### Snippet
```java
  private transient Timer lockApiRequestDuration;
  private static final Object REGISTRY_LOCK = new Object();
  private Metrics metrics;

  public HoodieLockMetrics(HoodieWriteConfig writeConfig) {
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java`
#### Snippet
```java
  public static final String DEFAULT_SIMPLE_INDEX_UPDATE_PARTITION_PATH = SIMPLE_INDEX_UPDATE_PARTITION_PATH_ENABLE.defaultValue();

  private EngineType engineType;

  /**
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/prometheus/PushGatewayMetricsReporter.java`
#### Snippet
```java
  private final PushGatewayReporter pushGatewayReporter;
  private final int periodSeconds;
  private final boolean deleteShutdown;
  private final String configuredJobName;
  private final Map<String, String> configuredLabels;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java`
#### Snippet
```java
  private String conflictResolutionFailureCounterName = null;
  private HoodieWriteConfig config;
  private String tableName;
  private Timer rollbackTimer = null;
  private Timer cleanTimer = null;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java

  // This is private in CacheConfig so have been copied here.
  private static String DROP_BEHIND_CACHE_COMPACTION_KEY = "hbase.hfile.drop.behind.compaction";

  public HoodieAvroHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileConfig, Schema schema,
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java
  private final TaskContextSupplier taskContextSupplier;
  private final boolean populateMetaFields;
  private final Schema schema;
  private final Option<Schema.Field> keyFieldSchema;
  private HFile.Writer writer;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java
    private boolean isPreCommitValidationConfigSet = false;
    private boolean isMetricsJmxConfigSet = false;
    private boolean isMetricsGraphiteConfigSet = false;
    private boolean isLayoutConfigSet = false;

```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java
    private boolean isLockConfigSet = false;
    private boolean isPreCommitValidationConfigSet = false;
    private boolean isMetricsJmxConfigSet = false;
    private boolean isMetricsGraphiteConfigSet = false;
    private boolean isLayoutConfigSet = false;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InMemoryFileSystem.java`
#### Snippet
```java
  // TODO: this needs to be per path to support num_cores > 1, and we should release the buffer once done
  private ByteArrayOutputStream bos;
  private Configuration conf = null;
  public static final String SCHEME = "inmemfs";
  private URI uri;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
  // TODO(na) : a dynamic sizing factor to ensure we have space for other objects in memory and
  // incorrect payload estimation
  private final Double sizingFactorForInMemoryMap = 0.8;
  // Size Estimator for key type
  private final SizeEstimator<T> keySizeEstimator;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java`
#### Snippet
```java
  private InternalSchema internalSchema = InternalSchema.getEmptyInternalSchema();
  private final boolean readBlocksLazily;
  private final boolean reverseLogReader;
  private final String recordKeyField;
  private final boolean enableInlineReading;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
  private Map<String, List<String>> partitionToReplaceFileIds;
  private List<HoodieWriteStat> filesWritten;
  private String preCommitInstantTime;
  private SyncableFileSystemView completedCommitsFileSystemView;
  private HoodieTableMetaClient tableMetaClient;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
  private Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;

  private HoodieTableMetaClient metaClient;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
  private Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;

  public TableSizeStats(JavaSparkContext jsc, Config cfg) {
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
  private final Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;
  // Spark context
  private final HoodieEngineContext context;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
  private TypedProperties props;

  private final HoodieTableMetaClient metaClient;

  public HoodieDropPartitionsTool(JavaSparkContext jsc, Config cfg) {
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/FilebasedSchemaProvider.java`
#### Snippet
```java
public class FilebasedSchemaProvider extends SchemaProvider {

  private final FileSystem fs;

  protected Schema sourceSchema;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
  private Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;

  private final HoodieTableMetaClient metaClient;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectDataFetcher.java`
#### Snippet
```java

  private final String fileFormat;
  private TypedProperties props;

  private static final Logger LOG = LoggerFactory.getLogger(GcsObjectDataFetcher.class);
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/MysqlDebeziumSource.java`
#### Snippet
```java
public class MysqlDebeziumSource extends DebeziumSource {

  private final SQLContext sqlContext;
  private final String generateUniqueSeqUdfFn = "mysql_generate_order_key";

```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
 */
public class KafkaConnectHdfsProvider extends InitialCheckPointProvider {
  private static String FILENAME_SEPARATOR = "[\\+\\.]";

  public KafkaConnectHdfsProvider(TypedProperties props) {
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
   * Schema provider that supplies the command for reading the input and writing out the target table.
   */
  private transient SchemaProvider schemaProvider;

  /**
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectTransactionServices.java`
#### Snippet
```java
  private final HoodieWriteConfig writeConfig;
  private final String tableBasePath;
  private final String tableName;
  private final HoodieEngineContext context;

```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectTransactionServices.java`
#### Snippet
```java
  private final String tableBasePath;
  private final String tableName;
  private final HoodieEngineContext context;

  private final HoodieJavaWriteClient<HoodieAvroPayload> javaClient;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
   * NOTE: These properties are already consolidated w/ CLI provided config-overrides.
   */
  private final TypedProperties properties;

  protected transient Option<HoodieIngestionService> ingestionService;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java`
#### Snippet
```java
     * Meta Client.
     */
    private final HoodieTableMetaClient metaClient;

    /**
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringOperator.java`
#### Snippet
```java
  private transient Schema schema;
  private transient Schema readerSchema;
  private transient int[] requiredPos;
  private transient AvroToRowDataConverters.AvroToRowDataConverter avroToRowDataConverter;
  private transient HoodieFlinkWriteClient writeClient;
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClientIds.java`
#### Snippet
```java
    private String clientId = INIT_CLIENT_ID;
    private long heartbeatIntervalInMs = DEFAULT_HEARTBEAT_INTERVAL_IN_MS;
    private int numTolerableHeartbeatMisses = DEFAULT_NUM_TOLERABLE_HEARTBEAT_MISSES;

    public Builder fs(FileSystem fs) {
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-user", "-hu"}, description = "Hive username")
  private String hiveUser = "hive";

  @Parameter(names = {"--hive-password", "-hp"}, description = "Hive password")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-sync", "-hs"}, description = "Enable syncing to hive")
  private Boolean enableHiveSync = false;

  @Parameter(names = {"--hive-db", "-hd"}, description = "Hive database")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-name", "-n"}, description = "Table name for Hoodie sample table")
  private String tableName = "hoodie_test";

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-table", "-ht"}, description = "Hive table")
  private String hiveTable = "hoodie_sample_test";

  @Parameter(names = {"--hive-user", "-hu"}, description = "Hive username")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--commit-type", "-ct"}, description = "How may commits will run")
  private String commitType = "overwrite";

  @Parameter(names = {"--help", "-h"}, help = true)
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
  private Boolean useMultiPartitionKeys = false;

  @Parameter(names = {"--commit-type", "-ct"}, description = "How may commits will run")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-password", "-hp"}, description = "Hive password")
  private String hivePass = "hive";

  @Parameter(names = {"--hive-url", "-hl"}, description = "Hive JDBC URL")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-db", "-hd"}, description = "Hive database")
  private String hiveDB = "default";

  @Parameter(names = {"--hive-table", "-ht"}, description = "Hive table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-url", "-hl"}, description = "Hive JDBC URL")
  private String hiveJdbcUrl = "jdbc:hive2://localhost:10000";

  @Parameter(names = {"--non-partitioned", "-np"}, description = "Use non-partitioned Table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
public class HoodieJavaGenerateApp {
  @Parameter(names = {"--table-path", "-p"}, description = "Path for Hoodie sample table")
  private String tablePath = "file:///tmp/hoodie/sample-table";

  @Parameter(names = {"--table-name", "-n"}, description = "Table name for Hoodie sample table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-sync", "-hv"}, description = "Enable syncing to hive")
  private Boolean enableHiveSync = false;

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
  private String hiveTable = "hoodie_sample_test";

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
  private String hiveDB = "default";

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
  private String hiveJdbcUrl = "jdbc:hive2://localhost:10000";

  @Parameter(names = {"--non-partitioned", "-np"}, description = "Use non-partitioned Table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
  private String hivePass = "hive";

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-path", "-p"}, description = "path for Hoodie sample table")
  private String tablePath = "file:///tmp/hoodie/sample-table";

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
  private Boolean useMultiPartitionKeys = false;

  @Parameter(names = {"--help", "-h"}, help = true)
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
  private String tableName = "hoodie_test";

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
  private String hiveUser = "hive";

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
  private String hivePass = "hive";

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
  @Parameter(names = {"--streaming-checkpointing-path", "-scp"},
      description = "path for streaming checking pointing folder")
  private String streamingCheckpointingPath = "/tmp/hoodie/streaming/checkpoint";

  @Parameter(names = {"--streaming-duration-in-ms", "-sdm"},
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
  private String hiveJdbcUrl = "jdbc:hive2://localhost:10000";

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-sync", "-hv"}, description = "Enable syncing to hive")
  private Boolean enableHiveSync = false;

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
  @Parameter(names = {"--streaming-duration-in-ms", "-sdm"},
      description = "time in millisecond for the streaming duration")
  private Long streamingDurationInMs = 15000L;

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
  private String hiveTable = "hoodie_sample_test";

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
  private String tableName = "hoodie_test";

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
  private Boolean useMultiPartitionKeys = false;

  @Parameter(names = {"--help", "-h"}, help = true)
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--streaming-source-path", "-ssp"}, description = "path for streaming source file folder")
  private String streamingSourcePath = "/tmp/hoodie/streaming/source";

  @Parameter(names = {"--streaming-checkpointing-path", "-scp"},
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
  private String hiveDB = "default";

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
```

### FieldCanBeLocal
Field can be converted to a local variable
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
  private String hiveUser = "hive";

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
```

## RuleId[id=IgnoreResultOfCall]
### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java
    File outFile = new File(filePath);
    if (outFile.exists()) {
      outFile.delete();
    }
    OutputStream os = null;
```

### IgnoreResultOfCall
Result of `HoodieCLI.getTableMetaClient()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
      @ShellOption(value = "--sparkMaster", defaultValue = SparkUtil.DEFAULT_SPARK_MASTER, help = "Spark master") final String master
  ) throws IOException {
    HoodieCLI.getTableMetaClient();
    initJavaSparkContext(Option.of(master));
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
```

### IgnoreResultOfCall
Result of `HoodieCLI.getTableMetaClient()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
                     @ShellOption(value = {"--readonly"}, defaultValue = "false",
                         help = "Open in read-only mode") final boolean readOnly) throws Exception {
    HoodieCLI.getTableMetaClient();
    Path metadataPath = new Path(getMetadataTableBasePath(HoodieCLI.basePath));
    try {
```

### IgnoreResultOfCall
Result of `HoodieCLI.getTableMetaClient()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
  public String listFiles(
      @ShellOption(value = {"--partition"}, help = "Name of the partition to list files", defaultValue = "") final String partition) throws IOException {
    HoodieCLI.getTableMetaClient();
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metaReader = new HoodieBackedTableMetadata(
```

### IgnoreResultOfCall
Result of `HoodieCLI.getTableMetaClient()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
  public String validateFiles(
      @ShellOption(value = {"--verbose"}, help = "Print all file details", defaultValue = "false") final boolean verbose) throws IOException {
    HoodieCLI.getTableMetaClient();
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metadataReader = new HoodieBackedTableMetadata(
```

### IgnoreResultOfCall
Result of `HoodieCLI.getTableMetaClient()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
  @ShellMethod(key = "metadata stats", value = "Print stats about the metadata")
  public String stats() throws IOException {
    HoodieCLI.getTableMetaClient();
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metadata = new HoodieBackedTableMetadata(new HoodieLocalEngineContext(HoodieCLI.conf),
```

### IgnoreResultOfCall
Result of `HoodieCLI.getTableMetaClient()` is ignored
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
      @ShellOption(value = "--sparkMaster", defaultValue = SparkUtil.DEFAULT_SPARK_MASTER, help = "Spark master") final String master
  ) throws IOException {
    HoodieCLI.getTableMetaClient();
    Path metadataPath = new Path(getMetadataTableBasePath(HoodieCLI.basePath));
    try {
```

### IgnoreResultOfCall
Result of `Condition.await()` is ignored
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
      queueLock.lock();
      while (!isShutdown() && !hasError() && (pendingInstants.size() > numPending)) {
        consumed.await(POLLING_SECONDS, TimeUnit.SECONDS);
      }
    } finally {
```

### IgnoreResultOfCall
Result of `ExecutorService.awaitTermination()` is ignored
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
          try {
            // Wait for some max time after requesting shutdown
            executor.awaitTermination(24, TimeUnit.HOURS);
          } catch (InterruptedException ie) {
            LOG.error("Interrupted while waiting for shutdown", ie);
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java`
#### Snippet
```java
  public static void deleteDirectory(File directory) throws IOException {
    if (directory.exists()) {
      Files.walk(directory.toPath()).sorted(Comparator.reverseOrder()).map(Path::toFile).forEach(File::delete);
      directory.delete();
      if (directory.exists()) {
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java`
#### Snippet
```java
    if (directory.exists()) {
      Files.walk(directory.toPath()).sorted(Comparator.reverseOrder()).map(Path::toFile).forEach(File::delete);
      directory.delete();
      if (directory.exists()) {
        throw new IOException("Unable to delete directory " + directory);
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java`
#### Snippet
```java
  public static void mkdir(File directory) throws IOException {
    if (!directory.exists()) {
      directory.mkdirs();
    }

```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
        }
      }
      writeOnlyFile.delete();
      this.iterators.forEach(ClosableIterator::close);
    } catch (Exception e) {
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    } catch (Exception e) {
      // delete the file for any sort of exception
      writeOnlyFile.delete();
    } finally {
      super.close();
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    // delete the file if it exists
    if (writeOnlyFile.exists()) {
      writeOnlyFile.delete();
    }
    if (!writeOnlyFile.getParentFile().exists()) {
```

### IgnoreResultOfCall
Result of `File.mkdir()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    }
    if (!writeOnlyFile.getParentFile().exists()) {
      writeOnlyFile.getParentFile().mkdir();
    }
    writeOnlyFile.createNewFile();
```

### IgnoreResultOfCall
Result of `File.createNewFile()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
      writeOnlyFile.getParentFile().mkdir();
    }
    writeOnlyFile.createNewFile();
    LOG.debug("Spilling to file location " + writeOnlyFile.getAbsolutePath());
    // Make sure file is deleted when JVM exits
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      LOG.info("Creating Pending Compaction map using external spillable Map. Max Mem=" + maxMemoryForPendingCompaction
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      LOG.info("Creating file group id to clustering instant map using external spillable Map. Max Mem=" + maxMemoryForClusteringFileGroups
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, HoodieInstant> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForClusteringFileGroups, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      LOG.info("Creating file group id to replace instant map using external spillable Map. Max Mem=" + maxMemoryForReplaceFileGroups
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, HoodieInstant> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForReplaceFileGroups, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      LOG.info("Creating Pending Log Compaction map using external spillable Map. Max Mem=" + maxMemoryForPendingLogCompaction
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingLogCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      LOG.info("Creating bootstrap base File Map using external spillable Map. Max Mem=" + maxMemoryForBootstrapBaseFile
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, BootstrapBaseFileMapping> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForBootstrapBaseFile, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      LOG.info("Creating Partition To File groups map using external spillable Map. Max Mem=" + maxMemoryForFileGroupMap
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      return (Map<String, List<HoodieFileGroup>>) (new ExternalSpillableMap<>(maxMemoryForFileGroupMap, baseStoreDir,
          new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
```

### IgnoreResultOfCall
Result of `DataInputStream.read()` is ignored
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java`
#### Snippet
```java
        passwordFileStream = fileSystem.open(new Path(properties.getString(JdbcSourceConfig.PASSWORD_FILE.key())));
        byte[] bytes = new byte[passwordFileStream.available()];
        passwordFileStream.read(bytes);
        dataFrameReader = dataFrameReader.option(Config.PASSWORD_PROP, new String(bytes));
      } else {
```

### IgnoreResultOfCall
Result of `ExecutorService.awaitTermination()` is ignored
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/utils/NonThrownExecutor.java`
#### Snippet
```java
      // We do not expect this to actually block for long. At this point, there should
      // be very few task running in the executor, if any.
      executor.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);
    }
  }
```

## RuleId[id=DefaultAnnotationParam]
### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
    public int parallelism = 200;

    @Parameter(names = {"--ignore-failed", "-ig"}, description = "Ignore data table validate failure and continue.", required = false)
    public boolean ignoreFailed = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
    public Integer minValidateIntervalSeconds = 10 * 60;

    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for validation", required = false)
    public int parallelism = 200;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
    public String sparkMaster = null;

    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
    public String sparkMemory = "1g";

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
    public Boolean assumeDatePartitioning = false;

    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java

    @Parameter(names = {"--assume-date-partitioning"}, description = "Should HoodieWriteClient assume the data is partitioned by dates, i.e three levels from base path."
        + "This is a stop-gap to support tables created by versions < 0.3.1. Will be removed eventually", required = false)
    public Boolean assumeDatePartitioning = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java

    @Parameter(names = {"--continuous"}, description = "Running MetadataTableValidator in continuous. "
        + "Can use --min-validate-interval-seconds to control validation frequency", required = false)
    public boolean continuous = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public String basePath = null;

    @Parameter(names = {"--num-days", "-nd"}, description = "Consider files modified within this many days.", required = false)
    public long numDays = 0;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public int parallelism = 200;

    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public String endDate = null;

    @Parameter(names = {"--enable-table-stats", "-fs"}, description = "Show file-level stats.", required = false)
    public boolean tableStats = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public String sparkMaster = null;

    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
    public String sparkMemory = "1g";

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public boolean tableStats = false;

    @Parameter(names = {"--enable-partition-stats", "-ps"}, description = "Show partition-level stats.", required = false)
    public boolean partitionStats = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public boolean partitionStats = false;

    @Parameter(names = {"--props-path", "-pp"}, description = "Properties file containing base paths one per line", required = false)
    public String propsFilePath = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public long numDays = 0;

    @Parameter(names = {"--start-date", "-sd"}, description = "Consider files modified on or after this date.", required = false)
    public String startDate = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public String propsFilePath = null;

    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for valuation", required = false)
    public int parallelism = 200;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    public String startDate = null;

    @Parameter(names = {"--end-date", "-ed"}, description = "Consider files modified before this date.", required = false)
    public String endDate = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java

  public static class Config implements Serializable {
    @Parameter(names = {"--base-path", "-bp"}, description = "Base path for the table", required = false)
    public String basePath = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    public String startingInstantTime = null;
    @Parameter(names = {"--end-instant-time", "-ei"}, description = "Ending Instant time "
        + "for repair (inclusive)", required = false)
    public String endingInstantTime = null;
    @Parameter(names = {"--backup-path", "-bp"}, description = "Backup path for storing dangling data "
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    public String runningMode = null;
    @Parameter(names = {"--start-instant-time", "-si"}, description = "Starting Instant time "
        + "for repair (inclusive)", required = false)
    public String startingInstantTime = null;
    @Parameter(names = {"--end-instant-time", "-ei"}, description = "Ending Instant time "
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    public String endingInstantTime = null;
    @Parameter(names = {"--backup-path", "-bp"}, description = "Backup path for storing dangling data "
        + "and log files from the table", required = false)
    public String backupPath = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for repair", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for repair", required = false)
    public int parallelism = 2;
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
        + "and log files from the table", required = false)
    public String backupPath = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for repair", required = false)
    public int parallelism = 2;
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    public String sparkMemory = "1g";
    @Parameter(names = {"--assume-date-partitioning", "-dp"}, description = "whether the partition path "
        + "is date with three levels", required = false)
    public Boolean assumeDatePartitioning = false;
    @Parameter(names = {"--help", "-h"}, help = true)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
    public String sparkMemory = "1g";
    @Parameter(names = {"--assume-date-partitioning", "-dp"}, description = "whether the partition path "
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--schema-file", "-sf"}, description = "path for Avro schema file", required = false)
    public String schemaFile = null;
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = true)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert", required = false)
    public int parallelism = 200;
    @Parameter(names = {"--schema-file", "-sf"}, description = "path for Avro schema file", required = false)
    public String schemaFile = null;
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = true)
    public String sparkMemory = null;
    @Parameter(names = {"--retry", "-rt"}, description = "number of retries", required = false)
    public int retry = 0;
    @Parameter(names = {"--schedule", "-sc"}, description = "Schedule compaction", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--retry", "-rt"}, description = "number of retries", required = false)
    public int retry = 0;
    @Parameter(names = {"--schedule", "-sc"}, description = "Schedule compaction", required = false)
    public Boolean runSchedule = false;
    @Parameter(names = {"--mode", "-m"}, description = "Set job mode: Set \"schedule\" means make a compact plan; "
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
        + "Set \"scheduleAndExecute\" means make a compact plan first and execute that plan immediately", required = false)
    public String runningMode = null;
    @Parameter(names = {"--strategy", "-st"}, description = "Strategy Class", required = false)
    public String strategyClassName = LogFileSizeBasedCompactionStrategy.class.getName();
    @Parameter(names = {"--help", "-h"}, help = true)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--instant-time", "-it"}, description = "Compaction Instant time", required = false)
    public String compactionInstantTime = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert", required = false)
    public int parallelism = 200;
    @Parameter(names = {"--schema-file", "-sf"}, description = "path for Avro schema file", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--table-name", "-tn"}, description = "Table name", required = true)
    public String tableName = null;
    @Parameter(names = {"--instant-time", "-it"}, description = "Compaction Instant time", required = false)
    public String compactionInstantTime = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    @Parameter(names = {"--mode", "-m"}, description = "Set job mode: Set \"schedule\" means make a compact plan; "
        + "Set \"execute\" means execute a compact plan at given instant which means --instant-time is needed here; "
        + "Set \"scheduleAndExecute\" means make a compact plan first and execute that plan immediately", required = false)
    public String runningMode = null;
    @Parameter(names = {"--strategy", "-st"}, description = "Strategy Class", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--dry-run", "-dr"}, description = "Dry Run Mode", required = false)
    public boolean dryRun = false;
    @Parameter(names = {"--skip-validation", "-sv"}, description = "Skip Validation", required = false)
    public boolean skipValidation = false;
    @Parameter(names = {"--output-path", "-ot"}, description = "Output Path", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--instant-time", "-in"}, description = "Compaction Instant time", required = false)
    public String compactionInstantTime = null;
    @Parameter(names = {"--partition-path", "-pp"}, description = "Partition Path", required = false)
    public String partitionPath = null;
    @Parameter(names = {"--file-id", "-id"}, description = "File Id", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--skip-validation", "-sv"}, description = "Skip Validation", required = false)
    public boolean skipValidation = false;
    @Parameter(names = {"--output-path", "-ot"}, description = "Output Path", required = false)
    public String outputPath = null;
    @Parameter(names = {"--print-output", "-pt"}, description = "Print Output", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--base-path", "-bp"}, description = "Base path for the table", required = true)
    public String basePath = null;
    @Parameter(names = {"--instant-time", "-in"}, description = "Compaction Instant time", required = false)
    public String compactionInstantTime = null;
    @Parameter(names = {"--partition-path", "-pp"}, description = "Partition Path", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--file-id", "-id"}, description = "File Id", required = false)
    public String fileId = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert", required = false)
    public int parallelism = 3;
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = true)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = true)
    public String sparkMemory = null;
    @Parameter(names = {"--dry-run", "-dr"}, description = "Dry Run Mode", required = false)
    public boolean dryRun = false;
    @Parameter(names = {"--skip-validation", "-sv"}, description = "Skip Validation", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--output-path", "-ot"}, description = "Output Path", required = false)
    public String outputPath = null;
    @Parameter(names = {"--print-output", "-pt"}, description = "Print Output", required = false)
    public boolean printOutput = true;
    @Parameter(names = {"--help", "-h"}, help = true)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java`
#### Snippet
```java
    @Parameter(names = {"--partition-path", "-pp"}, description = "Partition Path", required = false)
    public String partitionPath = null;
    @Parameter(names = {"--file-id", "-id"}, description = "File Id", required = false)
    public String fileId = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-sync-mode"}, description = "Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql.", required = false)
    public String hiveSyncMode = "hms";
    @Parameter(names = {"--hive-sync-ignore-exception"}, description = "Ignore hive sync exception.", required = false)
    public boolean hiveSyncIgnoreException = false;
    @Parameter(names = {"--hive-partition-value-extractor-class"}, description = "Class which implements PartitionValueExtractor to extract the partition values,"
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-partition-field"}, description = "Comma separated list of field in the hive table to use for determining hive partition columns.", required = false)
    public String hivePartitionsField = "";
    @Parameter(names = {"--hive-sync-use-jdbc"}, description = "Use JDBC when hive synchronization.", required = false)
    public boolean hiveUseJdbc = true;
    @Parameter(names = {"--hive-metastore-uris"}, description = "hive meta store uris to use.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-table-name"}, description = "Table to sync to.", required = false)
    public String hiveTableName = null;
    @Parameter(names = {"--hive-user-name", "-user"}, description = "hive user name to use.", required = false)
    public String hiveUserName = "hive";
    @Parameter(names = {"--hive-pass-word", "-pass"}, description = "hive password to use.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-database", "-db"}, description = "Database to sync to.", required = false)
    public String hiveDataBase = null;
    @Parameter(names = {"--hive-table-name"}, description = "Table to sync to.", required = false)
    public String hiveTableName = null;
    @Parameter(names = {"--hive-user-name", "-user"}, description = "hive user name to use.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    public boolean hiveSyncIgnoreException = false;
    @Parameter(names = {"--hive-partition-value-extractor-class"}, description = "Class which implements PartitionValueExtractor to extract the partition values,"
        + " default 'SlashEncodedDayPartitionValueExtractor'.", required = false)
    public String partitionValueExtractorClass = "org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor";
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-user-name", "-user"}, description = "hive user name to use.", required = false)
    public String hiveUserName = "hive";
    @Parameter(names = {"--hive-pass-word", "-pass"}, description = "hive password to use.", required = false)
    public String hivePassWord = "hive";
    @Parameter(names = {"--hive-jdbc-url", "-jdbc"}, description = "hive url to use.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--sync-hive-meta", "-sync"}, description = "Sync information to HMS.", required = false)
    public boolean syncToHive = false;
    @Parameter(names = {"--hive-database", "-db"}, description = "Database to sync to.", required = false)
    public String hiveDataBase = null;
    @Parameter(names = {"--hive-table-name"}, description = "Table to sync to.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert/upsert/delete", required = false)
    public int parallelism = 1500;
    @Parameter(names = {"--instant-time", "-it"}, description = "instant time for delete table partitions operation.", required = false)
    public String instantTime = null;
    @Parameter(names = {"--sync-hive-meta", "-sync"}, description = "Sync information to HMS.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-jdbc-url", "-jdbc"}, description = "hive url to use.", required = false)
    public String hiveURL = "jdbc:hive2://localhost:10000";
    @Parameter(names = {"--hive-partition-field"}, description = "Comma separated list of field in the hive table to use for determining hive partition columns.", required = false)
    public String hivePartitionsField = "";
    @Parameter(names = {"--hive-sync-use-jdbc"}, description = "Use JDBC when hive synchronization.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--partitions", "-p"}, description = "Comma separated list of partitions to delete.", required = true)
    public String partitions = null;
    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for hoodie insert/upsert/delete", required = false)
    public int parallelism = 1500;
    @Parameter(names = {"--instant-time", "-it"}, description = "instant time for delete table partitions operation.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
    public String sparkMemory = "1g";
    @Parameter(names = {"--props"}, description = "path to properties file on localfs or dfs, with configurations for "
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--instant-time", "-it"}, description = "instant time for delete table partitions operation.", required = false)
    public String instantTime = null;
    @Parameter(names = {"--sync-hive-meta", "-sync"}, description = "Sync information to HMS.", required = false)
    public boolean syncToHive = false;
    @Parameter(names = {"--hive-database", "-db"}, description = "Database to sync to.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
        + " default 'SlashEncodedDayPartitionValueExtractor'.", required = false)
    public String partitionValueExtractorClass = "org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor";
    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;
    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-metastore-uris"}, description = "hive meta store uris to use.", required = false)
    public String hiveHMSUris = null;
    @Parameter(names = {"--hive-sync-mode"}, description = "Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql.", required = false)
    public String hiveSyncMode = "hms";
    @Parameter(names = {"--hive-sync-ignore-exception"}, description = "Ignore hive sync exception.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-pass-word", "-pass"}, description = "hive password to use.", required = false)
    public String hivePassWord = "hive";
    @Parameter(names = {"--hive-jdbc-url", "-jdbc"}, description = "hive url to use.", required = false)
    public String hiveURL = "jdbc:hive2://localhost:10000";
    @Parameter(names = {"--hive-partition-field"}, description = "Comma separated list of field in the hive table to use for determining hive partition columns.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
    @Parameter(names = {"--hive-sync-use-jdbc"}, description = "Use JDBC when hive synchronization.", required = false)
    public boolean hiveUseJdbc = true;
    @Parameter(names = {"--hive-metastore-uris"}, description = "hive meta store uris to use.", required = false)
    public String hiveHMSUris = null;
    @Parameter(names = {"--hive-sync-mode"}, description = "Mode to choose for Hive ops. Valid values are hms, jdbc and hiveql.", required = false)
```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public int parallelism = 200;

    @Parameter(names = {"--ignore-failed", "-ig"}, description = "Ignore metadata validate failure and continue.", required = false)
    public boolean ignoreFailed = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean validateAllFileGroups = false;

    @Parameter(names = {"--validate-all-column-stats"}, description = "Validate column stats for all columns in the schema", required = false)
    public boolean validateAllColumnStats = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean validateLatestFileSlices = false;

    @Parameter(names = {"--validate-latest-base-files"}, description = "Validate latest base files for all partitions.", required = false)
    public boolean validateLatestBaseFiles = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java

    @Parameter(names = {"--assume-date-partitioning"}, description = "Should HoodieWriteClient assume the data is partitioned by dates, i.e three levels from base path."
        + "This is a stop-gap to support tables created by versions < 0.3.1. Will be removed eventually", required = false)
    public Boolean assumeDatePartitioning = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean skipDataFilesForCleaning = false;

    @Parameter(names = {"--validate-latest-file-slices"}, description = "Validate latest file slices for all partitions.", required = false)
    public boolean validateLatestFileSlices = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean validateLatestBaseFiles = false;

    @Parameter(names = {"--validate-all-file-groups"}, description = "Validate all file groups, and all file slices within file groups.", required = false)
    public boolean validateAllFileGroups = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean ignoreFailed = false;

    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
    public String sparkMaster = null;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean continuous = false;

    @Parameter(names = {"--skip-data-files-for-cleaning"}, description = "Skip to compare the data files which are under deletion by cleaner", required = false)
    public boolean skipDataFilesForCleaning = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public String sparkMaster = null;

    @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = false)
    public String sparkMemory = "1g";

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java

    @Parameter(names = {"--continuous"}, description = "Running MetadataTableValidator in continuous. "
        + "Can use --min-validate-interval-seconds to control validation frequency", required = false)
    public boolean continuous = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public Integer minValidateIntervalSeconds = 10 * 60;

    @Parameter(names = {"--parallelism", "-pl"}, description = "Parallelism for valuation", required = false)
    public int parallelism = 200;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    public boolean validateAllColumnStats = false;

    @Parameter(names = {"--validate-bloom-filters"}, description = "Validate bloom filters of base files", required = false)
    public boolean validateBloomFilters = false;

```

### DefaultAnnotationParam
Redundant default parameter value assignment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
    public String tableType;

    @Parameter(names = {"--base-file-format"}, description = "File format for the base files. PARQUET (or) HFILE", required = false)
    public String baseFileFormat = "PARQUET";

```

## RuleId[id=RedundantStreamOptionalCall]
### RedundantStreamOptionalCall
Redundant 'sequential()' call: the stream was created via 'stream()', so it's already sequential
in `hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java`
#### Snippet
```java
  public static <T, U> U reduce(Collection<T> c, U identity, BiFunction<U, T, U> reducer) {
    return c.stream()
        .sequential()
        .reduce(identity, reducer, (a, b) -> {
          throw new UnsupportedOperationException();
```

### RedundantStreamOptionalCall
Redundant 'sequential()' call: the stream was created via 'stream()', so it's already sequential
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    // there are multiple blocks. Compute min(block_mins) and max(block_maxs)
    return blockRanges.stream()
        .sequential()
        .reduce(this::combineRanges).get();
  }
```

### RedundantStreamOptionalCall
Redundant 'sequential()' call: the stream was created via 'stream()', so it's already sequential
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    // Collect stats from all individual Parquet blocks
    Map<String, List<HoodieColumnRangeMetadata<Comparable>>> columnToStatsListMap =
        (Map<String, List<HoodieColumnRangeMetadata<Comparable>>>) metadata.getBlocks().stream().sequential()
          .flatMap(blockMetaData ->
              blockMetaData.getColumns().stream()
```

### RedundantStreamOptionalCall
Redundant 'sequential()' call: the stream was created via 'stream()', so it's already sequential
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
  public Stream<R> valueStream() {
    final BufferedRandomAccessFile file = getRandomAccessFile();
    return valueMetadataMap.values().stream().sorted().sequential().map(valueMetaData -> (R) get(valueMetaData, file, isCompressionEnabled));
  }

```

### RedundantStreamOptionalCall
Redundant 'sequential()' call: the stream was created via 'stream()', so it's already sequential
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/PulsarSource.java`
#### Snippet
```java
    //       fetching all Pulsar client threads and interrupting them forcibly (to make them
    //       shutdown)
    collectActiveThreads().stream().sequential()
        .filter(t -> t.getName().startsWith("pulsar-client-io"))
        .forEach(Thread::interrupt);
```

## RuleId[id=MalformedFormatString]
### MalformedFormatString
Too many arguments for format string (found: 4, expected: 3)
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/FailOnFirstErrorWriteStatus.java`
#### Snippet
```java
  @Override
  public void markFailure(HoodieRecord record, Throwable t, Option<Map<String, String>> optionalRecordMetadata) {
    LOG.error(String.format("Error writing record %s with data %s and optionalRecordMetadata %s", record, record.getData(),
            optionalRecordMetadata.orElse(Collections.emptyMap()), t));
    throw new HoodieException("Error writing record " + record + ": " + t.getMessage());
```

## RuleId[id=RedundantMethodOverride]
### RedundantMethodOverride
Method `bulkInsertPrepped()` is identical to its super method
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java

  @Override
  public HoodieWriteMetadata<List<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context,
                                                                  String instantTime,
                                                                  List<HoodieRecord<T>> preppedRecords,
```

### RedundantMethodOverride
Method `getCommitActionType()` is identical to its super method
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java

  @Override
  protected String getCommitActionType() {
    return table.getMetaClient().getCommitActionType();
  }
```

### RedundantMethodOverride
Method `execute()` is identical to its super method
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteTableCommitActionExecutor.java`
#### Snippet
```java

  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
```

### RedundantMethodOverride
Method `scheduleRollback()` is identical to its super method
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java

  @Override
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
```

### RedundantMethodOverride
Method `getCommitActionType()` is identical to its super method
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java

  @Override
  protected String getCommitActionType() {
    return table.getMetaClient().getCommitActionType();
  }
```

### RedundantMethodOverride
Method `getProperty()` is identical to its super method
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/common/HoodieFlinkEngineContext.java`
#### Snippet
```java
    }

    public Option<String> getProperty(EngineProperty prop) {
      return Option.empty();
    }
```

### RedundantMethodOverride
Method `scheduleRollback()` is identical to its super method
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java

  @Override
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context,
                                                     String instantTime,
                                                     HoodieInstant instantToRollback, boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
```

### RedundantMethodOverride
Method `getCommitActionType()` is identical to its super method
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java`
#### Snippet
```java

  @Override
  protected String getCommitActionType() {
    return table.getMetaClient().getCommitActionType();
  }
```

### RedundantMethodOverride
Method `orderAndFilter()` is identical to its super method
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/UnBoundedCompactionStrategy.java`
#### Snippet
```java

  @Override
  public List<HoodieCompactionOperation> orderAndFilter(HoodieWriteConfig config,
      List<HoodieCompactionOperation> operations, List<HoodieCompactionPlan> pendingCompactionWorkloads) {
    return operations;
```

### RedundantMethodOverride
Method `getOutputDateFormat()` is identical to its super method
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/parser/HoodieDateTimeParser.java`
#### Snippet
```java

  @Override
  public String getOutputDateFormat() {
    return config.getString(TIMESTAMP_OUTPUT_DATE_FORMAT.key());
  }
```

### RedundantMethodOverride
Method `end()` is identical to its super method
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java`
#### Snippet
```java

  @Override
  protected void end() {}

  protected CopyOnWriteInsertHandler getInsertHandler() {
```

### RedundantMethodOverride
Method `start()` is identical to its super method
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java`
#### Snippet
```java

  @Override
  protected void start() {}

  @Override
```

### RedundantMethodOverride
Method `hashCode()` is identical to its super method
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    int result = partitionToWriteStats.hashCode();
    result = 31 * result + compacted.hashCode();
```

### RedundantMethodOverride
Method `isNestedType()` is identical to its super method
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/Type.java`
#### Snippet
```java
  abstract class PrimitiveType implements Type {
    @Override
    public boolean isNestedType() {
      return false;
    }
```

### RedundantMethodOverride
Method `withPositionChange()` is identical to its super method
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChange.java`
#### Snippet
```java

    @Override
    public boolean withPositionChange() {
      return false;
    }
```

### RedundantMethodOverride
Method `withPositionChange()` is identical to its super method
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java

    @Override
    public boolean withPositionChange() {
      return false;
    }
```

## RuleId[id=ThrowFromFinallyBlock]
### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java
            }
          } catch (IOException io) {
            throw new HoodieIOException("Error appending rollback block", io);
          }
        }
```

### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
      } catch (IOException e) {
        String errorMsg = "Failed to close file" + (needTempFile ? tmpPath : fullPath);
        throw new HoodieIOException(errorMsg, e);
      }

```

### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
        }
      } catch (IOException e) {
        throw new HoodieIOException("Failed to rename " + tmpPath + " to the target " + fullPath, e);
      }
    }
```

## RuleId[id=IntegerMultiplicationImplicitCastToLong]
### IntegerMultiplicationImplicitCastToLong
conf.getInteger(FlinkOptions.WRITE_LOG_BLOCK_SIZE) \* 1024 \* 1024: integer multiplication implicitly cast to long
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/FlinkWriteClients.java`
#### Snippet
```java
            .forTable(conf.getString(FlinkOptions.TABLE_NAME))
            .withStorageConfig(HoodieStorageConfig.newBuilder()
                .logFileDataBlockMaxSize(conf.getInteger(FlinkOptions.WRITE_LOG_BLOCK_SIZE) * 1024 * 1024)
                .logFileMaxSize(conf.getLong(FlinkOptions.WRITE_LOG_MAX_SIZE) * 1024 * 1024)
                .parquetBlockSize(conf.getInteger(FlinkOptions.WRITE_PARQUET_BLOCK_SIZE) * 1024 * 1024)
```

### IntegerMultiplicationImplicitCastToLong
timeoutSecs \* 1000: integer multiplication implicitly cast to long
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    long beginTime = System.currentTimeMillis();
    long currTime = beginTime;
    long timeoutMsecs = timeoutSecs * 1000;

    while ((currTime - beginTime) < timeoutMsecs) {
```

### IntegerMultiplicationImplicitCastToLong
sleepSecsAfterEachRun \* 1000: integer multiplication implicitly cast to long
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
        LOG.info("Got table not found exception. Retrying");
      } finally {
        Thread.sleep(sleepSecsAfterEachRun * 1000);
        currTime = System.currentTimeMillis();
      }
```

## RuleId[id=InfiniteLoopStatement]
### InfiniteLoopStatement
`while` statement cannot complete without throwing an exception
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
    protected Pair<CompletableFuture, ExecutorService> startService() {
      return Pair.of(CompletableFuture.supplyAsync(() -> {
        while (true) {
          try {
            long start = System.currentTimeMillis();
```

### InfiniteLoopStatement
`while` statement cannot complete without throwing an exception
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java`
#### Snippet
```java
    if (!useExternalTimelineServer && cfg.waitForManualQueries) {
      System.out.println("Timeline Server Host Address=" + hostAddr + ", port=" + timelineServer.getServerPort());
      while (true) {
        try {
          Thread.sleep(60000);
```

### InfiniteLoopStatement
`while` statement cannot complete without throwing an exception
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    protected Pair<CompletableFuture, ExecutorService> startService() {
      return Pair.of(CompletableFuture.supplyAsync(() -> {
        while (true) {
          try {
            long start = System.currentTimeMillis();
```

### InfiniteLoopStatement
`while` statement cannot complete without throwing an exception
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/kafka/KafkaConnectControlAgent.java`
#### Snippet
```java

    executorService.submit(() -> {
      while (true) {
        ConsumerRecords<String, byte[]> records;
        records = consumer.poll(Duration.ofMillis(KAFKA_POLL_TIMEOUT_MS));
```

### InfiniteLoopStatement
`while` statement cannot complete without throwing an exception
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/transaction/ConnectTransactionCoordinator.java`
#### Snippet
```java
  @Override
  public void run() {
    while (true) {
      try {
        CoordinatorEvent event = events.poll(COORDINATOR_EVENT_LOOP_TIMEOUT_MS, TimeUnit.MILLISECONDS);
```

## RuleId[id=ReturnFromFinallyBlock]
### ReturnFromFinallyBlock
'return' inside 'finally' block
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
        asyncMetadataTableValidateService.get().shutdown(true);
      }
      return result;
    }
  }
```

## RuleId[id=MismatchedCollectionQueryUpdate]
### MismatchedCollectionQueryUpdate
Contents of collection `metaClientMap` are updated, but never queried
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   */
  public static Map<Path, HoodieTableMetaClient> getTableMetaClientByPartitionPath(Configuration conf, Set<Path> partitions) {
    Map<Path, HoodieTableMetaClient> metaClientMap = new HashMap<>();
    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {
      try {
```

## RuleId[id=SlowAbstractSetRemoveAll]
### SlowAbstractSetRemoveAll
Call to 'set.removeAll(list)' may work slowly
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
      SchemaCompatibilityResult result = SchemaCompatibilityResult.compatible();
      final Set<String> symbols = new TreeSet<>(writer.getEnumSymbols());
      symbols.removeAll(reader.getEnumSymbols());
      if (!symbols.isEmpty()) {
        result = SchemaCompatibilityResult.incompatible(SchemaIncompatibilityType.MISSING_ENUM_SYMBOLS, reader,
```

### SlowAbstractSetRemoveAll
Call to 'set.removeAll(list)' may work slowly
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
      // Validate that no appended file has been deleted
      checkState(
          !appendedFileMap.keySet().removeAll(partitionToDeletedFiles.getOrDefault(partition, Collections.emptyList())),
          "Rollback file cannot both be appended and deleted");

```

## RuleId[id=RedundantStringFormatCall]
### RedundantStringFormatCall
Redundant call to `format()`
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaChangeApplier.java`
#### Snippet
```java
      }
    } else {
      throw new IllegalArgumentException(String.format("positionType should be specified"));
    }
    return SchemaChangeUtils.applyTableChanges2Schema(latestSchema, add);
```

## RuleId[id=FuseStreamOperations]
### FuseStreamOperations
Stream may be extended replacing 'toArray'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
        "Found pending restore in active timeline. Please complete the restore fully before proceeding. As of now, "
            + "table could be in an inconsistent state. Pending restores: " + Arrays.toString(inflightRestoreTimeline.getInstantsAsStream()
            .map(instant -> instant.getTimestamp()).collect(Collectors.toList()).toArray()));

    // if there are pending compactions, their instantTime must not be greater than that of this instant time
```

### FuseStreamOperations
Stream may be extended replacing 'toArray'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/InternalSchema.java`
#### Snippet
```java
        StringUtils.join(record.fields().stream()
            .map(f -> " " + f)
            .collect(Collectors.toList()).toArray(new String[0]), "\n"));
  }

```

### FuseStreamOperations
Stream may be extended replacing 'toArray'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/BootstrapColumnStichingRecordReader.java`
#### Snippet
```java
      values = new ArrayWritable(rightW.getValueClass(), new Writable[numColumns]);
    } else {
      String[] vals = IntStream.range(0, numColumns).mapToObj(idx -> "").collect(Collectors.toList())
          .toArray(new String[0]);
      values = new ArrayWritable(vals);
```

## RuleId[id=DuplicateThrows]
### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InMemoryFileSystem.java`
#### Snippet
```java

  @Override
  public FileStatus[] listStatus(Path inlinePath) throws FileNotFoundException, IOException {
    throw new UnsupportedOperationException("No support for listStatus");
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public FileStatus[] listStatus(Path f) throws FileNotFoundException, IOException {
    return new RetryHelper<FileStatus[], IOException>(maxRetryIntervalMs, maxRetryNumbers, initialRetryIntervalMs, retryExceptionsList).tryWith(() -> fileSystem.listStatus(f)).start();
  }
```

## RuleId[id=RedundantUnmodifiable]
### RedundantUnmodifiable
Redundant usage of the 'unmodifiableMap' wrapper
in `hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java`
#### Snippet
```java

  public static <K,V> Map<K, V> createImmutableMap(final K key, final V value) {
    return Collections.unmodifiableMap(Collections.singletonMap(key, value));
  }

```

## RuleId[id=NullArgumentToVariableArgMethod]
### NullArgumentToVariableArgMethod
Confusing argument `row.toArray(new Comparable[row.size()])`, unclear if a varargs or non-varargs call is desired
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java
        structType = structType.add(DataTypes.createStructField(headersNoSpaces.get(i), headerDataType, true));
      }
      List<Row> records = rows.stream().map(row -> RowFactory.create(row.toArray(new Comparable[row.size()])))
              .collect(Collectors.toList());
      Dataset<Row> dataset = this.sqlContext.createDataFrame(records, structType);
```

## RuleId[id=UnusedAssignment]
### UnusedAssignment
The value changed at `idx++` is never used
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java`
#### Snippet
```java
        row[idx++] = fs.getLogFiles().filter(lf -> lf.getBaseCommitTime().equals(fs.getBaseInstantTime()))
            .collect(Collectors.toList()).toString();
        row[idx++] = fs.getLogFiles().filter(lf -> !lf.getBaseCommitTime().equals(fs.getBaseInstantTime()))
            .collect(Collectors.toList()).toString();
      }
```

### UnusedAssignment
The value changed at `idx++` is never used
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java`
#### Snippet
```java
        row[idx++] = fs.getLogFiles().count();
        row[idx++] = fs.getLogFiles().mapToLong(HoodieLogFile::getFileSize).sum();
        row[idx++] = fs.getLogFiles().collect(Collectors.toList()).toString();
      }
      rows.add(row);
```

### UnusedAssignment
Variable `metadata` initializer `null` is redundant
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java`
#### Snippet
```java
            }

            GenericRecord metadata = null;
            switch (action) {
              case HoodieTimeline.CLEAN_ACTION:
```

### UnusedAssignment
Variable `numCopied` initializer `0` is redundant
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java`
#### Snippet
```java
    final Set<String> actionSet = new HashSet<String>(Arrays.asList(filter.split(",")));
    int numExports = limit == -1 ? Integer.MAX_VALUE : limit;
    int numCopied = 0;

    if (!new File(localFolder).isDirectory()) {
```

### UnusedAssignment
Variable `data` initializer `null` is redundant
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java`
#### Snippet
```java
      String localPath = localFolder + Path.SEPARATOR + instant.getFileName();

      byte[] data = null;
      switch (instant.getAction()) {
        case HoodieTimeline.CLEAN_ACTION: {
```

### UnusedAssignment
Variable `result` initializer `null` is redundant
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteHelper.java`
#### Snippet
```java
                                                        BaseCommitActionExecutor<EmptyHoodieRecordPayload, List<HoodieRecord<EmptyHoodieRecordPayload>>, List<HoodieKey>, List<WriteStatus>, R> deleteExecutor) {
    try {
      HoodieWriteMetadata<List<WriteStatus>> result = null;
      List<HoodieKey> dedupedKeys = keys;
      final int parallelism = config.getDeleteShuffleParallelism();
```

### UnusedAssignment
Variable `result` initializer `null` is redundant
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java`
#### Snippet
```java
                                                        BaseCommitActionExecutor<EmptyHoodieRecordPayload, List<HoodieRecord<EmptyHoodieRecordPayload>>, List<HoodieKey>, List<WriteStatus>, R> deleteExecutor) {
    try {
      HoodieWriteMetadata<List<WriteStatus>> result = null;
      List<HoodieKey> dedupedKeys = keys;
      final int parallelism = config.getDeleteShuffleParallelism();
```

### UnusedAssignment
Variable `orcReader` initializer `null` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/bootstrap/HoodieSparkBootstrapSchemaProvider.java`
#### Snippet
```java

  private static Schema getBootstrapSourceSchemaOrc(HoodieWriteConfig writeConfig, HoodieEngineContext context, Path filePath) {
    Reader orcReader = null;
    try {
      orcReader = OrcFile.createReader(filePath, OrcFile.readerOptions(context.getHadoopConf().get()));
```

### UnusedAssignment
Variable `preserveMetadata` initializer `false` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/CreateHandleFactory.java`
#### Snippet
```java
public class CreateHandleFactory<T, I, K, O> extends WriteHandleFactory<T, I, K, O> implements Serializable {

  private boolean preserveMetadata = false;

  public CreateHandleFactory() {
```

### UnusedAssignment
Variable `hoodieKeyList` initializer `new ArrayList<>()` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieKeyLocationFetchHandle.java`
#### Snippet
```java
    HoodieBaseFile baseFile = partitionPathBaseFilePair.getRight();
    BaseFileUtils baseFileUtils = BaseFileUtils.getInstance(baseFile.getPath());
    List<HoodieKey> hoodieKeyList = new ArrayList<>();
    if (keyGeneratorOpt.isPresent()) {
      hoodieKeyList = baseFileUtils.fetchHoodieKeys(hoodieTable.getHadoopConf(), new Path(baseFile.getPath()), keyGeneratorOpt);
```

### UnusedAssignment
Variable `bloomFilter` initializer `null` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieKeyLookupHandle.java`
#### Snippet
```java

  private BloomFilter getBloomFilter() {
    BloomFilter bloomFilter = null;
    HoodieTimer timer = HoodieTimer.start();
    try {
```

### UnusedAssignment
Variable `oldNumWrites` initializer `0` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
    }

    long oldNumWrites = 0;
    try (HoodieFileReader reader = HoodieFileReaderFactory.getReaderFactory(this.config.getRecordMerger().getRecordType()).getFileReader(hoodieTable.getHadoopConf(), oldFilePath)) {
      oldNumWrites = reader.getTotalRecords();
```

### UnusedAssignment
Variable `finalIndexPartitionInfos` initializer `null` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java`
#### Snippet
```java
      // transition requested indexInstant to inflight
      table.getActiveTimeline().transitionIndexRequestedToInflight(indexInstant, Option.empty());
      List<HoodieIndexPartitionInfo> finalIndexPartitionInfos = null;
      if (!firstTimeInitializingMetadataTable) {
        // start indexing for each partition
```

### UnusedAssignment
Variable `bootstrapFileReader` initializer `null` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieMergeHelper.java`
#### Snippet
```java
        .getReaderFactory(recordType)
        .getFileReader(hadoopConf, mergeHandle.getOldFilePath());
    HoodieFileReader bootstrapFileReader = null;

    Schema writerSchema = mergeHandle.getWriterSchemaWithMetaFields();
```

### UnusedAssignment
Variable `totalIO` initializer `0` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/FileSliceMetricUtils.java`
#### Snippet
```java
    long totalIORead = 0;
    long totalIOWrite = 0;
    long totalIO = 0;

    for (FileSlice slice : fileSlices) {
```

### UnusedAssignment
Variable `plan` initializer `null` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java`
#### Snippet
```java
      // we need to revert or complete last action.
      if (fs.exists(planPath)) {
        HoodieMergeArchiveFilePlan plan = null;
        try {
          plan = TimelineMetadataUtils.deserializeAvroMetadata(FileIOUtils.readDataFromPath(fs, planPath).get(), HoodieMergeArchiveFilePlan.class);
```

### UnusedAssignment
Variable `keyGenerator` initializer `null` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/factory/HoodieAvroKeyGeneratorFactory.java`
#### Snippet
```java
    }

    BaseKeyGenerator keyGenerator = null;

    switch (keyGeneratorTypeEnum) {
```

### UnusedAssignment
Variable `initialized` initializer `false` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/Metrics.java`
#### Snippet
```java
  private final List<MetricsReporter> reporters;
  private final String commonMetricPrefix;
  private boolean initialized = false;

  public Metrics(HoodieWriteConfig metricConfig) {
```

### UnusedAssignment
Variable `code` initializer `-1` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java
      char c = path.charAt(i);
      if (c == '%' && i + 2 < path.length()) {
        int code = -1;
        try {
          code = Integer.parseInt(path.substring(i + 1, i + 3), 16);
```

### UnusedAssignment
Variable `lines` initializer `new ArrayList<>()` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java`
#### Snippet
```java
   */
  public static List<String> readAsUTFStringLines(InputStream input) {
    List<String> lines = new ArrayList<>();
    BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(input, StandardCharsets.UTF_8));
    lines = bufferedReader.lines().collect(Collectors.toList());
```

### UnusedAssignment
Variable `result` initializer `new HashMap<>()` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/fs/FSUtils.java`
#### Snippet
```java
      HoodieEngineContext hoodieEngineContext, FileSystem fs, Path dirPath, int parallelism,
      Predicate<FileStatus> subPathPredicate, SerializableFunction<Pair<String, SerializableConfiguration>, T> pairFunction) {
    Map<String, T> result = new HashMap<>();
    try {
      FileStatus[] fileStatuses = fs.listStatus(dirPath);
```

### UnusedAssignment
Variable `functionResult` initializer `null` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/RetryHelper.java`
#### Snippet
```java
  public T start(CheckedFunction<T, R> func) throws R {
    int retries = 0;
    T functionResult = null;

    while (true) {
```

### UnusedAssignment
Variable `bytes` initializer `null` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case STRING:
        BytesColumnVector bytesColVec = (BytesColumnVector) colVector;
        byte[] bytes = null;

        if (value instanceof String) {
```

### UnusedAssignment
Variable `content` initializer `Option.empty()` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
    Path markerTypeFilePath = new Path(markerDir, MARKER_TYPE_FILENAME);
    FSDataInputStream fsDataInputStream = null;
    Option<MarkerType> content = Option.empty();
    try {
      if (!doesMarkerTypeFileExist(fileSystem, markerDir)) {
```

### UnusedAssignment
Variable `internalSchema` initializer `InternalSchema.getEmptyInternalSchema()` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java`
#### Snippet
```java
  private final FileSystem fs;
  private final Schema readerSchema;
  private InternalSchema internalSchema = InternalSchema.getEmptyInternalSchema();
  private final boolean readBlocksLazily;
  private final boolean reverseLogReader;
```

### UnusedAssignment
Variable `useColumnTypeFromFileSchema` initializer `true` is redundant
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaMerger.java`
#### Snippet
```java
  // eg: current column type is StringType, now we changed it to decimalType,
  // we can pass decimalType to reWriteRecordWithNewSchema directly, everything is ok.
  private boolean useColumnTypeFromFileSchema = true;

  // deal with rename
```

### UnusedAssignment
Variable `useColNameFromFileSchema` initializer `true` is redundant
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaMerger.java`
#### Snippet
```java
  // eg: current column name is colOldName, now we rename it to colNewName,
  // we can pass colNewName to reWriteRecordWithNewSchema directly, everything is ok.
  private boolean useColNameFromFileSchema = true;

  public InternalSchemaMerger(InternalSchema fileSchema, InternalSchema querySchema, boolean ignoreRequiredAttribute, boolean useColumnTypeFromFileSchema, boolean useColNameFromFileSchema) {
```

### UnusedAssignment
Variable `commitsToRollback` initializer `null` is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java`
#### Snippet
```java
  private List<String> getRollbackedCommits(HoodieInstant instant, HoodieActiveTimeline timeline) {
    try {
      List<String> commitsToRollback = null;
      if (instant.getAction().equals(HoodieTimeline.ROLLBACK_ACTION)) {
        try {
```

### UnusedAssignment
Variable `supportPayload` initializer `true` is redundant
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java`
#### Snippet
```java
  protected SchemaEvolutionContext schemaEvolutionContext;
  // support merge operation
  protected boolean supportPayload = true;
  // handle hive type to avro record
  protected HiveAvroSerializer serializer;
```

### UnusedAssignment
Variable `source` initializer `null` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java
    }

    Dataset<Row> source = null;
    // Do Incr pull. Set end instant if available
    if (queryTypeAndInstantEndpts.getKey().equals(DataSourceReadOptions.QUERY_TYPE_INCREMENTAL_OPT_VAL())) {
```

### UnusedAssignment
Variable `source` initializer `null` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java`
#### Snippet
```java
    }

    Dataset<Row> source = null;
    // Do incremental pull. Set end instant if available.
    if (queryTypeAndInstantEndpts.getKey().equals(DataSourceReadOptions.QUERY_TYPE_INCREMENTAL_OPT_VAL())) {
```

### UnusedAssignment
Variable `sparkOptionsMap` initializer `null` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceCloudStorageHelper.java`
#### Snippet
```java

    final ObjectMapper mapper = new ObjectMapper();
    Map<String, String> sparkOptionsMap = null;

    try {
```

### UnusedAssignment
Variable `sparkOptionsMap` initializer `null` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelectorCommon.java`
#### Snippet
```java
    if (StringUtils.nonEmpty(datasourceOpts)) {
      final ObjectMapper mapper = new ObjectMapper();
      Map<String, String> sparkOptionsMap = null;
      try {
        sparkOptionsMap = mapper.readValue(datasourceOpts, Map.class);
```

### UnusedAssignment
Variable `errorTableWriter` initializer `Option.empty()` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
  private String invalidCharMask = SCHEMA_FIELD_NAME_INVALID_CHAR_MASK.defaultValue();

  private Option<BaseErrorTableWriter> errorTableWriter = Option.empty();

  public SourceFormatAdapter(Source source) {
```

### UnusedAssignment
Variable `jssc` initializer `null` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
    final Config cfg = getConfig(args);
    Map<String, String> additionalSparkConfigs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg);
    JavaSparkContext jssc = null;
    if (StringUtils.isNullOrEmpty(cfg.sparkMaster)) {
      jssc = UtilHelpers.buildSparkContext("delta-streamer-" + cfg.targetTableName, additionalSparkConfigs);
```

### UnusedAssignment
Variable `catalogTable` initializer `null` is redundant
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/HoodiePipeline.java`
#### Snippet
```java
      tableEnv.executeSql(sql);
      String currentCatalog = tableEnv.getCurrentCatalog();
      ResolvedCatalogTable catalogTable = null;
      String defaultDatabase = null;
      try {
```

### UnusedAssignment
Variable `defaultDatabase` initializer `null` is redundant
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/HoodiePipeline.java`
#### Snippet
```java
      String currentCatalog = tableEnv.getCurrentCatalog();
      ResolvedCatalogTable catalogTable = null;
      String defaultDatabase = null;
      try {
        Catalog catalog = tableEnv.getCatalog(currentCatalog).get();
```

### UnusedAssignment
Variable `curAvroRecord` initializer `null` is redundant
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
        while (logRecordsKeyIterator.hasNext()) {
          String curAvroKey = logRecordsKeyIterator.next();
          Option<IndexedRecord> curAvroRecord = null;
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
```

### UnusedAssignment
Variable `curAvroRecord` initializer `null` is redundant
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
      public boolean hasNext() {
        while (recordsIterator.hasNext()) {
          Option<IndexedRecord> curAvroRecord = null;
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) recordsIterator.next();
          try {
```

### UnusedAssignment
Variable `curAvroRecord` initializer `null` is redundant
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
        while (logRecordsKeyIterator.hasNext()) {
          String curAvroKey = logRecordsKeyIterator.next();
          Option<IndexedRecord> curAvroRecord = null;
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
```

### UnusedAssignment
Variable `dataGen` initializer `null` is redundant
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

    // Generator of some records to be loaded in.
    HoodieTestDataGenerator dataGen = null;
    if (nonPartitionedTable) {
      // All data goes to base-path
```

### UnusedAssignment
Variable `numInitialCommits` initializer `0` is redundant
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    // start streaming and showing
    ExecutorService executor = Executors.newFixedThreadPool(2);
    int numInitialCommits = 0;

    // thread for spark structured streaming
```

## RuleId[id=ConstantValue]
### ConstantValue
Value `instantTime` is always 'null'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java`
#### Snippet
```java
            instantTime = n.getLogBlockHeader().get(HeaderMetadataType.INSTANT_TIME);
            if (instantTime == null) {
              throw new Exception("Invalid instant time " + instantTime);
            }
          } catch (Exception e) {
```

### ConstantValue
Condition `propertiesChanged` is always `false`
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java`
#### Snippet
```java
    Map<String, String> serdeProperties = getSerdeProperties(readAsOptimized);
    boolean serdePropertiesUpdated = syncClient.updateSerdeProperties(tableName, serdeProperties, useRealTimeInputFormat);
    propertiesChanged = propertiesChanged || serdePropertiesUpdated;

    Map<String, String> tableProperties = getTableProperties(schema);
```

### ConstantValue
Condition `rolloverPaths == null` is always `true`
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandle.java`
#### Snippet
```java
                          TaskContextSupplier taskContextSupplier) {
    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, taskContextSupplier, Option.empty());
    if (rolloverPaths == null) {
      // #makeOldAndNewFilePaths may already initialize it already
      rolloverPaths = new ArrayList<>();
```

### ConstantValue
Value `legacyMode` is always 'true'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                        basicArrayType.getComponentInfo(),
                        Type.Repetition.REQUIRED,
                        legacyMode))
                .named(LIST_GROUP_NAME);

```

### ConstantValue
Value `legacyMode` is always 'false'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                basicArrayType.getComponentInfo(),
                Type.Repetition.REQUIRED,
                legacyMode)
                .asPrimitiveType();
        fieldType =
```

### ConstantValue
Condition `enableBloomFilter` is always `false`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileWriterFactory.java`
#### Snippet
```java
      FSDataOutputStream outputStream, Configuration conf, HoodieConfig config, Schema schema) throws IOException {
    boolean enableBloomFilter = false;
    Option<BloomFilter> filter = enableBloomFilter ? Option.of(createBloomFilter(config)) : Option.empty();
    HoodieRowParquetWriteSupport writeSupport = new HoodieRowParquetWriteSupport(conf,
        HoodieInternalRowUtils.getCachedSchema(schema), filter,
```

### ConstantValue
Result of `keys.size()` is always '0'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
      return hTable.get(keys);
    }
    return new Result[keys.size()];
  }

```

### ConstantValue
Condition `newType instanceof StringType` is always `true`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
          oldDecimal.changePrecision(((DecimalType) newType).precision(), ((DecimalType) newType).scale());
          newV.putDecimal(i, oldDecimal, ((DecimalType) newType).precision());
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, (oldV.getDouble(i) + "").getBytes(StandardCharsets.UTF_8));
        }
```

### ConstantValue
Condition `newType instanceof DecimalType` is always `true`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, ((isInt ? oldV.getInt(i) : oldV.getLong(i)) + "").getBytes(StandardCharsets.UTF_8));
        } else if (newType instanceof DecimalType) {
          Decimal oldDecimal = Decimal.apply(isInt ? oldV.getInt(i) : oldV.getLong(i));
          oldDecimal.changePrecision(((DecimalType) newType).precision(), ((DecimalType) newType).scale());
```

### ConstantValue
Condition `newType instanceof DecimalType` is always `true`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
          int days = org.apache.spark.sql.catalyst.util.DateTimeUtils.fromJavaDate(Date.valueOf(oldV.getUTF8String(i).toString()));
          newV.putInt(i, days);
        } else if (newType instanceof DecimalType) {
          DecimalType decimalType = (DecimalType) newType;
          java.math.BigDecimal bigDecimal = new java.math.BigDecimal(oldV.getUTF8String(i).toString().trim());
```

### ConstantValue
Condition `newType instanceof DecimalType` is always `true`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, (oldV.getFloat(i) + "").getBytes(StandardCharsets.UTF_8));
        } else if (newType instanceof DecimalType) {
          Decimal oldDecimal = Decimal.apply(oldV.getFloat(i));
          oldDecimal.changePrecision(((DecimalType) newType).precision(), ((DecimalType) newType).scale());
```

### ConstantValue
Condition `oldV != null` is always `true`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
    DataType oldType = oldV.dataType();  // old colType eg: floatType
    DataType newType = newV.dataType();  // new colType eg: doubleType
    if (oldV != null && newType != null) {
      if (oldType instanceof BooleanType) {
        return false;
```

### ConstantValue
Condition `newType instanceof StringType` is always `true`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkInternalSchemaConverter.java`
#### Snippet
```java
          oldDecimal.changePrecision(((DecimalType) newType).precision(), ((DecimalType) newType).scale());
          newV.putDecimal(i, oldDecimal, ((DecimalType) newType).precision());
        } else if (newType instanceof StringType) {
          newV.putByteArray(i, oldDecimal.toString().getBytes(StandardCharsets.UTF_8));
        }
```

### ConstantValue
Value `isDeleted` is always 'true'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java
        return HoodieRollbackStat.newBuilder()
            .withPartitionPath(partitionPath)
            .withDeletedFileResult(fullDeletePath.toString(), isDeleted)
            .build();
      } catch (IOException e) {
```

### ConstantValue
Condition `enableBloomFilter` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroFileWriterFactory.java`
#### Snippet
```java
      FSDataOutputStream outputStream, Configuration conf, HoodieConfig config, Schema schema) throws IOException {
    boolean enableBloomFilter = false;
    Option<BloomFilter> filter = enableBloomFilter ? Option.of(createBloomFilter(config)) : Option.empty();
    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(new AvroSchemaConverter(conf).convert(schema), schema, filter);
    HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig = new HoodieParquetConfig<>(writeSupport,
```

### ConstantValue
Condition `actionMetadata.get() instanceof HoodieRestoreMetadata` is always `false`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
    final String INSTANT_ACTION = (actionMetadata.get() instanceof HoodieRollbackMetadata
        ? HoodieTimeline.ROLLBACK_ACTION
        : (actionMetadata.get() instanceof HoodieRestoreMetadata ? HoodieTimeline.RESTORE_ACTION : EMPTY_STRING));

    List<String> affectedInstantTimestamps;
```

### ConstantValue
Result of `actionMetadata.get()` is always 'null'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
    final String INSTANT_ACTION = (actionMetadata.get() instanceof HoodieRollbackMetadata
        ? HoodieTimeline.ROLLBACK_ACTION
        : (actionMetadata.get() instanceof HoodieRestoreMetadata ? HoodieTimeline.RESTORE_ACTION : EMPTY_STRING));

    List<String> affectedInstantTimestamps;
```

### ConstantValue
Condition `c >= 0` is always `true`
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java

  static boolean needsEscaping(char c) {
    return c >= 0 && c < charToEscape.size() && charToEscape.get(c);
  }

```

### ConstantValue
Condition `value == null` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
          throw new IllegalStateException(String.format(
              "Failed to add value %s to union with type %s",
              value == null ? "null" : value.toString(),
              type.toString()
          ));
```

### ConstantValue
Condition `value == null && matchValue != null` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
    }

    if (value == null && matchValue != null) {
      value = matchValue;
    }
```

### ConstantValue
Condition `matchValue != null` is always `false` when reached
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
    }

    if (value == null && matchValue != null) {
      value = matchValue;
    }
```

### ConstantValue
Condition `stringType == null || !stringType.equals(StringType.String)` is always `true`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case STRING:
        String stringType = avroSchema.getProp(GenericData.STRING_PROP);
        if (stringType == null || !stringType.equals(StringType.String)) {
          int stringLength = ((BytesColumnVector) colVector).length[vectorPos];
          int stringOffset = ((BytesColumnVector) colVector).start[vectorPos];
```

### ConstantValue
Condition `!stringType.equals(StringType.String)` is always `true` when reached
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case STRING:
        String stringType = avroSchema.getProp(GenericData.STRING_PROP);
        if (stringType == null || !stringType.equals(StringType.String)) {
          int stringLength = ((BytesColumnVector) colVector).length[vectorPos];
          int stringOffset = ((BytesColumnVector) colVector).start[vectorPos];
```

### ConstantValue
Condition `statValueWrapper instanceof DecimalWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof DateWrapper) {
      return LocalDate.ofEpochDay(((DateWrapper) statValueWrapper).getValue());
    } else if (statValueWrapper instanceof DecimalWrapper) {
      Schema valueSchema = DecimalWrapper.SCHEMA$.getField("value").schema();
      return AVRO_DECIMAL_CONVERSION.fromBytes(((DecimalWrapper) statValueWrapper).getValue(), valueSchema, valueSchema.getLogicalType());
```

### ConstantValue
Condition `statValueWrapper instanceof TimestampMicrosWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
      Schema valueSchema = DecimalWrapper.SCHEMA$.getField("value").schema();
      return AVRO_DECIMAL_CONVERSION.fromBytes(((DecimalWrapper) statValueWrapper).getValue(), valueSchema, valueSchema.getLogicalType());
    } else if (statValueWrapper instanceof TimestampMicrosWrapper) {
      return microsToInstant(((TimestampMicrosWrapper) statValueWrapper).getValue());
    } else if (statValueWrapper instanceof BooleanWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof BooleanWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof TimestampMicrosWrapper) {
      return microsToInstant(((TimestampMicrosWrapper) statValueWrapper).getValue());
    } else if (statValueWrapper instanceof BooleanWrapper) {
      return ((BooleanWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof IntWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof IntWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof BooleanWrapper) {
      return ((BooleanWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof IntWrapper) {
      return ((IntWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof LongWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof LongWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof IntWrapper) {
      return ((IntWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof LongWrapper) {
      return ((LongWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof FloatWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof FloatWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof LongWrapper) {
      return ((LongWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof FloatWrapper) {
      return ((FloatWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof DoubleWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof DoubleWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof FloatWrapper) {
      return ((FloatWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof DoubleWrapper) {
      return ((DoubleWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof BytesWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof BytesWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof DoubleWrapper) {
      return ((DoubleWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof BytesWrapper) {
      return ((BytesWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof StringWrapper) {
```

### ConstantValue
Condition `statValueWrapper instanceof StringWrapper` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof BytesWrapper) {
      return ((BytesWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof StringWrapper) {
      return ((StringWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof GenericRecord) {
```

### ConstantValue
Condition `statValueWrapper instanceof GenericRecord` is always `false`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    } else if (statValueWrapper instanceof StringWrapper) {
      return ((StringWrapper) statValueWrapper).getValue();
    } else if (statValueWrapper instanceof GenericRecord) {
      // NOTE: This branch could be hit b/c Avro records could be reconstructed
      //       as {@code GenericRecord)
```

### ConstantValue
Condition `result == null` is always `false`
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
      return null;
    }).filter(Objects::nonNull).collect(Collectors.toList());
    if (result == null) {
      // Returns an empty list instead of null.
      result = new ArrayList<>();
```

### ConstantValue
Value `realtime` is always 'true'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
    // now we support read log file, try to find log file
    if (FSUtils.isLogFile(new Path(path)) && realtime) {
      return getInputFormat(HoodieFileFormat.PARQUET, realtime, conf);
    }
    throw new HoodieIOException("Hoodie InputFormat not implemented for base file of type " + extension);
```

### ConstantValue
Condition `danglingFilePaths.size() > 0` is always `true` when reached
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
        }).collect(Collectors.toList());

        if (!danglingFilePaths.isEmpty() && danglingFilePaths.size() > 0) {
          LOG.error("Data table validation failed due to dangling files count " + danglingFilePaths.size() + ", found before active timeline");
          danglingFilePaths.forEach(entry -> LOG.error("Dangling file: " + entry.toString()));
```

### ConstantValue
Condition `parts != null` is always `true`
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
      // Assume partition date format of "<column>=<date>" and try parsing out date.
      String[] parts = partition.split("=");
      if (parts != null && parts.length == 2) {
        dateString = parts[1].trim();
      }
```

### ConstantValue
Value `partitionDate` is always 'null'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
      LOG.error("Partition name {} must conform to date format if --start-date, --end-date, or --num-days are specified. ", partition, dtpe);
    }
    return partitionDate;
  }

```

### ConstantValue
Value `endDate` is always 'null'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
      }
    } else {
      LOG.info("End date is not specified: {}.", endDate);
    }

```

### ConstantValue
Value `startDate` is always 'null'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
    } else {
      if (cfg.numDays == 0) {
        LOG.info("Start date not specified: {}.", startDate);
      } else if (cfg.numDays > 0) {
        endDate = LocalDate.now();
```

### ConstantValue
Condition `hasErrors` is always `true`
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/transaction/ConnectTransactionCoordinator.java`
#### Snippet
```java
            LOG.error("Commit " + currentCommitTime + " failed!");
          }
        } else if (hasErrors) {
          LOG.error("Coordinator found errors when writing. Errors/Total=" + totalErrorRecords + "/" + totalRecords);
          LOG.error("Printing out the top 100 errors");
```

## RuleId[id=OptionalGetWithoutIsPresent]
### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/common/HoodieJavaEngineContext.java`
#### Snippet
```java
    return data.stream().parallel().map(throwingMapToPairWrapper(mapToPairFunc))
        .collect(Collectors.groupingBy(p -> p.getKey())).values().stream()
        .map(list -> list.stream().map(e -> e.getValue()).reduce(throwingReduceWrapper(reduceFunc)).get())
        .collect(Collectors.toList());
  }
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
        .collect(Collectors.groupingBy(writeStat -> writeStat.getPartitionPath() + writeStat.getPath()))
        .values().stream()
        .map(duplicates -> duplicates.stream().reduce(WriteStatMerger::merge).get())
        .collect(Collectors.toList());
    return commitStats(instantTime, merged, extraMetadata, commitActionType, partitionToReplacedFileIds, extraPreCommitFunc);
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitPartitioner.java`
#### Snippet
```java
        smallFileLocations.add(sf);
      } else {
        HoodieLogFile logFile = smallFileSlice.getLogFiles().findFirst().get();
        sf.location = new HoodieRecordLocation(FSUtils.getBaseCommitTimeFromLogPath(logFile.getPath()),
            FSUtils.getFileIdFromLogPath(logFile.getPath()));
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java`
#### Snippet
```java
    FileSlice merged =
        fileSystemView.getLatestMergedFileSlicesBeforeOrOn(op.getPartitionPath(), lastInstant.getTimestamp())
            .filter(fs -> fs.getFileId().equals(op.getFileId())).findFirst().get();
    final int maxVersion = op.getDeltaFileNames().stream().map(lf -> FSUtils.getFileVersionFromLog(new Path(lf)))
        .reduce((x, y) -> x > y ? x : y).orElse(0);
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java`
#### Snippet
```java
    FileSlice merged =
        fileSystemView.getLatestMergedFileSlicesBeforeOrOn(operation.getPartitionPath(), lastInstant.getTimestamp())
            .filter(fs -> fs.getFileId().equals(operation.getFileId())).findFirst().get();
    List<HoodieLogFile> logFilesToRepair =
        merged.getLogFiles().filter(lf -> lf.getBaseCommitTime().equals(compactionInstant))
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java`
#### Snippet
```java
    FileSlice fileSliceForCompaction =
        fileSystemView.getLatestFileSlicesBeforeOrOn(operation.getPartitionPath(), operation.getBaseInstantTime(), true)
            .filter(fs -> fs.getFileId().equals(operation.getFileId())).findFirst().get();
    int maxUsedVersion = fileSliceForCompaction.getLogFiles().findFirst().map(HoodieLogFile::getLogVersion)
        .orElse(HoodieLogFile.LOGFILE_BASE_VERSION - 1);
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/CompactionAdminClient.java`
#### Snippet
```java
        .orElse(HoodieLogFile.DELTA_EXTENSION);
    String parentPath = fileSliceForCompaction.getBaseFile().map(df -> new Path(df.getPath()).getParent().toString())
        .orElse(fileSliceForCompaction.getLogFiles().findFirst().map(lf -> lf.getPath().getParent().toString()).get());
    for (HoodieLogFile toRepair : logFilesToRepair) {
      int version = maxUsedVersion + 1;
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java`
#### Snippet
```java
        HISTORICAL_SCHEMA_CACHE.put(tablePath, historicalSchemas);
      } else {
        long maxVersionId = historicalSchemas.keySet().stream().max(Long::compareTo).get();
        if (versionID > maxVersionId) {
          historicalSchemas = getHistoricalSchemas(metaClient);
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    return blockRanges.stream()
        .sequential()
        .reduce(this::combineRanges).get();
  }

```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
    }

    HoodieTimeline timeline = deltaFileGroups.stream().map(df -> df.getTimeline()).findAny().get();
    List<HoodieFileGroup> fgs =
        buildFileGroups(viewDataFiles.values().stream(), viewLogFiles.values().stream(), timeline, true);
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/MetadataMigrator.java`
#### Snippet
```java
    migrators = migratorList.stream().map(m -> Pair.of(m.getManagedVersion(), m))
        .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
    latestVersion = migrators.keySet().stream().reduce((x, y) -> x > y ? x : y).get();
    oldestVersion = migrators.keySet().stream().reduce((x, y) -> x < y ? x : y).get();
  }
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java`
#### Snippet
```java

  private boolean isFileGroupReplacedBeforeAny(HoodieFileGroupId fileGroupId, List<String> instants) {
    return isFileGroupReplacedBeforeOrOn(fileGroupId, instants.stream().max(Comparator.naturalOrder()).get());
  }

```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieLocalEngineContext.java`
#### Snippet
```java
    return data.stream().parallel().map(throwingMapToPairWrapper(mapToPairFunc))
        .collect(Collectors.groupingBy(p -> p.getKey())).values().stream()
        .map(list -> list.stream().map(e -> e.getValue()).reduce(throwingReduceWrapper(reduceFunc)).get())
        .collect(Collectors.toList());
  }
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChangesHelper.java`
#### Snippet
```java
    if (pchanges != null && !pchanges.isEmpty()) {
      for (TableChange.ColumnPositionChange pchange : pchanges) {
        Types.Field srcField = result.stream().filter(f -> f.fieldId() == pchange.getSrcId()).findFirst().get();
        Types.Field dsrField = result.stream().filter(f -> f.fieldId() == pchange.getDsrId()).findFirst().orElse(null);
        // we remove srcField first
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/HoodieFlinkQuickstart.java`
#### Snippet
```java
      ObjectPath objectPath = new ObjectPath(tEnv.getCurrentDatabase(), sourceTable);
      String currentCatalog = tEnv.getCurrentCatalog();
      Catalog catalog = tEnv.getCatalog(currentCatalog).get();
      ResolvedCatalogTable table = (ResolvedCatalogTable) catalog.getTable(objectPath);
      ResolvedSchema schema = table.getResolvedSchema();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/HoodieFlinkQuickstart.java`
#### Snippet
```java
    // wait to finish
    try {
      tableResult.getJobClient().get().getJobExecutionResult().get();
    } catch (InterruptedException | ExecutionException ex) {
      // ignored
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
      Map<TopicPartition, List<OffsetRange>> tpOffsets = Arrays.stream(oldRanges).collect(Collectors.groupingBy(OffsetRange::topicPartition));
      for (Map.Entry<TopicPartition, List<OffsetRange>> entry : tpOffsets.entrySet()) {
        long from = entry.getValue().stream().map(OffsetRange::fromOffset).min(Long::compare).get();
        long until = entry.getValue().stream().map(OffsetRange::untilOffset).max(Long::compare).get();
        newRanges.add(OffsetRange.create(entry.getKey(), from, until));
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/InitialCheckpointFromAnotherHoodieTimelineProvider.java`
#### Snippet
```java
            return null;
          }
        }).filter(Objects::nonNull).findFirst().get();
  }
}
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/meta/CkpMetadata.java`
#### Snippet
```java
          }
          return y;
        }).get())
        .sorted().collect(Collectors.toList());
  }
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/HoodiePipeline.java`
#### Snippet
```java
      String defaultDatabase = null;
      try {
        Catalog catalog = tableEnv.getCatalog(currentCatalog).get();
        defaultDatabase = catalog.getDefaultDatabase();
        catalogTable = (ResolvedCatalogTable) catalog.getTable(new ObjectPath(defaultDatabase, this.tableName));
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java`
#### Snippet
```java
  public Optional<DataSourceWriter> createWriter(String writeUUID, StructType schema, SaveMode mode,
      DataSourceOptions options) {
    String instantTime = options.get(DataSourceInternalWriterHelper.INSTANT_TIME_OPT_KEY).get();
    String path = options.get("path").get();
    String tblName = options.get(HoodieWriteConfig.TBL_NAME.key()).get();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java`
#### Snippet
```java
      DataSourceOptions options) {
    String instantTime = options.get(DataSourceInternalWriterHelper.INSTANT_TIME_OPT_KEY).get();
    String path = options.get("path").get();
    String tblName = options.get(HoodieWriteConfig.TBL_NAME.key()).get();
    boolean populateMetaFields = options.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java`
#### Snippet
```java
    String instantTime = options.get(DataSourceInternalWriterHelper.INSTANT_TIME_OPT_KEY).get();
    String path = options.get("path").get();
    String tblName = options.get(HoodieWriteConfig.TBL_NAME.key()).get();
    boolean populateMetaFields = options.getBoolean(HoodieTableConfig.POPULATE_META_FIELDS.key(),
        HoodieTableConfig.POPULATE_META_FIELDS.defaultValue());
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java`
#### Snippet
```java
    tryOverrideParquetWriteLegacyFormatProperty(properties, schema);
    // 1st arg to createHoodieConfig is not really required to be set. but passing it anyways.
    HoodieWriteConfig config = DataSourceUtils.createHoodieConfig(options.get(HoodieWriteConfig.AVRO_SCHEMA_STRING.key()).get(), path, tblName, properties);
    boolean arePartitionRecordsSorted = HoodieInternalConfig.getBulkInsertIsPartitionRecordsSorted(
        options.get(HoodieInternalConfig.BULKINSERT_ARE_PARTITIONER_RECORDS_SORTED).isPresent()
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
      Thread.sleep(3000);
      waitTillNCommits(fs, numExpCommits, 180, 3);
      commitInstantTime2 = HoodieDataSourceHelpers.listCommitsSince(fs, tablePath, commitInstantTime1).stream().sorted().findFirst().get();
      LOG.info("Second commit at instant time :" + commitInstantTime2);
    }
```

## RuleId[id=MathRoundingWithIntArgument]
### MathRoundingWithIntArgument
`ceil()` with argument of type 'int'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
      int numTasks = numTasksDuringPut;
      int maxParallelPutsTask = Math.max(1, Math.min(numTasks, maxExecutors));
      int multiPutBatchSizePerSecPerTask = Math.max(1, (int) Math.ceil(maxReqPerSec / maxParallelPutsTask));
      LOG.info("HbaseIndexThrottling: qpsFraction :" + qpsFraction);
      LOG.info("HbaseIndexThrottling: numRSAlive :" + numRSAlive);
```

### MathRoundingWithIntArgument
`floor()` with argument of type 'int'
in `hudi-common/src/main/java/org/apache/hudi/common/util/BinaryUtil.java`
#### Snippet
```java
    int totalBits = size * 8;
    for (int bitStep = 0; bitStep < totalBits; bitStep++) {
      int currentBytePos = (int) Math.floor(bitStep / 8);
      int currentBitPos = bitStep % 8;

```

### MathRoundingWithIntArgument
`floor()` with argument of type 'int'
in `hudi-common/src/main/java/org/apache/hudi/common/util/BinaryUtil.java`
#### Snippet
```java

      for (int i = 0; i < candidateSize; i++) {
        int tempResBytePos = (int) Math.floor(resBitPos / 8);
        int tempResBitPos = resBitPos % 8;
        result[tempResBytePos] = updatePos(result[tempResBytePos], tempResBitPos, buffer[i][currentBytePos], currentBitPos);
```

## RuleId[id=NonAtomicOperationOnVolatileField]
### NonAtomicOperationOnVolatileField
Non-atomic operation on volatile field `estimatedPayloadSize`
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
      this.estimatedPayloadSize = keySizeEstimator.sizeEstimate(key) + valueSizeEstimator.sizeEstimate(value);
    } else if (this.inMemoryMap.size() % NUMBER_OF_RECORDS_TO_ESTIMATE_PAYLOAD_SIZE == 0) {
      this.estimatedPayloadSize = (long) (this.estimatedPayloadSize * 0.9 + (keySizeEstimator.sizeEstimate(key) + valueSizeEstimator.sizeEstimate(value)) * 0.1);
      this.currentInMemoryMapSize = this.inMemoryMap.size() * this.estimatedPayloadSize;
    }
```

## RuleId[id=StringConcatenationInsideStringBufferAppend]
### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java
    }
    if (config.getString(HIVE_SYNC_BUCKET_SYNC_SPEC) != null) {
      sb.append(' ' + config.getString(HIVE_SYNC_BUCKET_SYNC_SPEC) + ' ');
    }
    sb.append(" ROW FORMAT SERDE '").append(serdeClass).append("'");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java`
#### Snippet
```java
      StringBuilder str = new StringBuilder();
      for (int i = 0; i < bucketsFilled.length; i++) {
        str.append("p" + i + " : " + bucketsFilled[i] + ",");
      }
      LOG.debug("Num buckets assigned per file group :" + str);
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java`
#### Snippet
```java
      String recordKeyValue = HoodieAvroUtils.getNestedFieldValAsString(record, recordKeyField, true, consistentLogicalTimestampEnabled);
      if (recordKeyValue == null) {
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + NULL_RECORDKEY_PLACEHOLDER + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
      } else if (recordKeyValue.isEmpty()) {
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + EMPTY_RECORDKEY_PLACEHOLDER + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java`
#### Snippet
```java
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + NULL_RECORDKEY_PLACEHOLDER + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
      } else if (recordKeyValue.isEmpty()) {
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + EMPTY_RECORDKEY_PLACEHOLDER + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
      } else {
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + recordKeyValue + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java`
#### Snippet
```java
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + EMPTY_RECORDKEY_PLACEHOLDER + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
      } else {
        recordKey.append(recordKeyField + DEFAULT_COMPOSITE_KEY_FILED_VALUE + recordKeyValue + DEFAULT_RECORD_KEY_PARTS_SEPARATOR);
        keyIsNullEmpty = false;
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
    StringBuilder builder = new StringBuilder();
    Writable[] values = writable.get();
    builder.append("\"values_" + random.nextDouble() + "_" + values.length + "\": {");
    int i = 0;
    for (Writable w : values) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
        builder.append(arrayWritableToString((ArrayWritable) w)).append(",");
      } else {
        builder.append("\"value" + i + "\":\"" + w + "\"").append(",");
        if (w == null) {
          builder.append("\"type" + i + "\":\"unknown\"").append(",");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
        builder.append("\"value" + i + "\":\"" + w + "\"").append(",");
        if (w == null) {
          builder.append("\"type" + i + "\":\"unknown\"").append(",");
        } else {
          builder.append("\"type" + i + "\":\"" + w.getClass().getSimpleName() + "\"").append(",");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
          builder.append("\"type" + i + "\":\"unknown\"").append(",");
        } else {
          builder.append("\"type" + i + "\":\"" + w.getClass().getSimpleName() + "\"").append(",");
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
      s.append("PathFilter: ");
      for (String pString : pStrings) {
        s.append(pString + " ");
      }
      return s.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
      StringBuilder sb = new StringBuilder();
      sb.append(inputSplitShim.toString());
      sb.append("InputFormatClass: " + inputFormatClassName);
      sb.append("\n");
      return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectMetadataFetcher.java`
#### Snippet
```java
    StringBuilder filter = new StringBuilder("size > 0");

    getPropVal(SELECT_RELATIVE_PATH_PREFIX.key()).ifPresent(val -> filter.append(" and name like '" + val + "%'"));
    getPropVal(IGNORE_RELATIVE_PATH_PREFIX.key()).ifPresent(val -> filter.append(" and name not like '" + val + "%'"));
    getPropVal(IGNORE_RELATIVE_PATH_SUBSTR.key()).ifPresent(val -> filter.append(" and name not like '%" + val + "%'"));
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectMetadataFetcher.java`
#### Snippet
```java

    getPropVal(SELECT_RELATIVE_PATH_PREFIX.key()).ifPresent(val -> filter.append(" and name like '" + val + "%'"));
    getPropVal(IGNORE_RELATIVE_PATH_PREFIX.key()).ifPresent(val -> filter.append(" and name not like '" + val + "%'"));
    getPropVal(IGNORE_RELATIVE_PATH_SUBSTR.key()).ifPresent(val -> filter.append(" and name not like '%" + val + "%'"));

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectMetadataFetcher.java`
#### Snippet
```java
    getPropVal(SELECT_RELATIVE_PATH_PREFIX.key()).ifPresent(val -> filter.append(" and name like '" + val + "%'"));
    getPropVal(IGNORE_RELATIVE_PATH_PREFIX.key()).ifPresent(val -> filter.append(" and name not like '" + val + "%'"));
    getPropVal(IGNORE_RELATIVE_PATH_SUBSTR.key()).ifPresent(val -> filter.append(" and name not like '%" + val + "%'"));

    // Match files with a given extension, or use the fileFormat as the default.
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectMetadataFetcher.java`
#### Snippet
```java
    // Match files with a given extension, or use the fileFormat as the default.
    getPropVal(CLOUD_DATAFILE_EXTENSION.key()).or(() -> Option.of(fileFormat))
        .map(val -> filter.append(" and name like '%" + val + "'"));

    return filter.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
      StringBuilder sb = new StringBuilder();
      // at least 1 partition will be present.
      sb.append(ranges[0].topic() + ",");
      sb.append(Arrays.stream(ranges).map(r -> String.format("%s:%d", r.partition(), r.untilOffset()))
              .collect(Collectors.joining(",")));
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java

    StringBuilder sb = new StringBuilder();
    sb.append(topicName + ",");
    for (Map.Entry<TopicPartition, OffsetAndTimestamp> map : offsetAndTimestamp.entrySet()) {
      if (map.getValue() != null) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuffer.append()` call
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java
    StringBuffer stringBuffer = new StringBuffer();
    stringBuffer.append("{");
    stringBuffer.append("\"ts\": \"" + (ts == null ? "0.0" : ts) + "\",");
    stringBuffer.append("\"uuid\": \"" + uuid + "\",");
    stringBuffer.append("\"partitionpath\": \"" + partitionPath + "\"");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuffer.append()` call
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java
    stringBuffer.append("{");
    stringBuffer.append("\"ts\": \"" + (ts == null ? "0.0" : ts) + "\",");
    stringBuffer.append("\"uuid\": \"" + uuid + "\",");
    stringBuffer.append("\"partitionpath\": \"" + partitionPath + "\"");
    stringBuffer.append("}");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuffer.append()` call
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java
    stringBuffer.append("\"ts\": \"" + (ts == null ? "0.0" : ts) + "\",");
    stringBuffer.append("\"uuid\": \"" + uuid + "\",");
    stringBuffer.append("\"partitionpath\": \"" + partitionPath + "\"");
    stringBuffer.append("}");
    return Option.of(stringBuffer.toString());
```

## RuleId[id=RedundantOperationOnEmptyContainer]
### RedundantOperationOnEmptyContainer
Collection `newRecordKeysSorted` is always empty
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java`
#### Snippet
```java
    }

    newRecordKeysSorted.clear();

    return super.close();
```

## RuleId[id=ReplaceInefficientStreamCount]
### ReplaceInefficientStreamCount
Can be replaced with 'stream.anyMatch()'
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java
      }

      if (rows.stream().filter(row -> row.size() != headers.size()).count() > 0) {
        throw new HoodieException("Invalid row, does not match headers " + headers.size() + " " + rows.size());
      }
```

### ReplaceInefficientStreamCount
Can be replaced with '!stream.findAny().isPresent()'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitPartitioner.java`
#### Snippet
```java
                  // NOTE: We can not pad slices with existing log-files w/o compacting these,
                  //       hence skipping
                  fileSlice.getLogFiles().count() < 1
                  && fileSlice.getBaseFile().get().getFileSize() < config.getParquetSmallFileLimit())
          .sorted(Comparator.comparing(fileSlice -> fileSlice.getBaseFile().get().getFileSize()))
```

### ReplaceInefficientStreamCount
Can be replaced with 'stream.findAny().isPresent()'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/BaseHoodieCompactionPlanGenerator.java`
#### Snippet
```java

  protected boolean filterFileSlice(FileSlice fileSlice, String lastCompletedInstantTime, Set<HoodieFileGroupId> pendingFileGroupIds) {
    return fileSlice.getLogFiles().count() > 0 && !pendingFileGroupIds.contains(fileSlice.getFileGroupId());
  }

```

### ReplaceInefficientStreamCount
Can be replaced with 'stream.findAny().isPresent()'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java`
#### Snippet
```java
                    // Add remaining log files back
                    logFiles.values().forEach(newFileSlice::addLogFile);
                    if (newFileSlice.getBaseFile().isPresent() || (newFileSlice.getLogFiles().count() > 0)) {
                      LOG.info("Adding back new file-slice after remove FS=" + newFileSlice);
                      return newFileSlice;
```

## RuleId[id=UNCHECKED_WARNING]
### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-cli/src/main/java/org/apache/hudi/cli/Table.java`
#### Snippet
```java
        Comparable fieldForRow1 = row1.get(rowHeader.indexOf(orderingColumnName));
        Comparable fieldForRow2 = row2.get(rowHeader.indexOf(orderingColumnName));
        int cmpRawResult = fieldForRow1.compareTo(fieldForRow2);
        return isDescendingOptional.map(isDescending -> isDescending ? -1 * cmpRawResult : cmpRawResult).orElse(cmpRawResult);
      }).collect(Collectors.toList());
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
    ObjectInputStream in = new ObjectInputStream(fsDataInputStream);
    try {
      T result = (T) in.readObject();
      LOG.info("Result : " + result);
      return result;
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.HashSet' to 'java.util.Set'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TimelineCommand.java`
#### Snippet
```java
      Integer limit, String sortByField, boolean descending, boolean headerOnly, boolean withRowNo,
      boolean showTimeSeconds, boolean showRollbackInfo) {
    Set<String> instantTimeSet = new HashSet(dtInstantInfoMap.keySet());
    instantTimeSet.addAll(mtInstantInfoMap.keySet());
    List<String> instantTimeList = instantTimeSet.stream()
```

### UNCHECKED_WARNING
Unchecked call to 'HashSet(Collection)' as a member of raw type 'java.util.HashSet'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TimelineCommand.java`
#### Snippet
```java
      Integer limit, String sortByField, boolean descending, boolean headerOnly, boolean withRowNo,
      boolean showTimeSeconds, boolean showRollbackInfo) {
    Set<String> instantTimeSet = new HashSet(dtInstantInfoMap.keySet());
    instantTimeSet.addAll(mtInstantInfoMap.keySet());
    List<String> instantTimeList = instantTimeSet.stream()
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieTimelineArchiver(HoodieWriteConfig, HoodieTable)' as a member of raw type 'org.apache.hudi.client.HoodieTimelineArchiver'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
    HoodieSparkTable<HoodieAvroPayload> table = HoodieSparkTable.create(config, context);
    try {
      HoodieTimelineArchiver archiver = new HoodieTimelineArchiver(config, table);
      archiver.archiveIfRequired(context,true);
    } catch (IOException ioe) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java

  public static String generateSchemaString(MessageType storageSchema) throws IOException {
    return generateSchemaString(storageSchema, Collections.EMPTY_LIST);
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java`
#### Snippet
```java
      HoodieData<HoodieRecord<R>> records, HoodieEngineContext context,
      HoodieTable hoodieTable) throws HoodieIndexException {
    List<HoodieRecord<T>> hoodieRecords = tagLocation(records.map(record -> (HoodieRecord<T>) record).collectAsList(), context, hoodieTable);
    return HoodieListData.eager(hoodieRecords.stream().map(r -> (HoodieRecord<R>) r).collect(Collectors.toList()));
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java`
#### Snippet
```java
      HoodieTable hoodieTable) throws HoodieIndexException {
    List<HoodieRecord<T>> hoodieRecords = tagLocation(records.map(record -> (HoodieRecord<T>) record).collectAsList(), context, hoodieTable);
    return HoodieListData.eager(hoodieRecords.stream().map(r -> (HoodieRecord<R>) r).collect(Collectors.toList()));
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java
        context, config, this, compactionInstantTime, new HoodieJavaMergeOnReadTableCompactor(),
        new HoodieJavaCopyOnWriteTable(config, context, getMetaClient()), WriteOperationType.COMPACT);
    return convertMetadata(compactionExecutor.execute());
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'compactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java
        context, config, this, compactionInstantTime, new HoodieJavaMergeOnReadTableCompactor(),
        new HoodieJavaCopyOnWriteTable(config, context, getMetaClient()), WriteOperationType.COMPACT);
    return convertMetadata(compactionExecutor.execute());
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config, this, instantTime, preppedRecords, bulkInsertPartitioner)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java
                                                                  List<HoodieRecord<T>> preppedRecords,
                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    return new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config,
        this, instantTime, preppedRecords, bulkInsertPartitioner).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'JavaBulkInsertPreppedCommitActionExecutor(HoodieJavaEngineContext, HoodieWriteConfig, HoodieTable, String, List\>, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertPreppedCommitActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java
                                                                  List<HoodieRecord<T>> preppedRecords,
                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    return new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config,
        this, instantTime, preppedRecords, bulkInsertPartitioner).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'ScheduleCompactionActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, Option\>, WriteOperationType)' as a member of raw type 'org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {
    ScheduleCompactionActionExecutor scheduleCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.COMPACT);
    return scheduleCompactionExecutor.execute();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'scheduleCompactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaMergeOnReadTable.java`
#### Snippet
```java
    ScheduleCompactionActionExecutor scheduleCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.COMPACT);
    return scheduleCompactionExecutor.execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteHelper.java`
#### Snippet
```java
        deleteExecutor.saveWorkloadProfileMetadataToInflight(new WorkloadProfile(Pair.of(new HashMap<>(), new WorkloadStat())), instantTime);
        result = new HoodieWriteMetadata<>();
        result.setWriteStatuses(Collections.EMPTY_LIST);
        deleteExecutor.commitOnAutoCommit(result);
      }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaWriteHelper.java`
#### Snippet
```java
      HoodieRecord<T> reducedRecord;
      try {
        reducedRecord =  merger.merge(rec1, schema, rec2, schema, props).get().getLeft();
      } catch (IOException e) {
        throw new HoodieException(String.format("Error to merge two records, %s, %s", rec1, rec2), e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertPreppedCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    try {
      return JavaBulkInsertHelper.newInstance().bulkInsert(preppedInputRecord, instantTime, table, config,
          this, false, userDefinedBulkInsertPartitioner);
    } catch (Throwable e) {
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(List\>, String, HoodieTable\>, List, List\>, HoodieWriteConfig, BaseCommitActionExecutor\>, List, List, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertPreppedCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    try {
      return JavaBulkInsertHelper.newInstance().bulkInsert(preppedInputRecord, instantTime, table, config,
          this, false, userDefinedBulkInsertPartitioner);
    } catch (Throwable e) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    try {
      return JavaBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, table, config,
          this, true, bulkInsertPartitioner);
    } catch (HoodieInsertException ie) {
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(List\>, String, HoodieTable\>, List, List\>, HoodieWriteConfig, BaseCommitActionExecutor\>, List, List, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    try {
      return JavaBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, table, config,
          this, true, bulkInsertPartitioner);
    } catch (HoodieInsertException ie) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaDeleteHelper.newInstance()' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'execute(String, List, HoodieEngineContext, HoodieWriteConfig, HoodieTable\>, List, List\>, BaseCommitActionExecutor\>, List, List, R\>)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaDeleteHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaDeleteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return JavaDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'updateIndexAndCommitIfNeeded(List, HoodieWriteMetadata)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseJavaCommitActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java
        config.getBulkInsertShuffleParallelism(), new CreateHandleFactory(false));
    //update index
    ((BaseJavaCommitActionExecutor) executor).updateIndexAndCommitIfNeeded(writeStatuses, result);
    return result;
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java
    //update index
    ((BaseJavaCommitActionExecutor) executor).updateIndexAndCommitIfNeeded(writeStatuses, result);
    return result;
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List\>'. Reason: 'JavaWriteHelper.newInstance()' has raw type, so result of combineOnCondition is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java

    if (performDedupe) {
      dedupedRecords = (List<HoodieRecord<T>>) JavaWriteHelper.newInstance()
          .combineOnCondition(config.shouldCombineBeforeInsert(), inputRecords, targetParallelism, table);
    }

```

### UNCHECKED_WARNING
Unchecked call to 'combineOnCondition(boolean, I, int, HoodieTable)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java

    if (performDedupe) {
      dedupedRecords = (List<HoodieRecord<T>>) JavaWriteHelper.newInstance()
          .combineOnCondition(config.shouldCombineBeforeInsert(), inputRecords, targetParallelism, table);
    }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List\>'. Reason: 'partitioner' has raw type, so result of repartitionRecords is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java

    final List<HoodieRecord<T>> repartitionedRecords =
        (List<HoodieRecord<T>>) partitioner.repartitionRecords(dedupedRecords, targetParallelism);

    FileIdPrefixProvider fileIdPrefixProvider = (FileIdPrefixProvider) ReflectionUtils.loadClass(
```

### UNCHECKED_WARNING
Unchecked call to 'repartitionRecords(I, int)' as a member of raw type 'org.apache.hudi.table.BulkInsertPartitioner'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java

    final List<HoodieRecord<T>> repartitionedRecords =
        (List<HoodieRecord<T>>) partitioner.repartitionRecords(dedupedRecords, targetParallelism);

    FileIdPrefixProvider fileIdPrefixProvider = (FileIdPrefixProvider) ReflectionUtils.loadClass(
```

### UNCHECKED_WARNING
Unchecked call to 'orElse(T)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java`
#### Snippet
```java
        fileIdPrefixProvider.createFilePrefix(""), table.getTaskContextSupplier(),
        // Always get the first WriteHandleFactory, as there is only a single data partition for hudi java engine.
        (WriteHandleFactory) partitioner.getWriteHandleFactory(0).orElse(writeHandleFactory)).forEachRemaining(writeStatuses::addAll);

    return writeStatuses;
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAppendHandle(HoodieWriteConfig, String, HoodieTable, String, String, Iterator\>, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.HoodieAppendHandle'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/deltacommit/JavaUpsertPreppedDeltaCommitActionExecutor.java`
#### Snippet
```java
    try {
      recordsByFileId.forEach((k, v) -> {
        HoodieAppendHandle<?, ?, ?, ?> appendHandle = new HoodieAppendHandle(config, instantTime, table,
            k.getRight(), k.getLeft(), v.iterator(), taskContextSupplier);
        appendHandle.doAppend();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'JavaBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/deltacommit/JavaUpsertPreppedDeltaCommitActionExecutor.java`
#### Snippet
```java

      if (insertedRecords.size() > 0) {
        HoodieWriteMetadata<List<WriteStatus>> insertResult = JavaBulkInsertHelper.newInstance()
            .bulkInsert(insertedRecords, instantTime, table, config, this, false, Option.empty());
        allWriteStatuses.addAll(insertResult.getWriteStatuses());
      }
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(List\>, String, HoodieTable\>, List, List\>, HoodieWriteConfig, BaseCommitActionExecutor\>, List, List, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/deltacommit/JavaUpsertPreppedDeltaCommitActionExecutor.java`
#### Snippet
```java

      if (insertedRecords.size() > 0) {
        HoodieWriteMetadata<List<WriteStatus>> insertResult = JavaBulkInsertHelper.newInstance()
            .bulkInsert(insertedRecords, instantTime, table, config, this, false, Option.empty());
        allWriteStatuses.addAll(insertResult.getWriteStatuses());
      }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'. Reason: 'JavaBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaSortAndSizeExecutionStrategy.java`
#### Snippet
```java
        .withProps(getWriteConfig().getProps()).build();
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringTargetFileMaxBytes()));
    return (List<WriteStatus>) JavaBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig,
        false, getPartitioner(strategyParams, schema), true, numOutputGroups, new CreateHandleFactory(preserveHoodieMetadata));
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(List\>, String, HoodieTable\>, List, List\>, HoodieWriteConfig, boolean, BulkInsertPartitioner, boolean, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaSortAndSizeExecutionStrategy.java`
#### Snippet
```java
        .withProps(getWriteConfig().getProps()).build();
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringTargetFileMaxBytes()));
    return (List<WriteStatus>) JavaBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig,
        false, getPartitioner(strategyParams, schema), true, numOutputGroups, new CreateHandleFactory(preserveHoodieMetadata));
  }
```

### UNCHECKED_WARNING
Unchecked call to 'CopyOnWriteRollbackActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean)' as a member of raw type 'org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                         boolean deleteInstants,
                                         boolean skipLocking) {
    return new CopyOnWriteRollbackActionExecutor(
        context, config, this, rollbackInstantTime, commitInstant, deleteInstants, skipLocking).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieCreateHandle(HoodieWriteConfig, String, HoodieTable, String, String, Map\>, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.HoodieCreateHandle'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
      Map<String, HoodieRecord<?>> recordMap) {
    HoodieCreateHandle<?, ?, ?, ?> createHandle =
        new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordMap, taskContextSupplier);
    createHandle.write();
    return Collections.singletonList(createHandle.close()).iterator();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'new RestorePlanActionExecutor(context, config, this, restoreInstantTimestamp, savepointToRestoreTimestamp)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieRestorePlan> scheduleRestore(HoodieEngineContext context, String restoreInstantTimestamp, String savepointToRestoreTimestamp) {
    return new RestorePlanActionExecutor(context, config, this, restoreInstantTimestamp, savepointToRestoreTimestamp).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'RestorePlanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, String)' as a member of raw type 'org.apache.hudi.table.action.rollback.RestorePlanActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieRestorePlan> scheduleRestore(HoodieEngineContext context, String restoreInstantTimestamp, String savepointToRestoreTimestamp) {
    return new RestorePlanActionExecutor(context, config, this, restoreInstantTimestamp, savepointToRestoreTimestamp).execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new JavaInsertOverwriteTableCommitActionExecutor( context, config, this, instantTime, records)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                                     String instantTime,
                                                                     List<HoodieRecord<T>> records) {
    return new JavaInsertOverwriteTableCommitActionExecutor(
        context, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'JavaInsertOverwriteTableCommitActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, List\>)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaInsertOverwriteTableCommitActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                                     String instantTime,
                                                                     List<HoodieRecord<T>> records) {
    return new JavaInsertOverwriteTableCommitActionExecutor(
        context, config, this, instantTime, records).execute();
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new JavaBulkInsertCommitActionExecutor((HoodieJavaEngineContext) context, config, this, instantTime, records, bulkInsertPartitioner)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                           List<HoodieRecord<T>> records,
                                                           Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    return new JavaBulkInsertCommitActionExecutor((HoodieJavaEngineContext) context, config,
        this, instantTime, records, bulkInsertPartitioner).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'JavaBulkInsertCommitActionExecutor(HoodieJavaEngineContext, HoodieWriteConfig, HoodieTable, String, List\>, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertCommitActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                           List<HoodieRecord<T>> records,
                                                           Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    return new JavaBulkInsertCommitActionExecutor((HoodieJavaEngineContext) context, config,
        this, instantTime, records, bulkInsertPartitioner).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'CleanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String)' as a member of raw type 'org.apache.hudi.table.action.clean.CleanActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
  public HoodieCleanMetadata clean(HoodieEngineContext context,
                                   String cleanInstantTime) {
    return new CleanActionExecutor(context, config, this, cleanInstantTime).execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish, shouldRollbackUsingMarkers)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
        shouldRollbackUsingMarkers).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'BaseRollbackPlanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean)' as a member of raw type 'org.apache.hudi.table.action.rollback.BaseRollbackPlanActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
        shouldRollbackUsingMarkers).execute();
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new JavaInsertOverwriteCommitActionExecutor( context, config, this, instantTime, records)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                                String instantTime,
                                                                List<HoodieRecord<T>> records) {
    return new JavaInsertOverwriteCommitActionExecutor(
        context, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'JavaInsertOverwriteCommitActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, List\>)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaInsertOverwriteCommitActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                                String instantTime,
                                                                List<HoodieRecord<T>> records) {
    return new JavaInsertOverwriteCommitActionExecutor(
        context, config, this, instantTime, records).execute();
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config, this, instantTime, preppedRecords, bulkInsertPartitioner)' has raw type, so result of execute is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                                  List<HoodieRecord<T>> preppedRecords,
                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    return new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config,
        this, instantTime, preppedRecords, bulkInsertPartitioner).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'JavaBulkInsertPreppedCommitActionExecutor(HoodieJavaEngineContext, HoodieWriteConfig, HoodieTable, String, List\>, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.JavaBulkInsertPreppedCommitActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                                                  List<HoodieRecord<T>> preppedRecords,
                                                                  Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    return new JavaBulkInsertPreppedCommitActionExecutor((HoodieJavaEngineContext) context, config,
        this, instantTime, preppedRecords, bulkInsertPartitioner).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'SavepointActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, String, String)' as a member of raw type 'org.apache.hudi.table.action.savepoint.SavepointActionExecutor'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
                                           String user,
                                           String comment) {
    return new SavepointActionExecutor(
        context, config, this, instantToSavepoint, user, comment).execute();
  }
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.action.HoodieWriteMetadata', required 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java

  @Override
  public HoodieWriteMetadata deletePartitions(HoodieEngineContext context, String instantTime, List<String> partitions) {
    throw new HoodieNotSupportedException("Delete partitions is not supported yet");
  }
```

### UNCHECKED_WARNING
Unchecked call to 'runMerge(HoodieTable, HoodieMergeHandle)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieMergeHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaCopyOnWriteTable.java`
#### Snippet
```java
          "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
    } else {
      HoodieMergeHelper.newInstance().runMerge(this, upsertHandle);
    }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Comparator' to 'java.util.Comparator\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/bulkinsert/JavaGlobalSortPartitioner.java`
#### Snippet
```java
                                                  int outputPartitions) {
    // Now, sort the records and line them up nicely for loading.
    records.sort(new Comparator() {
      @Override
      public int compare(Object o1, Object o2) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.queue.HoodieExecutor' to 'org.apache.hudi.common.util.queue.HoodieExecutor\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/JavaLazyInsertIterable.java`
#### Snippet
```java
    try {
      final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
      executor = ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getTransformer(schema, hoodieConfig));
      final List<WriteStatus> result = executor.execute();
      checkState(result != null && !result.isEmpty());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.execution.CopyOnWriteInsertHandler' to 'org.apache.hudi.common.util.queue.HoodieConsumer,java.lang.Object\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/execution/JavaLazyInsertIterable.java`
#### Snippet
```java
    try {
      final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
      executor = ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(), getTransformer(schema, hoodieConfig));
      final List<WriteStatus> result = executor.execute();
      checkState(result != null && !result.isEmpty());
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.hudi.common.util.collection.Pair\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
  public int getPartition(Object key) {
    Pair<HoodieKey, Option<HoodieRecordLocation>> keyLocation =
        (Pair<HoodieKey, Option<HoodieRecordLocation>>) key;
    if (keyLocation.getRight().isPresent()) {
      HoodieRecordLocation location = keyLocation.getRight().get();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'java.util.Iterator'. Reason: 'baseFileReader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
      try (HoodieFileReader baseFileReader = HoodieFileReaderFactory.getReaderFactory(recordType).getFileReader(getHoodieTable().getHadoopConf(), new Path(clusteringOp.getDataFilePath()))) {
        Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(getWriteConfig().getSchema()));
        Iterator<HoodieRecord> recordIterator = baseFileReader.getRecordIterator(readerSchema);
        // NOTE: Record have to be cloned here to make sure if it holds low-level engine-specific
        //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'. Reason: 'record.copy()' has raw type, so result of wrapIntoHoodieRecordPayloadWithKeyGen is erased
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
        //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
        //       it since these records will be put into the records(List).
        recordIterator.forEachRemaining(record -> records.add(record.copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema, new Properties(), Option.empty())));
      } catch (IOException e) {
        throw new HoodieClusteringException("Error reading input data for " + clusteringOp.getDataFilePath()
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.execution.bulkinsert.JavaCustomColumnsSortPartitioner' to 'org.apache.hudi.table.BulkInsertPartitioner\>\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
  protected BulkInsertPartitioner<List<HoodieRecord<T>>> getPartitioner(Map<String, String> strategyParams, Schema schema) {
    if (strategyParams.containsKey(PLAN_STRATEGY_SORT_COLUMNS.key())) {
      return new JavaCustomColumnsSortPartitioner(
          strategyParams.get(PLAN_STRATEGY_SORT_COLUMNS.key()).split(","),
          HoodieAvroUtils.addMetadataFields(schema),
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.BulkInsertPartitioner' to 'org.apache.hudi.table.BulkInsertPartitioner\>\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
          getWriteConfig().isConsistentLogicalTimestampEnabled());
    } else {
      return JavaBulkInsertInternalPartitionerFactory.get(getWriteConfig().getBulkInsertSortMode());
    }
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.table.log.HoodieFileSliceReader' to 'java.util.Iterator\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/JavaExecutionStrategy.java`
#### Snippet
```java
            : Option.of(HoodieFileReaderFactory.getReaderFactory(recordType).getFileReader(table.getHadoopConf(), new Path(clusteringOp.getDataFilePath())));
        HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();
        Iterator<HoodieRecord<T>> fileSliceReader = getFileSliceReader(baseFileReader, scanner, readerSchema,
            tableConfig.getProps(),
            tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
```

### UNCHECKED_WARNING
Unchecked call to 'getRecordKey(Schema, Option)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkConcatHandle.java`
#### Snippet
```java
  public void write(HoodieRecord oldRecord) {
    Schema oldSchema = config.populateMetaFields() ? writeSchemaWithMetaFields : writeSchema;
    String key = oldRecord.getRecordKey(oldSchema, keyGeneratorOpt);
    try {
      fileWriter.write(key, oldRecord, oldSchema);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
                                                    Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.BULK_INSERT_PREPPED, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.BULK_INSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.INSERT, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
                                  String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.DELETE, Option.ofNullable(instantTime));
    preWrite(instantTime, WriteOperationType.DELETE, table.getMetaClient());
    HoodieWriteMetadata<List<WriteStatus>> result = table.delete(context,instantTime, keys);
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.HoodieTable', required 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java

  @Override
  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf, HoodieTableMetaClient metaClient) {
    return HoodieJavaTable.create(config, context, metaClient);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
                                                String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.INSERT_PREPPED, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
                                  String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.UPSERT, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.UPSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
                                                String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.UPSERT_PREPPED, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.UPSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.HoodieTable', required 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java

  @Override
  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf) {
    return HoodieJavaTable.create(config, context);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
                                       String instantTime,
                                       WriteOperationType operationType) {
    super(context, config, table, instantTime, operationType, Option.empty());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
                                       WriteOperationType operationType,
                                       Option extraMetadata) {
    super(context, config, table, instantTime, operationType, extraMetadata);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
                                       WriteOperationType operationType,
                                       Option extraMetadata) {
    super(context, config, table, instantTime, operationType, extraMetadata);
  }

```

### UNCHECKED_WARNING
Unchecked call to 'runMerge(HoodieTable, HoodieMergeHandle)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieMergeHelper'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
          "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
    } else {
      HoodieMergeHelper.newInstance().runMerge(table, upsertHandle);
    }

```

### UNCHECKED_WARNING
Unchecked call to 'setWriteStatuses(O)' as a member of raw type 'org.apache.hudi.table.action.HoodieWriteMetadata'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
    List<WriteStatus> statuses = table.getIndex().updateLocation(HoodieListData.eager(writeStatuses), context, table).collectAsList();
    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));
    result.setWriteStatuses(statuses);
    result.setPartitionToReplaceFileIds(getPartitionToReplacedFileIds(result));
    commitOnAutoCommit(result);
```

### UNCHECKED_WARNING
Unchecked call to 'setPartitionToReplaceFileIds(Map\>)' as a member of raw type 'org.apache.hudi.table.action.HoodieWriteMetadata'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));
    result.setWriteStatuses(statuses);
    result.setPartitionToReplaceFileIds(getPartitionToReplacedFileIds(result));
    commitOnAutoCommit(result);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));
    result.setWriteStatuses(statuses);
    result.setPartitionToReplaceFileIds(getPartitionToReplacedFileIds(result));
    commitOnAutoCommit(result);
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
    if (!recordItr.hasNext()) {
      LOG.info("Empty partition");
      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();
    }
    return new JavaLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
    if (!recordItr.hasNext()) {
      LOG.info("Empty partition with fileId => " + fileId);
      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();
    }
    // these are updates
```

### UNCHECKED_WARNING
Unchecked call to 'getRecordKey(Schema, Option)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkConcatAndReplaceHandle.java`
#### Snippet
```java
  public void write(HoodieRecord oldRecord) {
    Schema oldSchema = config.populateMetaFields() ? writeSchemaWithMetaFields : writeSchema;
    String key = oldRecord.getRecordKey(oldSchema, keyGeneratorOpt);
    try {
      fileWriter.write(key, oldRecord, writeSchema);
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java`
#### Snippet
```java
      HoodieData<HoodieRecord<R>> records, HoodieEngineContext context,
      HoodieTable hoodieTable) throws HoodieIndexException {
    List<HoodieRecord<T>> hoodieRecords = tagLocation(records.map(record -> (HoodieRecord<T>) record).collectAsList(), context, hoodieTable);
    return HoodieListData.eager(hoodieRecords.stream().map(r -> (HoodieRecord<R>) r).collect(Collectors.toList()));
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java`
#### Snippet
```java
      HoodieTable hoodieTable) throws HoodieIndexException {
    List<HoodieRecord<T>> hoodieRecords = tagLocation(records.map(record -> (HoodieRecord<T>) record).collectAsList(), context, hoodieTable);
    return HoodieListData.eager(hoodieRecords.stream().map(r -> (HoodieRecord<R>) r).collect(Collectors.toList()));
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'FlinkWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteTableCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, List\>, HoodieEngineContext, HoodieTable\>, List, List\>, boolean, int, BaseCommitActionExecutor\>, List, List, R\>, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkWriteHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteTableCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'table' has raw type, so result of getPartitionMetafileFormat is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java`
#### Snippet
```java
              new Path(writeConfig.getBasePath()),
              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
              table.getPartitionMetafileFormat());
      partitionMetadata.trySave(taskPartitionId);
      createMarkerFile(partitionPath, FSUtils.makeBaseFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'FlinkWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, List\>, HoodieEngineContext, HoodieTable\>, List, List\>, boolean, int, BaseCommitActionExecutor\>, List, List, R\>, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkWriteHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'FlinkWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, List\>, HoodieEngineContext, HoodieTable\>, List, List\>, boolean, int, BaseCommitActionExecutor\>, List, List, R\>, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkWriteHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'FlinkDeleteHelper.newInstance()' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'execute(String, List, HoodieEngineContext, HoodieWriteConfig, HoodieTable\>, List, List\>, BaseCommitActionExecutor\>, List, List, R\>)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkDeleteHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'FlinkWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, List\>, HoodieEngineContext, HoodieTable\>, List, List\>, boolean, int, BaseCommitActionExecutor\>, List, List, R\>, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkWriteHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<List<WriteStatus>> execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
    if (writeHandle instanceof FlinkAppendHandle) {
      FlinkAppendHandle<?, ?, ?, ?> appendHandle = (FlinkAppendHandle<?, ?, ?, ?>) writeHandle;
      return new FlinkUpsertDeltaCommitActionExecutor<>(context, appendHandle, config, this, instantTime, hoodieRecords).execute();
    } else {
      return super.insert(context, writeHandle, instantTime, hoodieRecords);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
        context, config, this, compactionInstantTime, new HoodieFlinkMergeOnReadTableCompactor(),
        new HoodieFlinkCopyOnWriteTable(config, context, getMetaClient()), WriteOperationType.COMPACT);
    return convertMetadata(compactionExecutor.execute());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'compactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
        context, config, this, compactionInstantTime, new HoodieFlinkMergeOnReadTableCompactor(),
        new HoodieFlinkCopyOnWriteTable(config, context, getMetaClient()), WriteOperationType.COMPACT);
    return convertMetadata(compactionExecutor.execute());
  }

```

### UNCHECKED_WARNING
Unchecked call to 'ScheduleCompactionActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, Option\>, WriteOperationType)' as a member of raw type 'org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
      String instantTime,
      Option<Map<String, String>> extraMetadata) {
    ScheduleCompactionActionExecutor scheduleCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.COMPACT);
    return scheduleCompactionExecutor.execute();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'scheduleCompactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
    ScheduleCompactionActionExecutor scheduleCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.COMPACT);
    return scheduleCompactionExecutor.execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish, shouldRollbackUsingMarkers)' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
        shouldRollbackUsingMarkers).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'BaseRollbackPlanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean)' as a member of raw type 'org.apache.hudi.table.action.rollback.BaseRollbackPlanActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
        shouldRollbackUsingMarkers).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'MergeOnReadRollbackActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean)' as a member of raw type 'org.apache.hudi.table.action.rollback.MergeOnReadRollbackActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
  public HoodieRollbackMetadata rollback(HoodieEngineContext context, String rollbackInstantTime, HoodieInstant commitInstant,
                                         boolean deleteInstants, boolean skipLocking) {
    return new MergeOnReadRollbackActionExecutor(context, config, this, rollbackInstantTime, commitInstant, deleteInstants,
        skipLocking).execute();
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkMergeOnReadTable.java`
#### Snippet
```java
        "MOR write handle should always be a FlinkAppendHandle");
    FlinkAppendHandle<?, ?, ?, ?> appendHandle = (FlinkAppendHandle<?, ?, ?, ?>) writeHandle;
    return new FlinkUpsertDeltaCommitActionExecutor<>(context, appendHandle, config, this, instantTime, hoodieRecords).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'write(String, List\>, HoodieEngineContext, HoodieTable\>, List, List\>, boolean, int, BaseCommitActionExecutor\>, List, List, R\>, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkWriteHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/FlinkUpsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata execute() {
    return FlinkWriteHelper.newInstance().write(instantTime, inputRecords, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkWriteHelper.java`
#### Snippet
```java
      try {
        // Precombine do not need schema and do not return null
        reducedRecord =  merger.merge(rec1, schema, rec2, schema, props).get().getLeft();
      } catch (IOException e) {
        throw new HoodieException(String.format("Error to merge two records, %s, %s", rec1, rec2), e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'appendHandle' has raw type, so result of close is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/BaseFlinkDeltaCommitActionExecutor.java`
#### Snippet
```java
    FlinkAppendHandle appendHandle = (FlinkAppendHandle) writeHandle;
    appendHandle.doAppend();
    List<WriteStatus> writeStatuses = appendHandle.close();
    return Collections.singletonList(writeStatuses).iterator();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'ExplicitWriteHandleFactory(HoodieWriteHandle)' as a member of raw type 'org.apache.hudi.io.ExplicitWriteHandleFactory'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/delta/BaseFlinkDeltaCommitActionExecutor.java`
#### Snippet
```java
  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr) {
    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table,
        idPfx, taskContextSupplier, new ExplicitWriteHandleFactory(writeHandle));
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java`
#### Snippet
```java
        deleteExecutor.saveWorkloadProfileMetadataToInflight(new WorkloadProfile(Pair.of(new HashMap<>(), new WorkloadStat())), instantTime);
        result = new HoodieWriteMetadata<>();
        result.setWriteStatuses(Collections.EMPTY_LIST);
        deleteExecutor.commitOnAutoCommit(result);
      }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java
                                       WriteOperationType operationType,
                                       Option extraMetadata) {
    super(context, config, table, instantTime, operationType, extraMetadata);
    this.writeHandle = writeHandle;
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java
                                       WriteOperationType operationType,
                                       Option extraMetadata) {
    super(context, config, table, instantTime, operationType, extraMetadata);
    this.writeHandle = writeHandle;
  }
```

### UNCHECKED_WARNING
Unchecked call to 'runMerge(HoodieTable, HoodieMergeHandle)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieMergeHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java
          "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
    } else {
      HoodieMergeHelper.newInstance().runMerge(table, upsertHandle);
    }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java
    if (!recordItr.hasNext()) {
      LOG.info("Empty partition with fileId => " + fileId);
      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();
    }
    // these are updates
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java
    if (!recordItr.hasNext()) {
      LOG.info("Empty partition");
      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();
    }
    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.Collection'. Reason: 'handle' has raw type, so result of close is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/ExplicitWriteHandler.java`
#### Snippet
```java

  private void closeOpenHandle() {
    statuses.addAll(handle.close());
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'new CleanPlanActionExecutor(context, config, this, instantTime, extraMetadata)' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {
    return new CleanPlanActionExecutor(context, config, this, instantTime, extraMetadata).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'CleanPlanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, Option\>)' as a member of raw type 'org.apache.hudi.table.action.clean.CleanPlanActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {
    return new CleanPlanActionExecutor(context, config, this, instantTime, extraMetadata).execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new FlinkInsertOverwriteTableCommitActionExecutor(context, writeHandle, config, this, instantTime, records)' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
      String instantTime,
      List<HoodieRecord<T>> records) {
    return new FlinkInsertOverwriteTableCommitActionExecutor(context, writeHandle, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'FlinkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext, HoodieWriteHandle, HoodieWriteConfig, HoodieTable, String, List\>)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkInsertOverwriteTableCommitActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
      String instantTime,
      List<HoodieRecord<T>> records) {
    return new FlinkInsertOverwriteTableCommitActionExecutor(context, writeHandle, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'runMerge(HoodieTable, HoodieMergeHandle)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieMergeHelper'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
          "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
    } else {
      HoodieMergeHelper.newInstance().runMerge(this, upsertHandle);
    }

```

### UNCHECKED_WARNING
Unchecked call to 'CopyOnWriteRollbackActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean)' as a member of raw type 'org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
  public HoodieRollbackMetadata rollback(HoodieEngineContext context, String rollbackInstantTime, HoodieInstant commitInstant,
                                         boolean deleteInstants, boolean skipLocking) {
    return new CopyOnWriteRollbackActionExecutor(context, config, this, rollbackInstantTime, commitInstant, deleteInstants, skipLocking).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'CleanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String)' as a member of raw type 'org.apache.hudi.table.action.clean.CleanActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public HoodieCleanMetadata clean(HoodieEngineContext context, String cleanInstantTime) {
    return new CleanActionExecutor(context, config, this, cleanInstantTime).execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish, shouldRollbackUsingMarkers)' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
        shouldRollbackUsingMarkers).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'BaseRollbackPlanActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean)' as a member of raw type 'org.apache.hudi.table.action.rollback.BaseRollbackPlanActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
  public Option<HoodieRollbackPlan> scheduleRollback(HoodieEngineContext context, String instantTime, HoodieInstant instantToRollback,
                                                     boolean skipTimelinePublish, boolean shouldRollbackUsingMarkers) {
    return new BaseRollbackPlanActionExecutor(context, config, this, instantTime, instantToRollback, skipTimelinePublish,
        shouldRollbackUsingMarkers).execute();
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new FlinkInsertOverwriteCommitActionExecutor(context, writeHandle, config, this, instantTime, records)' has raw type, so result of execute is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
      String instantTime,
      List<HoodieRecord<T>> records) {
    return new FlinkInsertOverwriteCommitActionExecutor(context, writeHandle, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'FlinkInsertOverwriteCommitActionExecutor(HoodieEngineContext, HoodieWriteHandle, HoodieWriteConfig, HoodieTable, String, List\>)' as a member of raw type 'org.apache.hudi.table.action.commit.FlinkInsertOverwriteCommitActionExecutor'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
      String instantTime,
      List<HoodieRecord<T>> records) {
    return new FlinkInsertOverwriteCommitActionExecutor(context, writeHandle, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'HoodieCreateHandle(HoodieWriteConfig, String, HoodieTable, String, String, Map\>, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.HoodieCreateHandle'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
      Map<String, HoodieRecord<?>> recordMap) {
    HoodieCreateHandle<?, ?, ?, ?> createHandle =
        new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordMap, taskContextSupplier);
    createHandle.write();
    return Collections.singletonList(createHandle.close()).iterator();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.queue.HoodieExecutor' to 'org.apache.hudi.common.util.queue.HoodieExecutor\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/FlinkLazyInsertIterable.java`
#### Snippet
```java
    try {
      Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
      executor = ExecutorFactory.create(hoodieConfig, inputItr, getExplicitInsertHandler(),
          getTransformer(schema, hoodieConfig));
      final List<WriteStatus> result = executor.execute();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.execution.ExplicitWriteHandler' to 'org.apache.hudi.common.util.queue.HoodieConsumer,java.lang.Object\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/FlinkLazyInsertIterable.java`
#### Snippet
```java
    try {
      Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema());
      executor = ExecutorFactory.create(hoodieConfig, inputItr, getExplicitInsertHandler(),
          getTransformer(schema, hoodieConfig));
      final List<WriteStatus> result = executor.execute();
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'writeClient' has raw type, so result of upsertPreppedRecords is erased
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java

      List<WriteStatus> statuses = preppedRecordList.size() > 0
          ? writeClient.upsertPreppedRecords(preppedRecordList, instantTime)
          : Collections.emptyList();
      // flink does not support auto-commit yet, also the auto commit logic is not complete as BaseHoodieWriteClient now.
```

### UNCHECKED_WARNING
Unchecked call to 'upsertPreppedRecords(List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieFlinkWriteClient'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java

      List<WriteStatus> statuses = preppedRecordList.size() > 0
          ? writeClient.upsertPreppedRecords(preppedRecordList, instantTime)
          : Collections.emptyList();
      // flink does not support auto-commit yet, also the auto commit logic is not complete as BaseHoodieWriteClient now.
```

### UNCHECKED_WARNING
Unchecked call to 'commit(String, O, Option\>, String, Map\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/metadata/FlinkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
          : Collections.emptyList();
      // flink does not support auto-commit yet, also the auto commit logic is not complete as BaseHoodieWriteClient now.
      writeClient.commit(instantTime, statuses, Option.empty(), HoodieActiveTimeline.DELTA_COMMIT_ACTION, Collections.emptyMap());

      // reload timeline
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.storage.HoodieFileReader' to 'org.apache.hudi.io.storage.HoodieFileReader'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileReaderFactory.java`
#### Snippet
```java
  @Override
  public HoodieFileReader newBootstrapFileReader(HoodieFileReader skeletonFileReader, HoodieFileReader dataFileReader, Option<String[]> partitionFields, Object[] partitionValues) {
    return new HoodieSparkBootstrapFileReader(skeletonFileReader, dataFileReader, partitionFields, partitionValues);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.storage.HoodieFileReader' to 'org.apache.hudi.io.storage.HoodieFileReader'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileReaderFactory.java`
#### Snippet
```java
  @Override
  public HoodieFileReader newBootstrapFileReader(HoodieFileReader skeletonFileReader, HoodieFileReader dataFileReader, Option<String[]> partitionFields, Object[] partitionValues) {
    return new HoodieSparkBootstrapFileReader(skeletonFileReader, dataFileReader, partitionFields, partitionValues);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.parquet.hadoop.ParquetReader' to 'org.apache.parquet.hadoop.ParquetReader'. Reason: 'ParquetReader.builder((ReadSupport) new ParquetReadSupport(), path) .withConf(conf)' has raw type, so result of build is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkParquetReader.java`
#### Snippet
```java
    conf.setBoolean(SQLConf.PARQUET_BINARY_AS_STRING().key(), (Boolean) SQLConf.get().getConf(SQLConf.PARQUET_BINARY_AS_STRING()));
    conf.setBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP().key(), (Boolean) SQLConf.get().getConf(SQLConf.PARQUET_INT96_AS_TIMESTAMP()));
    ParquetReader<InternalRow> reader = ParquetReader.<InternalRow>builder((ReadSupport) new ParquetReadSupport(), path)
        .withConf(conf)
        .build();
    ParquetReaderIterator<InternalRow> parquetReaderIterator = new ParquetReaderIterator<>(reader);
    readerIterators.add(parquetReaderIterator);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.parquet.hadoop.api.ReadSupport' to 'org.apache.parquet.hadoop.api.ReadSupport'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkParquetReader.java`
#### Snippet
```java
    conf.setBoolean(SQLConf.PARQUET_BINARY_AS_STRING().key(), (Boolean) SQLConf.get().getConf(SQLConf.PARQUET_BINARY_AS_STRING()));
    conf.setBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP().key(), (Boolean) SQLConf.get().getConf(SQLConf.PARQUET_INT96_AS_TIMESTAMP()));
    ParquetReader<InternalRow> reader = ParquetReader.<InternalRow>builder((ReadSupport) new ParquetReadSupport(), path)
        .withConf(conf)
        .build();
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieSparkCompactor(BaseHoodieWriteClient\>, JavaRDD, JavaRDD\>, HoodieEngineContext)' as a member of raw type 'org.apache.hudi.client.HoodieSparkCompactor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/async/SparkAsyncCompactService.java`
#### Snippet
```java
  @Override
  protected BaseCompactor createCompactor(BaseHoodieWriteClient client) {
    return new HoodieSparkCompactor(client, this.context);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieSparkClusteringClient(BaseHoodieWriteClient\>, JavaRDD, JavaRDD\>)' as a member of raw type 'org.apache.hudi.client.HoodieSparkClusteringClient'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/async/SparkAsyncClusteringService.java`
#### Snippet
```java
  @Override
  protected BaseClusterer createClusteringClient(BaseHoodieWriteClient client) {
    return new HoodieSparkClusteringClient(client);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'table' has raw type, so result of getPartitionMetafileFormat is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java`
#### Snippet
```java
              new Path(writeConfig.getBasePath()),
              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath),
              table.getPartitionMetafileFormat());
      partitionMetadata.trySave(taskPartitionId);

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndex.java`
#### Snippet
```java
      HoodieTable hoodieTable) throws HoodieIndexException {
    return HoodieJavaRDD.of(tagLocation(
        HoodieJavaRDD.getJavaRDD(records.map(record -> (HoodieRecord<T>) record)), context, hoodieTable)
        .map(r -> (HoodieRecord<R>) r));
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndex.java`
#### Snippet
```java
    return HoodieJavaRDD.of(tagLocation(
        HoodieJavaRDD.getJavaRDD(records.map(record -> (HoodieRecord<T>) record)), context, hoodieTable)
        .map(r -> (HoodieRecord<R>) r));
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.hudi.common.util.collection.Pair'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java`
#### Snippet
```java
  @Override
  public int getPartition(Object key) {
    final Pair<HoodieFileGroupId, String> parts = (Pair<HoodieFileGroupId, String>) key;
    // TODO replace w/ more performant hash
    final long hashOfKey = NumericUtils.getMessageDigestHash("MD5", parts.getRight());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
  public List<WriteStatus> deletePartitions(List<String> partitions, String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.DELETE_PARTITION, Option.ofNullable(instantTime));
    preWrite(instantTime, WriteOperationType.DELETE_PARTITION, table.getMetaClient());
    HoodieWriteMetadata<List<WriteStatus>> result = table.deletePartitions(context, instantTime, partitions);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
    // create the write handle if not exists
    HoodieWriteMetadata<List<WriteStatus>> result;
    try (AutoCloseableWriteHandle closeableHandle = new AutoCloseableWriteHandle(records, instantTime, table, true)) {
      result = ((HoodieFlinkTable<T>) table).insertOverwriteTable(context, closeableHandle.getWriteHandle(), instantTime, records);
    }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieFlinkTable'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
    HoodieWriteMetadata<List<WriteStatus>> result;
    try (AutoCloseableWriteHandle closeableHandle = new AutoCloseableWriteHandle(records, instantTime, table, true)) {
      result = ((HoodieFlinkTable<T>) table).insertOverwriteTable(context, closeableHandle.getWriteHandle(), instantTime, records);
    }
    return postWrite(result, instantTime, table);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
      List<HoodieRecord<T>> records, final String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.INSERT_OVERWRITE, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT_OVERWRITE, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
    // only used for metadata table, the upsert happens in single thread
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.UPSERT, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.UPSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.INSERT, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.HoodieTable', required 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java

  @Override
  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf, HoodieTableMetaClient metaClient) {
    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context, metaClient);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
  public List<WriteStatus> delete(List<HoodieKey> keys, String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.DELETE, Option.ofNullable(instantTime));
    preWrite(instantTime, WriteOperationType.DELETE, table.getMetaClient());
    HoodieWriteMetadata<List<WriteStatus>> result = table.delete(context, instantTime, keys);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
  public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTime) {
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.UPSERT, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.UPSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.HoodieTable', required 'org.apache.hudi.table.HoodieTable\>,java.util.List,java.util.List\>'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java

  @Override
  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf) {
    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieDeleteHelper.newInstance()' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeleteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'execute(String, HoodieData, HoodieEngineContext, HoodieWriteConfig, HoodieTable\>, HoodieData, HoodieData\>, BaseCommitActionExecutor\>, HoodieData, HoodieData, R\>)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieDeleteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeleteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertPreppedCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(preppedInputRecordRdd, instantTime, table, config,
          this, false, userDefinedBulkInsertPartitioner);
    } catch (Throwable e) {
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, BaseCommitActionExecutor\>, HoodieData, HoodieData, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertPreppedCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(preppedInputRecordRdd, instantTime, table, config,
          this, false, userDefinedBulkInsertPartitioner);
    } catch (Throwable e) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(inputRecordsRDD, instantTime, table, config,
          this, true, bulkInsertPartitioner);
    } catch (HoodieInsertException ie) {
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, BaseCommitActionExecutor\>, HoodieData, HoodieData, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(inputRecordsRDD, instantTime, table, config,
          this, true, bulkInsertPartitioner);
    } catch (HoodieInsertException ie) {
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAppendHandle(HoodieWriteConfig, String, HoodieTable, String, String, Iterator\>, TaskContextSupplier, ...)' as a member of raw type 'org.apache.hudi.io.HoodieAppendHandle'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
                                                          Map<String, HoodieRecord<?>> recordMap,
                                                          Map<HoodieLogBlock.HeaderMetadataType, String> header) {
    HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,
        partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);
    appendHandle.write(recordMap);
```

### UNCHECKED_WARNING
Unchecked call to 'write(Map\>)' as a member of raw type 'org.apache.hudi.io.HoodieAppendHandle'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
    HoodieAppendHandle appendHandle = new HoodieAppendHandle(config, instantTime, this,
        partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);
    appendHandle.write(recordMap);
    List<WriteStatus> writeStatuses = appendHandle.close();
    return Collections.singletonList(writeStatuses).iterator();
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'appendHandle' has raw type, so result of close is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
        partitionPath, fileId, recordMap.values().iterator(), taskContextSupplier, header);
    appendHandle.write(recordMap);
    List<WriteStatus> writeStatuses = appendHandle.close();
    return Collections.singletonList(writeStatuses).iterator();
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new SparkBulkInsertPreppedDeltaCommitActionExecutor((HoodieSparkEngineContext) context, config, this, instantTime, preppedRecords, userDefinedBulkInsertPartitioner)' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context, String instantTime,
      HoodieData<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
    return new SparkBulkInsertPreppedDeltaCommitActionExecutor((HoodieSparkEngineContext) context, config,
        this, instantTime, preppedRecords, userDefinedBulkInsertPartitioner).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'SparkBulkInsertPreppedDeltaCommitActionExecutor(HoodieSparkEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieData\>, Option)' as a member of raw type 'org.apache.hudi.table.action.deltacommit.SparkBulkInsertPreppedDeltaCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context, String instantTime,
      HoodieData<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
    return new SparkBulkInsertPreppedDeltaCommitActionExecutor((HoodieSparkEngineContext) context, config,
        this, instantTime, preppedRecords, userDefinedBulkInsertPartitioner).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'ScheduleCompactionActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, Option\>, WriteOperationType)' as a member of raw type 'org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {
    ScheduleCompactionActionExecutor scheduleCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.COMPACT);
    return scheduleCompactionExecutor.execute();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'scheduleCompactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
    ScheduleCompactionActionExecutor scheduleCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.COMPACT);
    return scheduleCompactionExecutor.execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'ScheduleCompactionActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, Option\>, WriteOperationType)' as a member of raw type 'org.apache.hudi.table.action.compact.ScheduleCompactionActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
  @Override
  public Option<HoodieCompactionPlan> scheduleLogCompaction(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {
    ScheduleCompactionActionExecutor scheduleLogCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.LOG_COMPACT);
    return scheduleLogCompactionExecutor.execute();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'scheduleLogCompactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
    ScheduleCompactionActionExecutor scheduleLogCompactionExecutor = new ScheduleCompactionActionExecutor(
        context, config, this, instantTime, extraMetadata, WriteOperationType.LOG_COMPACT);
    return scheduleLogCompactionExecutor.execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'logCompactionExecutor' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkMergeOnReadTable.java`
#### Snippet
```java
    RunCompactionActionExecutor logCompactionExecutor = new RunCompactionActionExecutor(context, config, this,
        logCompactionInstantTime, new HoodieSparkMergeOnReadTableCompactor<>(), this, WriteOperationType.LOG_COMPACT);
    return logCompactionExecutor.execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkUpsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkUpsertCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new SparkBulkInsertPreppedCommitActionExecutor((HoodieSparkEngineContext) context, config, this, instantTime, preppedRecords, userDefinedBulkInsertPartitioner)' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context, String instantTime,
      HoodieData<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
    return new SparkBulkInsertPreppedCommitActionExecutor((HoodieSparkEngineContext) context, config,
        this, instantTime, preppedRecords, userDefinedBulkInsertPartitioner).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'SparkBulkInsertPreppedCommitActionExecutor(HoodieSparkEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieData\>, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertPreppedCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> bulkInsertPrepped(HoodieEngineContext context, String instantTime,
      HoodieData<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
    return new SparkBulkInsertPreppedCommitActionExecutor((HoodieSparkEngineContext) context, config,
        this, instantTime, preppedRecords, userDefinedBulkInsertPartitioner).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieCreateHandle(HoodieWriteConfig, String, HoodieTable, String, String, Map\>, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.HoodieCreateHandle'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
      Map<String, HoodieRecord<?>> recordMap) {
    HoodieCreateHandle<?, ?, ?, ?> createHandle =
        new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordMap, taskContextSupplier);
    createHandle.write();
    return Collections.singletonList(createHandle.close()).iterator();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'new SparkInsertOverwriteTableCommitActionExecutor(context, config, this, instantTime, records)' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> insertOverwriteTable(HoodieEngineContext context, String instantTime, HoodieData<HoodieRecord<T>> records) {
    return new SparkInsertOverwriteTableCommitActionExecutor(context, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieData\>)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkInsertOverwriteTableCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> insertOverwriteTable(HoodieEngineContext context, String instantTime, HoodieData<HoodieRecord<T>> records) {
    return new SparkInsertOverwriteTableCommitActionExecutor(context, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'runMerge(HoodieTable, HoodieMergeHandle)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieMergeHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
        upsertHandle.setPartitionValues(partitionValues);
      }
      HoodieMergeHelper.newInstance().runMerge(this, upsertHandle);
    }

```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.action.HoodieWriteMetadata', required 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java

  @Override
  public HoodieWriteMetadata insertOverwrite(HoodieEngineContext context, String instantTime, HoodieData<HoodieRecord<T>> records) {
    return new SparkInsertOverwriteCommitActionExecutor(context, config, this, instantTime, records).execute();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'SparkInsertOverwriteCommitActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieData\>)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkInsertOverwriteCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata insertOverwrite(HoodieEngineContext context, String instantTime, HoodieData<HoodieRecord<T>> records) {
    return new SparkInsertOverwriteCommitActionExecutor(context, config, this, instantTime, records).execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.bootstrap.HoodieBootstrapWriteMetadata' to 'org.apache.hudi.table.action.bootstrap.HoodieBootstrapWriteMetadata\>'. Reason: 'new SparkBootstrapCommitActionExecutor((HoodieSparkEngineContext) context, config, this, extraMetadata)' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public HoodieBootstrapWriteMetadata<HoodieData<WriteStatus>> bootstrap(HoodieEngineContext context, Option<Map<String, String>> extraMetadata) {
    return new SparkBootstrapCommitActionExecutor((HoodieSparkEngineContext) context, config, this, extraMetadata).execute();
  }

```

### UNCHECKED_WARNING
Unchecked call to 'SparkBootstrapCommitActionExecutor(HoodieSparkEngineContext, HoodieWriteConfig, HoodieTable, Option\>)' as a member of raw type 'org.apache.hudi.table.action.bootstrap.SparkBootstrapCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/HoodieSparkCopyOnWriteTable.java`
#### Snippet
```java
  @Override
  public HoodieBootstrapWriteMetadata<HoodieData<WriteStatus>> bootstrap(HoodieEngineContext context, Option<Map<String, String>> extraMetadata) {
    return new SparkBootstrapCommitActionExecutor((HoodieSparkEngineContext) context, config, this, extraMetadata).execute();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
    switch (btype) {
      case INSERT:
        return handleInsert(binfo.fileIdPrefix, recordItr);
      default:
        throw new AssertionError("Expect INSERT bucketType for insert overwrite, please correct the logical of " + partitioner.getClass().getName());
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
    switch (btype) {
      case INSERT:
        return handleInsert(binfo.fileIdPrefix, recordItr);
      default:
        throw new AssertionError("Expect INSERT bucketType for insert overwrite, please correct the logical of " + partitioner.getClass().getName());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.hudi.common.data.HoodieData\>'. Reason: 'HoodieWriteHelper.newInstance()' has raw type, so result of combineOnCondition is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java`
#### Snippet
```java

    if (performDedupe) {
      dedupedRecords = (HoodieData<HoodieRecord<T>>) HoodieWriteHelper.newInstance()
          .combineOnCondition(config.shouldCombineBeforeInsert(), inputRecords, targetParallelism, table);
    }

```

### UNCHECKED_WARNING
Unchecked call to 'combineOnCondition(boolean, I, int, HoodieTable)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java`
#### Snippet
```java

    if (performDedupe) {
      dedupedRecords = (HoodieData<HoodieRecord<T>>) HoodieWriteHelper.newInstance()
          .combineOnCondition(config.shouldCombineBeforeInsert(), inputRecords, targetParallelism, table);
    }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.spark.api.java.JavaRDD\>'. Reason: 'partitioner' has raw type, so result of repartitionRecords is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java`
#### Snippet
```java
    // only JavaRDD is supported for Spark partitioner, but it is not enforced by BulkInsertPartitioner API. To improve this, TODO HUDI-3463
    final HoodieData<HoodieRecord<T>> repartitionedRecords =
        HoodieJavaRDD.of((JavaRDD<HoodieRecord<T>>) partitioner.repartitionRecords(HoodieJavaRDD.getJavaRDD(dedupedRecords), targetParallelism));

    JavaRDD<WriteStatus> writeStatusRDD = HoodieJavaRDD.getJavaRDD(repartitionedRecords)
```

### UNCHECKED_WARNING
Unchecked call to 'repartitionRecords(I, int)' as a member of raw type 'org.apache.hudi.table.BulkInsertPartitioner'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java`
#### Snippet
```java
    // only JavaRDD is supported for Spark partitioner, but it is not enforced by BulkInsertPartitioner API. To improve this, TODO HUDI-3463
    final HoodieData<HoodieRecord<T>> repartitionedRecords =
        HoodieJavaRDD.of((JavaRDD<HoodieRecord<T>>) partitioner.repartitionRecords(HoodieJavaRDD.getJavaRDD(dedupedRecords), targetParallelism));

    JavaRDD<WriteStatus> writeStatusRDD = HoodieJavaRDD.getJavaRDD(repartitionedRecords)
```

### UNCHECKED_WARNING
Unchecked call to 'updateIndexAndCommitIfNeeded(HoodieData, HoodieWriteMetadata\>)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java`
#### Snippet
```java

    // Update index
    ((BaseSparkCommitActionExecutor) executor).updateIndexAndCommitIfNeeded(writeStatuses, result);
    return result;
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java`
#### Snippet
```java
    // Update index
    ((BaseSparkCommitActionExecutor) executor).updateIndexAndCommitIfNeeded(writeStatuses, result);
    return result;
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'scala.Tuple2\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBucketIndexPartitioner.java`
#### Snippet
```java
  @Override
  public int getPartition(Object key) {
    Tuple2<HoodieKey, Option<HoodieRecordLocation>> keyLocation = (Tuple2<HoodieKey, Option<HoodieRecordLocation>>) key;
    String partitionPath = keyLocation._1.getPartitionPath();
    Option<HoodieRecordLocation> location = keyLocation._2;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.deltacommit.SparkBulkInsertDeltaCommitActionExecutor' to 'org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  protected BaseSparkCommitActionExecutor<T> getBulkInsertActionExecutor(HoodieData<HoodieRecord> inputRecordsRDD) {
    return new SparkBulkInsertDeltaCommitActionExecutor(
        (HoodieSparkEngineContext) context,
        new HoodieWriteConfig.Builder()
```

### UNCHECKED_WARNING
Unchecked call to 'SparkBulkInsertDeltaCommitActionExecutor(HoodieSparkEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieData\>, Option, Option\>)' as a member of raw type 'org.apache.hudi.table.action.deltacommit.SparkBulkInsertDeltaCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  protected BaseSparkCommitActionExecutor<T> getBulkInsertActionExecutor(HoodieData<HoodieRecord> inputRecordsRDD) {
    return new SparkBulkInsertDeltaCommitActionExecutor(
        (HoodieSparkEngineContext) context,
        new HoodieWriteConfig.Builder()
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'scala.Tuple2\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java`
#### Snippet
```java
  public int getPartition(Object key) {
    Tuple2<HoodieKey, Option<HoodieRecordLocation>> keyLocation =
        (Tuple2<HoodieKey, Option<HoodieRecordLocation>>) key;
    if (keyLocation._2().isPresent()) {
      HoodieRecordLocation location = keyLocation._2().get();
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieBootstrapHandle(HoodieWriteConfig, String, HoodieTable, String, String, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.HoodieBootstrapHandle'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/BaseBootstrapMetadataHandler.java`
#### Snippet
```java
  public BootstrapWriteStatus runMetadataBootstrap(String srcPartitionPath, String partitionPath, KeyGeneratorInterface keyGenerator) {
    Path sourceFilePath = FileStatusUtils.toPath(srcFileStatus.getPath());
    HoodieBootstrapHandle<?, ?, ?, ?> bootstrapHandle = new HoodieBootstrapHandle(config, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS,
        table, partitionPath, FSUtils.createNewFileIdPfx(), table.getTaskContextSupplier());
    try {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(inputRecordsRDD, instantTime, table, config,
          this, true, bulkInsertPartitioner);
    } catch (HoodieInsertException ie) {
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, BaseCommitActionExecutor\>, HoodieData, HoodieData, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(inputRecordsRDD, instantTime, table, config,
          this, true, bulkInsertPartitioner);
    } catch (HoodieInsertException ie) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieDeleteHelper.newInstance()' has raw type, so result of execute is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkDeleteDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'execute(String, HoodieData, HoodieEngineContext, HoodieWriteConfig, HoodieTable\>, HoodieData, HoodieData\>, BaseCommitActionExecutor\>, HoodieData, HoodieData, R\>)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieDeleteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkDeleteDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieDeleteHelper.newInstance().execute(instantTime, keys, context, config, table, this);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/OrcBootstrapMetadataHandler.java`
#### Snippet
```java
            gr.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, recKey);
            BootstrapRecordPayload payload = new BootstrapRecordPayload(gr);
            HoodieRecord rec = new HoodieAvroRecord(new HoodieKey(recKey, partitionPath), payload);
            return rec;
          }, table.getPreExecuteRunnable());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertPreppedDeltaCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(preppedInputRecordRdd, instantTime, table, config,
          this, false, bulkInsertPartitioner);
    } catch (Throwable e) {
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, BaseCommitActionExecutor\>, HoodieData, HoodieData, R\>, boolean, Option)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkBulkInsertPreppedDeltaCommitActionExecutor.java`
#### Snippet
```java
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    try {
      return SparkBulkInsertHelper.newInstance().bulkInsert(preppedInputRecordRdd, instantTime, table, config,
          this, false, bulkInsertPartitioner);
    } catch (Throwable e) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(),this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkUpsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeUpsert(), config.getUpsertShuffleParallelism(),this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'HoodieWriteHelper.newInstance()' has raw type, so result of write is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkInsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(),this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'write(String, I, HoodieEngineContext, HoodieTable, boolean, int, BaseCommitActionExecutor, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.BaseWriteHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/deltacommit/SparkInsertDeltaCommitActionExecutor.java`
#### Snippet
```java
  @Override
  public HoodieWriteMetadata<HoodieData<WriteStatus>> execute() {
    return HoodieWriteHelper.newInstance().write(instantTime, inputRecordsRDD, context, table,
        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(),this, operationType);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'getRecordKey(Schema, Option)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/ParquetBootstrapMetadataHandler.java`
#### Snippet
```java
    try {
      Function<HoodieRecord, HoodieRecord> transformer = record -> {
        String recordKey = record.getRecordKey(schema, Option.of(keyGenerator));
        return createNewMetadataBootstrapRecord(recordKey, partitionPath, recordMerger.getRecordType())
            // NOTE: Record have to be cloned here to make sure if it holds low-level engine-specific
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'. Reason: 'reader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/ParquetBootstrapMetadataHandler.java`
#### Snippet
```java
            .copy();
      };
      ClosableIterator<HoodieRecord> recordIterator = reader.getRecordIterator(schema);
      executor = ExecutorFactory.create(config, recordIterator,
          new BootstrapRecordConsumer(bootstrapHandle), transformer, table.getPreExecuteRunnable());
```

### UNCHECKED_WARNING
Unchecked call to 'runMerge(HoodieTable, HoodieMergeHandle)' as a member of raw type 'org.apache.hudi.table.action.commit.HoodieMergeHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java`
#### Snippet
```java
      }

      HoodieMergeHelper.newInstance().runMerge(table, upsertHandle);
    }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java`
#### Snippet
```java
                                       WriteOperationType operationType,
                                       Option<Map<String, String>> extraMetadata) {
    super(context, config, table, instantTime, operationType, extraMetadata);
    try {
      keyGeneratorOpt = config.populateMetaFields()
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.hudi.table.action.cluster.strategy.UpdateStrategy\>\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java`
#### Snippet
```java
    }

    UpdateStrategy<T, HoodieData<HoodieRecord<T>>> updateStrategy = (UpdateStrategy<T, HoodieData<HoodieRecord<T>>>) ReflectionUtils
        .loadClass(config.getClusteringUpdatesStrategyClass(), new Class<?>[] {HoodieEngineContext.class, HoodieTable.class, Set.class},
            this.context, table, fileGroupsInPendingClustering);
    Pair<HoodieData<HoodieRecord<T>>, Set<HoodieFileGroupId>> recordsAndPendingClusteringFileGroups =
        updateStrategy.handleUpdate(inputRecords);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
            HoodieRecord currentRecord = currentBatchOfRecords.remove(0);
            if (result.getRow() == null) {
              taggedRecords.add(currentRecord);
              continue;
            }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
            if (!HoodieIndexUtils.checkIfValidCommit(completedCommitsTimeline, commitTs)) {
              // if commit is invalid, treat this as a new taggedRecord
              taggedRecords.add(currentRecord);
              continue;
            }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
            if (updatePartitionPath && !partitionPath.equals(currentRecord.getPartitionPath())) {
              // delete partition old data record
              HoodieRecord emptyRecord = new HoodieAvroRecord(new HoodieKey(currentRecord.getRecordKey(), partitionPath),
                  new EmptyHoodieRecordPayload());
              emptyRecord.unseal();
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
              emptyRecord.seal();
              // insert partition new data record
              currentRecord = new HoodieAvroRecord(new HoodieKey(currentRecord.getRecordKey(), currentRecord.getPartitionPath()),
                  (HoodieRecordPayload) currentRecord.getData());
              taggedRecords.add(emptyRecord);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
              currentRecord = new HoodieAvroRecord(new HoodieKey(currentRecord.getRecordKey(), currentRecord.getPartitionPath()),
                  (HoodieRecordPayload) currentRecord.getData());
              taggedRecords.add(emptyRecord);
              taggedRecords.add(currentRecord);
            } else {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
                  (HoodieRecordPayload) currentRecord.getData());
              taggedRecords.add(emptyRecord);
              taggedRecords.add(currentRecord);
            } else {
              currentRecord = new HoodieAvroRecord(new HoodieKey(currentRecord.getRecordKey(), partitionPath),
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
              taggedRecords.add(currentRecord);
            } else {
              currentRecord = new HoodieAvroRecord(new HoodieKey(currentRecord.getRecordKey(), partitionPath),
                  (HoodieRecordPayload) currentRecord.getData());
              currentRecord.unseal();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
              currentRecord.setCurrentLocation(new HoodieRecordLocation(commitTs, fileId));
              currentRecord.seal();
              taggedRecords.add(currentRecord);
              // the key from Result and the key being processed should be same
              assert (currentRecord.getRecordKey().contentEquals(keyFromResult));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'rec' has raw type, so result of getNewLocation is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
            for (HoodieRecord rec : writeStatus.getWrittenRecords()) {
              if (!writeStatus.isErrored(rec.getKey())) {
                Option<HoodieRecordLocation> loc = rec.getNewLocation();
                if (loc.isPresent()) {
                  if (rec.getCurrentLocation() != null) {
```

### UNCHECKED_WARNING
Unchecked call to 'validate(String, HoodieWriteMetadata, Dataset, Dataset)' as a member of raw type 'org.apache.hudi.client.validator.SparkPreCommitValidator'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/utils/SparkValidatorUtils.java`
#### Snippet
```java
    return CompletableFuture.supplyAsync(() -> {
      try {
        validator.validate(instantTime, writeMetadata, beforeState, afterState);
        LOG.info("validation complete for " + validator.getClass().getName());
        return true;
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'inputProvider' has raw type, so result of generateInputRecords is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
            properties, context);
    JavaRDD<HoodieRecord> inputRecordsRDD =
        (JavaRDD<HoodieRecord>) inputProvider.generateInputRecords("bootstrap_source", config.getBootstrapSourceBasePath(),
            partitionFilesList, config);
    // Start Full Bootstrap
    String bootstrapInstantTime = HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS;
```

### UNCHECKED_WARNING
Unchecked call to 'generateInputRecords(String, String, List\>\>, HoodieWriteConfig)' as a member of raw type 'org.apache.hudi.client.bootstrap.FullRecordBootstrapDataProvider'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
            properties, context);
    JavaRDD<HoodieRecord> inputRecordsRDD =
        (JavaRDD<HoodieRecord>) inputProvider.generateInputRecords("bootstrap_source", config.getBootstrapSourceBasePath(),
            partitionFilesList, config);
    // Start Full Bootstrap
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.bootstrap.HoodieBootstrapWriteMetadata' to 'org.apache.hudi.table.action.bootstrap.HoodieBootstrapWriteMetadata\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
      Option<HoodieWriteMetadata<HoodieData<WriteStatus>>> fullBootstrapResult = fullBootstrap(partitionSelections.get(FULL_RECORD));

      return new HoodieBootstrapWriteMetadata(metadataResult, fullBootstrapResult);
    } catch (IOException ioe) {
      throw new HoodieIOException(ioe.getMessage(), ioe);
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieBootstrapWriteMetadata(Option\>, Option\>)' as a member of raw type 'org.apache.hudi.table.action.bootstrap.HoodieBootstrapWriteMetadata'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
      Option<HoodieWriteMetadata<HoodieData<WriteStatus>>> fullBootstrapResult = fullBootstrap(partitionSelections.get(FULL_RECORD));

      return new HoodieBootstrapWriteMetadata(metadataResult, fullBootstrapResult);
    } catch (IOException ioe) {
      throw new HoodieIOException(ioe.getMessage(), ioe);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
            .withWriteStatusClass(BootstrapWriteStatus.class)
            .withBulkInsertParallelism(config.getBootstrapParallelism()).build(),
        table,
        HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS,
        WriteOperationType.BOOTSTRAP,
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.commit.SparkBulkInsertCommitActionExecutor' to 'org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java

  protected BaseSparkCommitActionExecutor<T> getBulkInsertActionExecutor(HoodieData<HoodieRecord> inputRecordsRDD) {
    return new SparkBulkInsertCommitActionExecutor((HoodieSparkEngineContext) context, new HoodieWriteConfig.Builder().withProps(config.getProps())
        .withSchema(bootstrapSchema).build(), table, HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS,
        inputRecordsRDD, Option.empty(), extraMetadata);
```

### UNCHECKED_WARNING
Unchecked call to 'SparkBulkInsertCommitActionExecutor(HoodieSparkEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieData\>, Option, Option\>)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertCommitActionExecutor'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java

  protected BaseSparkCommitActionExecutor<T> getBulkInsertActionExecutor(HoodieData<HoodieRecord> inputRecordsRDD) {
    return new SparkBulkInsertCommitActionExecutor((HoodieSparkEngineContext) context, new HoodieWriteConfig.Builder().withProps(config.getProps())
        .withSchema(bootstrapSchema).build(), table, HoodieTimeline.FULL_BOOTSTRAP_INSTANT_TS,
        inputRecordsRDD, Option.empty(), extraMetadata);
```

### UNCHECKED_WARNING
Unchecked call to 'ifPresent(Consumer)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDTableServiceClient.java`
#### Snippet
```java
    // Do not do any conflict resolution here as we do with regular writes. We take the lock here to ensure all writes to metadata table happens within a
    // single lock (single writer). Because more than one write to metadata table will result in conflicts since all of them updates the same partition.
    table.getMetadataWriter(hoodieInstant.getTimestamp())
        .ifPresent(writer -> ((HoodieTableMetadataWriter) writer).update(commitMetadata, hoodieInstant.getTimestamp(), isTableServiceAction));
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkConsistentBucketClusteringExecutionStrategy.java`
#### Snippet
```java
    }

    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance()
        .bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig, false, partitioner, true, numOutputGroups);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, boolean, BulkInsertPartitioner, boolean, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkConsistentBucketClusteringExecutionStrategy.java`
#### Snippet
```java
    }

    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance()
        .bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig, false, partitioner, true, numOutputGroups);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'javaSparkContext.parallelize(data, parallelism).mapToPair(input -\> { Pair pair = func.call(input); return new Tuple2(pair.getLeft(), pair.getRight()); })' has raw type, so result of collectAsMap is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java`
#### Snippet
```java
  public <I, K, V> Map<K, V> mapToPair(List<I> data, SerializablePairFunction<I, K, V> func, Integer parallelism) {
    if (Objects.nonNull(parallelism)) {
      return javaSparkContext.parallelize(data, parallelism).mapToPair(input -> {
        Pair<K, V> pair = func.call(input);
        return new Tuple2(pair.getLeft(), pair.getRight());
      }).collectAsMap();
    } else {
      return javaSparkContext.parallelize(data).mapToPair(input -> {
```

### UNCHECKED_WARNING
Unchecked assignment: 'scala.Tuple2' to 'scala.Tuple2'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java`
#### Snippet
```java
      return javaSparkContext.parallelize(data, parallelism).mapToPair(input -> {
        Pair<K, V> pair = func.call(input);
        return new Tuple2(pair.getLeft(), pair.getRight());
      }).collectAsMap();
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'Tuple2(T1, T2)' as a member of raw type 'scala.Tuple2'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java`
#### Snippet
```java
      return javaSparkContext.parallelize(data, parallelism).mapToPair(input -> {
        Pair<K, V> pair = func.call(input);
        return new Tuple2(pair.getLeft(), pair.getRight());
      }).collectAsMap();
    } else {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'javaSparkContext.parallelize(data).mapToPair(input -\> { Pair pair = func.call(input); return new Tuple2(pair.getLeft(), pair.getRight()); })' has raw type, so result of collectAsMap is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java`
#### Snippet
```java
      }).collectAsMap();
    } else {
      return javaSparkContext.parallelize(data).mapToPair(input -> {
        Pair<K, V> pair = func.call(input);
        return new Tuple2(pair.getLeft(), pair.getRight());
      }).collectAsMap();
    }
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'scala.Tuple2' to 'scala.Tuple2'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java`
#### Snippet
```java
      return javaSparkContext.parallelize(data).mapToPair(input -> {
        Pair<K, V> pair = func.call(input);
        return new Tuple2(pair.getLeft(), pair.getRight());
      }).collectAsMap();
    }
```

### UNCHECKED_WARNING
Unchecked call to 'Tuple2(T1, T2)' as a member of raw type 'scala.Tuple2'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/common/HoodieSparkEngineContext.java`
#### Snippet
```java
      return javaSparkContext.parallelize(data).mapToPair(input -> {
        Pair<K, V> pair = func.call(input);
        return new Tuple2(pair.getLeft(), pair.getRight());
      }).collectAsMap();
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java

  public HoodieWriteResult deletePartitions(List<String> partitions, String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table = initTable(WriteOperationType.DELETE_PARTITION, Option.ofNullable(instantTime));
    preWrite(instantTime, WriteOperationType.DELETE_PARTITION, table.getMetaClient());
    HoodieWriteMetadata<HoodieData<WriteStatus>> result = table.deletePartitions(context, instantTime, partitions);
```

### UNCHECKED_WARNING
Unchecked call to 'bootstrap(HoodieEngineContext, Option\>)' as a member of raw type 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  @Override
  public void bootstrap(Option<Map<String, String>> extraMetadata) {
    initTable(WriteOperationType.UPSERT, Option.ofNullable(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS)).bootstrap(context, extraMetadata);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public JavaRDD<WriteStatus> insert(JavaRDD<HoodieRecord<T>> records, String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table =
        initTable(WriteOperationType.INSERT, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table =
        initTable(WriteOperationType.UPSERT, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.UPSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.HoodieTable', required 'org.apache.hudi.table.HoodieTable\>,org.apache.spark.api.java.JavaRDD,org.apache.spark.api.java.JavaRDD\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java

  @Override
  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf) {
    return HoodieSparkTable.create(config, context);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
   */
  public HoodieWriteResult insertOverwrite(JavaRDD<HoodieRecord<T>> records, final String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table = initTable(WriteOperationType.INSERT_OVERWRITE, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT_OVERWRITE, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  @Override
  public JavaRDD<WriteStatus> delete(JavaRDD<HoodieKey> keys, String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table = initTable(WriteOperationType.DELETE, Option.ofNullable(instantTime));
    preWrite(instantTime, WriteOperationType.DELETE, table.getMetaClient());
    HoodieWriteMetadata<HoodieData<WriteStatus>> result = table.delete(context,instantTime, HoodieJavaRDD.of(keys));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public JavaRDD<WriteStatus> bulkInsertPreppedRecords(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime, Option<BulkInsertPartitioner> bulkInsertPartitioner) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table =
        initTable(WriteOperationType.BULK_INSERT_PREPPED, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.BULK_INSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public JavaRDD<WriteStatus> bulkInsert(JavaRDD<HoodieRecord<T>> records, String instantTime, Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table =
        initTable(WriteOperationType.BULK_INSERT, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.BULK_INSERT, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public JavaRDD<WriteStatus> insertPreppedRecords(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table =
        initTable(WriteOperationType.INSERT_PREPPED, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
  public JavaRDD<WriteStatus> upsertPreppedRecords(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table =
        initTable(WriteOperationType.UPSERT_PREPPED, Option.ofNullable(instantTime));
    table.validateUpsertSchema();
    preWrite(instantTime, WriteOperationType.UPSERT_PREPPED, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java
   */
  public HoodieWriteResult insertOverwriteTable(JavaRDD<HoodieRecord<T>> records, final String instantTime) {
    HoodieTable<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>> table = initTable(WriteOperationType.INSERT_OVERWRITE_TABLE, Option.ofNullable(instantTime));
    table.validateInsertSchema();
    preWrite(instantTime, WriteOperationType.INSERT_OVERWRITE_TABLE, table.getMetaClient());
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.table.HoodieTable', required 'org.apache.hudi.table.HoodieTable\>,org.apache.spark.api.java.JavaRDD,org.apache.spark.api.java.JavaRDD\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java`
#### Snippet
```java

  @Override
  protected HoodieTable createTable(HoodieWriteConfig config, Configuration hadoopConf, HoodieTableMetaClient metaClient) {
    return HoodieSparkTable.create(config, context, metaClient);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/ExecutionStrategyUtil.java`
#### Snippet
```java

    HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
    return hoodieRecord;
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/ExecutionStrategyUtil.java`
#### Snippet
```java
    HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
    return hoodieRecord;
  }
}
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSortAndSizeExecutionStrategy.java`
#### Snippet
```java
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringMaxBytesInGroup()));

    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(),
        newConfig, false, getRDDPartitioner(strategyParams, schema), true, numOutputGroups, new CreateHandleFactory(shouldPreserveHoodieMetadata));
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, boolean, BulkInsertPartitioner, boolean, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSortAndSizeExecutionStrategy.java`
#### Snippet
```java
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(getWriteConfig().getClusteringMaxBytesInGroup()));

    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(),
        newConfig, false, getRDDPartitioner(strategyParams, schema), true, numOutputGroups, new CreateHandleFactory(shouldPreserveHoodieMetadata));
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData'. Reason: 'SparkBulkInsertHelper.newInstance()' has raw type, so result of bulkInsert is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSingleFileSortExecutionStrategy.java`
#### Snippet
```java
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(Long.MAX_VALUE));

    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig,
        false, getRDDPartitioner(strategyParams, schema), true, numOutputGroups, new SingleFileHandleCreateFactory(fileGroupIdList.get(0).getFileId(), shouldPreserveHoodieMetadata));
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(HoodieData\>, String, HoodieTable\>, HoodieData, HoodieData\>, HoodieWriteConfig, boolean, BulkInsertPartitioner, boolean, ...)' as a member of raw type 'org.apache.hudi.table.action.commit.SparkBulkInsertHelper'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSingleFileSortExecutionStrategy.java`
#### Snippet
```java
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(Long.MAX_VALUE));

    return (HoodieData<WriteStatus>) SparkBulkInsertHelper.newInstance().bulkInsert(inputRecords, instantTime, getHoodieTable(), newConfig,
        false, getRDDPartitioner(strategyParams, schema), true, numOutputGroups, new SingleFileHandleCreateFactory(fileGroupIdList.get(0).getFileId(), shouldPreserveHoodieMetadata));
  }
```

### UNCHECKED_WARNING
Unchecked call to 'CloseableMappingIterator(ClosableIterator*, Function)' as a member of raw type 'org.apache.hudi.common.util.collection.CloseableMappingIterator'*
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java`
#### Snippet
```java
          //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
          //       it since these records will be shuffled later.
          CloseableMappingIterator mappingIterator = new CloseableMappingIterator((ClosableIterator<HoodieRecord>) baseFileReader.getRecordIterator(readerSchema),
              rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema,
                  getWriteConfig().getProps(), keyGeneratorOp));
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'. Reason: 'baseFileReader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java`
#### Snippet
```java
          //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
          //       it since these records will be shuffled later.
          CloseableMappingIterator mappingIterator = new CloseableMappingIterator((ClosableIterator<HoodieRecord>) baseFileReader.getRecordIterator(readerSchema),
              rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema,
                  getWriteConfig().getProps(), keyGeneratorOp));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.CloseableMappingIterator' to 'java.util.Iterator\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java`
#### Snippet
```java
              rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema,
                  getWriteConfig().getProps(), keyGeneratorOp));
          return mappingIterator;
        } catch (IOException e) {
          throw new HoodieClusteringException("Error reading input data for " + clusteringOp.getDataFilePath()
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java`
#### Snippet
```java
    // Produce records tagged with new record location
    List<String> indexKeyFields = Arrays.asList(table.getConfig().getBucketIndexHashField().split(","));
    HoodieData<HoodieRecord<T>> redirectedRecordsRDD = filteredRecordsRDD.map(r -> {
      ConsistentHashingNode node = partitionToIdentifier.get(r.getPartitionPath()).getBucket(r.getKey(), indexKeyFields);
      return getTaggedRecord(new HoodieAvroRecord(r.getKey(), r.getData(), r.getOperation()),
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java`
#### Snippet
```java
    HoodieData<HoodieRecord<T>> redirectedRecordsRDD = filteredRecordsRDD.map(r -> {
      ConsistentHashingNode node = partitionToIdentifier.get(r.getPartitionPath()).getBucket(r.getKey(), indexKeyFields);
      return getTaggedRecord(new HoodieAvroRecord(r.getKey(), r.getData(), r.getOperation()),
          Option.ofNullable(new HoodieRecordLocation(partitionToInstant.get(r.getPartitionPath()), FSUtils.createNewFileId(node.getFileIdPrefix(), 0))));
    });
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieAvroRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java`
#### Snippet
```java
    HoodieData<HoodieRecord<T>> redirectedRecordsRDD = filteredRecordsRDD.map(r -> {
      ConsistentHashingNode node = partitionToIdentifier.get(r.getPartitionPath()).getBucket(r.getKey(), indexKeyFields);
      return getTaggedRecord(new HoodieAvroRecord(r.getKey(), r.getData(), r.getOperation()),
          Option.ofNullable(new HoodieRecordLocation(partitionToInstant.get(r.getPartitionPath()), FSUtils.createNewFileId(node.getFileIdPrefix(), 0))));
    });
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T, HoodieOperation)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java`
#### Snippet
```java
    HoodieData<HoodieRecord<T>> redirectedRecordsRDD = filteredRecordsRDD.map(r -> {
      ConsistentHashingNode node = partitionToIdentifier.get(r.getPartitionPath()).getBucket(r.getKey(), indexKeyFields);
      return getTaggedRecord(new HoodieAvroRecord(r.getKey(), r.getData(), r.getOperation()),
          Option.ofNullable(new HoodieRecordLocation(partitionToInstant.get(r.getPartitionPath()), FSUtils.createNewFileId(node.getFileIdPrefix(), 0))));
    });
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkConsistentBucketDuplicateUpdateStrategy.java`
#### Snippet
```java
      ConsistentHashingNode node = partitionToIdentifier.get(r.getPartitionPath()).getBucket(r.getKey(), indexKeyFields);
      return getTaggedRecord(new HoodieAvroRecord(r.getKey(), r.getData(), r.getOperation()),
          Option.ofNullable(new HoodieRecordLocation(partitionToInstant.get(r.getPartitionPath()), FSUtils.createNewFileId(node.getFileIdPrefix(), 0))));
    });

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.ArrayList' to 'java.util.List'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkConsistentBucketClusteringPlanStrategy.java`
#### Snippet
```java
    Triple<List<HoodieClusteringGroup>, Integer, List<FileSlice>> splitResult =
        buildSplitClusteringGroups(identifier, fileSlices, splitSlot);
    List<HoodieClusteringGroup> ret = new ArrayList<>(splitResult.getLeft());

    // Apply merge rule
```

### UNCHECKED_WARNING
Unchecked call to 'ArrayList(Collection)' as a member of raw type 'java.util.ArrayList'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkConsistentBucketClusteringPlanStrategy.java`
#### Snippet
```java
    Triple<List<HoodieClusteringGroup>, Integer, List<FileSlice>> splitResult =
        buildSplitClusteringGroups(identifier, fileSlices, splitSlot);
    List<HoodieClusteringGroup> ret = new ArrayList<>(splitResult.getLeft());

    // Apply merge rule
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.queue.HoodieExecutor' to 'org.apache.hudi.common.util.queue.HoodieExecutor\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java`
#### Snippet
```java
      }

      bufferedIteratorExecutor = ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(),
          getTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.execution.CopyOnWriteInsertHandler' to 'org.apache.hudi.common.util.queue.HoodieConsumer,java.lang.Object\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java`
#### Snippet
```java
      }

      bufferedIteratorExecutor = ExecutorFactory.create(hoodieConfig, inputItr, getInsertHandler(),
          getTransformer(schema, hoodieConfig), hoodieTable.getPreExecuteRunnable());

```

### UNCHECKED_WARNING
Unchecked call to 'upsertPreppedRecords(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
      }

      writeClient.upsertPreppedRecords(preppedRecordRDD, instantTime).collect();

      // reload timeline
```

### UNCHECKED_WARNING
Unchecked call to 'deletePartitions(List, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
      String actionType = CommitUtils.getCommitActionType(WriteOperationType.DELETE_PARTITION, HoodieTableType.MERGE_ON_READ);
      writeClient.startCommitWithTime(instantTime, actionType);
      writeClient.deletePartitions(partitionsToDrop, instantTime);
    }
    closeInternal();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD\[\]' to 'org.apache.spark.api.java.JavaRDD\[\]'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
  private JavaRDD<WriteStatus>[] convertStreamToArray(Stream<JavaRDD<WriteStatus>> writeStatusRDDStream) {
    Object[] writeStatusObjects = writeStatusRDDStream.toArray(Object[]::new);
    JavaRDD<WriteStatus>[] writeStatusRDDArray = new JavaRDD[writeStatusObjects.length];
    for (int i = 0; i < writeStatusObjects.length; i++) {
      writeStatusRDDArray[i] = (JavaRDD<WriteStatus>) writeStatusObjects[i];
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.spark.api.java.JavaRDD'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
    JavaRDD<WriteStatus>[] writeStatusRDDArray = new JavaRDD[writeStatusObjects.length];
    for (int i = 0; i < writeStatusObjects.length; i++) {
      writeStatusRDDArray[i] = (JavaRDD<WriteStatus>) writeStatusObjects[i];
    }
    return writeStatusRDDArray;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.BulkInsertPartitioner' to 'org.apache.hudi.table.BulkInsertPartitioner*'*
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
            .map(listStr -> listStr.split(","));

    return orderByColumnsOpt.map(orderByColumns -> {
      HoodieClusteringConfig.LayoutOptimizationStrategy layoutOptStrategy = getWriteConfig().getLayoutOptimizationStrategy();
      switch (layoutOptStrategy) {
        case ZORDER:
        case HILBERT:
          return isRowPartitioner
              ? new RowSpatialCurveSortPartitioner(getWriteConfig())
              : new RDDSpatialCurveSortPartitioner((HoodieSparkEngineContext) getEngineContext(), orderByColumns, layoutOptStrategy,
              getWriteConfig().getLayoutOptimizationCurveBuildMethod(), HoodieAvroUtils.addMetadataFields(schema), recordType);
        case LINEAR:
          return isRowPartitioner
              ? new RowCustomColumnsSortPartitioner(orderByColumns)
              : new RDDCustomColumnsSortPartitioner(orderByColumns, HoodieAvroUtils.addMetadataFields(schema),
              getWriteConfig().isConsistentLogicalTimestampEnabled());
        default:
          throw new UnsupportedOperationException(String.format("Layout optimization strategy '%s' is not supported", layoutOptStrategy));
      }
    }).orElse(isRowPartitioner
        ? BulkInsertInternalPartitionerWithRowsFactory.get(getWriteConfig(), getHoodieTable().isPartitioned(), true)
        : BulkInsertInternalPartitionerFactory.get(getHoodieTable(), getWriteConfig(), true));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.table.log.HoodieFileSliceReader' to 'java.util.Iterator\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
              ? Option.empty()
              : Option.of(getBaseOrBootstrapFileReader(hadoopConf, bootstrapBasePath, partitionFields, clusteringOp));
          recordIterators.add(getFileSliceReader(baseFileReader, scanner, readerSchema,
              tableConfig.getProps(),
              tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(),
```

### UNCHECKED_WARNING
Unchecked call to 'CloseableMappingIterator(ClosableIterator*, Function)' as a member of raw type 'org.apache.hudi.common.util.collection.CloseableMappingIterator'*
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
              //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
              //       it since these records will be shuffled later.
              CloseableMappingIterator mappingIterator = new CloseableMappingIterator(
                  (ClosableIterator<HoodieRecord>) baseFileReader.getRecordIterator(readerSchema),
                  rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema, writeConfig.getProps(), keyGeneratorOp));
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'. Reason: 'baseFileReader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
              //       it since these records will be shuffled later.
              CloseableMappingIterator mappingIterator = new CloseableMappingIterator(
                  (ClosableIterator<HoodieRecord>) baseFileReader.getRecordIterator(readerSchema),
                  rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema, writeConfig.getProps(), keyGeneratorOp));
              iteratorsForPartition.add(mappingIterator);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.CloseableMappingIterator' to 'java.util.Iterator\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/MultipleSparkJobExecutionStrategy.java`
#### Snippet
```java
                  (ClosableIterator<HoodieRecord>) baseFileReader.getRecordIterator(readerSchema),
                  rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema, writeConfig.getProps(), keyGeneratorOp));
              iteratorsForPartition.add(mappingIterator);
            } catch (IOException e) {
              throw new HoodieClusteringException("Error reading input data for " + clusteringOp.getDataFilePath()
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSpatialCurveSortPartitioner.java`
#### Snippet
```java
            String partition = record.get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();
            HoodieKey hoodieKey = new HoodieKey(key, partition);
            HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, new RewriteAvroPayload(record));
            return hoodieRecord;
          });
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSpatialCurveSortPartitioner.java`
#### Snippet
```java
      Dataset<Row> sortedDataset = reorder(sourceDataset, outputSparkPartitions);

      return sortedDataset.queryExecution().toRdd()
          .toJavaRDD()
          .map(internalRow -> {
            String key = internalRow.getString(HoodieMetadataField.RECORD_KEY_METADATA_FIELD.ordinal());
            String partition = internalRow.getString(HoodieMetadataField.PARTITION_PATH_METADATA_FIELD.ordinal());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSpatialCurveSortPartitioner.java`
#### Snippet
```java
            String partition = internalRow.getString(HoodieMetadataField.PARTITION_PATH_METADATA_FIELD.ordinal());
            HoodieKey hoodieKey = new HoodieKey(key, partition);
            return (HoodieRecord) new HoodieSparkRecord(hoodieKey, internalRow, structType, false);
          });
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
      FlatLists.ComparableList obj1 = FlatLists.ofComparableArray(t1.getColumnValues(schema.get(), sortColumns, consistentLogicalTimestampEnabled));
      FlatLists.ComparableList obj2 = FlatLists.ofComparableArray(t2.getColumnValues(schema.get(), sortColumns, consistentLogicalTimestampEnabled));
      return obj1.compareTo(obj2);
    };

```

### UNCHECKED_WARNING
Unchecked call to 'orElse(T)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
    return new SparkLazyInsertIterable<>(recordItr, areRecordsSorted, config, instantTime, hoodieTable,
        partitioner.getFileIdPfx(partition), hoodieTable.getTaskContextSupplier(), useWriterSchema,
        (WriteHandleFactory) partitioner.getWriteHandleFactory(partition).orElse(this.writeHandleFactory));
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.HashMap' to 'java.util.Map\>'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDConsistentBucketBulkInsertPartitioner.java`
#### Snippet
```java
   */
  private Map<String, Map<String, Integer>> generateFileIdPfx(Map<String, ConsistentBucketIdentifier> partitionToIdentifier) {
    Map<String, Map<String, Integer>> partitionToFileIdPfxIdxMap = new HashMap(partitionToIdentifier.size() * 2);
    int count = 0;
    for (ConsistentBucketIdentifier identifier : partitionToIdentifier.values()) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.HashMap' to 'java.util.Map'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDConsistentBucketBulkInsertPartitioner.java`
#### Snippet
```java
    int count = 0;
    for (ConsistentBucketIdentifier identifier : partitionToIdentifier.values()) {
      Map<String, Integer> fileIdPfxToIdx = new HashMap();
      for (ConsistentHashingNode node : identifier.getNodes()) {
        fileIdPfxToIdx.put(node.getFileIdPrefix(), count++);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.HoodieCreateHandle' to 'org.apache.hudi.io.HoodieWriteHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/CreateHandleFactory.java`
#### Snippet
```java
                                              final String fileIdPrefix, TaskContextSupplier taskContextSupplier) {

    return new HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, partitionPath,
        getNextFileId(fileIdPrefix), taskContextSupplier, preserveMetadata);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieCreateHandle(HoodieWriteConfig, String, HoodieTable, String, String, TaskContextSupplier, boolean)' as a member of raw type 'org.apache.hudi.io.HoodieCreateHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/CreateHandleFactory.java`
#### Snippet
```java
                                              final String fileIdPrefix, TaskContextSupplier taskContextSupplier) {

    return new HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, partitionPath,
        getNextFileId(fileIdPrefix), taskContextSupplier, preserveMetadata);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.HoodieUnboundedCreateHandle' to 'org.apache.hudi.io.HoodieWriteHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/SingleFileHandleCreateFactory.java`
#### Snippet
```java

    if (isHandleCreated.compareAndSet(false, true)) {
      return new HoodieUnboundedCreateHandle(hoodieConfig, commitTime, hoodieTable, partitionPath,
          fileId, // ignore idPfx, always use same fileId
          taskContextSupplier, preserveHoodieMetadata);
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieUnboundedCreateHandle(HoodieWriteConfig, String, HoodieTable, String, String, TaskContextSupplier, boolean)' as a member of raw type 'org.apache.hudi.io.HoodieUnboundedCreateHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/SingleFileHandleCreateFactory.java`
#### Snippet
```java

    if (isHandleCreated.compareAndSet(false, true)) {
      return new HoodieUnboundedCreateHandle(hoodieConfig, commitTime, hoodieTable, partitionPath,
          fileId, // ignore idPfx, always use same fileId
          taskContextSupplier, preserveHoodieMetadata);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator\>'. Reason: 'baseFileReader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java`
#### Snippet
```java
      HoodieFileReader baseFileReader = baseFileReaderOpt.get();
      HoodieRecordMerger recordMerger = config.getRecordMerger();
      ClosableIterator<HoodieRecord<T>> baseFileItr = baseFileReader.getRecordIterator(readerSchema);
      HoodieTableConfig tableConfig = hoodieTable.getMetaClient().getTableConfig();
      Option<Pair<String, String>> simpleKeyGenFieldsOpt =
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java`
#### Snippet
```java
          tableConfig.populateMetaFields() ? Option.empty() : Option.of(Pair.of(tableConfig.getRecordKeyFieldProp(), tableConfig.getPartitionFieldProp()));
      while (baseFileItr.hasNext()) {
        HoodieRecord<T> record = baseFileItr.next().wrapIntoHoodieRecordPayloadWithParams(readerSchema,
            config.getProps(), simpleKeyGenFieldsOpt, logRecordScanner.isWithOperationField(), logRecordScanner.getPartitionNameOverride(), false, Option.empty());
        String key = record.getRecordKey();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java`
#### Snippet
```java
            continue;
          }
          mergedRecords.add(mergeResult.get().getLeft());
        } else {
          mergedRecords.add(record.copy());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergedReadHandle.java`
#### Snippet
```java

    for (String key : deltaRecordKeys) {
      mergedRecords.add(deltaRecordMap.get(key));
    }

```

### UNCHECKED_WARNING
Unchecked call to 'getRecordKey(Schema, Option)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java`
#### Snippet
```java
    Schema oldSchema = config.populateMetaFields() ? writeSchemaWithMetaFields : writeSchema;
    Schema newSchema = useWriterSchemaForCompaction ? writeSchemaWithMetaFields : writeSchema;
    String key = oldRecord.getRecordKey(oldSchema, keyGeneratorOpt);

    // To maintain overall sorted order across updates and inserts, write any new inserts whose keys are less than
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieSortedMergeHandle.java`
#### Snippet
```java
    }

    super.write(oldRecord);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'. Reason: 'record' has raw type, so result of getMetadata is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java`
#### Snippet
```java
  @Override
  protected void doWrite(HoodieRecord record, Schema schema, TypedProperties props) {
    Option<Map<String, String>> recordMetadata = record.getMetadata();
    try {
      if (!HoodieOperation.isDelete(record.getOperation()) && !record.isDelete(schema, config.getProps())) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.HoodieAppendHandle' to 'org.apache.hudi.io.HoodieAppendHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/AppendHandleFactory.java`
#### Snippet
```java
                                     final String fileIdPrefix, final TaskContextSupplier sparkTaskContextSupplier) {

    return new HoodieAppendHandle(hoodieConfig, commitTime, hoodieTable, partitionPath,
        getNextFileId(fileIdPrefix), sparkTaskContextSupplier);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAppendHandle(HoodieWriteConfig, String, HoodieTable, String, String, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.HoodieAppendHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/AppendHandleFactory.java`
#### Snippet
```java
                                     final String fileIdPrefix, final TaskContextSupplier sparkTaskContextSupplier) {

    return new HoodieAppendHandle(hoodieConfig, commitTime, hoodieTable, partitionPath,
        getNextFileId(fileIdPrefix), sparkTaskContextSupplier);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieConcatHandle.java`
#### Snippet
```java
                            Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,
                            HoodieBaseFile dataFileToBeMerged, TaskContextSupplier taskContextSupplier) {
    super(config, instantTime, hoodieTable, Collections.emptyMap(), partitionPath, fileId, dataFileToBeMerged, taskContextSupplier,
        Option.empty());
    this.recordItr = keyToNewRecords.values().iterator();
```

### UNCHECKED_WARNING
Unchecked call to 'getRecordKey(Schema, Option)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieConcatHandle.java`
#### Snippet
```java
  public void write(HoodieRecord oldRecord) {
    Schema oldSchema = config.populateMetaFields() ? writeSchemaWithMetaFields : writeSchema;
    String key = oldRecord.getRecordKey(oldSchema, keyGeneratorOpt);
    try {
      // NOTE: We're enforcing preservation of the record metadata to keep existing semantic
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieConcatHandle.java`
#### Snippet
```java
    try {
      // NOTE: We're enforcing preservation of the record metadata to keep existing semantic
      writeToFile(new HoodieKey(key, partitionPath), oldRecord, oldSchema, config.getPayloadConfig().getProps(), true);
    } catch (IOException | RuntimeException e) {
      String errMsg = String.format("Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s",
```

### UNCHECKED_WARNING
Unchecked call to 'updateWriteClient(BaseHoodieWriteClient)' as a member of raw type 'org.apache.hudi.client.BaseCompactor'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/AsyncCompactService.java`
#### Snippet
```java

  public synchronized void updateWriteClient(BaseHoodieWriteClient writeClient) {
    this.compactor.updateWriteClient(writeClient);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'updateWriteClient(BaseHoodieWriteClient)' as a member of raw type 'org.apache.hudi.client.BaseClusterer'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/AsyncClusteringService.java`
#### Snippet
```java
   */
  public synchronized void updateWriteClient(BaseHoodieWriteClient writeClient) {
    this.clusteringClient.updateWriteClient(writeClient);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieKeyLookupHandle(HoodieWriteConfig, HoodieTable, Pair)' as a member of raw type 'org.apache.hudi.io.HoodieKeyLookupHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndexCheckFunction.java`
#### Snippet
```java
          // lazily init state
          if (keyLookupHandle == null) {
            keyLookupHandle = new HoodieKeyLookupHandle(config, hoodieTable, partitionPathFilePair);
          }

```

### UNCHECKED_WARNING
Unchecked call to 'HoodieKeyLookupHandle(HoodieWriteConfig, HoodieTable, Pair)' as a member of raw type 'org.apache.hudi.io.HoodieKeyLookupHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndexCheckFunction.java`
#### Snippet
```java
            // do the actual checking of file & break out
            ret.add(keyLookupHandle.getLookupResult());
            keyLookupHandle = new HoodieKeyLookupHandle(config, hoodieTable, partitionPathFilePair);
            keyLookupHandle.addKey(recordKey);
            break;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
      init(record);
      flushToDiskIfRequired(record, false);
      writeToBuffer(record);
    }
    appendDataAndDeleteBlocks(header, true);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'. Reason: 'record' has raw type, so result of getMetadata is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
  @Override
  protected void doWrite(HoodieRecord record, Schema schema, TypedProperties props) {
    Option<Map<String, String>> recordMetadata = record.getMetadata();
    try {
      init(record);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
      init(record);
      flushToDiskIfRequired(record, false);
      writeToBuffer(record);
    } catch (Throwable t) {
      // Not throwing exception from here, since we don't want to fail the entire job
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.DefaultSizeEstimator' to 'org.apache.hudi.common.util.SizeEstimator'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
    this.fileId = fileId;
    this.recordItr = recordItr;
    this.sizeEstimator = new DefaultSizeEstimator();
    this.statuses = new ArrayList<>();
    this.recordProperties.putAll(config.getProps());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java

        // Prepend meta-fields into the record
        MetadataValues metadataValues = populateMetadataFields(finalRecord);
        HoodieRecord populatedRecord =
            finalRecord.prependMetaFields(schema, writeSchemaWithMetaFields, metadataValues, recordProperties);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
      HoodieUpsertException failureEx = new HoodieUpsertException("mismatched partition path, record partition: "
          + newRecord.getPartitionPath() + " but trying to insert into partition: " + partitionPath);
      writeStatus.markFailure(newRecord, failureEx, recordMetadata);
      return false;
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
    try {
      if (combineRecord.isPresent() && !combineRecord.get().isDelete(schema, config.getProps()) && !isDelete) {
        writeToFile(newRecord.getKey(), combineRecord.get(), schema, prop, preserveMetadata && useWriterSchemaForCompaction);
        recordsWritten++;
      } else {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
        recordsDeleted++;
      }
      writeStatus.markSuccess(newRecord, recordMetadata);
      // deflate record payload after recording success. This will help users access payload as a
      // part of marking
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
    } catch (Exception e) {
      LOG.error("Error writing record  " + newRecord, e);
      writeStatus.markFailure(newRecord, e, recordMetadata);
    }
    return false;
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'. Reason: '((ExternalSpillableMap)keyToNewRecords)' has raw type, so result of iterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
    // write out any pending records (this can happen when inserts are turned into updates)
    Iterator<HoodieRecord<T>> newRecordsItr = (keyToNewRecords instanceof ExternalSpillableMap)
        ? ((ExternalSpillableMap)keyToNewRecords).iterator() : keyToNewRecords.values().iterator();
    while (newRecordsItr.hasNext()) {
      HoodieRecord<T> hoodieRecord = newRecordsItr.next();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
      long memoryForMerge = IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config);
      LOG.info("MaxMemoryPerPartitionMerge => " + memoryForMerge);
      this.keyToNewRecords = new ExternalSpillableMap<>(memoryForMerge, config.getSpillableMapBasePath(),
          new DefaultSizeEstimator(), new HoodieRecordSizeEstimator(writeSchema),
          config.getCommonConfig().getSpillableDiskMapType(),
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
      long memoryForMerge = IOUtils.getMaxMemoryPerPartitionMerge(taskContextSupplier, config);
      LOG.info("MaxMemoryPerPartitionMerge => " + memoryForMerge);
      this.keyToNewRecords = new ExternalSpillableMap<>(memoryForMerge, config.getSpillableMapBasePath(),
          new DefaultSizeEstimator(), new HoodieRecordSizeEstimator(writeSchema),
          config.getCommonConfig().getSpillableDiskMapType(),
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bucket/HoodieBucketIndex.java`
#### Snippet
```java
            HoodieRecord record = inputItr.next();
            Option<HoodieRecordLocation> loc = mapper.getRecordLocation(record.getKey());
            return HoodieIndexUtils.getTaggedRecord(record, loc);
          }
        },
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bucket/HoodieBucketIndex.java`
#### Snippet
```java
            HoodieRecord record = inputItr.next();
            Option<HoodieRecordLocation> loc = mapper.getRecordLocation(record.getKey());
            return HoodieIndexUtils.getTaggedRecord(record, loc);
          }
        },
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieAvroRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
          } else {
            // merged record has a different partition: issue a delete to the old partition and insert the merged record to the new partition
            HoodieRecord<R> deleteRecord = new HoodieAvroRecord(existing.getKey(), new EmptyHoodieRecordPayload());
            deleteRecord.setCurrentLocation(existing.getCurrentLocation());
            deleteRecord.seal();
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
          } else {
            // merged record has a different partition: issue a delete to the old partition and insert the merged record to the new partition
            HoodieRecord<R> deleteRecord = new HoodieAvroRecord(existing.getKey(), new EmptyHoodieRecordPayload());
            deleteRecord.setCurrentLocation(existing.getCurrentLocation());
            deleteRecord.seal();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'. Reason: 'mergeResult.get().getLeft()' has raw type, so result of wrapIntoHoodieRecordPayloadWithParams is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
    if (mergeResult.isPresent()) {
      // the merged record needs to be converted back to the original payload
      HoodieRecord<R> merged = mergeResult.get().getLeft().wrapIntoHoodieRecordPayloadWithParams(
          writeSchemaWithMetaFields, config.getProps(), Option.empty(),
          config.allowOperationMetadataField(), Option.empty(), false, Option.of(writeSchema));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
        .lastInstant()
        .map(HoodieInstant::getTimestamp);
    return partitionLocations.flatMap(p -> {
      String partitionPath = p.getLeft();
      String fileId = p.getRight().getFileId();
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'. Reason: 'new HoodieMergedReadHandle(config, instantTime, hoodieTable, Pair.of(partitionPath, fileId)) .getMergedRecords()' has raw type, so result of iterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
      String partitionPath = p.getLeft();
      String fileId = p.getRight().getFileId();
      return new HoodieMergedReadHandle(config, instantTime, hoodieTable, Pair.of(partitionPath, fileId))
          .getMergedRecords().iterator();
    });
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieMergedReadHandle(HoodieWriteConfig, Option, HoodieTable, Pair)' as a member of raw type 'org.apache.hudi.io.HoodieMergedReadHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
      String partitionPath = p.getLeft();
      String fileId = p.getRight().getFileId();
      return new HoodieMergedReadHandle(config, instantTime, hoodieTable, Pair.of(partitionPath, fileId))
          .getMergedRecords().iterator();
    });
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Set' to 'java.util.Set'. Reason: 'fileReader' has raw type, so result of filterRowKeys is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
      if (!candidateRecordKeys.isEmpty()) {
        HoodieTimer timer = HoodieTimer.start();
        Set<String> fileRowKeys = fileReader.filterRowKeys(new TreeSet<>(candidateRecordKeys));
        foundRecordKeys.addAll(fileRowKeys);
        LOG.info(String.format("Checked keys against file %s, in %d ms. #candidates (%d) #found (%d)", filePath,
```

### UNCHECKED_WARNING
Unchecked call to 'filterRowKeys(Set)' as a member of raw type 'org.apache.hudi.io.storage.HoodieFileReader'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java`
#### Snippet
```java
      if (!candidateRecordKeys.isEmpty()) {
        HoodieTimer timer = HoodieTimer.start();
        Set<String> fileRowKeys = fileReader.filterRowKeys(new TreeSet<>(candidateRecordKeys));
        foundRecordKeys.addAll(fileRowKeys);
        LOG.info(String.format("Checked keys against file %s, in %d ms. #candidates (%d) #found (%d)", filePath,
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'record' has raw type, so result of getNewLocation is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/inmemory/HoodieInMemoryHashIndex.java`
#### Snippet
```java
        if (!writeStatus.isErrored(record.getKey())) {
          HoodieKey key = record.getKey();
          Option<HoodieRecordLocation> newLocation = record.getNewLocation();
          if (newLocation.isPresent()) {
            recordLocationMap.put(key, newLocation.get());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodiePairData' to 'org.apache.hudi.common.data.HoodiePairData'. Reason: 'context.parallelize(baseFiles, fetchParallelism) .flatMap(partitionPathBaseFile -\> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt) .locations().iterator())' has raw type, so result of mapToPair is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java`
#### Snippet
```java
    int fetchParallelism = Math.max(1, Math.min(baseFiles.size(), parallelism));

    return context.parallelize(baseFiles, fetchParallelism)
        .flatMap(partitionPathBaseFile -> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt)
            .locations().iterator())
        .mapToPair(e -> (Pair<HoodieKey, HoodieRecordLocation>) e);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'mapToPair(SerializablePairFunction)' as a member of raw type 'org.apache.hudi.common.data.HoodieData'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java`
#### Snippet
```java
    int fetchParallelism = Math.max(1, Math.min(baseFiles.size(), parallelism));

    return context.parallelize(baseFiles, fetchParallelism)
        .flatMap(partitionPathBaseFile -> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt)
            .locations().iterator())
        .mapToPair(e -> (Pair<HoodieKey, HoodieRecordLocation>) e);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator'. Reason: 'new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt) .locations()' has raw type, so result of iterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java`
#### Snippet
```java

    return context.parallelize(baseFiles, fetchParallelism)
        .flatMap(partitionPathBaseFile -> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt)
            .locations().iterator())
        .mapToPair(e -> (Pair<HoodieKey, HoodieRecordLocation>) e);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieKeyLocationFetchHandle(HoodieWriteConfig, HoodieTable, Pair, Option)' as a member of raw type 'org.apache.hudi.io.HoodieKeyLocationFetchHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java`
#### Snippet
```java

    return context.parallelize(baseFiles, fetchParallelism)
        .flatMap(partitionPathBaseFile -> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt)
            .locations().iterator())
        .mapToPair(e -> (Pair<HoodieKey, HoodieRecordLocation>) e);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.hudi.common.util.collection.Pair'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java`
#### Snippet
```java
        .flatMap(partitionPathBaseFile -> new HoodieKeyLocationFetchHandle(config, hoodieTable, partitionPathBaseFile, keyGeneratorOpt)
            .locations().iterator())
        .mapToPair(e -> (Pair<HoodieKey, HoodieRecordLocation>) e);
  }
}
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java`
#### Snippet
```java
              // When it differs, the record will still be updated at its old partition.
              return Pair.of(
                  (HoodieRecord<R>) HoodieIndexUtils.getTaggedRecord(new HoodieAvroRecord(recordLocationHoodieKeyPair.get().getRight(), (HoodieRecordPayload) hoodieRecord.getData()),
                      Option.ofNullable(recordLocationHoodieKeyPair.get().getLeft())), Option.empty());
            }
          } else {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieAvroRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java`
#### Snippet
```java
              // When it differs, the record will still be updated at its old partition.
              return Pair.of(
                  (HoodieRecord<R>) HoodieIndexUtils.getTaggedRecord(new HoodieAvroRecord(recordLocationHoodieKeyPair.get().getRight(), (HoodieRecordPayload) hoodieRecord.getData()),
                      Option.ofNullable(recordLocationHoodieKeyPair.get().getLeft())), Option.empty());
            }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java`
#### Snippet
```java
              // When it differs, the record will still be updated at its old partition.
              return Pair.of(
                  (HoodieRecord<R>) HoodieIndexUtils.getTaggedRecord(new HoodieAvroRecord(recordLocationHoodieKeyPair.get().getRight(), (HoodieRecordPayload) hoodieRecord.getData()),
                      Option.ofNullable(recordLocationHoodieKeyPair.get().getLeft())), Option.empty());
            }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieAvroRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java`
#### Snippet
```java
              // Ignore the incoming record's partition, regardless of whether it differs from its old partition or not.
              // When it differs, the record will still be updated at its old partition.
              HoodieRecord<R> newRecord = new HoodieAvroRecord(new HoodieKey(inputRecord.getRecordKey(), partitionPath), (HoodieRecordPayload) inputRecord.getData());
              return Pair.of(HoodieIndexUtils.getTaggedRecord(newRecord, Option.of(location)), Option.empty());
            }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java`
#### Snippet
```java
              // Ignore the incoming record's partition, regardless of whether it differs from its old partition or not.
              // When it differs, the record will still be updated at its old partition.
              HoodieRecord<R> newRecord = new HoodieAvroRecord(new HoodieKey(inputRecord.getRecordKey(), partitionPath), (HoodieRecordPayload) inputRecord.getData());
              return Pair.of(HoodieIndexUtils.getTaggedRecord(newRecord, Option.of(location)), Option.empty());
            }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieRangeInfoHandle(HoodieWriteConfig, HoodieTable, Pair)' as a member of raw type 'org.apache.hudi.io.HoodieRangeInfoHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java`
#### Snippet
```java
    return context.map(partitionPathFileIDList, pf -> {
      try {
        HoodieRangeInfoHandle rangeInfoHandle = new HoodieRangeInfoHandle(config, hoodieTable, Pair.of(pf.getKey(), pf.getValue().getKey()));
        String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys(pf.getValue().getValue());
        return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue().getKey(), minMaxKeys[0], minMaxKeys[1]));
```

### UNCHECKED_WARNING
Unchecked call to 'write(HoodieRecord)' as a member of raw type 'org.apache.hudi.io.HoodieMergeHandle'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseMergeHelper.java`
#### Snippet
```java
    @Override
    public void consume(HoodieRecord record) {
      upsertHandle.write(record);
    }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieWriteHelper.java`
#### Snippet
```java
      HoodieRecord<T> reducedRecord;
      try {
        reducedRecord =  merger.merge(rec1, schema.get(), rec2, schema.get(), props).get().getLeft();
      } catch (IOException e) {
        throw new HoodieException(String.format("Error to merge two records, %s, %s", rec1, rec2), e);
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieDeleteHelper.java`
#### Snippet
```java
    HoodieRecordType recordType = config.getRecordMerger().getRecordType();
    if (recordType == HoodieRecordType.AVRO) {
      return keys.map(key -> new HoodieAvroRecord(key, new EmptyHoodieRecordPayload()));
    } else {
      return keys.map(key -> new HoodieEmptyRecord<>(key, recordType));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieDeleteHelper.java`
#### Snippet
```java
    try {
      int targetParallelism =
          deduceShuffleParallelism((HoodieData) keys, config.getDeleteShuffleParallelism());

      HoodieData<HoodieKey> dedupedKeys;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieDeleteHelper.java`
#### Snippet
```java
      Instant beginTag = Instant.now();
      // perform index loop up to get existing location of records
      HoodieData<HoodieRecord<T>> taggedRecords = table.getIndex().tagLocation(dedupedRecords, context, table);
      Duration tagLocationDuration = Duration.between(beginTag, Instant.now());

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieDeleteHelper.java`
#### Snippet
```java
      Instant beginTag = Instant.now();
      // perform index loop up to get existing location of records
      HoodieData<HoodieRecord<T>> taggedRecords = table.getIndex().tagLocation(dedupedRecords, context, table);
      Duration tagLocationDuration = Duration.between(beginTag, Instant.now());

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringExecutionStrategy.java`
#### Snippet
```java
  public ClusteringExecutionStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {
    this.writeConfig = writeConfig;
    this.hoodieTable = table;
    this.engineContext = engineContext;
    this.recordType = table.getConfig().getRecordMerger().getRecordType();
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'org.apache.hudi.table.action.cluster.strategy.ClusteringExecutionStrategy\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java`
#### Snippet
```java
    final Schema schema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));
    HoodieWriteMetadata<HoodieData<WriteStatus>> writeMetadata = (
        (ClusteringExecutionStrategy<T, HoodieData<HoodieRecord<T>>, HoodieData<HoodieKey>, HoodieData<WriteStatus>>)
            ReflectionUtils.loadClass(config.getClusteringExecutionStrategyClass(),
                new Class<?>[] {HoodieTable.class, HoodieEngineContext.class, HoodieWriteConfig.class}, table, context, config))
        .performClustering(clusteringPlan, schema, instantTime);
    HoodieData<WriteStatus> writeStatusList = writeMetadata.getWriteStatuses();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java`
#### Snippet
```java
  protected void commitOnAutoCommit(HoodieWriteMetadata result) {
    // validate commit action before committing result
    runPrecommitValidators(result);
    if (config.shouldAutoCommit()) {
      LOG.info("Auto commit enabled: Committing " + instantTime);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java`
#### Snippet
```java
    if (config.shouldAutoCommit()) {
      LOG.info("Auto commit enabled: Committing " + instantTime);
      autoCommit(extraMetadata, result);
    } else {
      LOG.info("Auto commit disabled for " + instantTime);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'strategy' has raw type, so result of generateClusteringPlan is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/ClusteringPlanActionExecutor.java`
#### Snippet
```java
            new Class<?>[] {HoodieTable.class, HoodieEngineContext.class, HoodieWriteConfig.class}, table, context, config);

    return strategy.generateClusteringPlan();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'. Reason: 'compactionHandler' has raw type, so result of handleUpdate is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionExecutionHelper.java`
#### Snippet
```java
    // If the dataFile is present, perform updates else perform inserts into a new base file.
    if (oldDataFileOpt.isPresent()) {
      result = compactionHandler.handleUpdate(instantTime, operation.getPartitionPath(),
          operation.getFileId(), scanner.getRecords(),
          oldDataFileOpt.get());
```

### UNCHECKED_WARNING
Unchecked call to 'handleUpdate(String, String, String, Map\>, HoodieBaseFile)' as a member of raw type 'org.apache.hudi.table.HoodieCompactionHandler'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionExecutionHelper.java`
#### Snippet
```java
    // If the dataFile is present, perform updates else perform inserts into a new base file.
    if (oldDataFileOpt.isPresent()) {
      result = compactionHandler.handleUpdate(instantTime, operation.getPartitionPath(),
          operation.getFileId(), scanner.getRecords(),
          oldDataFileOpt.get());
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'. Reason: 'compactionHandler' has raw type, so result of handleInsert is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionExecutionHelper.java`
#### Snippet
```java
          oldDataFileOpt.get());
    } else {
      result = compactionHandler.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),
          scanner.getRecords());
    }
```

### UNCHECKED_WARNING
Unchecked call to 'handleInsert(String, String, String, Map\>)' as a member of raw type 'org.apache.hudi.table.HoodieCompactionHandler'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionExecutionHelper.java`
#### Snippet
```java
          oldDataFileOpt.get());
    } else {
      result = compactionHandler.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),
          scanner.getRecords());
    }
```

### UNCHECKED_WARNING
Unchecked call to 'rewriteRecordWithNewSchema(Schema, Properties, Schema, Map)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieMergeHelper.java`
#### Snippet
```java
        Map<String, String> renameCols = InternalSchemaUtils.collectRenameCols(writeInternalSchema, querySchema);
        return Option.of(record -> {
          return record.rewriteRecordWithNewSchema(
              recordSchema,
              writeConfig.getProps(),
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'. Reason: 'baseFileReader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieMergeHelper.java`
#### Snippet
```java
      // the records in the projected schema directly
      ClosableIterator<HoodieRecord> baseFileRecordIterator =
          baseFileReader.getRecordIterator(isPureProjection ? writerSchema : readerSchema);
      Schema recordSchema;
      if (baseFile.getBootstrapBaseFile().isPresent()) {
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'. Reason: 'bootstrapFileReader' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/HoodieMergeHelper.java`
#### Snippet
```java
        recordIterator = new ClosableMergingIterator<>(
            baseFileRecordIterator,
            (ClosableIterator<HoodieRecord>) bootstrapFileReader.getRecordIterator(recordSchema),
            (left, right) -> left.joinWith(right, recordSchema));
      } else {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'. Reason: 'compactionHandler' has raw type, so result of handleInsertsForLogCompaction is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/LogCompactionExecutionHelper.java`
#### Snippet
```java
        StringUtils.join(scanner.getValidBlockInstants(), ","));
    // Compacting is very similar to applying updates to existing file
    return compactionHandler.handleInsertsForLogCompaction(instantTime, operation.getPartitionPath(),
        operation.getFileId(), scanner.getRecords(), header);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'handleInsertsForLogCompaction(String, String, String, Map\>, Map)' as a member of raw type 'org.apache.hudi.table.HoodieCompactionHandler'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/LogCompactionExecutionHelper.java`
#### Snippet
```java
        StringUtils.join(scanner.getValidBlockInstants(), ","));
    // Compacting is very similar to applying updates to existing file
    return compactionHandler.handleInsertsForLogCompaction(instantTime, operation.getPartitionPath(),
        operation.getFileId(), scanner.getRecords(), header);
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java`
#### Snippet
```java
  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {
    this.writeConfig = writeConfig;
    this.hoodieTable = table;
    this.engineContext = engineContext;
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.ArrayList' to 'java.util.List'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/UnBoundedPartitionAwareCompactionStrategy.java`
#### Snippet
```java
    List<HoodieCompactionOperation> operationsToExclude =
        boundedPartitionAwareCompactionStrategy.orderAndFilter(config, operations, pendingCompactionWorkloads);
    List<HoodieCompactionOperation> allOperations = new ArrayList<>(operations);
    allOperations.removeAll(operationsToExclude);
    return allOperations;
```

### UNCHECKED_WARNING
Unchecked call to 'ArrayList(Collection)' as a member of raw type 'java.util.ArrayList'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/UnBoundedPartitionAwareCompactionStrategy.java`
#### Snippet
```java
    List<HoodieCompactionOperation> operationsToExclude =
        boundedPartitionAwareCompactionStrategy.orderAndFilter(config, operations, pendingCompactionWorkloads);
    List<HoodieCompactionOperation> allOperations = new ArrayList<>(operations);
    allOperations.removeAll(operationsToExclude);
    return allOperations;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable\>,org.apache.hudi.common.data.HoodieData,org.apache.hudi.common.data.HoodieData\>'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java`
#### Snippet
```java
                                     HoodieCompactionHandler compactionHandler,
                                     WriteOperationType operationType) {
    super(context, config, table, instantTime);
    this.compactor = compactor;
    this.compactionHandler = compactionHandler;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.data.HoodieData' to 'org.apache.hudi.common.data.HoodieData'. Reason: 'compactor' has raw type, so result of compact is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java`
#### Snippet
```java
      }

      HoodieData<WriteStatus> statuses = compactor.compact(
          context, compactionPlan, table, configCopy, instantTime, compactionHandler);

```

### UNCHECKED_WARNING
Unchecked call to 'maybePersist(HoodieData, HoodieEngineContext, HoodieWriteConfig, String)' as a member of raw type 'org.apache.hudi.table.action.compact.HoodieCompactor'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java`
#### Snippet
```java
          context, compactionPlan, table, configCopy, instantTime, compactionHandler);

      compactor.maybePersist(statuses, context, config, instantTime);
      context.setJobStatus(this.getClass().getSimpleName(), "Preparing compaction metadata: " + config.getTableName());
      List<HoodieWriteStat> updateStatusMap = statuses.map(WriteStatus::getStat).collectAsList();
```

### UNCHECKED_WARNING
Unchecked call to 'MergeOnReadRollbackActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean, ...)' as a member of raw type 'org.apache.hudi.table.action.rollback.MergeOnReadRollbackActionExecutor'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/MergeOnReadRestoreActionExecutor.java`
#### Snippet
```java
    table.scheduleRollback(context, instantTime, instantToRollback, false, false);
    table.getMetaClient().reloadActiveTimeline();
    MergeOnReadRollbackActionExecutor rollbackActionExecutor = new MergeOnReadRollbackActionExecutor(
        context,
        config,
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator\>'. Reason: 'executionHelper' has raw type, so result of writeFileAndGetWriteStats is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java
    // Compacting is very similar to applying updates to existing file
    Iterator<List<WriteStatus>> result;
    result = executionHelper.writeFileAndGetWriteStats(compactionHandler, operation, instantTime, scanner, oldDataFileOpt);
    scanner.close();
    Iterable<List<WriteStatus>> resultIterable = () -> result;
```

### UNCHECKED_WARNING
Unchecked call to 'writeFileAndGetWriteStats(HoodieCompactionHandler, CompactionOperation, String, HoodieMergedLogRecordScanner, Option)' as a member of raw type 'org.apache.hudi.table.action.compact.CompactionExecutionHelper'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java
    // Compacting is very similar to applying updates to existing file
    Iterator<List<WriteStatus>> result;
    result = executionHelper.writeFileAndGetWriteStats(compactionHandler, operation, instantTime, scanner, oldDataFileOpt);
    scanner.close();
    Iterable<List<WriteStatus>> resultIterable = () -> result;
```

### UNCHECKED_WARNING
Unchecked call to 'CopyOnWriteRollbackActionExecutor(HoodieEngineContext, HoodieWriteConfig, HoodieTable, String, HoodieInstant, boolean, boolean, ...)' as a member of raw type 'org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java`
#### Snippet
```java
    table.scheduleRollback(context, newInstantTime, instantToRollback, false, false);
    table.getMetaClient().reloadActiveTimeline();
    CopyOnWriteRollbackActionExecutor rollbackActionExecutor = new CopyOnWriteRollbackActionExecutor(
        context,
        config,
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/CopyOnWriteRestoreActionExecutor.java`
#### Snippet
```java
                                          String instantTime,
                                          String savepointToRestoreTimestamp) {
    super(context, config, table, instantTime, savepointToRestoreTimestamp);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/plan/generators/BaseHoodieCompactionPlanGenerator.java`
#### Snippet
```java

  public BaseHoodieCompactionPlanGenerator(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {
    this.hoodieTable = table;
    this.writeConfig = writeConfig;
    this.engineContext = engineContext;
```

### UNCHECKED_WARNING
Unchecked call to 'MarkerBasedRollbackStrategy(HoodieTable, HoodieEngineContext, HoodieWriteConfig, String)' as a member of raw type 'org.apache.hudi.table.action.rollback.MarkerBasedRollbackStrategy'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java`
#### Snippet
```java
  private BaseRollbackPlanActionExecutor.RollbackStrategy getRollbackStrategy() {
    if (shouldRollbackUsingMarkers) {
      return new MarkerBasedRollbackStrategy(table, context, config, instantTime);
    } else {
      return new ListingBasedRollbackStrategy(table, context, config, instantTime);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
    }
    String content = response.returnContent().asString();
    return (T) mapper.readValue(content, reference);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/ZeroToOneUpgradeHandler.java`
#### Snippet
```java
      recreateMarkers(commit, table, context, config.getMarkersDeleteParallelism());
    }
    return Collections.EMPTY_MAP;
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/OneToZeroDowngradeHandler.java`
#### Snippet
```java
      writeMarkers.quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());
    }
    return Collections.EMPTY_MAP;
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.config.ConfigProperty' to 'org.apache.hudi.common.config.ConfigProperty'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
    // Write out the current version in hoodie.properties.updated file
    for (Map.Entry<ConfigProperty, String> entry : tableProps.entrySet()) {
      metaClient.getTableConfig().setValue(entry.getKey(), entry.getValue());
    }
    metaClient.getTableConfig().setTableVersion(toVersion);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/TwoToOneDowngradeHandler.java`
#### Snippet
```java
      }
    }
    return Collections.EMPTY_MAP;
  }

```

### UNCHECKED_WARNING
Unchecked call to 'finalizeWrite(HoodieEngineContext, String, List)' as a member of raw type 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieClient.java`
#### Snippet
```java
    try {
      final Timer.Context finalizeCtx = metrics.getFinalizeCtx();
      table.finalizeWrite(context, instantTime, stats);
      if (finalizeCtx != null) {
        Option<Long> durationInMs = Option.of(metrics.getDurationInMs(finalizeCtx.stop()));
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
      return Arrays.stream(fileStatus).map(fs -> fs.getPath().getName()).collect(Collectors.toList());
    }
    return Collections.EMPTY_LIST;
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'java.util.Iterator'. Reason: 'baseFileReader.get()' has raw type, so result of getRecordIterator is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java`
#### Snippet
```java
      Option<HoodieFileReader> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, Properties props, Option<Pair<String, String>> simpleKeyGenFieldsOpt) throws IOException {
    if (baseFileReader.isPresent()) {
      Iterator<HoodieRecord> baseIterator = baseFileReader.get().getRecordIterator(schema);
      while (baseIterator.hasNext()) {
        scanner.processNextRecord(baseIterator.next().wrapIntoHoodieRecordPayloadWithParams(schema, props,
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'. Reason: 'baseIterator.next()' has raw type, so result of wrapIntoHoodieRecordPayloadWithParams is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java`
#### Snippet
```java
      Iterator<HoodieRecord> baseIterator = baseFileReader.get().getRecordIterator(schema);
      while (baseIterator.hasNext()) {
        scanner.processNextRecord(baseIterator.next().wrapIntoHoodieRecordPayloadWithParams(schema, props,
            simpleKeyGenFieldsOpt, scanner.isWithOperationField(), scanner.getPartitionNameOverride(), false, Option.empty()));
      }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieFileSliceReader(Iterator\>)' as a member of raw type 'org.apache.hudi.common.table.log.HoodieFileSliceReader'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java`
#### Snippet
```java
      }
    }
    return new HoodieFileSliceReader(scanner.iterator());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
      throw new HoodieException("Cannot bootstrap the table in multi-writer mode");
    }
    HoodieTable<T, I, K, O> table = initTable(WriteOperationType.UPSERT, Option.ofNullable(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS));
    tableServiceClient.rollbackFailedBootstrap();
    table.bootstrap(context, extraMetadata);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
    Timer.Context timerContext = metrics.getRollbackCtx();
    try {
      HoodieTable<T, I, K, O> table = initTable(WriteOperationType.UNKNOWN, Option.empty(), initialMetadataTableIfNecessary);
      Pair<String, Option<HoodieRestorePlan>> timestampAndRestorePlan = scheduleAndGetRestorePlan(savepointToRestoreTimestamp, table);
      final String restoreInstantTimestamp = timestampAndRestorePlan.getLeft();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
    }

    HoodieTable<T, I, K, O> table = initTable(WriteOperationType.UNKNOWN, Option.empty(), initialMetadataTableIfNecessary);
    SavepointHelpers.validateSavepointPresence(table, savepointTime);
    ValidationUtils.checkArgument(!config.shouldArchiveBeyondSavepoint(), "Restore is not supported when " + HoodieArchivalConfig.ARCHIVE_BEYOND_SAVEPOINT.key()
```

### UNCHECKED_WARNING
Unchecked call to 'ifPresent(Consumer)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
    } else {
      context.setJobStatus(this.getClass().getSimpleName(), "Committing to metadata table: " + config.getTableName());
      table.getMetadataWriter(instantTime).ifPresent(w -> ((HoodieTableMetadataWriter) w).update(metadata, instantTime, false));
    }
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.HoodieTable' to 'org.apache.hudi.table.HoodieTable'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
  @Deprecated
  public boolean rollback(final String commitInstantTime) throws HoodieRollbackException {
    HoodieTable<T, I, K, O> table = initTable(WriteOperationType.UNKNOWN, Option.empty());
    Option<HoodiePendingRollbackInfo> pendingRollbackInfo = tableServiceClient.getPendingRollbackInfo(table.getMetaClient(), commitInstantTime);
    return tableServiceClient.rollback(commitInstantTime, pendingRollbackInfo, false);
```

### UNCHECKED_WARNING
Unchecked call to 'ifPresent(Consumer)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
    try {
      context.setJobStatus(this.getClass().getSimpleName(), "Dropping partitions from metadata table: " + config.getTableName());
      table.getMetadataWriter(dropInstant).ifPresent(w -> {
        try {
          ((HoodieTableMetadataWriter) w).dropMetadataPartitions(partitionTypes);
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieTimelineArchiver(HoodieWriteConfig, HoodieTable)' as a member of raw type 'org.apache.hudi.client.HoodieTimelineArchiver'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
    try {
      // We cannot have unbounded commit files. Archive commits if we have to archive
      HoodieTimelineArchiver archiver = new HoodieTimelineArchiver(config, table);
      archiver.archiveIfRequired(context, true);
    } catch (IOException ioe) {
```

### UNCHECKED_WARNING
Unchecked call to 'ifPresent(Consumer)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
    checkArgument(table.isTableServiceAction(actionType, instantTime), String.format("Unsupported action: %s.%s is not table service.", actionType, instantTime));
    context.setJobStatus(this.getClass().getSimpleName(), "Committing to metadata table: " + config.getTableName());
    table.getMetadataWriter(instantTime).ifPresent(w -> ((HoodieTableMetadataWriter) w).update(metadata, instantTime, true));
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'table' has raw type, so result of scheduleRollback is erased
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
            rollbackInstantTime, commitInstantOpt.isPresent(), pendingRollbackInfo.isPresent()));
        Option<HoodieRollbackPlan> rollbackPlanOption = pendingRollbackInfo.map(entry -> Option.of(entry.getRollbackPlan()))
            .orElseGet(() -> table.scheduleRollback(context, rollbackInstantTime, commitInstantOpt.get(), false, config.shouldRollbackUsingMarkers()));
        if (rollbackPlanOption.isPresent()) {
          // There can be a case where the inflight rollback failed after the instant files
```

### UNCHECKED_WARNING
Unchecked call to 'setValue(T)' as a member of raw type 'org.apache.hudi.metrics.HoodieGauge'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/Metrics.java`
#### Snippet
```java
    try {
      HoodieGauge guage = (HoodieGauge) registry.gauge(metricName, () -> new HoodieGauge<>(value));
      guage.setValue(value);
    } catch (Exception e) {
      // Here we catch all exception, so the major upsert pipeline will not be affected if the
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.execution.HoodieLazyInsertIterable.HoodieInsertValueGenResult' to 'org.apache.hudi.execution.HoodieLazyInsertIterable.HoodieInsertValueGenResult'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java`
#### Snippet
```java
    return record -> {
      HoodieRecord<T> clonedRecord = shouldClone ? record.copy() : record;
      return new HoodieInsertValueGenResult(clonedRecord, schema, writeConfig.getProps());
    };
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieInsertValueGenResult(R, Schema, Properties)' as a member of raw type 'org.apache.hudi.execution.HoodieLazyInsertIterable.HoodieInsertValueGenResult'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/HoodieLazyInsertIterable.java`
#### Snippet
```java
    return record -> {
      HoodieRecord<T> clonedRecord = shouldClone ? record.copy() : record;
      return new HoodieInsertValueGenResult(clonedRecord, schema, writeConfig.getProps());
    };
  }
```

### UNCHECKED_WARNING
Unchecked call to 'create(HoodieWriteConfig, String, HoodieTable, String, String, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.WriteHandleFactory'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/CopyOnWriteInsertHandler.java`
#### Snippet
```java
      }
      // Lazily initialize the handle, for the first time
      handle = writeHandleFactory.create(config, instantTime, hoodieTable,
          record.getPartitionPath(), idPrefix, taskContextSupplier);
      handles.put(partitionPath, handle);
```

### UNCHECKED_WARNING
Unchecked call to 'create(HoodieWriteConfig, String, HoodieTable, String, String, TaskContextSupplier)' as a member of raw type 'org.apache.hudi.io.WriteHandleFactory'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/CopyOnWriteInsertHandler.java`
#### Snippet
```java
      statuses.addAll(handle.close());
      // Open new handle
      handle = writeHandleFactory.create(config, instantTime, hoodieTable,
          record.getPartitionPath(), idPrefix, taskContextSupplier);
      handles.put(partitionPath, handle);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.storage.HoodieFileReader' to 'org.apache.hudi.io.storage.HoodieFileReader'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroFileReaderFactory.java`
#### Snippet
```java
  @Override
  public HoodieFileReader newBootstrapFileReader(HoodieFileReader skeletonFileReader, HoodieFileReader dataFileReader, Option<String[]> partitionFields, Object[] partitionValues) {
    return new HoodieAvroBootstrapFileReader(skeletonFileReader, dataFileReader, partitionFields, partitionValues);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.storage.HoodieFileReader' to 'org.apache.hudi.io.storage.HoodieFileReader'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroFileReaderFactory.java`
#### Snippet
```java
  @Override
  public HoodieFileReader newBootstrapFileReader(HoodieFileReader skeletonFileReader, HoodieFileReader dataFileReader, Option<String[]> partitionFields, Object[] partitionValues) {
    return new HoodieAvroBootstrapFileReader(skeletonFileReader, dataFileReader, partitionFields, partitionValues);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.avro.HoodieAvroWriteSupport' to 'org.apache.parquet.hadoop.api.WriteSupport'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetStreamWriter.java`
#### Snippet
```java
                                   HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig) throws IOException {
    this.writeSupport = parquetConfig.getWriteSupport();
    this.writer = new Builder<IndexedRecord>(new OutputStreamBackedOutputFile(outputStream), writeSupport)
        .withWriteMode(ParquetFileWriter.Mode.CREATE)
        .withCompressionCodec(parquetConfig.getCompressionCodecName())
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Set' to 'java.util.Set'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroOrcReader.java`
#### Snippet
```java
  @Override
  public Set<String> filterRowKeys(Set candidateRowKeys) {
    return orcUtils.filterRowKeys(conf, path, candidateRowKeys);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Set' to 'java.util.Set'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroOrcReader.java`
#### Snippet
```java
  @Override
  public Set<String> filterRowKeys(Set candidateRowKeys) {
    return orcUtils.filterRowKeys(conf, path, candidateRowKeys);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieBootstrapRecordIterator.java`
#### Snippet
```java
    HoodieRecord<T> dataRecord = dataFileIterator.next();
    HoodieRecord<T> skeletonRecord = skeletonIterator.next();
    HoodieRecord<T> ret = dataRecord.prependMetaFields(schema, schema,
        new MetadataValues().setCommitTime(skeletonRecord.getRecordKey(schema, HoodieRecord.COMMIT_TIME_METADATA_FIELD))
            .setCommitSeqno(skeletonRecord.getRecordKey(schema, HoodieRecord.COMMIT_SEQNO_METADATA_FIELD))
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.io.storage.HoodieParquetConfig' to 'org.apache.hudi.io.storage.HoodieParquetConfig'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroFileWriterFactory.java`
#### Snippet
```java
      compressionCodecName = null;
    }
    HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig = new HoodieParquetConfig(writeSupport,
        CompressionCodecName.fromConf(compressionCodecName),
        config.getIntOrDefault(HoodieStorageConfig.PARQUET_BLOCK_SIZE),
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieParquetConfig(T, CompressionCodecName, int, int, long, Configuration, double, ...)' as a member of raw type 'org.apache.hudi.io.storage.HoodieParquetConfig'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroFileWriterFactory.java`
#### Snippet
```java
      compressionCodecName = null;
    }
    HoodieParquetConfig<HoodieAvroWriteSupport> parquetConfig = new HoodieParquetConfig(writeSupport,
        CompressionCodecName.fromConf(compressionCodecName),
        config.getIntOrDefault(HoodieStorageConfig.PARQUET_BLOCK_SIZE),
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/HoodieRollbackStat.java`
#### Snippet
```java
    public HoodieRollbackStat build() {
      if (successDeleteFiles == null) {
        successDeleteFiles = Collections.EMPTY_LIST;
      }
      if (failedDeleteFiles == null) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/HoodieRollbackStat.java`
#### Snippet
```java
      }
      if (failedDeleteFiles == null) {
        failedDeleteFiles = Collections.EMPTY_LIST;
      }
      if (commandBlocksCount == null) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/HoodieRollbackStat.java`
#### Snippet
```java
      }
      if (commandBlocksCount == null) {
        commandBlocksCount = Collections.EMPTY_MAP;
      }
      return new HoodieRollbackStat(partitionPath, successDeleteFiles, failedDeleteFiles, commandBlocksCount);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
  public GenericRecord convert(String json, Schema schema) {
    try {
      Map<String, Object> jsonObjectMap = mapper.readValue(json, Map.class);
      return convertJsonToAvro(jsonObjectMap, schema, shouldSanitize, invalidCharMask);
    } catch (IOException e) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
        Schema valueSchema = schema.getValueType();
        Map<String, Object> mapRes = new HashMap<>();
        for (Map.Entry<String, Object> v : ((Map<String, Object>) value).entrySet()) {
          mapRes.put(v.getKey(), convertJsonToAvroField(v.getValue(), name, valueSchema, shouldSanitize, invalidCharMask));
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
      public Pair<Boolean, Object> convert(Object value, String name, Schema schema, boolean shouldSanitize, String invalidCharMask) {
        GenericRecord result = new GenericData.Record(schema);
        return Pair.of(true, convertJsonToAvro((Map<String, Object>) value, schema, shouldSanitize, invalidCharMask));
      }
    };
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
        // The ObjectMapper use List to represent FixedType
        // eg: "decimal_val": [0, 0, 14, -63, -52] will convert to ArrayList<Integer>
        List<Integer> converval = (List<Integer>) value;
        byte[] src = new byte[converval.size()];
        for (int i = 0; i < converval.size(); i++) {
```

### UNCHECKED_WARNING
Unchecked call to 'scheduleCompactionAtInstant(String, Option\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
    // and so we try compaction w/ instant C4001. So, we can avoid compaction if we already have compaction w/ same instant time.
    if (!metadataMetaClient.getActiveTimeline().filterCompletedInstants().containsInstant(compactionInstantTime)
        && writeClient.scheduleCompactionAtInstant(compactionInstantTime, Option.empty())) {
      writeClient.compact(compactionInstantTime);
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Iterator' to 'java.util.Iterator'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java

  public static Iterator<GenericRecord> rewriteRecordWithNewSchema(Iterator<GenericRecord> oldRecords, Schema newSchema, Map<String, String> renameCols) {
    return rewriteRecordWithNewSchema(oldRecords, newSchema, Collections.EMPTY_MAP, false);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java

  public static Iterator<GenericRecord> rewriteRecordWithNewSchema(Iterator<GenericRecord> oldRecords, Schema newSchema, Map<String, String> renameCols) {
    return rewriteRecordWithNewSchema(oldRecords, newSchema, Collections.EMPTY_MAP, false);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java

  public static GenericRecord rewriteRecordDeep(GenericRecord oldRecord, Schema newSchema, boolean validate) {
    return rewriteRecordWithNewSchema(oldRecord, newSchema, Collections.EMPTY_MAP, validate);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java

  public static GenericRecord rewriteRecordDeep(GenericRecord oldRecord, Schema newSchema) {
    return rewriteRecordWithNewSchema(oldRecord, newSchema, Collections.EMPTY_MAP);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.ArrayList' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
        ValidationUtils.checkArgument(oldRecord instanceof Collection, "cannot rewrite record with different type");
        Collection array = (Collection)oldRecord;
        List<Object> newArray = new ArrayList();
        fieldNames.push("element");
        for (Object element : array) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
      case MAP:
        ValidationUtils.checkArgument(oldRecord instanceof Map, "cannot rewrite record with different type");
        Map<Object, Object> map = (Map<Object, Object>) oldRecord;
        Map<Object, Object> newMap = new HashMap<>();
        fieldNames.push("value");
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java`
#### Snippet
```java
      throw new IllegalArgumentException("The byte[] must not be null");
    }
    return (T) SERIALIZER_REF.get().deserialize(objectData);
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord\>' to 'R'
in `hudi-common/src/main/java/org/apache/hudi/common/util/SpillableMapUtils.java`
#### Snippet
```java
    HoodieRecord<? extends HoodieRecordPayload> hoodieRecord = new HoodieAvroRecord<>(new HoodieKey(recKey, partitionPath),
        HoodieRecordUtils.loadPayload(payloadClazz, new Object[] {null, orderingVal}, GenericRecord.class, Comparable.class));
    return (R) hoodieRecord;
  }
}
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord\>' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/util/SpillableMapUtils.java`
#### Snippet
```java
            Comparable.class), operation);

    return (HoodieRecord<R>) hoodieRecord;
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.avro.generic.GenericData.Record' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/OrcReaderIterator.java`
#### Snippet
```java
      ensureBatch();
      if (this.next == null) {
        this.next = (T) readRecordFromBatch();
      }
      return this.next != null;
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.avro.generic.GenericData.Record' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/OrcReaderIterator.java`
#### Snippet
```java
      }
      T retVal = this.next;
      this.next = (T) readRecordFromBatch();
      return retVal;
    } catch (IOException io) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java`
#### Snippet
```java
  public static Option<String> readCheckpointValue(String value, String id) {
    try {
      Map<String, String> checkpointMap = OBJECT_MAPPER.readValue(value, Map.class);
      if (!checkpointMap.containsKey(id)) {
        return Option.empty();
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.util.Option\>' to 'org.apache.hudi.common.util.Option'
in `hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java`
#### Snippet
```java
  public static Option<String> getValidCheckpointForCurrentWriter(HoodieTimeline timeline, String checkpointKey,
                                                                  String keyToLookup) {
    return (Option<String>) timeline.getWriteTimeline().getReverseOrderedInstants().map(instant -> {
      try {
        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata
            .fromBytes(timeline.getInstantDetails(instant).get(), HoodieCommitMetadata.class);
        // process commits only with checkpoint entries
        String checkpointValue = commitMetadata.getMetadata(checkpointKey);
        if (StringUtils.nonEmpty(checkpointValue)) {
          // return if checkpoint for "keyForLookup" exists.
          return readCheckpointValue(checkpointValue, keyToLookup);
        } else {
          return Option.empty();
        }
      } catch (IOException e) {
        throw new HoodieIOException("Failed to parse HoodieCommitMetadata for " + instant.toString(), e);
      }
    }).filter(Option::isPresent).findFirst().orElse(Option.empty());
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'capture' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/HoodieRecordUtils.java`
#### Snippet
```java
      Class<?>... constructorArgTypes) {
    try {
      return (T) ReflectionUtils.getClass(recordPayloadClass).getConstructor(constructorArgTypes)
          .newInstance(payloadArgs);
    } catch (InstantiationException | IllegalAccessException | InvocationTargetException | NoSuchMethodException e) {
      throw new HoodieException("Unable to instantiate payload class ", e);
```

### UNCHECKED_WARNING
Unchecked cast: 'capture' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java`
#### Snippet
```java
  public static <T> T loadClass(String className) {
    try {
      return (T) getClass(className).newInstance();
    } catch (InstantiationException | IllegalAccessException e) {
      throw new HoodieException("Could not load class " + className, e);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        break;
      case MAP:
        Map<String, ?> mapValue = (Map<String, ?>) value;

        MapColumnVector mapColumnVector = (MapColumnVector) colVector;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.DefaultSizeEstimator' to 'org.apache.hudi.common.util.SizeEstimator'
in `hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueue.java`
#### Snippet
```java
   */
  public BoundedInMemoryQueue(final long memoryLimit, final Function<I, O> transformFunction) {
    this(memoryLimit, transformFunction, new DefaultSizeEstimator() {});
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.io.Serializable' to 'R'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java`
#### Snippet
```java

  public Iterator<R> iterator() {
    return getRocksDBDAO().prefixSearch(columnFamilyName, "").map(p -> (R) (p.getValue())).iterator();
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Pair.java`
#### Snippet
```java
    Comparable otherRight = (Comparable) other.getRight();

    if (thisLeft.compareTo(otherLeft) == 0) {
      return thisRight.compareTo(otherRight);
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Pair.java`
#### Snippet
```java

    if (thisLeft.compareTo(otherLeft) == 0) {
      return thisRight.compareTo(otherRight);
    } else {
      return thisLeft.compareTo(otherLeft);
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Pair.java`
#### Snippet
```java
      return thisRight.compareTo(otherRight);
    } else {
      return thisLeft.compareTo(otherLeft);
    }
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.FlatLists.ComparableListImpl' to 'org.apache.hudi.common.util.collection.FlatLists.ComparableList'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java

  private static <T> ComparableList<T> of_(List<T> t) {
    return new ComparableListImpl(new ArrayList<>(t));
  }

```

### UNCHECKED_WARNING
Unchecked call to 'ComparableListImpl(List)' as a member of raw type 'org.apache.hudi.common.util.collection.FlatLists.ComparableListImpl'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java

  private static <T> ComparableList<T> of_(List<T> t) {
    return new ComparableListImpl(new ArrayList<>(t));
  }

```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Triple.java`
#### Snippet
```java
    Comparable otherLeft = (Comparable) other.getLeft();

    if (thisLeft.compareTo(otherLeft) == 0) {
      return Pair.of(getMiddle(), getRight()).compareTo(Pair.of(other.getMiddle(), other.getRight()));
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/Triple.java`
#### Snippet
```java
      return Pair.of(getMiddle(), getRight()).compareTo(Pair.of(other.getMiddle(), other.getRight()));
    } else {
      return thisLeft.compareTo(otherLeft);
    }
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java
    @Override
    public void read(Kryo kryo, Input input) {
      list = (List<T>) kryo.readClassAndObject(input);
    }
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java

  public static <T extends Comparable> ComparableList<T> ofComparableArray(Object[] t) {
    return ofComparable(Arrays.stream(t).map(v -> (T)v).collect(Collectors.toList()));
  }

```

### UNCHECKED_WARNING
Unchecked method 'compare(T, T)' invocation
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java
        Comparable o0 = list0.get(i);
        Comparable o1 = list1.get(i);
        int c = compare(o0, o1);
        if (c != 0) {
          return c;
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDbDiskMap.java`
#### Snippet
```java
    R value = get(key);
    if (value != null) {
      keySet.remove((T) key);
      getRocksDb().delete(ROCKSDB_COL_FAMILY, (T) key);
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/FlatLists.java`
#### Snippet
```java

    public int compareTo(List o) {
      return compare(list, o);
    }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDbDiskMap.java`
#### Snippet
```java
    if (value != null) {
      keySet.remove((T) key);
      getRocksDb().delete(ROCKSDB_COL_FAMILY, (T) key);
    }
    return value;
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDbDiskMap.java`
#### Snippet
```java
      return null;
    }
    return getRocksDb().get(ROCKSDB_COL_FAMILY, (T) key);
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDbDiskMap.java`
#### Snippet
```java
  @Override
  public boolean containsKey(Object key) {
    return keySet.contains((T) key);
  }

```

### UNCHECKED_WARNING
Unchecked method 'create(String, String, T, T, long, long, long, ...)' invocation
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
                .map(columnChunkMetaData -> {
                  Statistics stats = columnChunkMetaData.getStatistics();
                  return HoodieColumnRangeMetadata.<Comparable>create(
                      parquetFilePath.getName(),
                      columnChunkMetaData.getPath().toDotString(),
```

### UNCHECKED_WARNING
Unchecked method 'getColumnRangeInFile(List\>)' invocation
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    Stream<HoodieColumnRangeMetadata<Comparable>> stream = columnToStatsListMap.values()
        .stream()
        .map(this::getColumnRangeInFile);

    return stream.collect(Collectors.toList());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'. Reason: 'new LazyFileIterable(filePath, valueMetadataMap, isCompressionEnabled)' has raw type, so result of iterator is erased
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
  @Override
  public Iterator<R> iterator() {
    ClosableIterator<R> iterator = new LazyFileIterable(filePath, valueMetadataMap, isCompressionEnabled).iterator();
    this.iterators.add(iterator);
    return iterator;
```

### UNCHECKED_WARNING
Unchecked call to 'LazyFileIterable(String, Map, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.LazyFileIterable'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
  @Override
  public Iterator<R> iterator() {
    ClosableIterator<R> iterator = new LazyFileIterable(filePath, valueMetadataMap, isCompressionEnabled).iterator();
    this.iterators.add(iterator);
    return iterator;
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'R'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
  public Stream<R> valueStream() {
    final BufferedRandomAccessFile file = getRandomAccessFile();
    return valueMetadataMap.values().stream().sorted().sequential().map(valueMetaData -> (R) get(valueMetaData, file, isCompressionEnabled));
  }

```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java`
#### Snippet
```java
      return this;
    }
    if (oldValue.orderingVal.compareTo(orderingVal) > 0) {
      // pick the payload with greatest ordering value
      return oldValue;
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/model/DefaultHoodieRecordPayload.java`
#### Snippet
```java
        orderField,
        true, consistentLogicalTimestampEnabled);
    return persistedOrderingVal == null || ((Comparable) persistedOrderingVal).compareTo(incomingOrderingVal) <= 0;
  }

```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieColumnRangeMetadata.java`
#### Snippet
```java
      return val1;
    }
    return val1.compareTo(val2) < 0 ? val1 : val2;
  }

```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieColumnRangeMetadata.java`
#### Snippet
```java
      return val1;
    }
    return val1.compareTo(val2) > 0 ? val1 : val2;
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieColumnRangeMetadata' to 'org.apache.hudi.common.model.HoodieColumnRangeMetadata'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieColumnRangeMetadata.java`
#### Snippet
```java
    long totalSize = left.getTotalSize() + right.getTotalSize();
    long totalUncompressedSize = left.getTotalUncompressedSize() + right.getTotalUncompressedSize();
    return create(filePath, columnName, min, max, nullCount, valueCount, totalSize, totalUncompressedSize);
  }

```

### UNCHECKED_WARNING
Unchecked method 'create(String, String, T, T, long, long, long, ...)' invocation
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieColumnRangeMetadata.java`
#### Snippet
```java
    long totalSize = left.getTotalSize() + right.getTotalSize();
    long totalUncompressedSize = left.getTotalUncompressedSize() + right.getTotalUncompressedSize();
    return create(filePath, columnName, min, max, nullCount, valueCount, totalSize, totalUncompressedSize);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.ArrayList' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/model/CompactionOperation.java`
#### Snippet
```java
    op.dataFileName = Option.ofNullable(operation.getDataFilePath());
    op.dataFileCommitTime = op.dataFileName.map(p -> FSUtils.getCommitTime(new Path(p).getName()));
    op.deltaFileNames = new ArrayList<>(operation.getDeltaFilePaths());
    op.id = new HoodieFileGroupId(operation.getPartitionPath(), operation.getFileId());
    op.metrics = operation.getMetrics() == null ? new HashMap<>() : new HashMap<>(operation.getMetrics());
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.HashMap' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/model/CompactionOperation.java`
#### Snippet
```java
    op.deltaFileNames = new ArrayList<>(operation.getDeltaFilePaths());
    op.id = new HoodieFileGroupId(operation.getPartitionPath(), operation.getFileId());
    op.metrics = operation.getMetrics() == null ? new HashMap<>() : new HashMap<>(operation.getMetrics());
    op.bootstrapFilePath = Option.ofNullable(operation.getBootstrapFilePath());
    return op;
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.ArrayList' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/model/ClusteringOperation.java`
#### Snippet
```java

  public static ClusteringOperation create(HoodieSliceInfo sliceInfo) {
    return new ClusteringOperation(sliceInfo.getDataFilePath(), new ArrayList<>(sliceInfo.getDeltaFilePaths()), sliceInfo.getFileId(),
        sliceInfo.getPartitionPath(), sliceInfo.getBootstrapFilePath(), sliceInfo.getVersion());
  }
```

### UNCHECKED_WARNING
Unchecked overriding: return type requires unchecked conversion. Found 'org.apache.hudi.common.model.HoodieRecord', required 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroIndexedRecord.java`
#### Snippet
```java

  @Override
  public HoodieRecord newInstance() {
    return new HoodieAvroIndexedRecord(this);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroIndexedRecord.java`
#### Snippet
```java

    HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
    return hoodieRecord;
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: '((HoodieAvroRecord) newer).getData().combineAndGetUpdateValue(previousAvroData.get(), schema, props)' has raw type, so result of map is erased
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecordMerger.java`
#### Snippet
```java
    }

    return ((HoodieAvroRecord) newer).getData().combineAndGetUpdateValue(previousAvroData.get(), schema, props)
        .map(combinedAvroPayload -> new HoodieAvroIndexedRecord((IndexedRecord) combinedAvroPayload));
  }

```

### UNCHECKED_WARNING
Unchecked call to 'map(Function)' as a member of raw type 'org.apache.hudi.common.util.Option'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecordMerger.java`
#### Snippet
```java
    }

    return ((HoodieAvroRecord) newer).getData().combineAndGetUpdateValue(previousAvroData.get(), schema, props)
        .map(combinedAvroPayload -> new HoodieAvroIndexedRecord((IndexedRecord) combinedAvroPayload));
  }

```

### UNCHECKED_WARNING
Unchecked call to 'preCombine(T, Schema, Properties)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecordPayload'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecordMerger.java`
#### Snippet
```java

  private HoodieRecord preCombine(HoodieRecord older, HoodieRecord newer, Schema schema, Properties props) {
    HoodieRecordPayload payload = unsafeCast(((HoodieAvroRecord) newer).getData().preCombine(((HoodieAvroRecord) older).getData(), schema, props));
    return new HoodieAvroRecord(newer.getKey(), payload, newer.getOperation());
  }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T, HoodieOperation)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecordMerger.java`
#### Snippet
```java
  private HoodieRecord preCombine(HoodieRecord older, HoodieRecord newer, Schema schema, Properties props) {
    HoodieRecordPayload payload = unsafeCast(((HoodieAvroRecord) newer).getData().preCombine(((HoodieAvroRecord) older).getData(), schema, props));
    return new HoodieAvroRecord(newer.getKey(), payload, newer.getOperation());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecord.java`
#### Snippet
```java

  public Option<Map<String, String>> getMetadata() {
    return getData().getMetadata();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecord.java`
#### Snippet
```java
  public HoodieRecord prependMetaFields(Schema recordSchema, Schema targetSchema, MetadataValues metadataValues, Properties props) {
    try {
      Option<IndexedRecord> avroRecordOpt = getData().getInsertValue(recordSchema, props);
      GenericRecord newAvroRecord = HoodieAvroUtils.rewriteRecordWithNewSchema(avroRecordOpt.get(), targetSchema);
      updateMetadataValuesInternal(newAvroRecord, metadataValues);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecord.java`
#### Snippet
```java
  @Override
  public Option<HoodieAvroIndexedRecord> toIndexedRecord(Schema recordSchema, Properties props) throws IOException {
    Option<IndexedRecord> avroData = getData().getInsertValue(recordSchema, props);
    if (avroData.isPresent()) {
      HoodieAvroIndexedRecord record =
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroRecord.java`
#### Snippet
```java
    if (avroData.isPresent()) {
      HoodieAvroIndexedRecord record =
          new HoodieAvroIndexedRecord(key, avroData.get(), operation, getData().getMetadata());
      return Option.of(record);
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'ParquetWriter(Path, WriteSupport, CompressionCodecName, int, int)' as a member of raw type 'org.apache.parquet.hadoop.ParquetWriter'
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodiePartitionMetadata.java`
#### Snippet
```java
          MessageType type = Types.buildMessage().optional(PrimitiveTypeName.INT64).named("dummyint").named("dummy");
          HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(type, schema, Option.empty());
          try (ParquetWriter writer = new ParquetWriter(filePath, writeSupport, CompressionCodecName.UNCOMPRESSED, 1024, 1024)) {
            for (String key : props.stringPropertyNames()) {
              writeSupport.addFooterMetadata(key, props.getProperty(key));
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
    }
    // pick the payload with greater ordering value as insert record
    final boolean shouldPickOldRecord = oldValue.orderingVal.compareTo(orderingVal) > 0 ? true : false;
    try {
      GenericRecord oldRecord = HoodieAvroUtils.bytesToAvro(oldValue.recordBytes, schema);
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
      return oldOrderingVal != null
          && ReflectionUtils.isSameClass(oldOrderingVal, orderingVal)
          && oldOrderingVal.compareTo(orderingVal) > 0;
    }
    return false;
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.CloseableMappingIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieCDCLogRecordIterator.java`
#### Snippet
```java
    closeItr();
    // TODO support cdc with spark record.
    itr = new CloseableMappingIterator(dataBlock.getRecordIterator(HoodieRecordType.AVRO), record -> ((HoodieAvroIndexedRecord) record).getData());
    return itr.hasNext();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'CloseableMappingIterator(ClosableIterator*, Function)' as a member of raw type 'org.apache.hudi.common.util.collection.CloseableMappingIterator'*
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieCDCLogRecordIterator.java`
#### Snippet
```java
    closeItr();
    // TODO support cdc with spark record.
    itr = new CloseableMappingIterator(dataBlock.getRecordIterator(HoodieRecordType.AVRO), record -> ((HoodieAvroIndexedRecord) record).getData());
    return itr.hasNext();
  }
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java`
#### Snippet
```java
      boolean choosePrev = !deleteOrderingVal.equals(0)
          && ReflectionUtils.isSameClass(curOrderingVal, deleteOrderingVal)
          && curOrderingVal.compareTo(deleteOrderingVal) > 0;
      if (choosePrev) {
        // The DELETE message is obsolete if the old message has greater orderingVal.
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java`
#### Snippet
```java
  protected <T> void processNextRecord(HoodieRecord<T> newRecord) throws IOException {
    String key = newRecord.getRecordKey();
    HoodieRecord<T> prevRecord = records.get(key);
    if (prevRecord != null) {
      // Merge and store the combined record
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieMergedLogRecordScanner.java`
#### Snippet
```java
    if (prevRecord != null) {
      // Merge and store the combined record
      HoodieRecord<T> combinedRecord = (HoodieRecord<T>) recordMerger.merge(prevRecord, readerSchema,
          newRecord, readerSchema, this.getPayloadProps()).get().getLeft();
      // If pre-combine returns existing record, no need to update it
      if (combinedRecord.getData() != prevRecord.getData()) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator\>'. Reason: 'HoodieFileReaderFactory.getReaderFactory(type).getFileReader(inlineConf, inlineLogFilePath, PARQUET)' has raw type, so result of getRecordIterator is erased
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieParquetDataBlock.java`
#### Snippet
```java
    Schema writerSchema = new Schema.Parser().parse(this.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));

    ClosableIterator<HoodieRecord<T>> iterator = HoodieFileReaderFactory.getReaderFactory(type).getFileReader(inlineConf, inlineLogFilePath, PARQUET)
        .getRecordIterator(writerSchema, readerSchema);
    return iterator;
  }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieHFileDataBlock.java`
#### Snippet
```java
        fullKey ? reader.getRecordsByKeysIterator(sortedKeys, readerSchema) : reader.getRecordsByKeyPrefixIterator(sortedKeys, readerSchema);

    return new CloseableMappingIterator<>(recordIterator, data -> (HoodieRecord<T>) data);
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.model.HoodieAvroIndexedRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java`
#### Snippet
```java
    // TODO AvroSparkReader need
    RecordIterator iterator = RecordIterator.getInstance(this, content);
    return new CloseableMappingIterator<>(iterator, data -> (HoodieRecord<T>) new HoodieAvroIndexedRecord(data));
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.stream.Stream' to 'java.util.stream.Stream\>'. Reason: '((ExternalSpillableMap) fgIdToPendingLogCompaction)' has raw type, so result of valueStream is erased
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  Stream<Pair<String, CompactionOperation>> fetchPendingLogCompactionOperations() {
    return ((ExternalSpillableMap) fgIdToPendingLogCompaction).valueStream();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.stream.Stream' to 'java.util.stream.Stream'. Reason: '((ExternalSpillableMap) partitionToFileGroupsMap).valueStream()' has raw type, so result of flatMap is erased
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  public Stream<HoodieFileGroup> fetchAllStoredFileGroups() {
    return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream().flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());
  }

```

### UNCHECKED_WARNING
Unchecked call to 'flatMap(Function\>)' as a member of raw type 'java.util.stream.Stream'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  public Stream<HoodieFileGroup> fetchAllStoredFileGroups() {
    return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream().flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  public Stream<HoodieFileGroup> fetchAllStoredFileGroups() {
    return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream().flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.stream.Stream' to 'java.util.stream.Stream'. Reason: '((ExternalSpillableMap) partitionToFileGroupsMap).valueStream()' has raw type, so result of flatMap is erased
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  public Stream<HoodieFileGroup> getAllFileGroups() {
    return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream()
        .flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());
  }

```

### UNCHECKED_WARNING
Unchecked call to 'flatMap(Function\>)' as a member of raw type 'java.util.stream.Stream'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  public Stream<HoodieFileGroup> getAllFileGroups() {
    return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream()
        .flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  public Stream<HoodieFileGroup> getAllFileGroups() {
    return ((ExternalSpillableMap) partitionToFileGroupsMap).valueStream()
        .flatMap(fg -> ((List<HoodieFileGroup>) fg).stream());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map\>'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, HoodieInstant> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForClusteringFileGroups, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, HoodieInstant> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForClusteringFileGroups, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, HoodieInstant> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForReplaceFileGroups, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, HoodieInstant> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForReplaceFileGroups, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.stream.Stream' to 'java.util.stream.Stream\>'. Reason: '((ExternalSpillableMap) fgIdToPendingCompaction)' has raw type, so result of valueStream is erased
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  Stream<Pair<String, CompactionOperation>> fetchPendingCompactionOperations() {
    return ((ExternalSpillableMap) fgIdToPendingCompaction).valueStream();
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map\>'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingLogCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingLogCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, BootstrapBaseFileMapping> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForBootstrapBaseFile, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, BootstrapBaseFileMapping> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForBootstrapBaseFile, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.stream.Stream' to 'java.util.stream.Stream'. Reason: '((ExternalSpillableMap) fgIdToBootstrapBaseFile)' has raw type, so result of valueStream is erased
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
  @Override
  Stream<BootstrapBaseFileMapping> fetchBootstrapBaseFiles() {
    return ((ExternalSpillableMap) fgIdToBootstrapBaseFile).valueStream();
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'java.util.Map\>'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      return (Map<String, List<HoodieFileGroup>>) (new ExternalSpillableMap<>(maxMemoryForFileGroupMap, baseStoreDir,
          new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled));
    } catch (IOException e) {
      throw new RuntimeException(e);
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
          + ", BaseDir=" + baseStoreDir);
      new File(baseStoreDir).mkdirs();
      return (Map<String, List<HoodieFileGroup>>) (new ExternalSpillableMap<>(maxMemoryForFileGroupMap, baseStoreDir,
          new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled));
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineMetadataUtils.java`
#### Snippet
```java

  public static <T extends SpecificRecordBase> T deserializeAvroRecordMetadata(Object object, Schema schema) {
    return  (T) SpecificData.get().deepCopy(schema, object);
  }
}
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.model.HoodieRecord' to 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
                populateMetaFields,
                Option.empty());
        processNextRecord(completedRecord);
        totalLogRecords.incrementAndGet();
      }
```

### UNCHECKED_WARNING
Unchecked call to 'rewriteRecordWithNewSchema(Schema, Properties, Schema, Map)' as a member of raw type 'org.apache.hudi.common.model.HoodieRecord'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java

    return Option.of(Pair.of((record) -> {
      return record.rewriteRecordWithNewSchema(
          dataBlock.getSchema(),
          this.hoodieTableMetaClient.getTableConfig().getProps(),
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
    if (keySpecOpt.isPresent()) {
      KeySpec keySpec = keySpecOpt.get();
      blockRecordsIterator = (ClosableIterator) dataBlock
          .getRecordIterator(keySpec.getKeys(), keySpec.isFullKey(), recordType);
    } else {
      blockRecordsIterator = (ClosableIterator) dataBlock.getRecordIterator(recordType);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ClosableIterator' to 'org.apache.hudi.common.util.collection.ClosableIterator'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
          .getRecordIterator(keySpec.getKeys(), keySpec.isFullKey(), recordType);
    } else {
      blockRecordsIterator = (ClosableIterator) dataBlock.getRecordIterator(recordType);
    }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    Response response = retryHelper != null ? retryHelper.start(() -> get(timeoutMs, url, method)) : get(timeoutMs, url, method);
    String content = response.returnContent().asString(Consts.UTF_8);
    return (T) mapper.readValue(content, reference);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.RetryHelper' to 'org.apache.hudi.common.util.RetryHelper'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    this.timeoutMs = viewConf.getRemoteTimelineClientTimeoutSecs() * 1000;
    if (viewConf.isRemoteTimelineClientRetryEnabled()) {
      retryHelper = new RetryHelper(
          viewConf.getRemoteTimelineClientMaxRetryIntervalMs(),
          viewConf.getRemoteTimelineClientMaxRetryNumbers(),
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/io/FileBasedInternalSchemaStorageManager.java`
#### Snippet
```java
  @Override
  public String getHistorySchemaStr() {
    return getHistorySchemaStrByGivenValidCommits(Collections.EMPTY_LIST);
  }

```

### UNCHECKED_WARNING
Unchecked call to 'add(E)' as a member of raw type 'java.util.Set'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java
        throw new IllegalArgumentException(String.format("cannot delete missing columns: %s", name));
      }
      deletes.add(field.fieldId());
      return this;
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Set' to 'java.util.Set'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java

    public Set<Integer> getDeletes() {
      return deletes;
    }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.LinkedList' to 'java.util.Deque'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/convert/AvroInternalSchemaConverter.java`
#### Snippet
```java
  public static Type buildTypeFromAvroSchema(Schema schema) {
    // set flag to check this has not been visited.
    Deque<String> visited = new LinkedList();
    AtomicInteger nextId = new AtomicInteger(1);
    return visitAvroSchemaToBuildType(schema, visited, true, nextId);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.lang.Class' to 'java.lang.Class'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/BootstrapColumnStichingRecordReader.java`
#### Snippet
```java
    int numColumns = numLeftColumns + numRightColumns;
    if (rightW.getValueClass() != null) {
      values = new ArrayWritable(rightW.getValueClass(), new Writable[numColumns]);
    } else {
      String[] vals = IntStream.range(0, numColumns).mapToObj(idx -> "").collect(Collectors.toList())
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.lang.Class' to 'java.lang.Class'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SafeParquetRecordReaderWrapper.java`
#### Snippet
```java
    // Call createValue of parquetReader to get size and class type info only
    Writable[] emptyWritableBuf = new Writable[numValueFields];
    return new ArrayWritable(valueClass, emptyWritableBuf);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'combinedPayload' has raw type, so result of getInsertValue is erased
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    HoodieMetadataPayload anotherPayload = new HoodieMetadataPayload(Option.of((GenericRecord) oldRecord));
    HoodieRecordPayload combinedPayload = preCombine(anotherPayload);
    return combinedPayload.getInsertValue(schema, properties);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hadoop.mapred.RecordReader' to 'org.apache.hadoop.mapred.RecordReader'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieCombineRealtimeRecordReader.java`
#### Snippet
```java
        LOG.info("Creating new RealtimeRecordReader for split");
        recordReaders.add(
            new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) rtSplit, jobConf, readers.remove(0)));
      }
      currentRecordReader = recordReaders.remove(0);
```

### UNCHECKED_WARNING
Unchecked call to 'getRecordReader(JobConf, CombineFileSplit, Reporter, Class\>)' as a member of raw type 'org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat.HoodieCombineFileInputFormatShim'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
      HoodieCombineFileInputFormatShim shims = createInputFormatShim();
      IOContextMap.get(job).setInputPath(((CombineHiveInputSplit) split).getPath(0));
      return shims.getRecordReader(job, ((CombineHiveInputSplit) split).getInputSplitShim(),
          reporter, CombineHiveRecordReader.class);
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'getRecordReader(JobConf, CombineFileSplit, Reporter, Class\>)' as a member of raw type 'org.apache.hadoop.hive.shims.HadoopShims.CombineFileInputFormatShim'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
          reporter, CombineHiveRecordReader.class);
    } else {
      return ShimLoader.getHadoopShims().getCombineFileInputFormat().getRecordReader(job, (CombineFileSplit) split,
          reporter, CombineHiveRecordReader.class);
    }
```

### UNCHECKED_WARNING
Unchecked call to 'CombineFileRecordReader(JobConf, CombineFileSplit, Reporter, Class\>)' as a member of raw type 'org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
        return new HoodieCombineRealtimeRecordReader(job, split, recordReaders);
      }
      return new HadoopShimsSecure.CombineFileRecordReader(job, split, reporter, rrClass);
    }

```

### UNCHECKED_WARNING
Unchecked call to 'add(E)' as a member of raw type 'java.util.ArrayList'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
          CombineFileSplit split = (CombineFileSplit) splits[pos];
          if (split.getPaths().length > 0) {
            inputSplitShims.add(new HadoopShimsSecure.InputSplitShim(job, split.getPaths(), split.getStartOffsets(),
                split.getLengths(), split.getLocations()));
          }
```

### UNCHECKED_WARNING
Unchecked call to 'toArray(T\[\])' as a member of raw type 'java.util.ArrayList'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
          }
        }
        return (CombineFileSplit[]) inputSplitShims
            .toArray(new HadoopShimsSecure.InputSplitShim[inputSplitShims.size()]);
      }
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'client' has raw type, so result of scheduleCompaction is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
      if (StringUtils.isNullOrEmpty(cfg.compactionInstantTime)) {
        LOG.warn("No instant time is provided for scheduling compaction.");
        return client.scheduleCompaction(Option.empty());
      }

```

### UNCHECKED_WARNING
Unchecked call to 'scheduleCompaction(Option\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
      if (StringUtils.isNullOrEmpty(cfg.compactionInstantTime)) {
        LOG.warn("No instant time is provided for scheduling compaction.");
        return client.scheduleCompaction(Option.empty());
      }

```

### UNCHECKED_WARNING
Unchecked call to 'scheduleCompactionAtInstant(String, Option\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
      }

      client.scheduleCompactionAtInstant(cfg.compactionInstantTime, Option.empty());
      return Option.of(cfg.compactionInstantTime);
    }
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
            partition, metaClient.getActiveTimeline().filterCompletedInstants().lastInstant().get().getTimestamp());
      } else {
        return Collections.EMPTY_LIST;
      }
    } else {
```

### UNCHECKED_WARNING
Unchecked method 'create(String, String, T, T, long, long, long, ...)' invocation
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
   */
  public static HoodieColumnRangeMetadata<Comparable> convertColumnStatsRecordToColumnRangeMetadata(HoodieMetadataColumnStats columnStats) {
    return HoodieColumnRangeMetadata.<Comparable>create(
        columnStats.getFileName(),
        columnStats.getColumnName(),
```

### UNCHECKED_WARNING
Unchecked method 'create(String, String, T, T, long, long, long, ...)' invocation
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
        .map(field -> {
          ColumnStats colStats = allColumnStats.get(field.name());
          return HoodieColumnRangeMetadata.<Comparable>create(
              filePath,
              field.name(),
```

### UNCHECKED_WARNING
Unchecked call to 'getMethod(String, Class...)' as a member of raw type 'java.lang.Class'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/ProtoKafkaSource.java`
#### Snippet
```java
      if (parseMethod == null) {
        try {
          parseMethod = getProtoClass().getMethod("parseFrom", byte[].class);
        } catch (NoSuchMethodException ex) {
          throw new HoodieException("Unable to get proto parsing method from specified class: " + className, ex);
```

### UNCHECKED_WARNING
Unchecked method 'newAPIHadoopFile(String, Class, Class, Class, Configuration)' invocation
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HiveIncrPullSource.java`
#### Snippet
```java
      List<FileStatus> commitDeltaFiles = Arrays.asList(fs.listStatus(new Path(incrPullRootPath, commitToPull.get())));
      String pathStr = commitDeltaFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(","));
      JavaPairRDD<AvroKey, NullWritable> avroRDD = sparkContext.newAPIHadoopFile(pathStr, AvroKeyInputFormat.class,
          AvroKey.class, NullWritable.class, sparkContext.hadoopConfiguration());
      sparkContext.setJobGroup(this.getClass().getSimpleName(), "Fetch new data");
```

### UNCHECKED_WARNING
Unchecked method 'newAPIHadoopFile(String, Class, Class, Class, Configuration)' invocation
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/AvroDFSSource.java`
#### Snippet
```java
  private JavaRDD<GenericRecord> fromFiles(String pathStr) {
    sparkContext.setJobGroup(this.getClass().getSimpleName(), "Fetch Avro data from files");
    JavaPairRDD<AvroKey, NullWritable> avroRDD = sparkContext.newAPIHadoopFile(pathStr, AvroKeyInputFormat.class,
        AvroKey.class, NullWritable.class, sparkContext.hadoopConfiguration());
    return avroRDD.keys().map(r -> ((GenericRecord) r.datum()));
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceCloudStorageHelper.java`
#### Snippet
```java

    try {
      sparkOptionsMap = mapper.readValue(props.getString(SPARK_DATASOURCE_OPTIONS.key()), Map.class);
    } catch (IOException e) {
      throw new HoodieException(String.format("Failed to parse sparkOptions: %s",
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/S3EventsMetaSelector.java`
#### Snippet
```java
      if (messageBody.has(SQS_MODEL_MESSAGE)) {
        // If this messages is from S3Event -> SNS -> SQS
        messageMap = (Map<String, Object>) mapper.readValue(messageBody.getString(SQS_MODEL_MESSAGE), Map.class);
      } else {
        // If this messages is from S3Event -> SQS
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/S3EventsMetaSelector.java`
#### Snippet
```java
      } else {
        // If this messages is from S3Event -> SQS
        messageMap = (Map<String, Object>) mapper.readValue(messageBody.toString(), Map.class);
      }
      if (messageMap.containsKey(SQS_MODEL_EVENT_RECORDS)) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/S3EventsMetaSelector.java`
#### Snippet
```java
      }
      if (messageMap.containsKey(SQS_MODEL_EVENT_RECORDS)) {
        List<Map<String, Object>> events = (List<Map<String, Object>>) messageMap.get(SQS_MODEL_EVENT_RECORDS);
        for (Map<String, Object> event : events) {
          event.remove(S3_EVENT_RESPONSE_ELEMENTS);
```

### UNCHECKED_WARNING
Unchecked method 'newAPIHadoopFile(String, Class, Class, Class, Configuration)' invocation
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java`
#### Snippet
```java
    HoodieEngineContext context = new HoodieSparkEngineContext(jsc);
    context.setJobStatus(this.getClass().getSimpleName(), "Build records for import: " + cfg.tableName);
    return jsc.newAPIHadoopFile(cfg.srcPath, ParquetInputFormat.class, Void.class, GenericRecord.class,
            job.getConfiguration())
        // To reduce large number of tasks.
```

### UNCHECKED_WARNING
Unchecked assignment: 'scala.util.Right' to 'scala.util.Either'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java`
#### Snippet
```java
      genericRecord = fromJson(json);
    } catch (Exception e) {
      return new Right(json);
    }
    return new Left(genericRecord);
```

### UNCHECKED_WARNING
Unchecked call to 'Right(B)' as a member of raw type 'scala.util.Right'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java`
#### Snippet
```java
      genericRecord = fromJson(json);
    } catch (Exception e) {
      return new Right(json);
    }
    return new Left(genericRecord);
```

### UNCHECKED_WARNING
Unchecked assignment: 'scala.util.Left' to 'scala.util.Either'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java`
#### Snippet
```java
      return new Right(json);
    }
    return new Left(genericRecord);
  }

```

### UNCHECKED_WARNING
Unchecked call to 'Left(A)' as a member of raw type 'scala.util.Left'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/AvroConvertor.java`
#### Snippet
```java
      return new Right(json);
    }
    return new Left(genericRecord);
  }

```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/SanitizationUtils.java`
#### Snippet
```java
    return src.stream().map(obj -> {
      if (obj instanceof List) {
        return transformList((List<Object>) obj, invalidCharMask);
      } else if (obj instanceof Map) {
        return transformMap((Map<String, Object>) obj, invalidCharMask);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/SanitizationUtils.java`
#### Snippet
```java
        return transformList((List<Object>) obj, invalidCharMask);
      } else if (obj instanceof Map) {
        return transformMap((Map<String, Object>) obj, invalidCharMask);
      } else {
        return obj;
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/SanitizationUtils.java`
#### Snippet
```java
    try {
      OM.enable(JsonParser.Feature.ALLOW_COMMENTS);
      Map<String, Object> objMap = OM.readValue(schemaStr, Map.class);
      Map<String, Object> modifiedMap = transformMap(objMap, invalidCharMask);
      return Option.of(new Schema.Parser().parse(OM.writeValueAsString(modifiedMap)));
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/SanitizationUtils.java`
#### Snippet
```java
        .map(kv -> {
          if (kv.getValue() instanceof List) {
            return Pair.of(kv.getKey(), transformList((List<Object>) kv.getValue(), invalidCharMask));
          } else if (kv.getValue() instanceof Map) {
            return Pair.of(kv.getKey(), transformMap((Map<String, Object>) kv.getValue(), invalidCharMask));
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/SanitizationUtils.java`
#### Snippet
```java
            return Pair.of(kv.getKey(), transformList((List<Object>) kv.getValue(), invalidCharMask));
          } else if (kv.getValue() instanceof Map) {
            return Pair.of(kv.getKey(), transformMap((Map<String, Object>) kv.getValue(), invalidCharMask));
          } else if (kv.getValue() instanceof String) {
            String currentStrValue = (String) kv.getValue();
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelectorCommon.java`
#### Snippet
```java
      Map<String, String> sparkOptionsMap = null;
      try {
        sparkOptionsMap = mapper.readValue(datasourceOpts, Map.class);
      } catch (IOException e) {
        throw new HoodieException(String.format("Failed to parse sparkOptions: %s", datasourceOpts), e);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.lang.Object' to 'java.util.List'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/ProtoConversionUtil.java`
#### Snippet
```java
      switch (schema.getType()) {
        case ARRAY:
          List<Object> arrayValue = (List<Object>) value;
          List<Object> arrayCopy = new GenericData.Array<>(arrayValue.size(), schema);
          for (Object obj : arrayValue) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/ProtoConversionUtil.java`
#### Snippet
```java
          return tmpValue;
        case MAP:
          Map<Object, Object> mapValue = (Map) value;
          Map<Object, Object> mapCopy = new HashMap<>(mapValue.size());
          for (Map.Entry<Object, Object> entry : mapValue.entrySet()) {
```

### UNCHECKED_WARNING
Unchecked call to 'getMethod(String, Class...)' as a member of raw type 'java.lang.Class'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/ProtoConversionUtil.java`
#### Snippet
```java
      return SCHEMA_CACHE.computeIfAbsent(new SchemaCacheKey(c, wrappedPrimitivesAsRecords, maxRecursionDepth, timestampsAsRecords), key -> {
        try {
          Object descriptor = c.getMethod("getDescriptor").invoke(null);
          if (c.isEnum()) {
            return getEnumSchema((Descriptors.EnumDescriptor) descriptor);
```

### UNCHECKED_WARNING
Unchecked call to 'KafkaConsumer(Map)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
    Map<TopicPartition, Long> offsetMap = CheckpointUtils.strToOffsets(checkpointStr);
    Map<TopicPartition, OffsetAndMetadata> offsetAndMetadataMap = new HashMap<>(offsetMap.size());
    try (KafkaConsumer consumer = new KafkaConsumer(kafkaParams)) {
      offsetMap.forEach((topicPartition, offset) -> offsetAndMetadataMap.put(topicPartition, new OffsetAndMetadata(offset)));
      consumer.commitSync(offsetAndMetadataMap);
```

### UNCHECKED_WARNING
Unchecked call to 'commitSync(Map)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
    try (KafkaConsumer consumer = new KafkaConsumer(kafkaParams)) {
      offsetMap.forEach((topicPartition, offset) -> offsetAndMetadataMap.put(topicPartition, new OffsetAndMetadata(offset)));
      consumer.commitSync(offsetAndMetadataMap);
    } catch (CommitFailedException | TimeoutException e) {
      LOG.warn("Committing offsets to Kafka failed, this does not impact processing of records", e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map\>'. Reason: 'consumer' has raw type, so result of listTopics is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   */
  public boolean checkTopicExists(KafkaConsumer consumer)  {
    Map<String, List<PartitionInfo>> result = consumer.listTopics();
    return result.containsKey(topicName);
  }
```

### UNCHECKED_WARNING
Unchecked call to 'KafkaConsumer(Map)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
    Map<TopicPartition, Long> fromOffsets;
    Map<TopicPartition, Long> toOffsets;
    try (KafkaConsumer consumer = new KafkaConsumer(kafkaParams)) {
      if (!checkTopicExists(consumer)) {
        throw new HoodieException("Kafka topic:" + topicName + " does not exist");
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of beginningOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
        switch (autoResetValue) {
          case EARLIEST:
            fromOffsets = consumer.beginningOffsets(topicPartitions);
            break;
          case LATEST:
```

### UNCHECKED_WARNING
Unchecked call to 'beginningOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
        switch (autoResetValue) {
          case EARLIEST:
            fromOffsets = consumer.beginningOffsets(topicPartitions);
            break;
          case LATEST:
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of endOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
            break;
          case LATEST:
            fromOffsets = consumer.endOffsets(topicPartitions);
            break;
          case GROUP:
```

### UNCHECKED_WARNING
Unchecked call to 'endOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
            break;
          case LATEST:
            fromOffsets = consumer.endOffsets(topicPartitions);
            break;
          case GROUP:
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of endOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java

      // Obtain the latest offsets.
      toOffsets = consumer.endOffsets(topicPartitions);
    }

```

### UNCHECKED_WARNING
Unchecked call to 'endOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java

      // Obtain the latest offsets.
      toOffsets = consumer.endOffsets(topicPartitions);
    }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of beginningOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
  private Map<TopicPartition, Long> fetchValidOffsets(KafkaConsumer consumer,
                                                        Option<String> lastCheckpointStr, Set<TopicPartition> topicPartitions) {
    Map<TopicPartition, Long> earliestOffsets = consumer.beginningOffsets(topicPartitions);
    Map<TopicPartition, Long> checkpointOffsets = CheckpointUtils.strToOffsets(lastCheckpointStr.get());
    boolean isCheckpointOutOfBounds = checkpointOffsets.entrySet().stream()
```

### UNCHECKED_WARNING
Unchecked call to 'beginningOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
  private Map<TopicPartition, Long> fetchValidOffsets(KafkaConsumer consumer,
                                                        Option<String> lastCheckpointStr, Set<TopicPartition> topicPartitions) {
    Map<TopicPartition, Long> earliestOffsets = consumer.beginningOffsets(topicPartitions);
    Map<TopicPartition, Long> checkpointOffsets = CheckpointUtils.strToOffsets(lastCheckpointStr.get());
    boolean isCheckpointOutOfBounds = checkpointOffsets.entrySet().stream()
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'consumer' has raw type, so result of partitionsFor is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
    do {
      // TODO(HUDI-4625) cleanup, introduce retrying client
      partitionInfos = consumer.partitionsFor(topicName);
      try {
        if (partitionInfos == null) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of beginningOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
                                                    .collect(Collectors.toMap(Function.identity(), x -> timestamp));

    Map<TopicPartition, Long> earliestOffsets = consumer.beginningOffsets(topicPartitions);
    Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestamp = consumer.offsetsForTimes(topicPartitionsTimestamp);

```

### UNCHECKED_WARNING
Unchecked call to 'beginningOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
                                                    .collect(Collectors.toMap(Function.identity(), x -> timestamp));

    Map<TopicPartition, Long> earliestOffsets = consumer.beginningOffsets(topicPartitions);
    Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestamp = consumer.offsetsForTimes(topicPartitionsTimestamp);

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of offsetsForTimes is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java

    Map<TopicPartition, Long> earliestOffsets = consumer.beginningOffsets(topicPartitions);
    Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestamp = consumer.offsetsForTimes(topicPartitionsTimestamp);

    StringBuilder sb = new StringBuilder();
```

### UNCHECKED_WARNING
Unchecked call to 'offsetsForTimes(Map)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java

    Map<TopicPartition, Long> earliestOffsets = consumer.beginningOffsets(topicPartitions);
    Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestamp = consumer.offsetsForTimes(topicPartitionsTimestamp);

    StringBuilder sb = new StringBuilder();
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of endOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
    Long delayCount = 0L;
    Map<TopicPartition, Long> checkpointOffsets = CheckpointUtils.strToOffsets(lastCheckpointStr.get());
    Map<TopicPartition, Long> lastOffsets = consumer.endOffsets(topicPartitions);

    for (Map.Entry<TopicPartition, Long> entry : lastOffsets.entrySet()) {
```

### UNCHECKED_WARNING
Unchecked call to 'endOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
    Long delayCount = 0L;
    Map<TopicPartition, Long> checkpointOffsets = CheckpointUtils.strToOffsets(lastCheckpointStr.get());
    Map<TopicPartition, Long> lastOffsets = consumer.endOffsets(topicPartitions);

    for (Map.Entry<TopicPartition, Long> entry : lastOffsets.entrySet()) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'. Reason: 'consumer' has raw type, so result of endOffsets is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
      } else {
        LOG.warn("There are no commits associated with this consumer group, starting to consume from latest offset");
        fromOffsets = consumer.endOffsets(topicPartitions);
        break;
      }
```

### UNCHECKED_WARNING
Unchecked call to 'endOffsets(Collection)' as a member of raw type 'org.apache.kafka.clients.consumer.KafkaConsumer'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
      } else {
        LOG.warn("There are no commits associated with this consumer group, starting to consume from latest offset");
        fromOffsets = consumer.endOffsets(topicPartitions);
        break;
      }
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltastreamerMultiWriterCkptUpdateFunc.java`
#### Snippet
```java
      String value = latestCommitMetadata.get().getExtraMetadata().get(CHECKPOINT_KEY);
      try {
        checkpointMap = OBJECT_MAPPER.readValue(value, Map.class);
      } catch (Exception e) {
        throw new HoodieException("Failed to parse checkpoint as map", e);
```

### UNCHECKED_WARNING
Unchecked call to 'SparkRDDWriteClient(HoodieEngineContext, HoodieWriteConfig, Option)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SparkSampleWritesUtils.java`
#### Snippet
```java
        .withPath(sampleWritesBasePath)
        .build();
    try (SparkRDDWriteClient sampleWriteClient = new SparkRDDWriteClient(new HoodieSparkEngineContext(jsc), sampleWriteConfig, Option.empty())) {
      int size = writeConfig.getIntOrDefault(SAMPLE_WRITES_SIZE);
      List<HoodieRecord> samples = records.coalesce(1).take(size);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'sampleWriteClient' has raw type, so result of bulkInsert is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SparkSampleWritesUtils.java`
#### Snippet
```java
      List<HoodieRecord> samples = records.coalesce(1).take(size);
      sampleWriteClient.startCommitWithTime(instantTime);
      JavaRDD<WriteStatus> writeStatusRDD = sampleWriteClient.bulkInsert(jsc.parallelize(samples, 1), instantTime);
      if (writeStatusRDD.filter(WriteStatus::hasErrors).count() > 0) {
        LOG.error(String.format("sample writes for table %s failed with errors.", writeConfig.getTableName()));
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SparkSampleWritesUtils.java`
#### Snippet
```java
      List<HoodieRecord> samples = records.coalesce(1).take(size);
      sampleWriteClient.startCommitWithTime(instantTime);
      JavaRDD<WriteStatus> writeStatusRDD = sampleWriteClient.bulkInsert(jsc.parallelize(samples, 1), instantTime);
      if (writeStatusRDD.filter(WriteStatus::hasErrors).count() > 0) {
        LOG.error(String.format("sample writes for table %s failed with errors.", writeConfig.getTableName()));
```

### UNCHECKED_WARNING
Unchecked call to 'bootstrap(Option\>)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
        checkpointCommitMetadata.put(HoodieDeltaStreamer.CHECKPOINT_RESET_KEY, cfg.checkpoint);
      }
      bootstrapClient.bootstrap(Option.of(checkpointCommitMetadata));
      syncHive();
    } finally {
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case ROW:
        //we do the sanitizing here if enabled
        InputBatch<Dataset<Row>> datasetInputBatch = maybeSanitizeFieldNames(((Source<Dataset<Row>>) source).fetchNext(lastCkptStr, sourceLimit));
        return new InputBatch<>(processErrorEvents(datasetInputBatch.getBatch(),
            ErrorEvent.ErrorReason.JSON_ROW_DESERIALIZATION_FAILURE),
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case AVRO: {
        //don't need to sanitize because it's already avro
        InputBatch<JavaRDD<GenericRecord>> r = ((Source<JavaRDD<GenericRecord>>) source).fetchNext(lastCkptStr, sourceLimit);
        return avroDataInRowFormat(r);
      }
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java

        }
        InputBatch<JavaRDD<String>> r = ((Source<JavaRDD<String>>) source).fetchNext(lastCkptStr, sourceLimit);
        Schema sourceSchema = r.getSchemaProvider().getSourceSchema();
        if (errorTableWriter.isPresent()) {
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case PROTO: {
        //TODO([HUDI-5830]) implement field name sanitization
        InputBatch<JavaRDD<Message>> r = ((Source<JavaRDD<Message>>) source).fetchNext(lastCkptStr, sourceLimit);
        Schema sourceSchema = r.getSchemaProvider().getSourceSchema();
        AvroConvertor convertor = new AvroConvertor(r.getSchemaProvider().getSourceSchema());
```

### UNCHECKED_WARNING
Unchecked call to 'addErrorEvents(JavaRDD)' as a member of raw type 'org.apache.hudi.utilities.deltastreamer.BaseErrorTableWriter'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
          if (errorTableWriter.isPresent() && Arrays.stream(dataset.columns()).collect(Collectors.toList())
              .contains(ERROR_TABLE_CURRUPT_RECORD_COL_NAME)) {
            errorTableWriter.get().addErrorEvents(dataset.filter(new Column(ERROR_TABLE_CURRUPT_RECORD_COL_NAME).isNotNull())
                .select(new Column(ERROR_TABLE_CURRUPT_RECORD_COL_NAME)).toJavaRDD().map(ev ->
                    new ErrorEvent<>(ev.getString(0), errorReason)));
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case AVRO:
        //don't need to sanitize because it's already avro
        return ((Source<JavaRDD<GenericRecord>>) source).fetchNext(lastCkptStr, sourceLimit);
      case JSON: {
        //sanitizing is done inside the convertor in transformJsonToGenericRdd if enabled
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case JSON: {
        //sanitizing is done inside the convertor in transformJsonToGenericRdd if enabled
        InputBatch<JavaRDD<String>> r = ((Source<JavaRDD<String>>) source).fetchNext(lastCkptStr, sourceLimit);
        JavaRDD<GenericRecord> eventsRdd = transformJsonToGenericRdd(r);
        return new InputBatch<>(Option.ofNullable(eventsRdd),r.getCheckpointForNextBatch(), r.getSchemaProvider());
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case ROW: {
        //we do the sanitizing here if enabled
        InputBatch<Dataset<Row>> r = maybeSanitizeFieldNames(((Source<Dataset<Row>>) source).fetchNext(lastCkptStr, sourceLimit));
        return new InputBatch<>(Option.ofNullable(r.getBatch().map(
            rdd -> {
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.utilities.sources.Source' to 'org.apache.hudi.utilities.sources.Source\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      case PROTO: {
        //TODO([HUDI-5830]) implement field name sanitization
        InputBatch<JavaRDD<Message>> r = ((Source<JavaRDD<Message>>) source).fetchNext(lastCkptStr, sourceLimit);
        AvroConvertor convertor = new AvroConvertor(r.getSchemaProvider().getSourceSchema());
        return new InputBatch<>(Option.ofNullable(r.getBatch().map(rdd -> rdd.map(convertor::fromProtoMessage)).orElse(null)),
```

### UNCHECKED_WARNING
Unchecked call to 'addErrorEvents(JavaRDD)' as a member of raw type 'org.apache.hudi.utilities.deltastreamer.BaseErrorTableWriter'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
      if (errorTableWriter.isPresent()) {
        JavaRDD<Either<GenericRecord,String>> javaRDD = rdd.map(convertor::fromJsonWithError);
        errorTableWriter.get().addErrorEvents(javaRDD.filter(x -> x.isRight()).map(x ->
            new ErrorEvent<>(x.right().get(), ErrorEvent.ErrorReason.JSON_AVRO_DESERIALIZATION_FAILURE)));
        return javaRDD.filter(x -> x.isLeft()).map(x -> x.left().get());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.collection.ExternalSpillableMap' to 'org.apache.hudi.common.util.collection.ExternalSpillableMap\>'
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java`
#### Snippet
```java
      long memoryForMerge = IOUtils.getMaxMemoryPerPartitionMerge(context.getTaskContextSupplier(), config);
      LOG.info("MaxMemoryPerPartitionMerge => " + memoryForMerge);
      this.bufferedRecords = new ExternalSpillableMap<>(memoryForMerge,
          config.getSpillableMapBasePath(),
          new DefaultSizeEstimator(),
```

### UNCHECKED_WARNING
Unchecked call to 'ExternalSpillableMap(Long, String, SizeEstimator, SizeEstimator, DiskMapType, boolean)' as a member of raw type 'org.apache.hudi.common.util.collection.ExternalSpillableMap'
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java`
#### Snippet
```java
      long memoryForMerge = IOUtils.getMaxMemoryPerPartitionMerge(context.getTaskContextSupplier(), config);
      LOG.info("MaxMemoryPerPartitionMerge => " + memoryForMerge);
      this.bufferedRecords = new ExternalSpillableMap<>(memoryForMerge,
          config.getSpillableMapBasePath(),
          new DefaultSizeEstimator(),
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'writeClient' has raw type, so result of upsertPreppedRecords is erased
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java`
#### Snippet
```java
      if (!bufferedRecords.isEmpty()) {
        if (isMorTable) {
          writeStatuses = writeClient.upsertPreppedRecords(
              new LinkedList<>(bufferedRecords.values()),
              instantTime);
```

### UNCHECKED_WARNING
Unchecked call to 'upsertPreppedRecords(List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieJavaWriteClient'
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java`
#### Snippet
```java
      if (!bufferedRecords.isEmpty()) {
        if (isMorTable) {
          writeStatuses = writeClient.upsertPreppedRecords(
              new LinkedList<>(bufferedRecords.values()),
              instantTime);
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'writeClient' has raw type, so result of bulkInsertPreppedRecords is erased
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java`
#### Snippet
```java
              instantTime);
        } else {
          writeStatuses = writeClient.bulkInsertPreppedRecords(
              new LinkedList<>(bufferedRecords.values()),
              instantTime, Option.empty());
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsertPreppedRecords(List\>, String, Option)' as a member of raw type 'org.apache.hudi.client.HoodieJavaWriteClient'
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/BufferedConnectWriter.java`
#### Snippet
```java
              instantTime);
        } else {
          writeStatuses = writeClient.bulkInsertPreppedRecords(
              new LinkedList<>(bufferedRecords.values()),
              instantTime, Option.empty());
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.flink.table.runtime.typeutils.AbstractRowDataSerializer' to 'org.apache.flink.table.runtime.typeutils.AbstractRowDataSerializer'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/sort/SortOperator.java`
#### Snippet
```java
            computeMemorySize(),
            this.getContainingTask().getEnvironment().getIOManager(),
            inputSerializer,
            binarySerializer,
            computer,
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'this.writeClient' has raw type, so result of insert is erased
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
    switch (WriteOperationType.fromValue(writeOperation)) {
      case INSERT:
        this.writeFunction = (records, instantTime) -> this.writeClient.insert(records, instantTime);
        break;
      case UPSERT:
```

### UNCHECKED_WARNING
Unchecked call to 'insert(List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieFlinkWriteClient'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
    switch (WriteOperationType.fromValue(writeOperation)) {
      case INSERT:
        this.writeFunction = (records, instantTime) -> this.writeClient.insert(records, instantTime);
        break;
      case UPSERT:
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'this.writeClient' has raw type, so result of upsert is erased
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
      case UPSERT:
      case DELETE: // shares the code path with UPSERT
        this.writeFunction = (records, instantTime) -> this.writeClient.upsert(records, instantTime);
        break;
      case INSERT_OVERWRITE:
```

### UNCHECKED_WARNING
Unchecked call to 'upsert(List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieFlinkWriteClient'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
      case UPSERT:
      case DELETE: // shares the code path with UPSERT
        this.writeFunction = (records, instantTime) -> this.writeClient.upsert(records, instantTime);
        break;
      case INSERT_OVERWRITE:
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'this.writeClient' has raw type, so result of insertOverwrite is erased
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
        break;
      case INSERT_OVERWRITE:
        this.writeFunction = (records, instantTime) -> this.writeClient.insertOverwrite(records, instantTime);
        break;
      case INSERT_OVERWRITE_TABLE:
```

### UNCHECKED_WARNING
Unchecked call to 'insertOverwrite(List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieFlinkWriteClient'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
        break;
      case INSERT_OVERWRITE:
        this.writeFunction = (records, instantTime) -> this.writeClient.insertOverwrite(records, instantTime);
        break;
      case INSERT_OVERWRITE_TABLE:
```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.List' to 'java.util.List'. Reason: 'this.writeClient' has raw type, so result of insertOverwriteTable is erased
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
        break;
      case INSERT_OVERWRITE_TABLE:
        this.writeFunction = (records, instantTime) -> this.writeClient.insertOverwriteTable(records, instantTime);
        break;
      default:
```

### UNCHECKED_WARNING
Unchecked call to 'insertOverwriteTable(List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieFlinkWriteClient'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/StreamWriteFunction.java`
#### Snippet
```java
        break;
      case INSERT_OVERWRITE_TABLE:
        this.writeFunction = (records, instantTime) -> this.writeClient.insertOverwriteTable(records, instantTime);
        break;
      default:
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'writeClient' has raw type, so result of scheduleClustering is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
  public Option<String> getClusteringInstantOpt() {
    if (writeClient != null) {
      return writeClient.scheduleClustering(Option.empty());
    } else {
      return Option.empty();
```

### UNCHECKED_WARNING
Unchecked call to 'scheduleClustering(Option\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
  public Option<String> getClusteringInstantOpt() {
    if (writeClient != null) {
      return writeClient.scheduleClustering(Option.empty());
    } else {
      return Option.empty();
```

### UNCHECKED_WARNING
Unchecked call to 'collect(T)' as a member of raw type 'org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionPlanSourceFunction.java`
#### Snippet
```java
      LOG.info("CompactionPlanFunction compacting " + operations + " files");
      for (CompactionOperation operation : operations) {
        sourceContext.collect(new CompactionPlanEvent(pair.getLeft(), operation));
      }
    }
```

### UNCHECKED_WARNING
Unchecked call to 'addErrorEvents(JavaRDD)' as a member of raw type 'org.apache.hudi.utilities.deltastreamer.BaseErrorTableWriter'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
                    HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE, reconcileSchema,
                    Option.of(this.userProvidedSchemaProvider.getTargetSchema()));
                errorTableWriter.get().addErrorEvents(safeCreateRDDs._2().toJavaRDD()
                    .map(evStr -> new ErrorEvent<>(evStr,
                        ErrorEvent.ErrorReason.AVRO_DESERIALIZATION_FAILURE)));
```

### UNCHECKED_WARNING
Unchecked cast: 'org.apache.hudi.common.util.Option\>' to 'org.apache.hudi.common.util.Option\>'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java

  protected Option<Pair<String, HoodieCommitMetadata>> getLatestInstantAndCommitMetadataWithValidCheckpointInfo(HoodieTimeline timeline) throws IOException {
    return (Option<Pair<String, HoodieCommitMetadata>>) timeline.getReverseOrderedInstants().map(instant -> {
      try {
        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata
            .fromBytes(timeline.getInstantDetails(instant).get(), HoodieCommitMetadata.class);
        if (!StringUtils.isNullOrEmpty(commitMetadata.getMetadata(CHECKPOINT_KEY)) || !StringUtils.isNullOrEmpty(commitMetadata.getMetadata(CHECKPOINT_RESET_KEY))) {
          return Option.of(Pair.of(instant.toString(), commitMetadata));
        } else {
          return Option.empty();
        }
      } catch (IOException e) {
        throw new HoodieIOException("Failed to parse HoodieCommitMetadata for " + instant.toString(), e);
      }
    }).filter(Option::isPresent).findFirst().orElse(Option.empty());
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
  public static Option<String> readCheckpointValue(String value, String id) {
    try {
      Map<String,String> checkpointMap = OBJECT_MAPPER.readValue(value, Map.class);
      if (!checkpointMap.containsKey(id)) {
        return Option.empty();
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.action.HoodieWriteMetadata' to 'org.apache.hudi.table.action.HoodieWriteMetadata\>'. Reason: 'writeClient' has raw type, so result of compact is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        Option<String> pendingCompactionInstant = getLastPendingCompactionInstant(allCommitsTimelineOpt);
        if (pendingCompactionInstant.isPresent()) {
          HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = writeClient.compact(pendingCompactionInstant.get());
          writeClient.commitCompaction(pendingCompactionInstant.get(), writeMetadata.getCommitMetadata().get(), Option.empty());
          refreshTimeline();
```

### UNCHECKED_WARNING
Unchecked call to 'commitCompaction(String, HoodieCommitMetadata, Option\>)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        if (pendingCompactionInstant.isPresent()) {
          HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = writeClient.compact(pendingCompactionInstant.get());
          writeClient.commitCompaction(pendingCompactionInstant.get(), writeMetadata.getCommitMetadata().get(), Option.empty());
          refreshTimeline();
          reInitWriteClient(schemaProvider.getSourceSchema(), schemaProvider.getTargetSchema(), null);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'writeClient' has raw type, so result of insert is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
    switch (cfg.operation) {
      case INSERT:
        writeStatusRDD = writeClient.insert(records, instantTime);
        break;
      case UPSERT:
```

### UNCHECKED_WARNING
Unchecked call to 'insert(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
    switch (cfg.operation) {
      case INSERT:
        writeStatusRDD = writeClient.insert(records, instantTime);
        break;
      case UPSERT:
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'writeClient' has raw type, so result of upsert is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        break;
      case UPSERT:
        writeStatusRDD = writeClient.upsert(records, instantTime);
        break;
      case BULK_INSERT:
```

### UNCHECKED_WARNING
Unchecked call to 'upsert(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        break;
      case UPSERT:
        writeStatusRDD = writeClient.upsert(records, instantTime);
        break;
      case BULK_INSERT:
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'writeClient' has raw type, so result of bulkInsert is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        break;
      case BULK_INSERT:
        writeStatusRDD = writeClient.bulkInsert(records, instantTime);
        break;
      case INSERT_OVERWRITE:
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        break;
      case BULK_INSERT:
        writeStatusRDD = writeClient.bulkInsert(records, instantTime);
        break;
      case INSERT_OVERWRITE:
```

### UNCHECKED_WARNING
Unchecked call to 'insertOverwrite(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        break;
      case INSERT_OVERWRITE:
        writeStatusRDD = writeClient.insertOverwrite(records, instantTime).getWriteStatuses();
        break;
      case INSERT_OVERWRITE_TABLE:
```

### UNCHECKED_WARNING
Unchecked call to 'insertOverwriteTable(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        break;
      case INSERT_OVERWRITE_TABLE:
        writeStatusRDD = writeClient.insertOverwriteTable(records, instantTime).getWriteStatuses();
        break;
      case DELETE_PARTITION:
```

### UNCHECKED_WARNING
Unchecked call to 'deletePartitions(List, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
      case DELETE_PARTITION:
        List<String> partitions = records.map(record -> record.getPartitionPath()).distinct().collect();
        writeStatusRDD = writeClient.deletePartitions(partitions, instantTime).getWriteStatuses();
        break;
      default:
```

### UNCHECKED_WARNING
Unchecked call to 'upsertAndCommit(String, Option)' as a member of raw type 'org.apache.hudi.utilities.deltastreamer.BaseErrorTableWriter'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        // Commit the error events triggered so far to the error table
        Option<String> commitedInstantTime = getLatestInstantWithValidCheckpointInfo(commitsTimelineOpt);
        boolean errorTableSuccess = errorTableWriter.get().upsertAndCommit(instantTime, commitedInstantTime);
        if (!errorTableSuccess) {
          switch (errorWriteFailureStrategy) {
```

### UNCHECKED_WARNING
Unchecked call to 'commit(String, JavaRDD, Option\>, String, Map\>, Option\>)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        }
      }
      boolean success = writeClient.commit(instantTime, writeStatusRDD, Option.of(checkpointCommitMetadata), commitActionType, Collections.emptyMap(), extraPreCommitFunc);
      if (success) {
        LOG.info("Commit " + instantTime + " successful!");
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'writeClient' has raw type, so result of scheduleCompaction is erased
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        // Schedule compaction if needed
        if (cfg.isAsyncCompactionEnabled()) {
          scheduledCompactionInstant = writeClient.scheduleCompaction(Option.empty());
        }

```

### UNCHECKED_WARNING
Unchecked call to 'scheduleCompaction(Option\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
        // Schedule compaction if needed
        if (cfg.isAsyncCompactionEnabled()) {
          scheduledCompactionInstant = writeClient.scheduleCompaction(Option.empty());
        }

```

### UNCHECKED_WARNING
Unchecked assignment: 'java.util.Map' to 'java.util.Map'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/FlinkClusteringConfig.java`
#### Snippet
```java
   */
  public static Configuration toFlinkConfig(FlinkClusteringConfig config) {
    Map<String, String> propsMap = new HashMap<String, String>((Map) getProps(config));
    org.apache.flink.configuration.Configuration conf = fromMap(propsMap);

```

### UNCHECKED_WARNING
Unchecked call to 'completeTableService(TableServiceType, HoodieCommitMetadata, HoodieTable\>, List, List\>, String)' as a member of raw type 'org.apache.hudi.client.HoodieFlinkWriteClient'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringCommitSink.java`
#### Snippet
```java
    // commit the clustering
    this.table.getMetaClient().reloadActiveTimeline();
    this.writeClient.completeTableService(
        TableServiceType.CLUSTER, writeMetadata.getCommitMetadata().get(), table, instant);

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.flink.table.runtime.typeutils.AbstractRowDataSerializer' to 'org.apache.flink.table.runtime.typeutils.AbstractRowDataSerializer'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringOperator.java`
#### Snippet
```java
            computeMemorySize(),
            this.getContainingTask().getEnvironment().getIOManager(),
            (AbstractRowDataSerializer) binarySerializer,
            binarySerializer,
            computer,
```

### UNCHECKED_WARNING
Unchecked call to 'scheduleClustering(Option\>)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClusteringUtil.java`
#### Snippet
```java
    validateClusteringScheduling(conf);
    if (committed) {
      writeClient.scheduleClustering(Option.empty());
    }
  }
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'. Reason: 'writeClient.getTableServiceClient()' has raw type, so result of getPendingRollbackInfo is erased
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClusteringUtil.java`
#### Snippet
```java
      LOG.info("Rollback the inflight clustering instant: " + inflightInstant + " for failover");
      table.rollbackInflightClustering(inflightInstant,
          commitToRollback -> writeClient.getTableServiceClient().getPendingRollbackInfo(table.getMetaClient(), commitToRollback, false));
      table.getMetaClient().reloadActiveTimeline();
    });
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
      }
      try {
        return record.getData().getInsertValue(tableSchema);
      } catch (IOException e) {
        throw new HoodieIOException("Get insert value from payload exception", e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + curAvroKey, e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) recordsIterator.next();
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + hoodieRecord.getRecordKey(), e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + curAvroKey, e);
```

### UNCHECKED_WARNING
Unchecked call to 'readToVector(int, VECTOR)' as a member of raw type 'org.apache.flink.formats.parquet.vector.reader.ColumnReader'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RowColumnReader.java`
#### Snippet
```java
    boolean[] isNulls = new boolean[readNumber];
    for (int i = 0; i < vectors.length; i++) {
      fieldReaders.get(i).readToVector(readNumber, vectors[i]);

      for (int j = 0; j < readNumber; j++) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
        ((HeapBytesVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          byte[] src = ((List<byte[]>) valueList).get(i);
          if (src == null) {
            ((HeapBytesVector) lcv.child).setNullAt(i);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapBooleanVector) lcv.child).vector[i] =
                ((List<Boolean>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapByteVector) lcv.child).vector[i] =
                (byte) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapShortVector) lcv.child).vector[i] =
                (short) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapIntVector) lcv.child).setNullAt(i);
          } else {
            ((HeapIntVector) lcv.child).vector[i] = ((List<Integer>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapFloatVector) lcv.child).setNullAt(i);
          } else {
            ((HeapFloatVector) lcv.child).vector[i] = ((List<Float>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapLongVector) lcv.child).setNullAt(i);
          } else {
            ((HeapLongVector) lcv.child).vector[i] = ((List<Long>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapDoubleVector) lcv.child).vector[i] =
                ((List<Double>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapTimestampVector) lcv.child)
                .setTimestamp(i, ((List<TimestampData>) valueList).get(i));
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Integer>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Long>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              byte[] src = ((List<byte[]>) valueList).get(i);
              if (valueList.get(i) == null) {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
```

### UNCHECKED_WARNING
Unchecked call to 'readToVector(int, VECTOR)' as a member of raw type 'org.apache.flink.formats.parquet.vector.reader.ColumnReader'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RowColumnReader.java`
#### Snippet
```java
    boolean[] isNulls = new boolean[readNumber];
    for (int i = 0; i < vectors.length; i++) {
      fieldReaders.get(i).readToVector(readNumber, vectors[i]);

      for (int j = 0; j < readNumber; j++) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
        ((HeapBytesVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          byte[] src = ((List<byte[]>) valueList).get(i);
          if (src == null) {
            ((HeapBytesVector) lcv.child).setNullAt(i);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapBooleanVector) lcv.child).vector[i] =
                ((List<Boolean>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapByteVector) lcv.child).vector[i] =
                (byte) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapShortVector) lcv.child).vector[i] =
                (short) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapIntVector) lcv.child).setNullAt(i);
          } else {
            ((HeapIntVector) lcv.child).vector[i] = ((List<Integer>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapFloatVector) lcv.child).setNullAt(i);
          } else {
            ((HeapFloatVector) lcv.child).vector[i] = ((List<Float>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapLongVector) lcv.child).setNullAt(i);
          } else {
            ((HeapLongVector) lcv.child).vector[i] = ((List<Long>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapDoubleVector) lcv.child).vector[i] =
                ((List<Double>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapTimestampVector) lcv.child)
                .setTimestamp(i, ((List<TimestampData>) valueList).get(i));
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Integer>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Long>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              byte[] src = ((List<byte[]>) valueList).get(i);
              if (valueList.get(i) == null) {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
```

### UNCHECKED_WARNING
Unchecked call to 'readToVector(int, VECTOR)' as a member of raw type 'org.apache.flink.formats.parquet.vector.reader.ColumnReader'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RowColumnReader.java`
#### Snippet
```java
    boolean[] isNulls = new boolean[readNumber];
    for (int i = 0; i < vectors.length; i++) {
      fieldReaders.get(i).readToVector(readNumber, vectors[i]);

      for (int j = 0; j < readNumber; j++) {
```

### UNCHECKED_WARNING
Unchecked call to 'readToVector(int, VECTOR)' as a member of raw type 'org.apache.flink.formats.parquet.vector.reader.ColumnReader'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RowColumnReader.java`
#### Snippet
```java
    boolean[] isNulls = new boolean[readNumber];
    for (int i = 0; i < vectors.length; i++) {
      fieldReaders.get(i).readToVector(readNumber, vectors[i]);

      for (int j = 0; j < readNumber; j++) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
        ((HeapBytesVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          byte[] src = ((List<byte[]>) valueList).get(i);
          if (src == null) {
            ((HeapBytesVector) lcv.child).setNullAt(i);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapBooleanVector) lcv.child).vector[i] =
                ((List<Boolean>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapByteVector) lcv.child).vector[i] =
                (byte) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapShortVector) lcv.child).vector[i] =
                (short) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapIntVector) lcv.child).setNullAt(i);
          } else {
            ((HeapIntVector) lcv.child).vector[i] = ((List<Integer>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapFloatVector) lcv.child).setNullAt(i);
          } else {
            ((HeapFloatVector) lcv.child).vector[i] = ((List<Float>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapLongVector) lcv.child).setNullAt(i);
          } else {
            ((HeapLongVector) lcv.child).vector[i] = ((List<Long>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapDoubleVector) lcv.child).vector[i] =
                ((List<Double>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapTimestampVector) lcv.child)
                .setTimestamp(i, ((List<TimestampData>) valueList).get(i));
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Integer>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Long>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              byte[] src = ((List<byte[]>) valueList).get(i);
              if (valueList.get(i) == null) {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
```

### UNCHECKED_WARNING
Unchecked call to 'readToVector(int, VECTOR)' as a member of raw type 'org.apache.flink.formats.parquet.vector.reader.ColumnReader'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RowColumnReader.java`
#### Snippet
```java
    boolean[] isNulls = new boolean[readNumber];
    for (int i = 0; i < vectors.length; i++) {
      fieldReaders.get(i).readToVector(readNumber, vectors[i]);

      for (int j = 0; j < readNumber; j++) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
        ((HeapBytesVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          byte[] src = ((List<byte[]>) valueList).get(i);
          if (src == null) {
            ((HeapBytesVector) lcv.child).setNullAt(i);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapBooleanVector) lcv.child).vector[i] =
                ((List<Boolean>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapByteVector) lcv.child).vector[i] =
                (byte) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapShortVector) lcv.child).vector[i] =
                (short) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapIntVector) lcv.child).setNullAt(i);
          } else {
            ((HeapIntVector) lcv.child).vector[i] = ((List<Integer>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapFloatVector) lcv.child).setNullAt(i);
          } else {
            ((HeapFloatVector) lcv.child).vector[i] = ((List<Float>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapLongVector) lcv.child).setNullAt(i);
          } else {
            ((HeapLongVector) lcv.child).vector[i] = ((List<Long>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapDoubleVector) lcv.child).vector[i] =
                ((List<Double>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapTimestampVector) lcv.child)
                .setTimestamp(i, ((List<TimestampData>) valueList).get(i));
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Integer>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Long>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              byte[] src = ((List<byte[]>) valueList).get(i);
              if (valueList.get(i) == null) {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieTimelineArchiver(HoodieWriteConfig, HoodieTable)' as a member of raw type 'org.apache.hudi.client.HoodieTimelineArchiver'
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/ArchiveExecutorUtils.java`
#### Snippet
```java
    HoodieSparkTable<HoodieAvroPayload> table = HoodieSparkTable.create(config, context);
    try {
      HoodieTimelineArchiver archiver = new HoodieTimelineArchiver(config, table);
      archiver.archiveIfRequired(context, true);
    } catch (IOException ioe) {
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
        ((HeapBytesVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          byte[] src = ((List<byte[]>) valueList).get(i);
          if (src == null) {
            ((HeapBytesVector) lcv.child).setNullAt(i);
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapBooleanVector) lcv.child).vector[i] =
                ((List<Boolean>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapByteVector) lcv.child).vector[i] =
                (byte) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapShortVector) lcv.child).vector[i] =
                (short) ((List<Integer>) valueList).get(i).intValue();
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapIntVector) lcv.child).setNullAt(i);
          } else {
            ((HeapIntVector) lcv.child).vector[i] = ((List<Integer>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapFloatVector) lcv.child).setNullAt(i);
          } else {
            ((HeapFloatVector) lcv.child).vector[i] = ((List<Float>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapLongVector) lcv.child).setNullAt(i);
          } else {
            ((HeapLongVector) lcv.child).vector[i] = ((List<Long>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapDoubleVector) lcv.child).vector[i] =
                ((List<Double>) valueList).get(i);
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
          } else {
            ((HeapTimestampVector) lcv.child)
                .setTimestamp(i, ((List<TimestampData>) valueList).get(i));
          }
        }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Integer>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Long>) valueList).get(i);
              }
            }
```

### UNCHECKED_WARNING
Unchecked cast: 'java.util.List' to 'java.util.List'
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
            ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              byte[] src = ((List<byte[]>) valueList).get(i);
              if (valueList.get(i) == null) {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java

    public HoodieRecord generateUpdateRecord(HoodieKey key, String randomString) throws IOException {
      return new HoodieAvroRecord(key, generateRandomValue(key, randomString));
    }

```

### UNCHECKED_WARNING
Unchecked call to 'HoodieAvroRecord(HoodieKey, T)' as a member of raw type 'org.apache.hudi.common.model.HoodieAvroRecord'
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java
        numExistingKeys++;
        try {
          return new HoodieAvroRecord(key, generateRandomValue(key, randomString));
        } catch (IOException e) {
          throw new HoodieIOException(e.getMessage(), e);
```

### UNCHECKED_WARNING
Unchecked method 'newAPIHadoopFile(String, Class, Class, Class, Configuration)' invocation
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
    HoodieEngineContext context = new HoodieSparkEngineContext(jsc);
    context.setJobStatus(this.getClass().getSimpleName(), "Build records for import: " + this.tableName);
    return jsc.newAPIHadoopFile(this.srcPath, ParquetInputFormat.class, Void.class, GenericRecord.class,
            job.getConfiguration())
        // To reduce large number of tasks.
```

### UNCHECKED_WARNING
Unchecked call to 'bootstrap(Option\>)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/BootstrapExecutorUtils.java`
#### Snippet
```java
      HashMap<String, String> checkpointCommitMetadata = new HashMap<>();
      checkpointCommitMetadata.put(CHECKPOINT_KEY, Config.checkpoint);
      bootstrapClient.bootstrap(Option.of(checkpointCommitMetadata));
      syncHive();
    }
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieSparkClusteringClient(BaseHoodieWriteClient\>, JavaRDD, JavaRDD\>)' as a member of raw type 'org.apache.hudi.client.HoodieSparkClusteringClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/async/SparkStreamingAsyncClusteringService.java`
#### Snippet
```java
  @Override
  protected BaseClusterer createClusteringClient(BaseHoodieWriteClient client) {
    return new HoodieSparkClusteringClient(client);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'HoodieSparkCompactor(BaseHoodieWriteClient\>, JavaRDD, JavaRDD\>, HoodieEngineContext)' as a member of raw type 'org.apache.hudi.client.HoodieSparkCompactor'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/async/SparkStreamingAsyncCompactService.java`
#### Snippet
```java
  @Override
  protected BaseCompactor createCompactor(BaseHoodieWriteClient client) {
    return new HoodieSparkCompactor(client, this.context);
  }
}
```

### UNCHECKED_WARNING
Unchecked call to 'compareTo(T)' as a member of raw type 'java.lang.Comparable'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/HoodieSparkRecordMerger.java`
#### Snippet
```java
      return Option.of(Pair.of(newer, newSchema));
    }
    if (older.getOrderingValue(oldSchema, props).compareTo(newer.getOrderingValue(newSchema, props)) > 0) {
      return Option.of(Pair.of(older, oldSchema));
    } else {
```

### UNCHECKED_WARNING
Unchecked call to 'commitStats(String, List, Option\>, String)' as a member of raw type 'org.apache.hudi.client.BaseHoodieWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java`
#### Snippet
```java
  public void commit(List<HoodieWriteStat> writeStatList) {
    try {
      writeClient.commitStats(instantTime, writeStatList, Option.of(extraMetadata),
          CommitUtils.getCommitActionType(operationType, metaClient.getTableType()));
    } catch (Exception ioe) {
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.common.util.Option' to 'org.apache.hudi.common.util.Option\>\>'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
      return StringUtils.isNullOrEmpty(bulkInsertPartitionerClass)
          ? Option.empty() :
          Option.of((BulkInsertPartitioner) ReflectionUtils.loadClass(bulkInsertPartitionerClass, config));
    } catch (Throwable e) {
      throw new HoodieException("Could not create UserDefinedBulkInsertPartitionerRows class " + bulkInsertPartitionerClass, e);
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.hudi.table.BulkInsertPartitioner' to 'org.apache.hudi.table.BulkInsertPartitioner\>'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
      return StringUtils.isNullOrEmpty(bulkInsertPartitionerClass)
          ? Option.empty() :
          Option.of((BulkInsertPartitioner) ReflectionUtils.loadClass(bulkInsertPartitionerClass, config));
    } catch (Throwable e) {
      throw new HoodieException("Could not create UserDefinedBulkInsertPartitionerRows class " + bulkInsertPartitionerClass, e);
```

### UNCHECKED_WARNING
Unchecked call to 'deletePartitions(List, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
  public static HoodieWriteResult doDeletePartitionsOperation(SparkRDDWriteClient client, List<String> partitionsToDelete,
                                                    String instantTime) {
    return client.deletePartitions(partitionsToDelete, instantTime);
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'client' has raw type, so result of delete is erased
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
  public static HoodieWriteResult doDeleteOperation(SparkRDDWriteClient client, JavaRDD<HoodieKey> hoodieKeys,
      String instantTime) {
    return new HoodieWriteResult(client.delete(hoodieKeys, instantTime));
  }

```

### UNCHECKED_WARNING
Unchecked call to 'delete(JavaRDD, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
  public static HoodieWriteResult doDeleteOperation(SparkRDDWriteClient client, JavaRDD<HoodieKey> hoodieKeys,
      String instantTime) {
    return new HoodieWriteResult(client.delete(hoodieKeys, instantTime));
  }

```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'client' has raw type, so result of bulkInsert is erased
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =
                createUserDefinedBulkInsertPartitioner(client.getConfig());
        return new HoodieWriteResult(client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner));
      case INSERT:
        return new HoodieWriteResult(client.insert(hoodieRecords, instantTime));
```

### UNCHECKED_WARNING
Unchecked call to 'bulkInsert(JavaRDD\>, String, Option)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =
                createUserDefinedBulkInsertPartitioner(client.getConfig());
        return new HoodieWriteResult(client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner));
      case INSERT:
        return new HoodieWriteResult(client.insert(hoodieRecords, instantTime));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'client' has raw type, so result of insert is erased
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return new HoodieWriteResult(client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner));
      case INSERT:
        return new HoodieWriteResult(client.insert(hoodieRecords, instantTime));
      case UPSERT:
        return new HoodieWriteResult(client.upsert(hoodieRecords, instantTime));
```

### UNCHECKED_WARNING
Unchecked call to 'insert(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return new HoodieWriteResult(client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner));
      case INSERT:
        return new HoodieWriteResult(client.insert(hoodieRecords, instantTime));
      case UPSERT:
        return new HoodieWriteResult(client.upsert(hoodieRecords, instantTime));
```

### UNCHECKED_WARNING
Unchecked assignment: 'org.apache.spark.api.java.JavaRDD' to 'org.apache.spark.api.java.JavaRDD'. Reason: 'client' has raw type, so result of upsert is erased
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return new HoodieWriteResult(client.insert(hoodieRecords, instantTime));
      case UPSERT:
        return new HoodieWriteResult(client.upsert(hoodieRecords, instantTime));
      case INSERT_OVERWRITE:
        return client.insertOverwrite(hoodieRecords, instantTime);
```

### UNCHECKED_WARNING
Unchecked call to 'upsert(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return new HoodieWriteResult(client.insert(hoodieRecords, instantTime));
      case UPSERT:
        return new HoodieWriteResult(client.upsert(hoodieRecords, instantTime));
      case INSERT_OVERWRITE:
        return client.insertOverwrite(hoodieRecords, instantTime);
```

### UNCHECKED_WARNING
Unchecked call to 'insertOverwrite(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return new HoodieWriteResult(client.upsert(hoodieRecords, instantTime));
      case INSERT_OVERWRITE:
        return client.insertOverwrite(hoodieRecords, instantTime);
      case INSERT_OVERWRITE_TABLE:
        return client.insertOverwriteTable(hoodieRecords, instantTime);
```

### UNCHECKED_WARNING
Unchecked call to 'insertOverwriteTable(JavaRDD\>, String)' as a member of raw type 'org.apache.hudi.client.SparkRDDWriteClient'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return client.insertOverwrite(hoodieRecords, instantTime);
      case INSERT_OVERWRITE_TABLE:
        return client.insertOverwriteTable(hoodieRecords, instantTime);
      default:
        throw new HoodieException("Not a valid operation type for doWriteOperation: " + operation.toString());
```

## RuleId[id=JavadocReference]
### JavadocReference
Cannot resolve symbol `org.apache.hudi.unsafe.UTF8StringBuilder`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/PartitionPathFormatterBase.java`
#### Snippet
```java
  /**
   * This is a generic interface closing the gap and unifying the {@link java.lang.StringBuilder} with
   * {@link org.apache.hudi.unsafe.UTF8StringBuilder} implementations, allowing us to avoid code-duplication by performing
   * most of the key-generation in a generic and unified way
   *
```

### JavadocReference
Cannot resolve symbol `HoodieSliceInfo`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java`
#### Snippet
```java

  /**
   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.
   */
  protected static List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {
```

### JavadocReference
Cannot resolve symbol `HoodieRestoreMetadata`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/BaseRestoreActionExecutor.java`
#### Snippet
```java
   * Update metadata table if available. Any update to metadata table happens within data table lock.
   *
   * @param restoreMetadata instance of {@link HoodieRestoreMetadata} to be applied to metadata.
   */
  private void writeToMetadata(HoodieRestoreMetadata restoreMetadata, HoodieInstant restoreInflightInstant) {
```

### JavadocReference
Cannot resolve symbol `HoodieRollbackPlan`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java`
#### Snippet
```java

/**
 * Base rollback plan action executor to assist in scheduling rollback requests. This phase serialized {@link HoodieRollbackPlan}
 * to rollback.requested instant.
 */
```

### JavadocReference
Cannot resolve symbol `HoodieRollbackRequest`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java`
#### Snippet
```java

    /**
     * Fetch list of {@link HoodieRollbackRequest}s to be added to rollback plan.
     * @param instantToRollback instant to be rolled back.
     * @return list of {@link HoodieRollbackRequest}s to be added to rollback plan
```

### JavadocReference
Cannot resolve symbol `HoodieRollbackRequest`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java`
#### Snippet
```java
     * Fetch list of {@link HoodieRollbackRequest}s to be added to rollback plan.
     * @param instantToRollback instant to be rolled back.
     * @return list of {@link HoodieRollbackRequest}s to be added to rollback plan
     */
    List<HoodieRollbackRequest> getRollbackRequests(HoodieInstant instantToRollback);
```

### JavadocReference
Cannot resolve symbol `HoodieRollbackPlan`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java`
#### Snippet
```java
   * Execute rollback and fetch rollback stats.
   * @param instantToRollback instant to be rolled back.
   * @param rollbackPlan instance of {@link HoodieRollbackPlan} for which rollback needs to be executed.
   * @return list of {@link HoodieRollbackStat}s.
   */
```

### JavadocReference
Cannot resolve symbol `HoodieRollbackPlan`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java`
#### Snippet
```java
  /**
   * Execute actual rollback and fetch list of RollbackStats.
   * @param hoodieRollbackPlan instance of {@link HoodieRollbackPlan} that needs to be executed.
   * @return a list of {@link HoodieRollbackStat}s.
   * @throws IOException
```

### JavadocReference
Cannot resolve symbol `HoodieRollbackRequest`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackStrategy.java`
#### Snippet
```java

/**
 * Listing based rollback strategy to fetch list of {@link HoodieRollbackRequest}s.
 */
public class ListingBasedRollbackStrategy implements BaseRollbackPlanActionExecutor.RollbackStrategy {
```

### JavadocReference
Cannot resolve symbol `HoodieArchivedMetaEntry`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/MetadataConversionUtils.java`
#### Snippet
```java

/**
 * Helper class to convert between different action related payloads and {@link HoodieArchivedMetaEntry}.
 */
public class MetadataConversionUtils {
```

### JavadocReference
Cannot resolve symbol `HoodieIndexCommitMetadata`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
   *
   * @param indexInstantTime - instant time for the requested INDEX action
   * @return {@link Option<HoodieIndexCommitMetadata>} after successful indexing.
   */
  public Option<HoodieIndexCommitMetadata> index(String indexInstantTime) {
```

### JavadocReference
Cannot resolve symbol `HoodieCleanMetadata`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
   * @param cleanInstantTime instant time for clean.
   * @param skipLocking if this is triggered by another parent transaction, locking can be skipped.
   * @return instance of {@link HoodieCleanMetadata}.
   */
  @Deprecated
```

### JavadocReference
Cannot resolve symbol `HoodieCleanMetadata`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
   *    * configurations and CleaningPolicy used.
   * @param skipLocking if this is triggered by another parent transaction, locking can be skipped.
   * @return instance of {@link HoodieCleanMetadata}.
   */
  @Deprecated
```

### JavadocReference
Cannot resolve symbol
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroParquetReader.java`
#### Snippet
```java
 * {@link HoodieFileReader} implementation for parquet format.
 *
 * @param <R> Record implementation that permits field access by integer index.
 */
public class HoodieAvroParquetReader extends HoodieAvroFileReaderBase {
```

### JavadocReference
Cannot resolve symbol
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroOrcReader.java`
#### Snippet
```java
 * {@link HoodieFileReader} implementation for ORC format.
 *
 * @param <R> Record implementation that permits field access by integer index.
 */
public class HoodieAvroOrcReader extends HoodieAvroFileReaderBase {
```

### JavadocReference
Cannot resolve symbol `DebeziumSource`
in `hudi-common/src/main/java/org/apache/hudi/common/model/debezium/DebeziumConstants.java`
#### Snippet
```java

/**
 * Constants used by {@link DebeziumSource} and {@link DebeziumAvroPayload}.
 */
public class DebeziumConstants {
```

### JavadocReference
Cannot resolve symbol `DebeziumAvroPayload`
in `hudi-common/src/main/java/org/apache/hudi/common/model/debezium/DebeziumConstants.java`
#### Snippet
```java

/**
 * Constants used by {@link DebeziumSource} and {@link DebeziumAvroPayload}.
 */
public class DebeziumConstants {
```

### JavadocReference
Cannot resolve symbol `HoodieMetadataColumnStats`
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java

  /**
   * Converts instance of {@link HoodieMetadataColumnStats} to {@link HoodieColumnRangeMetadata}
   */
  public static HoodieColumnRangeMetadata<Comparable> convertColumnStatsRecordToColumnRangeMetadata(HoodieMetadataColumnStats columnStats) {
```

### JavadocReference
Cannot resolve symbol `GCS_INCR_DATAFILE_EXTENSION`
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectMetadataFetcher.java`
#### Snippet
```java

  /**
   * @param fileFormat The default file format to assume if {@link GcsIngestionConfig#GCS_INCR_DATAFILE_EXTENSION}
   *                   is not given.
   */
```

### JavadocReference
Cannot resolve symbol `GCS_INCR_DATAFILE_EXTENSION`
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectMetadataFetcher.java`
#### Snippet
```java

  /**
   * The default file format to assume if {@link GcsIngestionConfig#GCS_INCR_DATAFILE_EXTENSION} is not given.
   */
  private final String fileFormat;
```

### JavadocReference
Cannot resolve symbol `org.apache.hudi.connect.ControlMessage.ConnectWriteStatus`
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java`
#### Snippet
```java
   *
   * @param writeStatuses The list of Hudi {@link WriteStatus}.
   * @return the protobuf message {@link org.apache.hudi.connect.ControlMessage.ConnectWriteStatus}
   * that wraps the Hudi {@link WriteStatus}.
   * @throws IOException thrown if the conversion failed.
```

### JavadocReference
Cannot resolve symbol `ControlMessage.ParticipantInfo`
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java`
#### Snippet
```java
   * Unwrap the Hudi {@link WriteStatus} from the received Protobuf message.
   *
   * @param participantInfo The {@link ControlMessage.ParticipantInfo} that contains the
   *                        underlying {@link WriteStatus} sent by the participants.
   * @return the list of {@link WriteStatus} returned by Hudi on a write transaction.
```

### JavadocReference
Cannot resolve symbol `HoodieCompactionPlan`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/CompactionCommitSink.java`
#### Snippet
```java
 *
 * <p> Each time after receiving a compaction commit event {@link CompactionCommitEvent},
 * it loads and checks the compaction plan {@link HoodieCompactionPlan},
 * if all the compaction operations {@link org.apache.hudi.common.model.CompactionOperation}
 * of the plan are finished, tries to commit the compaction action.
```

### JavadocReference
Cannot resolve symbol `org.apache.hudi.avro.model.HoodieClusteringPlan`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/ClusteringCommitSink.java`
#### Snippet
```java
 *
 * <p> Each time after receiving a clustering commit event {@link ClusteringCommitEvent},
 * it loads and checks the clustering plan {@link org.apache.hudi.avro.model.HoodieClusteringPlan},
 * if all the clustering operations {@link org.apache.hudi.common.model.ClusteringOperation}
 * of the plan are finished, tries to commit the clustering action.
```

## RuleId[id=DataFlowIssue]
### DataFlowIssue
Argument `libDirectory.list()` might be null
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java`
#### Snippet
```java
      // When directly using hudi-cli module, the jars under the lib directory
      // generated by the compilation is required
      Arrays.stream(libDirectory.list()).forEach(library ->
          sparkLauncher.addJar(new File(libDirectory, library).getAbsolutePath()));
    } else {
```

### DataFlowIssue
Method invocation `close` may produce `NullPointerException`
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java
      os.write(data.getBytes(), 0, data.length());
    } finally {
      os.close();
    }
  }
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
          assert (args.length == 7);
          doCompactValidate(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]));
          returnCode = 0;
          break;
        case COMPACT_REPAIR:
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
          doCompactRepair(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),
              Boolean.parseBoolean(args[7]));
          returnCode = 0;
          break;
        case COMPACT_UNSCHEDULE_FILE:
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
          doCompactUnscheduleFile(jsc, args[3], args[4], args[5], args[6], Integer.parseInt(args[7]),
              Boolean.parseBoolean(args[8]), Boolean.parseBoolean(args[9]));
          returnCode = 0;
          break;
        case COMPACT_UNSCHEDULE_PLAN:
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
          doCompactUnschedule(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),
              Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));
          returnCode = 0;
          break;
        case CLUSTERING_RUN:
```

### DataFlowIssue
Method invocation `replaceAll` may produce `NullPointerException`
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java

        String expectedType = getExpectedType(newTableSchema, tickSurroundedFieldName);
        expectedType = expectedType.replaceAll("\\s+", "");
        expectedType = expectedType.replaceAll("`", "");

```

### DataFlowIssue
Stream might have already been linked or consumed
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/common/HoodieJavaEngineContext.java`
#### Snippet
```java
  public <I, K, V> Stream<ImmutablePair<K, V>> mapPartitionsToPairAndReduceByKey(Stream<I> data, SerializablePairFlatMapFunction<Iterator<I>, K, V> flatMapToPairFunc,
                                                                                 SerializableBiFunction<V, V, V> reduceFunc, int parallelism) {
    return throwingFlatMapToPairWrapper(flatMapToPairFunc).apply(data.parallel().iterator())
        .collect(Collectors.groupingBy(Pair::getKey)).entrySet().stream()
        .map(entry -> new ImmutablePair<>(entry.getKey(), entry.getValue().stream().map(
```

### DataFlowIssue
Stream might have already been linked or consumed
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/common/HoodieFlinkEngineContext.java`
#### Snippet
```java
      Stream<I> data, SerializablePairFlatMapFunction<Iterator<I>, K, V> flatMapToPairFunc,
      SerializableBiFunction<V, V, V> reduceFunc, int parallelism) {
    return throwingFlatMapToPairWrapper(flatMapToPairFunc).apply(data.parallel().iterator())
        .collect(Collectors.groupingBy(Pair::getKey)).entrySet().stream()
        .map(entry -> new ImmutablePair<>(entry.getKey(), entry.getValue().stream().map(
```

### DataFlowIssue
Method invocation `schema` may produce `NullPointerException`
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
      originalDF = sqlContextOpt.get().read().orc(uniquePaths.toArray(new String[uniquePaths.size()]));
    }
    StructType schema = originalDF.schema();
    JavaPairRDD<HoodieKey, Row> keyRowRDD = originalDF.javaRDD().mapToPair(row -> {
      HoodieKey key = new HoodieKey(row.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD),
```

### DataFlowIssue
The call to 'handleNullRecordKey' always fails, according to its method contracts
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java`
#### Snippet
```java
    //       record-key field
    if (recordKeyValues[0] == null) {
      return handleNullRecordKey(null);
    } else if (recordKeyValues[0] instanceof UTF8String) {
      return requireNonNullNonEmptyKey((UTF8String) recordKeyValues[0]);
```

### DataFlowIssue
The call to 'handleNullRecordKey' always fails, according to its method contracts
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java`
#### Snippet
```java
    //       record-key field
    if (recordKeys[0] == null) {
      return handleNullRecordKey(null);
    } else {
      return requireNonNullNonEmptyKey(recordKeys[0].toString());
```

### DataFlowIssue
Method invocation `getTotalCreateTime` may produce `NullPointerException`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java`
#### Snippet
```java
      LOG.info(String.format("CreateHandle for partitionPath %s fileID %s, took %d ms.",
          writeStatus.getStat().getPartitionPath(), writeStatus.getStat().getFileId(),
          writeStatus.getStat().getRuntimeStats().getTotalCreateTime()));

      return Collections.singletonList(writeStatus);
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java
        if (combinedRecord.isPresent() && combinedRecord.get().shouldIgnore(combineRecordSchema, props)) {
          // If it is an IGNORE_RECORD, just copy the old record, and do not update the new record.
          copyOldRecord = true;
        } else if (writeUpdateRecord(newRecord, oldRecord, combinedRecord, combineRecordSchema)) {
          /*
```

### DataFlowIssue
Unboxing of `deletedFileResult` may produce `NullPointerException`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java`
#### Snippet
```java
        // For Bootstrap Base file deletions, store the full file path.
        partitionCleanStat.addDeleteFilePatterns(deletePath.toString(), true);
        partitionCleanStat.addDeletedFileResult(deletePath.toString(), deletedFileResult, true);
      } else {
        partitionCleanStat.addDeleteFilePatterns(deletePath.getName(), false);
```

### DataFlowIssue
Unboxing of `deletedFileResult` may produce `NullPointerException`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java`
#### Snippet
```java
      } else {
        partitionCleanStat.addDeleteFilePatterns(deletePath.getName(), false);
        partitionCleanStat.addDeletedFileResult(deletePath.getName(), deletedFileResult, false);
      }
    });
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java
          } catch (FileNotFoundException e) {
            // if first rollback attempt failed and retried again, chances that some files are already deleted.
            isDeleted = true;
          }

```

### DataFlowIssue
Method invocation `next` may produce `NullPointerException`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/ConcatenatingIterator.java`
#### Snippet
```java
  public T next() {
    ValidationUtils.checkArgument(hasNext(), "No more elements left");
    return allIterators.peek().next();
  }
}
```

### DataFlowIssue
Condition `actionMetadata.get() instanceof HoodieRollbackMetadata` is redundant and can be replaced with a null check
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
    }

    final String INSTANT_ACTION = (actionMetadata.get() instanceof HoodieRollbackMetadata
        ? HoodieTimeline.ROLLBACK_ACTION
        : (actionMetadata.get() instanceof HoodieRestoreMetadata ? HoodieTimeline.RESTORE_ACTION : EMPTY_STRING));
```

### DataFlowIssue
Switch label `HoodieTimeline.RESTORE_ACTION` is unreachable
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
        }
        break;
      case HoodieTimeline.RESTORE_ACTION:
        List<HoodieInstantInfo> restoredInstants =
            ((HoodieRestoreMetadata) actionMetadata.get()).getRestoreInstantInfo();
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java
          code = Integer.parseInt(path.substring(i + 1, i + 3), 16);
        } catch (Exception e) {
          code = -1;
        }
        if (code >= 0) {
```

### DataFlowIssue
Method invocation `getElementType` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        TypeDescription listType = type.getChildren().get(0);
        for (Object listItem : list) {
          addToVector(listType, listColVec.child, avroSchema.getElementType(), listItem, listColVec.childCount++);
        }
        break;
```

### DataFlowIssue
Method invocation `getValueType` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              type.getChildren().get(1),
              mapColumnVector.values,
              avroSchema.getValueType(),
              entry.getValue(),
              mapColumnVector.childCount
```

### DataFlowIssue
Method invocation `getFields` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
          Object fieldValue = record.get(fieldName);
          TypeDescription fieldType = type.getChildren().get(i);
          addToVector(fieldType, structColVec.fields[i], avroSchema.getFields().get(i).schema(), fieldValue, vectorPos);
        }

```

### DataFlowIssue
Method invocation `getProp` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        }
      case STRING:
        String stringType = avroSchema.getProp(GenericData.STRING_PROP);
        if (stringType == null || !stringType.equals(StringType.String)) {
          int stringLength = ((BytesColumnVector) colVector).length[vectorPos];
```

### DataFlowIssue
Method invocation `getScale` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        BigDecimal bigDecimal = ((DecimalColumnVector) colVector).vector[vectorPos]
            .getHiveDecimal().bigDecimalValue()
            .setScale(((LogicalTypes.Decimal) logicalType).getScale());
        Schema.Type baseType = avroSchema.getType();
        if (baseType.equals(Schema.Type.FIXED)) {
```

### DataFlowIssue
Method invocation `getType` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
            .getHiveDecimal().bigDecimalValue()
            .setScale(((LogicalTypes.Decimal) logicalType).getScale());
        Schema.Type baseType = avroSchema.getType();
        if (baseType.equals(Schema.Type.FIXED)) {
          return new Conversions.DecimalConversion().toFixed(bigDecimal, avroSchema, logicalType);
```

### DataFlowIssue
Method invocation `getElementType` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        TypeDescription childType = type.getChildren().get(0);
        for (int i = 0; i < listLength; i++) {
          list.add(readFromVector(childType, listVector.child, avroSchema.getElementType(), listOffset + i));
        }
        return list;
```

### DataFlowIssue
Method invocation `toString` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        for (int i = 0; i < mapLength; i++) {
          map.put(
              readFromVector(type.getChildren().get(0), mapVector.keys, keySchema, i + mapOffset).toString(),
              readFromVector(type.getChildren().get(1), mapVector.values,
                  avroSchema.getValueType(), i + mapOffset));
```

### DataFlowIssue
Method invocation `getValueType` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              readFromVector(type.getChildren().get(0), mapVector.keys, keySchema, i + mapOffset).toString(),
              readFromVector(type.getChildren().get(1), mapVector.values,
                  avroSchema.getValueType(), i + mapOffset));
        }
        return map;
```

### DataFlowIssue
Argument `avroSchema` might be null
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        StructColumnVector structVector = (StructColumnVector) colVector;
        List<TypeDescription> children = type.getChildren();
        GenericData.Record record = new GenericData.Record(avroSchema);
        for (int i = 0; i < children.size(); i++) {
          record.put(i, readFromVector(children.get(i), structVector.fields[i],
```

### DataFlowIssue
Method invocation `getTypes` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        int tag = unionVector.tags[vectorPos];
        ColumnVector fieldVector = unionVector.fields[tag];
        return readFromVector(type.getChildren().get(tag), fieldVector, avroSchema.getTypes().get(tag), vectorPos);
      default:
        throw new HoodieIOException("Unrecognized TypeDescription " + type.toString());
```

### DataFlowIssue
`null` is returned by the method declared as @Nonnull
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    int scale = decimalMetadata.getScale();
    if (val == null) {
      return null;
    } else if (val instanceof Integer) {
      return BigDecimal.valueOf((Integer) val, scale);
```

### DataFlowIssue
Argument `incomingOrderingVal` might be null
in `hudi-common/src/main/java/org/apache/hudi/common/model/DefaultHoodieRecordPayload.java`
#### Snippet
```java
        orderField,
        true, consistentLogicalTimestampEnabled);
    return persistedOrderingVal == null || ((Comparable) persistedOrderingVal).compareTo(incomingOrderingVal) <= 0;
  }

```

### DataFlowIssue
Method invocation `keySet` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCExtractor.java`
#### Snippet
```java
      // this is a cdc log
      if (supplementalLoggingMode == HoodieCDCSupplementalLoggingMode.DATA_BEFORE_AFTER) {
        cdcFileSplit = new HoodieCDCFileSplit(instantTs, AS_IS, writeStat.getCdcStats().keySet());
      } else {
        try {
```

### DataFlowIssue
Method invocation `keySet` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCExtractor.java`
#### Snippet
```java
            beforeFileSlice = new FileSlice(fileGroupId, writeStat.getPrevCommit(), beforeBaseFile, new ArrayList<>());
          }
          cdcFileSplit = new HoodieCDCFileSplit(instantTs, AS_IS, writeStat.getCdcStats().keySet(),
              Option.ofNullable(beforeFileSlice), Option.ofNullable(currentFileSlice));
        } catch (Exception e) {
```

### DataFlowIssue
Stream might have already been linked or consumed
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineLayout.java`
#### Snippet
```java
    @Override
    public Stream<HoodieInstant> filterHoodieInstants(Stream<HoodieInstant> instantStream) {
      return instantStream.collect(Collectors.groupingBy(instant -> Pair.of(instant.getTimestamp(),
          HoodieInstant.getComparableAction(instant.getAction())))).values().stream()
          .map(hoodieInstants -> hoodieInstants.stream().reduce((x, y) -> {
```

### DataFlowIssue
Method invocation `getBlockType` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
      // poll the element at the bottom of the stack since that's the order it was inserted
      HoodieLogBlock lastBlock = logBlocks.pollLast();
      switch (lastBlock.getBlockType()) {
        case AVRO_DATA_BLOCK:
        case HFILE_DATA_BLOCK:
```

### DataFlowIssue
Method invocation `getLogBlockHeader` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
    return currentInstantLogBlocks.size() > 0 && currentInstantLogBlocks.peek().getBlockType() != CORRUPT_BLOCK
        && !logBlock.getLogBlockHeader().get(INSTANT_TIME)
        .contentEquals(currentInstantLogBlocks.peek().getLogBlockHeader().get(INSTANT_TIME));
  }

```

### DataFlowIssue
Stream might have already been linked or consumed
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java`
#### Snippet
```java

  private Stream<HoodieFileGroup> getFileGroups(Stream<FileSlice> sliceStream) {
    return sliceStream.map(s -> Pair.of(Pair.of(s.getPartitionPath(), s.getFileId()), s))
        .collect(Collectors.groupingBy(Pair::getKey)).entrySet().stream().map(slicePair -> {
          HoodieFileGroup fg = new HoodieFileGroup(slicePair.getKey().getKey(), slicePair.getKey().getValue(),
```

### DataFlowIssue
Stream might have already been linked or consumed
in `hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieLocalEngineContext.java`
#### Snippet
```java
      Stream<I> data, SerializablePairFlatMapFunction<Iterator<I>, K, V> flatMapToPairFunc,
      SerializableBiFunction<V, V, V> reduceFunc, int parallelism) {
    return throwingFlatMapToPairWrapper(flatMapToPairFunc).apply(data.parallel().iterator())
        .collect(Collectors.groupingBy(Pair::getKey)).entrySet().stream()
        .map(entry -> new ImmutablePair<>(entry.getKey(), entry.getValue().stream().map(
```

### DataFlowIssue
Method invocation `field` may produce `NullPointerException`
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java`
#### Snippet
```java
    if (topParentFieldIds != null && !topParentFieldIds.isEmpty()) {
      for (int id : topParentFieldIds) {
        Types.Field f = recordType.field(id);
        if (f != null) {
          newFields.add(f);
```

### DataFlowIssue
Method invocation `split` may produce `NullPointerException`
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java`
#### Snippet
```java
    }

    if (!Arrays.asList(readColNames.split(",")).contains(fieldName)) {
      // If not already in the list - then add it
      conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, readColNamesPrefix + fieldName);
```

### DataFlowIssue
Method invocation `getChildren` may produce `NullPointerException`
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
            ((ExprNodeColumnDesc)expr).setColumn(newColumn);
          }
          List<ExprNodeDesc> children = expr.getChildren();
          if (children != null) {
            exprNodes.addAll(children);
```

### DataFlowIssue
Casting `o` to `HoodieMetadataTableValidator.Config` will produce `ClassCastException` for any non-null value
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
        return false;
      }
      HoodieMetadataTableValidator.Config config = (HoodieMetadataTableValidator.Config) o;
      return basePath.equals(config.basePath)
          && Objects.equals(continuous, config.continuous)
```

### DataFlowIssue
Method invocation `process` may produce `NullPointerException`
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JsonKafkaSource.java`
#### Snippet
```java
    }

    return processor.process(jsonStringRDD);
  }
}
```

### DataFlowIssue
Method invocation `getSourceSchema` may produce `NullPointerException`
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/writers/KafkaConnectWriterProvider.java`
#### Snippet
```java
              KafkaConnectFileIdPrefixProvider.KAFKA_CONNECT_PARTITION_ID,
              String.valueOf(partition)))
          .withSchema(schemaProvider.getSourceSchema().toString())
          .withAutoCommit(false)
          .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())
```

### DataFlowIssue
Method invocation `getRecordKey` may produce `NullPointerException`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/BulkInsertWriterHelper.java`
#### Snippet
```java
      String recordKey = preserveHoodieMetadata
          ? record.getString(HoodieRecord.RECORD_KEY_META_FIELD_ORD).toString()
          : keyGen.getRecordKey(record);
      String partitionPath = preserveHoodieMetadata
          ? record.getString(HoodieRecord.PARTITION_PATH_META_FIELD_ORD).toString()
```

### DataFlowIssue
Method invocation `getRecordKey` may produce `NullPointerException`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bucket/BucketBulkInsertWriterHelper.java`
#### Snippet
```java
    try {
      RowData record = tuple.getRow(1, this.recordArity);
      String recordKey = keyGen.getRecordKey(record);
      String partitionPath = keyGen.getPartitionPath(record);
      String fileId = tuple.getString(0).toString();
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
    if (enableHiveSync) {
      LOG.info("Enabling Hive sync to " + hiveJdbcUrl);
      writer = writer.option(META_SYNC_TABLE_NAME.key(), hiveTable)
          .option(META_SYNC_DATABASE_NAME.key(), hiveDB)
          .option(HIVE_URL.key(), hiveJdbcUrl)
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
            .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), "");
      } else if (useMultiPartitionKeys) {
        writer = writer.option(HiveSyncConfig.META_SYNC_PARTITION_FIELDS.key(), "year,month,day").option(
            HiveSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS.key(),
            MultiPartKeysValueExtractor.class.getCanonicalName());
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
            MultiPartKeysValueExtractor.class.getCanonicalName());
      } else {
        writer = writer.option(HiveSyncConfig.META_SYNC_PARTITION_FIELDS.key(), "dateStr").option(
            HiveSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS.key(),
            SlashEncodedDayPartitionValueExtractor.class.getCanonicalName());
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    if (enableHiveSync) {
      LOG.info("Enabling Hive sync to " + hiveJdbcUrl);
      writer = writer.option(META_SYNC_TABLE_NAME.key(), hiveTable)
          .option(META_SYNC_DATABASE_NAME.key(), hiveDB)
          .option(HIVE_URL.key(), hiveJdbcUrl)
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
            .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), "");
      } else if (useMultiPartitionKeys) {
        writer = writer.option(HiveSyncConfig.META_SYNC_PARTITION_FIELDS.key(), "year,month,day").option(
            HiveSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS.key(),
            MultiPartKeysValueExtractor.class.getCanonicalName());
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
            MultiPartKeysValueExtractor.class.getCanonicalName());
      } else {
        writer = writer.option(HiveSyncConfig.META_SYNC_PARTITION_FIELDS.key(), "dateStr").option(
            HiveSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS.key(),
            SlashEncodedDayPartitionValueExtractor.class.getCanonicalName());
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    if (enableHiveSync) {
      LOG.info("Enabling Hive sync to " + hiveJdbcUrl);
      writer = writer.option(META_SYNC_TABLE_NAME.key(), hiveTable)
          .option(META_SYNC_DATABASE_NAME.key(), hiveDB)
          .option(HIVE_URL.key(), hiveJdbcUrl)
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
          .option(HIVE_SYNC_ENABLED.key(), "true");
      if (useMultiPartitionKeys) {
        writer = writer.option(HiveSyncConfig.META_SYNC_PARTITION_FIELDS.key(), "year,month,day").option(
            HiveSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS.key(),
            MultiPartKeysValueExtractor.class.getCanonicalName());
```

### DataFlowIssue
Variable is already assigned to this value
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
            MultiPartKeysValueExtractor.class.getCanonicalName());
      } else {
        writer = writer.option(HiveSyncConfig.META_SYNC_PARTITION_FIELDS.key(), "dateStr").option(
            HiveSyncConfig.META_SYNC_PARTITION_EXTRACTOR_CLASS.key(),
            SlashEncodedDayPartitionValueExtractor.class.getCanonicalName());
```

## RuleId[id=UnaryPlus]
### UnaryPlus
Unary `+` operator
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java`
#### Snippet
```java

      metrics.ifPresent(metrics -> metrics.updateMetrics(HoodieMetadataMetrics.SCAN_STR,
          +baseFileOpenMs + logScannerOpenMs));
      return Pair.of(baseFileReader, logRecordScanner);
    } catch (IOException e) {
```

## RuleId[id=UnnecessarySemicolon]
### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/datadog/DatadogReporter.java`
#### Snippet
```java

  enum MetricType {
    gauge;
  }
}
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java

    /** Used internally to tag a reader/writer schema pair and prevent recursion. */
    RECURSION_IN_PROGRESS;
  }

```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hudi-common/src/main/java/org/apache/hudi/avro/AvroSchemaCompatibility.java`
#### Snippet
```java
  public enum SchemaIncompatibilityType {
    NAME_MISMATCH, FIXED_SIZE_MISMATCH, MISSING_ENUM_SYMBOLS, READER_FIELD_MISSING_DEFAULT_VALUE, TYPE_MISMATCH,
    MISSING_UNION_BRANCH;
  }

```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hudi-common/src/main/java/org/apache/hudi/common/util/OrcUtils.java`
#### Snippet
```java
  public Set<String> filterRowKeys(Configuration conf, Path filePath, Set<String> filter)
      throws HoodieIOException {
    try (Reader reader = OrcFile.createReader(filePath, OrcFile.readerOptions(conf));) {
      TypeDescription schema = reader.getSchema();
      try (RecordReader recordReader = reader.rows(new Options(conf).schema(schema))) {
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hudi-common/src/main/java/org/apache/hudi/common/table/cdc/HoodieCDCInferenceCase.java`
#### Snippet
```java
  BASE_FILE_DELETE,
  LOG_FILE,
  REPLACE_COMMIT;

}
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/MessageValidity.java`
#### Snippet
```java
  public enum ProcessingDecision {
    DO_PROCESS,
    DO_SKIP;
  }
}
```

## RuleId[id=SimplifyStreamApiCallChains]
### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/JDBCExecutor.java`
#### Snippet
```java
    LOG.info("Dropping partitions " + partitionsToDrop.size() + " from table " + tableName);
    List<String> sqls = constructDropPartitions(tableName, partitionsToDrop);
    sqls.stream().forEach(sql -> runSQL(sql));
  }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/QueryBasedDDLExecutor.java`
#### Snippet
```java
    LOG.info("Adding partitions " + partitionsToAdd.size() + " to table " + tableName);
    List<String> sqls = constructAddPartitions(tableName, partitionsToAdd);
    sqls.stream().forEach(sql -> runSQL(sql));
  }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/common/HoodieJavaEngineContext.java`
#### Snippet
```java
  @Override
  public <I> void foreach(List<I> data, SerializableConsumer<I> consumer, int parallelism) {
    data.stream().forEach(throwingForeachWrapper(consumer));
  }

```

### SimplifyStreamApiCallChains
'Collections.singletonList().stream()' can be replaced with 'Stream.of()'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java`
#### Snippet
```java
        String indexUptoInstant = fileIndexPartitionInfo.getIndexUptoInstant();
        // save index commit metadata and update table config
        finalIndexPartitionInfos = Collections.singletonList(fileIndexPartitionInfo).stream()
            .map(info -> new HoodieIndexPartitionInfo(
                info.getVersion(),
```

### SimplifyStreamApiCallChains
'Collections.singletonList().stream()' can be replaced with 'Stream.of()'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java
                    .withRollbackBlockAppendResults(filesToNumBlocksRollback)
                    .build()))
            .stream();
      } else {
        return Collections.singletonList(
```

### SimplifyStreamApiCallChains
'Collections.singletonList().stream()' can be replaced with 'Stream.of()'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackHelper.java`
#### Snippet
```java
                    .withPartitionPath(rollbackRequest.getPartitionPath())
                    .build()))
            .stream();
      }
    }, numPartitions);
```

### SimplifyStreamApiCallChains
Can be replaced with 'String.join'
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
      List<String> parentNames = new ArrayList<>();
      fieldNames.descendingIterator().forEachRemaining(parentNames::add);
      result = parentNames.stream().collect(Collectors.joining("."));
    }
    return result;
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java`
#### Snippet
```java
    Map<String, List<Map.Entry<HoodieFileGroupId, HoodieInstant>>> partitionToReplacedFileGroups =
        replacedFileGroups.entrySet().stream().collect(Collectors.groupingBy(e -> e.getKey().getPartitionPath()));
    partitionToReplacedFileGroups.entrySet().stream().forEach(partitionToReplacedFileGroupsEntry -> {
      String partitionPath = partitionToReplacedFileGroupsEntry.getKey();
      List<Map.Entry<HoodieFileGroupId, HoodieInstant>> replacedFileGroupsInPartition = partitionToReplacedFileGroupsEntry.getValue();
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java`
#### Snippet
```java
      // Now add them
      rocksDB.writeBatch(batch ->
          replacedFileGroupsInPartition.stream().forEach(fgToReplacedInstant -> {
            rocksDB.putInBatch(batch, schemaHelper.getColFamilyForReplacedFileGroups(),
                schemaHelper.getKeyForReplacedFileGroup(fgToReplacedInstant.getKey()), fgToReplacedInstant.getValue());
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java

    // First remove pending compaction instants which were completed
    diffResult.getFinishedCompactionInstants().stream().forEach(instant -> {
      try {
        removePendingCompactionInstant(instant);
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java

    // Now remove pending log compaction instants which were completed or removed
    diffResult.getFinishedOrRemovedLogCompactionInstants().stream().forEach(instant -> {
      try {
        removePendingLogCompactionInstant(instant);
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
          }));
        }).collect(Collectors.groupingBy(Pair::getKey));
    partitionFiles.entrySet().stream().forEach(e -> {
      removeFileSlicesForPartition(timeline, instant, e.getKey(),
          e.getValue().stream().map(x -> x.getValue()).collect(Collectors.toList()));
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
                                              HoodieTimeline timeline,
                                              HoodieInstant instant) {
    partitionToWriteStats.entrySet().stream().forEach(entry -> {
      String partition = entry.getKey();
      if (isPartitionAvailableInStore(partition)) {
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
        break;
      case REMOVE:
        deltaDataFiles.keySet().stream().forEach(p -> viewDataFiles.remove(p));
        deltaLogFiles.keySet().stream().forEach(p -> viewLogFiles.remove(p));
        break;
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
      case REMOVE:
        deltaDataFiles.keySet().stream().forEach(p -> viewDataFiles.remove(p));
        deltaLogFiles.keySet().stream().forEach(p -> viewLogFiles.remove(p));
        break;
      default:
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
        HoodieReplaceCommitMetadata.fromBytes(timeline.getInstantDetails(instant).get(), HoodieReplaceCommitMetadata.class);
    updatePartitionWriteFileGroups(replaceMetadata.getPartitionToWriteStats(), timeline, instant);
    replaceMetadata.getPartitionToReplaceFileIds().entrySet().stream().forEach(entry -> {
      String partition = entry.getKey();
      Map<HoodieFileGroupId, HoodieInstant> replacedFileIds = entry.getValue().stream()
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/FileSliceDTO.java`
#### Snippet
```java
    FileSlice slice = new FileSlice(dto.partitionPath, dto.baseInstantTime, dto.fileId);
    slice.setBaseFile(BaseFileDTO.toHoodieBaseFile(dto.baseFile));
    dto.logFiles.stream().forEach(lf -> slice.addLogFile(LogFileDTO.toHoodieLogFile(lf)));
    return slice;
  }
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieLocalEngineContext.java`
#### Snippet
```java
  @Override
  public <I> void foreach(List<I> data, SerializableConsumer<I> consumer, int parallelism) {
    data.stream().forEach(throwingForeachWrapper(consumer));
  }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/AvroSchemaEvolutionUtils.java`
#### Snippet
```java

    TableChanges.ColumnAddChange addChange = TableChanges.ColumnAddChange.get(oldTableSchema);
    finalAddAction.entrySet().stream().forEach(f -> {
      String name = f.getValue();
      int splitPoint = name.lastIndexOf(".");
```

### SimplifyStreamApiCallChains
Can be replaced with 'min()'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/AvroSchemaEvolutionUtils.java`
#### Snippet
```java
                  && c.startsWith(parentName)
                  && inComingInternalSchema.findIdByName(c) >  inComingInternalSchema.findIdByName(name)
                  && oldTableSchema.findIdByName(c) > 0).sorted((s1, s2) -> oldTableSchema.findIdByName(s1) - oldTableSchema.findIdByName(s2)).findFirst();
      addChange.addColumns(parentName, rawName, inComingInternalSchema.findType(name), null);
      inferPosition.map(i -> addChange.addPositionChange(name, i, "before"));
```

### SimplifyStreamApiCallChains
Can be replaced with 'String.join'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java`
#### Snippet
```java
      List<String> parentNames = new ArrayList<>();
      fieldNames.descendingIterator().forEachRemaining(parentNames::add);
      result = parentNames.stream().collect(Collectors.joining(".")) + "." + result;
    }
    return result;
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java`
#### Snippet
```java
    // find top parent field ID. eg: a.b.c, f.g.h, only collect id of a and f ignore all child field.
    List<Integer> topParentFieldIds = new ArrayList<>();
    names.stream().forEach(f -> {
      int id = schema.findIdByName(f.split("\\.")[0]);
      if (!topParentFieldIds.contains(id)) {
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/convert/AvroInternalSchemaConverter.java`
#### Snippet
```java
        }
        nextId.set(nextAssignId + fields.size());
        fields.stream().forEach(field -> {
          fieldTypes.add(visitAvroSchemaToBuildType(field.schema(), visited, false, nextId));
        });
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/convert/AvroInternalSchemaConverter.java`
#### Snippet
```java
      case UNION:
        List<Type> fTypes = new ArrayList<>();
        schema.getTypes().stream().forEach(t -> {
          fTypes.add(visitAvroSchemaToBuildType(t, visited, false, nextId));
        });
```

### SimplifyStreamApiCallChains
Can be replaced with 'String.join'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java`
#### Snippet
```java
            allStructFieldRefs.get(i).getFieldObjectInspector(), record, field);
      } catch (Exception e) {
        LOG.error(String.format("current columnNames: %s", columnNames.stream().collect(Collectors.joining(","))));
        LOG.error(String.format("current type: %s", columnTypes.stream().map(f -> f.getTypeName()).collect(Collectors.joining(","))));
        LOG.error(String.format("current value: %s", HoodieRealtimeRecordReaderUtils.arrayWritableToString((ArrayWritable) o)));
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java

    List<FileStatus> result = new ArrayList<>();
    fileGroups.stream().forEach(f -> {
      try {
        List<FileSlice> baseFiles = f.getAllFileSlices().filter(slice -> slice.getBaseFile().isPresent()).collect(Collectors.toList());
```

### SimplifyStreamApiCallChains
Can be replaced with 'java.util.ArrayList' constructor
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
            return Stream.empty();
          }
        }, hoodieInstants.size()).stream().collect(Collectors.toList());

        if (!danglingFiles.isEmpty()) {
```

### SimplifyStreamApiCallChains
Can be replaced with 'collection.toArray()'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    // Remove Hoodie meta columns except partition path from input source
    String[] colsToDrop = shouldDropMetaFields ? HoodieRecord.HOODIE_META_COLUMNS.stream().toArray(String[]::new) :
        HoodieRecord.HOODIE_META_COLUMNS.stream().filter(x -> !x.equals(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).toArray(String[]::new);
    final Dataset<Row> src = source.drop(colsToDrop);
```

### SimplifyStreamApiCallChains
Can be replaced with 'collection.toArray()'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/DataTypeUtils.java`
#### Snippet
```java
      fields.add(DataTypes.FIELD(fieldNames.get(i), fieldTypes.get(i)));
    }
    return DataTypes.ROW(fields.stream().toArray(DataTypes.Field[]::new)).notNull();
  }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/spark/sql/execution/datasources/parquet/Spark24HoodieVectorizedParquetRecordReader.java`
#### Snippet
```java
    boolean result = super.nextBatch();
    if (idToColumnVectors != null) {
      idToColumnVectors.entrySet().stream().forEach(e -> e.getValue().reset());
    }
    numBatched = resultBatch().numRows();
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/spark/sql/execution/datasources/parquet/Spark24HoodieVectorizedParquetRecordReader.java`
#### Snippet
```java
      typeChangeInfos.entrySet()
          .stream()
          .forEach(f -> {
            WritableColumnVector vector =
                memoryMode == MemoryMode.OFF_HEAP ? new OffHeapColumnVector(capacity, f.getValue().getLeft()) : new OnHeapColumnVector(capacity, f.getValue().getLeft());
```

## RuleId[id=StringOperationCanBeSimplified]
### StringOperationCanBeSimplified
`toUpperCase()` call can be replaced with 'equalsIgnoreCase()'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
        .getValByRegex(HOODIE_CONSUME_MODE_PATTERN_STRING.pattern());
    List<String> result = tablesModeMap.entrySet().stream().map(s -> {
      if (s.getValue().trim().toUpperCase().equals(INCREMENTAL_SCAN_MODE)) {
        Matcher matcher = HOODIE_CONSUME_MODE_PATTERN_STRING.matcher(s.getKey());
        return (!matcher.find() ? null : matcher.group(1));
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
        if (!danglingFiles.isEmpty()) {
          LOG.error("Data table validation failed due to extra files found for completed commits " + danglingFiles.size());
          danglingFiles.forEach(entry -> LOG.error("Dangling file: " + entry.toString()));
          finalResult = false;
          if (!cfg.ignoreFailed) {
```

## RuleId[id=TypeParameterHidesVisibleType]
### TypeParameterHidesVisibleType
Type parameter `T` hides type parameter 'T'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkTable.java`
#### Snippet
```java
   */
  @Override
  protected <T extends SpecificRecordBase> Option<HoodieTableMetadataWriter> getMetadataWriter(
      String triggeringInstantTimestamp,
      HoodieFailedWritesCleaningPolicy failedWritesCleaningPolicy,
```

### TypeParameterHidesVisibleType
Type parameter `T` hides type parameter 'T'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/LazyFileIterable.java`
#### Snippet
```java
   * Iterator implementation for the iterable defined above.
   */
  public class LazyFileIterator<T, R> implements ClosableIterator<R> {

    private final String filePath;
```

### TypeParameterHidesVisibleType
Type parameter `R` hides type parameter 'R'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/LazyFileIterable.java`
#### Snippet
```java
   * Iterator implementation for the iterable defined above.
   */
  public class LazyFileIterator<T, R> implements ClosableIterator<R> {

    private final String filePath;
```

### TypeParameterHidesVisibleType
Type parameter `R` hides type parameter 'R'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
   * in-memory map 2) diskLazyFileIterator - Iterates over all the data spilled to disk.
   */
  private class IteratorWrapper<R> implements Iterator<R> {

    private final Iterator<R> inMemoryIterator;
```

## RuleId[id=DeprecatedIsStillUsed]
### DeprecatedIsStillUsed
Deprecated member 'useJdbc' is still used
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java`
#### Snippet
```java
    @Deprecated
    @Parameter(names = {"--use-jdbc"}, description = "Hive jdbc connect url")
    public Boolean useJdbc;
    @Parameter(names = {"--metastore-uris"}, description = "Hive metastore uris")
    public String metastoreUris;
```

### DeprecatedIsStillUsed
Deprecated member 'tagLocation' is still used
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,
                                                    HoodieEngineContext context,
                                                    HoodieTable hoodieTable) throws HoodieIndexException;
```

### DeprecatedIsStillUsed
Deprecated member 'updateLocation' is still used
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<WriteStatus> updateLocation(List<WriteStatus> writeStatuses,
                                                   HoodieEngineContext context,
                                                   HoodieTable hoodieTable) throws HoodieIndexException;
```

### DeprecatedIsStillUsed
Deprecated member 'tagLocation' is still used
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,
                                                    HoodieEngineContext context,
                                                    HoodieTable hoodieTable) throws HoodieIndexException;
```

### DeprecatedIsStillUsed
Deprecated member 'updateLocation' is still used
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract List<WriteStatus> updateLocation(List<WriteStatus> writeStatuses,
                                                   HoodieEngineContext context,
                                                   HoodieTable hoodieTable) throws HoodieIndexException;
```

### DeprecatedIsStillUsed
Deprecated member 'updateLocation' is still used
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/SparkHoodieIndex.java`
#### Snippet
```java
  @Deprecated
  @PublicAPIMethod(maturity = ApiMaturityLevel.DEPRECATED)
  public abstract JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD,
                                                      HoodieEngineContext context,
                                                      HoodieTable hoodieTable) throws HoodieIndexException;
```

### DeprecatedIsStillUsed
Deprecated member 'CustomKeyGenerator' is still used
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/CustomKeyGenerator.java`
#### Snippet
```java
 */
@Deprecated
public class CustomKeyGenerator extends BuiltinKeyGenerator {

  private final CustomAvroKeyGenerator customAvroKeyGenerator;
```

### DeprecatedIsStillUsed
Deprecated member 'HoodieReadClient' is still used
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java`
#### Snippet
```java
 */
@Deprecated
public class HoodieReadClient<T> extends SparkRDDReadClient<T> {

  public HoodieReadClient(HoodieSparkEngineContext context, String basePath) {
```

### DeprecatedIsStillUsed
Deprecated member 'CLEANER_POLICY' is still used
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCleanConfig.java`
#### Snippet
```java
  // CLEANER_COMMITS_RETAINED, CLEANER_HOURS_RETAINED, CLEANER_FILE_VERSIONS_RETAINED
  @Deprecated
  public static final ConfigProperty<String> CLEANER_POLICY = ConfigProperty
      .key("hoodie.cleaner.policy")
      .defaultValue(HoodieCleaningPolicy.KEEP_LATEST_COMMITS.name())
```

### DeprecatedIsStillUsed
Deprecated member 'ORDERING_FIELD' is still used
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodiePayloadConfig.java`
#### Snippet
```java
  /** @deprecated Use {@link HoodieWriteConfig#PRECOMBINE_FIELD_NAME} and its methods instead */
  @Deprecated
  public static final ConfigProperty<String> ORDERING_FIELD = ConfigProperty
      .key(PAYLOAD_ORDERING_FIELD_PROP_KEY)
      .defaultValue("ts")
```

### DeprecatedIsStillUsed
Deprecated member 'getBlock' is still used
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java`
#### Snippet
```java
   */
  @Deprecated
  public static HoodieAvroDataBlock getBlock(byte[] content, Schema readerSchema, InternalSchema internalSchema) throws IOException {

    SizeAwareDataInputStream dis = new SizeAwareDataInputStream(new DataInputStream(new ByteArrayInputStream(content)));
```

### DeprecatedIsStillUsed
Deprecated member 'DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS' is still used
in `hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java`
#### Snippet
```java
  public static final String LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP = LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP_KEY;
  @Deprecated
  public static final int DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS = 60 * 1000;
  /** @deprecated Use {@link #HIVE_DATABASE_NAME_PROP_KEY} */
  @Deprecated
```

### DeprecatedIsStillUsed
Deprecated member 'TIMESTAMP_TIMEZONE_FORMAT' is still used
in `hudi-common/src/main/java/org/apache/hudi/common/config/TimestampKeyGeneratorConfig.java`
#### Snippet
```java
  //still keeping this prop for backward compatibility so that functionality for existing users does not break.
  @Deprecated
  public static final ConfigProperty<String> TIMESTAMP_TIMEZONE_FORMAT = ConfigProperty
      .key(TIMESTAMP_KEYGEN_CONFIG_PREFIX + "timezone")
      .defaultValue("UTC")
```

### DeprecatedIsStillUsed
Deprecated member 'MetadataRecordsGenerationParams' is still used
in `hudi-common/src/main/java/org/apache/hudi/metadata/MetadataRecordsGenerationParams.java`
#### Snippet
```java
 */
@Deprecated
public class MetadataRecordsGenerationParams implements Serializable {

  private final HoodieTableMetaClient dataMetaClient;
```

### DeprecatedIsStillUsed
Deprecated member 'HoodieAvroParquetReaderBuilder' is still used
in `hudi-common/src/main/java/org/apache/parquet/avro/HoodieAvroParquetReaderBuilder.java`
#### Snippet
```java

  @Deprecated
  public HoodieAvroParquetReaderBuilder(Path path) {
    super(path);
  }
```

### DeprecatedIsStillUsed
Deprecated member 'HoodieSnapshotCopier' is still used
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotCopier.java`
#### Snippet
```java
 * @deprecated Use {@link HoodieSnapshotExporter} instead.
 */
public class HoodieSnapshotCopier implements Serializable {

  private static final Logger LOG = LoggerFactory.getLogger(HoodieSnapshotCopier.class);
```

### DeprecatedIsStillUsed
Deprecated member 'READ_LATEST_INSTANT_ON_MISSING_CKPT' is still used
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/config/HoodieIncrSourceConfig.java`
#### Snippet
```java

  @Deprecated
  public static final ConfigProperty<Boolean> READ_LATEST_INSTANT_ON_MISSING_CKPT = ConfigProperty
      .key("hoodie.deltastreamer.source.hoodieincr.read_latest_on_missing_ckpt")
      .defaultValue(false)
```

### DeprecatedIsStillUsed
Deprecated member 'getNextFilePathsAndMaxModificationTime' is still used
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java`
#### Snippet
```java
   */
  @Deprecated
  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(Option<String> lastCheckpointStr,
                                                                             long sourceLimit) {
    try {
```

### DeprecatedIsStillUsed
Deprecated member 'HDFSParquetImporter' is still used
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java`
#### Snippet
```java
 * @see HoodieDeltaStreamer
 */
public class HDFSParquetImporter implements Serializable {

  private static final long serialVersionUID = 1L;
```

### DeprecatedIsStillUsed
Deprecated member 'targetTableName' is still used
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
    @Deprecated
    @Parameter(names = {"--target-table"}, description = "name of the target table")
    public String targetTableName;

    @Parameter(names = {"--table-type"}, description = "Type of table. COPY_ON_WRITE (or) MERGE_ON_READ", required = true)
```

## RuleId[id=JavaReflectionMemberAccess]
### JavaReflectionMemberAccess
Cannot resolve method 'getWithoutRegisterFns'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/IMetaStoreClientUtil.java`
#### Snippet
```java
    IMetaStoreClient metaStoreClient;
    try {
      metaStoreClient = ((Hive) Hive.class.getMethod("getWithoutRegisterFns", HiveConf.class).invoke(null, hiveConf)).getMSC();
    } catch (NoSuchMethodException | IllegalAccessException | IllegalArgumentException
      | InvocationTargetException ex) {
```

### JavaReflectionMemberAccess
Cannot resolve method 'getFromPathRecursively'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
      } catch (NoSuchMethodException e) {
        // HiveFileFormatUtils.getFromPathRecursively method only available in Hive 3.x
        method = hiveUtilsClass.getMethod("getFromPathRecursively", Map.class, Path.class, Map.class);
      }
      return (PartitionDesc) method.invoke(null, pathToPartitionInfo, dir, cacheMap);
```

## RuleId[id=SwitchStatementWithTooFewBranches]
### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/Parquet2SparkSchemaUtils.java`
#### Snippet
```java

      case FIXED_LEN_BYTE_ARRAY:
        switch (originalType) {
          case DECIMAL:
            return "decimal(" + field.getDecimalMetadata().getPrecision() + ","
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
        case FIXED_LEN_BYTE_ARRAY:
          if (originalType != null) {
            switch (originalType) {
              case DECIMAL:
                typeInfo = BasicTypeInfo.BIG_DEC_TYPE_INFO;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteCommitActionExecutor.java`
#### Snippet
```java
    BucketInfo binfo = upsertPartitioner.getBucketInfo(partition);
    BucketType btype = binfo.bucketType;
    switch (btype) {
      case INSERT:
        return handleInsert(binfo.fileIdPrefix, recordItr);
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/MarkerBasedRollbackUtils.java`
#### Snippet
```java
    }

    switch (markerTypeOption.get()) {
      case TIMELINE_SERVER_BASED:
        // Reads all markers written by the timeline server
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/TwoToOneDowngradeHandler.java`
#### Snippet
```java
    Option<MarkerType> markerTypeOption = MarkerUtils.readMarkerType(fileSystem, markerDir);
    if (markerTypeOption.isPresent()) {
      switch (markerTypeOption.get()) {
        case TIMELINE_SERVER_BASED:
          // Reads all markers written by the timeline server
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java`
#### Snippet
```java
  protected <T, I, K, O> HoodieFileWriter getFileWriterByFormat(HoodieFileFormat format,
      FSDataOutputStream outputStream, Configuration conf, HoodieConfig config, Schema schema) throws IOException {
    switch (format) {
      case PARQUET:
        return newParquetFileWriter(outputStream, conf, config, schema);
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
  @Override
  public boolean hasMagicHeader() {
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
  @Override
  public boolean hasContentLength() {
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
  @Override
  public boolean hasHeader() {
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return false;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
  @Override
  public boolean hasOrdinal() {
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatVersion.java`
#### Snippet
```java
  @Override
  public boolean hasContent() {
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlockVersion.java`
#### Snippet
```java

  public boolean hasRecordCount() {
    switch (super.getVersion()) {
      case DEFAULT_VERSION:
        return true;
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
            LOG.info(String.format("Reading a command block %s with targetInstantTime %s from file %s", commandBlock.getType(), targetInstantForCommandBlock,
                logFile.getPath()));
            switch (commandBlock.getType()) { // there can be different types of command blocks
              case ROLLBACK_BLOCK:
                // Rollback older read log block(s)
```

### SwitchStatementWithTooFewBranches
'switch' statement has too few case labels (1), and should probably be replaced with an 'if' statement
in `hudi-common/src/main/java/org/apache/hudi/secondary/index/HoodieSecondaryIndex.java`
#### Snippet
```java

  private void validate() {
    switch (indexType) {
      case LUCENE:
        if (columns.size() != 1) {
```

## RuleId[id=FinallyBlockCannotCompleteNormally]
### FinallyBlockCannotCompleteNormally
`finally` block can not complete normally
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
    } catch (Exception e) {
      throw new HoodieException("Unable to do hoodie metadata table validation in " + cfg.basePath, e);
    } finally {

      if (asyncMetadataTableValidateService.isPresent()) {
```

## RuleId[id=DuplicatedCode]
### DuplicatedCode
Duplicated code
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java`
#### Snippet
```java
    HoodieDefaultTimeline defaultTimeline = getTimeline(HoodieCLI.getTableMetaClient(), includeArchivedTimeline);
    HoodieTimeline timeline = defaultTimeline.getCommitsTimeline().filterCompletedInstants();

    Option<HoodieInstant> hoodieInstantOption = getCommitForInstant(timeline, instantTime);
    Option<HoodieCommitMetadata> commitMetadataOptional = getHoodieCommitMetadata(timeline, hoodieInstantOption);

    if (!commitMetadataOptional.isPresent()) {
      return "Commit " + instantTime + " not found in Commits " + timeline;
    }

    HoodieCommitMetadata meta = commitMetadataOptional.get();
```

### DuplicatedCode
Duplicated code
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java
      Process process = sparkLauncher.launch();
      InputStreamConsumer.captureOutput(process);
      int exitCode = process.waitFor();
      if (exitCode != 0) {
        return "Failed to unschedule compaction for " + compactionInstant;
      }
      List<RenameOpResult> res = deSerializeOperationResult(outputPathStr, HoodieCLI.fs);
```

### DuplicatedCode
Duplicated code
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java`
#### Snippet
```java
    TreeSet<String> allPropKeys = new TreeSet<>();
    allPropKeys.addAll(newProps.keySet().stream().map(Object::toString).collect(Collectors.toSet()));
    allPropKeys.addAll(oldProps.keySet());

    String[][] rows = new String[allPropKeys.size()][];
    int ind = 0;
```

### DuplicatedCode
Duplicated code
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)
        .withArchivalConfig(HoodieArchivalConfig.newBuilder().archiveCommitsWith(minCommits,maxCommits).build())
        .withCleanConfig(HoodieCleanConfig.newBuilder().retainCommits(commitsRetained).build())
        .withEmbeddedTimelineServerEnabled(false)
        .withMetadataConfig(HoodieMetadataConfig.newBuilder().enable(enableMetadata).build())
        .build();
    HoodieEngineContext context = new HoodieSparkEngineContext(jsc);
    HoodieSparkTable<HoodieAvroPayload> table = HoodieSparkTable.create(config, context);
    try {
      HoodieTimelineArchiver archiver = new HoodieTimelineArchiver(config, table);
      archiver.archiveIfRequired(context,true);
    } catch (IOException ioe) {
      LOG.error("Failed to archive with IOException: " + ioe);
      return  -1;
    }
    return 0;
```

### DuplicatedCode
Duplicated code
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TimelineCommand.java`
#### Snippet
```java
      row[numColumns - 3] = getFormattedDate(
          instantTimestamp, HoodieInstant.State.REQUESTED, instantInfoMap, showTimeSeconds);
      row[numColumns - 2] = getFormattedDate(
          instantTimestamp, HoodieInstant.State.INFLIGHT, instantInfoMap, showTimeSeconds);
      row[numColumns - 1] = getFormattedDate(
          instantTimestamp, HoodieInstant.State.COMPLETED, instantInfoMap, showTimeSeconds);
      return row;
```

### DuplicatedCode
Duplicated code
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/UpgradeOrDowngradeCommand.java`
#### Snippet
```java
    Process process = sparkLauncher.launch();
    InputStreamConsumer.captureOutput(process);
    int exitCode = process.waitFor();
    HoodieCLI.refreshTableMetadata();
    if (exitCode != 0) {
      return String.format("Failed: Could not Upgrade/Downgrade Hoodie table to \"%s\".", toVersionName);
    }
    return String.format("Hoodie table upgraded/downgraded to %s", toVersionName);
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java`
#### Snippet
```java
      switch (engineType) {
        case SPARK:
          return SPARK_SIZED_BASED_CLUSTERING_PLAN_STRATEGY;
        case FLINK:
        case JAVA:
          return JAVA_SIZED_BASED_CLUSTERING_PLAN_STRATEGY;
        default:
          throw new HoodieNotSupportedException("Unsupported engine " + engineType);
      }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java`
#### Snippet
```java
    return partitionRecordKeyPairs.map(partitionRecordKeyPair -> {
      String recordKey = partitionRecordKeyPair.getRight();
      String partitionPath = partitionRecordKeyPair.getLeft();

      return indexFileFilter.getMatchingFilesAndPartition(partitionPath, recordKey)
          .stream()
          .map(partitionFileIdPair ->
              new ImmutablePair<>(
                  new HoodieFileGroupId(partitionFileIdPair.getLeft(), partitionFileIdPair.getRight()), recordKey));
    })
        .flatMapToPair(Stream::iterator);
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleWithChangeLog.java`
#### Snippet
```java
    Option<HoodieRecord> savedCombineRecordOp = combinedRecordOpt.map(HoodieRecord::newInstance);
    final boolean result = super.writeUpdateRecord(newRecord, oldRecord, combinedRecordOpt, writerSchema);
    if (result) {
      boolean isDelete = HoodieOperation.isDelete(newRecord.getOperation());
      Option<IndexedRecord> avroRecordOpt = savedCombineRecordOp.flatMap(r ->
          toAvroRecord(r, writerSchema, config.getPayloadConfig().getProps()));
      cdcLogger.put(newRecord, (GenericRecord) oldRecord.getData(), isDelete ? Option.empty() : avroRecordOpt);
    }
    return result;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandleWithChangeLog.java`
#### Snippet
```java
    Schema schema = useWriterSchemaForCompaction ? writeSchemaWithMetaFields : writeSchema;
    // TODO Remove these unnecessary newInstance invocations
    HoodieRecord<T> savedRecord = newRecord.newInstance();
    super.writeInsertRecord(newRecord);
    if (!HoodieOperation.isDelete(newRecord.getOperation())) {
      cdcLogger.put(newRecord, null, savedRecord.toIndexedRecord(schema, config.getPayloadConfig().getProps()).map(HoodieAvroIndexedRecord::getData));
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkTableServiceClient.java`
#### Snippet
```java
    List<HoodieWriteStat> writeStats = metadata.getPartitionToWriteStats().entrySet().stream().flatMap(e ->
        e.getValue().stream()).collect(Collectors.toList());
    if (writeStats.stream().mapToLong(HoodieWriteStat::getTotalWriteErrors).sum() > 0) {
      throw new HoodieClusteringException("Clustering failed to write to files:"
          + writeStats.stream().filter(s -> s.getTotalWriteErrors() > 0L).map(HoodieWriteStat::getFileId).collect(Collectors.joining(",")));
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
    Timer.Context indexTimer = metrics.getIndexCtx();
    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(HoodieListData.eager(hoodieRecords), context, table).collectAsList();
    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));
    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java`
#### Snippet
```java
    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =
        initTable(WriteOperationType.DELETE, Option.ofNullable(instantTime));
    preWrite(instantTime, WriteOperationType.DELETE, table.getMetaClient());
    HoodieWriteMetadata<List<WriteStatus>> result = table.delete(context, instantTime, keys);
    return postWrite(result, instantTime, table);
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/FlinkSizeBasedClusteringPlanStrategy.java`
#### Snippet
```java
HoodieWriteConfig writeConfig = getWriteConfig();

    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();
    List<FileSlice> currentGroup = new ArrayList<>();

    // Sort fileSlices before dividing, which makes dividing more compact
    List<FileSlice> sortedFileSlices = new ArrayList<>(fileSlices);
    sortedFileSlices.sort((o1, o2) -> (int)
        ((o2.getBaseFile().isPresent() ? o2.getBaseFile().get().getFileSize() : writeConfig.getParquetMaxFileSize())
            - (o1.getBaseFile().isPresent() ? o1.getBaseFile().get().getFileSize() : writeConfig.getParquetMaxFileSize())));

    long totalSizeSoFar = 0;

    for (FileSlice currentSlice : sortedFileSlices) {
      long currentSize = currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : writeConfig.getParquetMaxFileSize();
      // check if max size is reached and create new group, if needed.
      if (totalSizeSoFar + currentSize > writeConfig.getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {
        int numOutputGroups = getNumberOfOutputFileGroups(totalSizeSoFar, writeConfig.getClusteringTargetFileMaxBytes());
        LOG.info("Adding one clustering group " + totalSizeSoFar + " max bytes: "
            + writeConfig.getClusteringMaxBytesInGroup() + " num input slices: " + currentGroup.size() + " output groups: " + numOutputGroups);
        fileSliceGroups.add(Pair.of(currentGroup, numOutputGroups));
        currentGroup = new ArrayList<>();
        totalSizeSoFar = 0;
      }

      // Add to the current file-group
      currentGroup.add(currentSlice);
      // assume each file group size is ~= parquet.max.file.size
      totalSizeSoFar += currentSize;
    }

    if (!currentGroup.isEmpty()) {
      if (currentGroup.size() > 1 || writeConfig.shouldClusteringSingleGroup()) {
        int numOutputGroups = getNumberOfOutputFileGroups(totalSizeSoFar, writeConfig.getClusteringTargetFileMaxBytes());
        LOG.info("Adding final clustering group " + totalSizeSoFar + " max bytes: "
            + writeConfig.getClusteringMaxBytesInGroup() + " num input slices: " + currentGroup.size() + " output groups: " + numOutputGroups);
        fileSliceGroups.add(Pair.of(currentGroup, numOutputGroups));
      }
    }

    return fileSliceGroups.stream().map(fileSliceGroup ->
        HoodieClusteringGroup.newBuilder()
            .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))
            .setNumOutputFileGroups(fileSliceGroup.getRight())
            .setMetrics(buildMetrics(fileSliceGroup.getLeft()))
            .build());
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/JavaSizeBasedClusteringPlanStrategy.java`
#### Snippet
```java
        int numOutputGroups = getNumberOfOutputFileGroups(totalSizeSoFar, writeConfig.getClusteringTargetFileMaxBytes());
        LOG.info("Adding one clustering group " + totalSizeSoFar + " max bytes: "
                + writeConfig.getClusteringMaxBytesInGroup() + " num input slices: " + currentGroup.size() + " output groups: " + numOutputGroups);
        fileSliceGroups.add(Pair.of(currentGroup, numOutputGroups));
        currentGroup = new ArrayList<>();
        totalSizeSoFar = 0;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkConcatAndReplaceHandle.java`
#### Snippet
```java
    Schema oldSchema = config.populateMetaFields() ? writeSchemaWithMetaFields : writeSchema;
    String key = oldRecord.getRecordKey(oldSchema, keyGeneratorOpt);
    try {
      fileWriter.write(key, oldRecord, writeSchema);
    } catch (IOException | RuntimeException e) {
      String errMsg = String.format("Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s",
          key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));
      LOG.debug("Old record is " + oldRecord);
      throw new HoodieUpsertException(errMsg, e);
    }
    recordsWritten++;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java`
#### Snippet
```java
    fileWriter.close();
    HoodieWriteStat stat = writeStatus.getStat();
    stat.setPartitionPath(partitionPath);
    stat.setNumWrites(writeStatus.getTotalRecords());
    stat.setNumDeletes(0);
    stat.setNumInserts(writeStatus.getTotalRecords());
    stat.setPrevCommit(HoodieWriteStat.NULL_COMMIT);
    stat.setFileId(fileId);
    stat.setPath(new Path(writeConfig.getBasePath()), path);
    long fileSizeInBytes = FSUtils.getFileSize(table.getMetaClient().getFs(), path);
    stat.setTotalWriteBytes(fileSizeInBytes);
    stat.setFileSizeInBytes(fileSizeInBytes);
    stat.setTotalWriteErrors(writeStatus.getTotalErrorRecords());
    HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();
    runtimeStats.setTotalCreateTime(currTimer.endTimer());
    stat.setRuntimeStats(runtimeStats);
    return writeStatus;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
    if (upsertHandle.getOldFilePath() == null) {
      throw new HoodieUpsertException(
          "Error in finding the old file path at commit " + instantTime + " for fileId: " + fileId);
    } else {
      HoodieMergeHelper.newInstance().runMerge(this, upsertHandle);
    }

    // TODO(vc): This needs to be revisited
    if (upsertHandle.getPartitionPath() == null) {
      LOG.info("Upsert Handle has partition path as null " + upsertHandle.getOldFilePath() + ", "
          + upsertHandle.writeStatuses());
    }

    return Collections.singletonList(upsertHandle.writeStatuses()).iterator();
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java`
#### Snippet
```java
    String actionType = getCommitActionType();
    LOG.info("Committing " + instantTime + ", action Type " + actionType);
    result.setCommitted(true);
    result.setWriteStats(writeStats);
    // Finalize write
    finalizeWrite(instantTime, writeStats, result);
    try {
      LOG.info("Committing " + instantTime + ", action Type " + getCommitActionType());
      HoodieActiveTimeline activeTimeline = table.getActiveTimeline();
      HoodieCommitMetadata metadata = result.getCommitMetadata().get();

      writeTableMetadata(metadata, actionType);

      activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
      LOG.info("Committed " + instantTime);
      result.setCommitMetadata(Option.of(metadata));
    } catch (IOException e) {
      throw new HoodieCommitException("Failed to complete commit " + config.getBasePath() + " at time " + instantTime,
          e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java`
#### Snippet
```java
      HoodieActiveTimeline activeTimeline = table.getActiveTimeline();
      HoodieCommitMetadata metadata = result.getCommitMetadata().get();
      writeTableMetadata(metadata, actionType);
      activeTimeline.saveAsComplete(new HoodieInstant(true, getCommitActionType(), instantTime),
          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
      LOG.info("Committed " + instantTime);
      result.setCommitMetadata(Option.of(metadata));
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java`
#### Snippet
```java
    boolean isIndexingGlobal = table.getIndex().isGlobal();
    if (isIndexingGlobal) {
      HashSet<String> recordKeys = keys.stream().map(HoodieKey::getRecordKey).collect(Collectors.toCollection(HashSet::new));
      List<HoodieKey> deduplicatedKeys = new LinkedList<>();
      keys.forEach(x -> {
        if (recordKeys.contains(x.getRecordKey())) {
          deduplicatedKeys.add(x);
        }
      });
      return deduplicatedKeys;
    } else {
      HashSet<HoodieKey> set = new HashSet<>(keys);
      keys.clear();
      keys.addAll(set);
      return keys;
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeleteHelper.java`
#### Snippet
```java
try {
      HoodieWriteMetadata<List<WriteStatus>> result = null;
      List<HoodieKey> dedupedKeys = keys;
      final int parallelism = config.getDeleteShuffleParallelism();
      if (config.shouldCombineBeforeDelete()) {
        // De-dupe/merge if needed
        dedupedKeys = deduplicateKeys(keys, table, parallelism);
      }

      List<HoodieRecord<EmptyHoodieRecordPayload>> dedupedRecords =
          dedupedKeys.stream().map(key -> new HoodieAvroRecord<>(key, new EmptyHoodieRecordPayload())).collect(Collectors.toList());
      Instant beginTag = Instant.now();
      // perform index look up to get existing location of records
      List<HoodieRecord<EmptyHoodieRecordPayload>> taggedRecords = table.getIndex().tagLocation(HoodieListData.eager(dedupedRecords), context, table).collectAsList();
      Duration tagLocationDuration = Duration.between(beginTag, Instant.now());

      // filter out non existent keys/records
      List<HoodieRecord<EmptyHoodieRecordPayload>> taggedValidRecords = taggedRecords.stream().filter(HoodieRecord::isCurrentLocationKnown).collect(Collectors.toList());
      if (!taggedValidRecords.isEmpty()) {
        result = deleteExecutor.execute(taggedValidRecords);
        result.setIndexLookupDuration(tagLocationDuration);
      } else {
        // if entire set of keys are non existent
        deleteExecutor.saveWorkloadProfileMetadataToInflight(new WorkloadProfile(Pair.of(new HashMap<>(), new WorkloadStat())), instantTime);
        result = new HoodieWriteMetadata<>();
        result.setWriteStatuses(Collections.EMPTY_LIST);
        deleteExecutor.commitOnAutoCommit(result);
      }
      return result;
    } catch (Throwable e) {
      if (e instanceof HoodieUpsertException) {
        throw (HoodieUpsertException) e;
      }
      throw new HoodieUpsertException("Failed to delete for commit time " + instantTime, e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkDeletePartitionCommitActionExecutor.java`
#### Snippet
```java
      HoodieInstant dropPartitionsInstant = new HoodieInstant(REQUESTED, REPLACE_COMMIT_ACTION, instantTime);
      if (!table.getMetaClient().getFs().exists(new Path(table.getMetaClient().getMetaPath(),
          dropPartitionsInstant.getFileName()))) {
        HoodieRequestedReplaceMetadata requestedReplaceMetadata = HoodieRequestedReplaceMetadata.newBuilder()
            .setOperationType(WriteOperationType.DELETE_PARTITION.name())
            .setExtraMetadata(extraMetadata.orElse(Collections.emptyMap()))
            .build();
        table.getMetaClient().getActiveTimeline().saveToPendingReplaceCommit(dropPartitionsInstant,
            TimelineMetadataUtils.serializeRequestedReplaceMetadata(requestedReplaceMetadata));
      }

      this.saveWorkloadProfileMetadataToInflight(new WorkloadProfile(Pair.of(new HashMap<>(), new WorkloadStat())),
          instantTime);
      this.commitOnAutoCommit(result);
      return result;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
    if (result.getIndexLookupDuration().isPresent()) {
      metrics.updateIndexMetrics(getOperationType().name(), result.getIndexUpdateDuration().get().toMillis());
    }
    if (result.isCommitted()) {
      // Perform post commit operations.
      if (result.getFinalizeDuration().isPresent()) {
        metrics.updateFinalizeWriteMetrics(result.getFinalizeDuration().get().toMillis(),
            result.getWriteStats().get().size());
      }

      postCommit(hoodieTable, result.getCommitMetadata().get(), instantTime, Option.empty());
      mayBeCleanAndArchive(hoodieTable);

      emitCommitMetrics(instantTime, result.getCommitMetadata().get(), hoodieTable.getMetaClient().getCommitActionType());
    }
    return result.getWriteStatuses();
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
      if (!partitionPathStatMap.containsKey(partitionPath)) {
        partitionPathStatMap.put(partitionPath, new WorkloadStat());
      }

      if (locOption.isPresent()) {
        // update
        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);
        globalStat.addUpdates(locOption.get(), count);
      } else {
        // insert
        partitionPathStatMap.get(partitionPath).addInserts(count);
        globalStat.addInserts(count);
      }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/BaseJavaCommitActionExecutor.java`
#### Snippet
```java
    BucketInfo binfo = javaUpsertPartitioner.getBucketInfo(partition);
    BucketType btype = binfo.bucketType;
    try {
      if (btype.equals(BucketType.INSERT)) {
        return handleInsert(binfo.fileIdPrefix, recordItr);
      } else if (btype.equals(BucketType.UPDATE)) {
        return handleUpdate(binfo.partitionPath, binfo.fileIdPrefix, recordItr);
      } else {
        throw new HoodieUpsertException("Unknown bucketType " + btype + " for partition :" + partition);
      }
    } catch (Throwable t) {
      String msg = "Error upserting bucketType " + btype + " for partition :" + partition;
      LOG.error(msg, t);
      throw new HoodieUpsertException(msg, t);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java`
#### Snippet
```java
    try {
      if (btype.equals(BucketType.INSERT)) {
        return handleInsert(binfo.fileIdPrefix, recordItr);
      } else if (btype.equals(BucketType.UPDATE)) {
        return handleUpdate(binfo.partitionPath, binfo.fileIdPrefix, recordItr);
      } else {
        throw new HoodieUpsertException("Unknown bucketType " + btype + " for partition :" + partition);
      }
    } catch (Throwable t) {
      String msg = "Error upserting bucketType " + btype + " for partition :" + partition;
      LOG.error(msg, t);
      throw new HoodieUpsertException(msg, t);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
    for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {
      WorkloadStat outputWorkloadStats = profile.getOutputPartitionPathStatMap().getOrDefault(partitionStat.getKey(), new WorkloadStat());
      for (Map.Entry<String, Pair<String, Long>> updateLocEntry :
          partitionStat.getValue().getUpdateLocationToCount().entrySet()) {
        addUpdateBucket(partitionStat.getKey(), updateLocEntry.getKey());
        if (profile.hasOutputWorkLoadStats()) {
          HoodieRecordLocation hoodieRecordLocation = new HoodieRecordLocation(updateLocEntry.getValue().getKey(), updateLocEntry.getKey());
          outputWorkloadStats.addUpdates(hoodieRecordLocation, updateLocEntry.getValue().getValue());
        }
      }
      if (profile.hasOutputWorkLoadStats()) {
        profile.updateOutputPartitionPathStatMap(partitionStat.getKey(), outputWorkloadStats);
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
          for (int b = 0; b < insertBuckets; b++) {
            bucketNumbers.add(totalBuckets);
            if (b < insertBuckets - 1) {
              recordsPerBucket.add(insertRecordsPerBucket);
            } else {
              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);
            }
            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);
            bucketInfoMap.put(totalBuckets, bucketInfo);
            if (profile.hasOutputWorkLoadStats()) {
              outputWorkloadStats.addInserts(new HoodieRecordLocation(HoodieWriteStat.NULL_COMMIT, bucketInfo.getFileIdPrefix()), recordsPerBucket.get(recordsPerBucket.size() - 1));
            }
            totalBuckets++;
          }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();
        double currentCumulativeWeight = 0;
        for (int i = 0; i < bucketNumbers.size(); i++) {
          InsertBucket bkt = new InsertBucket();
          bkt.bucketNumber = bucketNumbers.get(i);
          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();
          currentCumulativeWeight += bkt.weight;
          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, currentCumulativeWeight));
        }
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
    List<SmallFile> smallFileLocations = new ArrayList<>();

    HoodieTimeline commitTimeline = table.getMetaClient().getCommitsTimeline().filterCompletedInstants();

    if (!commitTimeline.empty()) { // if we have some commits
      HoodieInstant latestCommitTime = commitTimeline.lastInstant().get();
      List<HoodieBaseFile> allFiles = table.getBaseFileOnlyView()
          .getLatestBaseFilesBeforeOrOn(partitionPath, latestCommitTime.getTimestamp()).collect(Collectors.toList());

      for (HoodieBaseFile file : allFiles) {
        if (file.getFileSize() < config.getParquetSmallFileLimit()) {
          String filename = file.getFileName();
          SmallFile sf = new SmallFile();
          sf.location = new HoodieRecordLocation(FSUtils.getCommitTime(filename), FSUtils.getFileId(filename));
          sf.sizeBytes = file.getFileSize();
          smallFileLocations.add(sf);
        }
      }
    }

    return smallFileLocations;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
      final double r = 1.0 * Math.floorMod(hashOfKey, totalInserts) / totalInserts;

      int index = Collections.binarySearch(targetBuckets, new InsertBucketCumulativeWeightPair(new InsertBucket(), r));

      if (index >= 0) {
        return targetBuckets.get(index).getKey().bucketNumber;
      }

      if ((-1 * index - 1) < targetBuckets.size()) {
        return targetBuckets.get((-1 * index - 1)).getKey().bucketNumber;
      }

      // return first one, by default
      return targetBuckets.get(0).getKey().bucketNumber;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
    long avgSize = hoodieWriteConfig.getCopyOnWriteRecordSizeEstimate();
    long fileSizeThreshold = (long) (hoodieWriteConfig.getRecordSizeEstimationThreshold() * hoodieWriteConfig.getParquetSmallFileLimit());
    try {
      if (!commitTimeline.empty()) {
        // Go over the reverse ordered commits to get a more recent estimate of average record size.
        Iterator<HoodieInstant> instants = commitTimeline.getReverseOrderedInstants().iterator();
        while (instants.hasNext()) {
          HoodieInstant instant = instants.next();
          HoodieCommitMetadata commitMetadata = HoodieCommitMetadata
              .fromBytes(commitTimeline.getInstantDetails(instant).get(), HoodieCommitMetadata.class);
          long totalBytesWritten = commitMetadata.fetchTotalBytesWritten();
          long totalRecordsWritten = commitMetadata.fetchTotalRecordsWritten();
          if (totalBytesWritten > fileSizeThreshold && totalRecordsWritten > 0) {
            avgSize = (long) Math.ceil((1.0 * totalBytesWritten) / totalRecordsWritten);
            break;
          }
        }
      }
    } catch (Throwable t) {
      // make this fail safe.
      LOG.error("Error trying to compute average bytes/record ", t);
    }
    return avgSize;
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SparkSingleFileSortExecutionStrategy.java`
#### Snippet
```java
    if (numOutputGroups != 1 || fileGroupIdList.size() != 1) {
      throw new HoodieClusteringException("Expect only one file group for strategy: " + getClass().getName());
    }
    LOG.info("Starting clustering for a group, parallelism:" + numOutputGroups + " commit:" + instantTime);

    HoodieWriteConfig newConfig = HoodieWriteConfig.newBuilder()
        .withBulkInsertParallelism(numOutputGroups)
        .withProps(getWriteConfig().getProps()).build();

    // Since clustering will write to single file group using HoodieUnboundedCreateHandle, set max file size to a large value.
    newConfig.setValue(HoodieStorageConfig.PARQUET_MAX_FILE_SIZE, String.valueOf(Long.MAX_VALUE));
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SqlQueryEqualityPreCommitValidator.java`
#### Snippet
```java
    String queryWithPrevSnapshot = query.replaceAll(HoodiePreCommitValidatorConfig.VALIDATOR_TABLE_VARIABLE, prevTableSnapshot);
    String queryWithNewSnapshot = query.replaceAll(HoodiePreCommitValidatorConfig.VALIDATOR_TABLE_VARIABLE, newTableSnapshot);
    LOG.info("Running query on previous state: " + queryWithPrevSnapshot);
    Dataset<Row> prevRows = sqlContext.sql(queryWithPrevSnapshot);
    LOG.info("Running query on new state: " + queryWithNewSnapshot);
    Dataset<Row> newRows  = sqlContext.sql(queryWithNewSnapshot);
    printAllRowsIfDebugEnabled(prevRows);
    printAllRowsIfDebugEnabled(newRows);
    boolean areDatasetsEqual = prevRows.intersect(newRows).count() == prevRows.count();
```

### DuplicatedCode
Duplicated code
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/HoodieFileProbingFunction.java`
#### Snippet
```java
        if (!fileIDBaseFileMap.containsKey(fileId)) {
          Option<HoodieBaseFile> baseFile =
              baseFileOnlyViewBroadcast.getValue().getLatestBaseFile(partitionPath, fileId);
          if (!baseFile.isPresent()) {
            throw new HoodieIndexException("Failed to find the base file for partition: " + partitionPath
                + ", fileId: " + fileId);
          }

          fileIDBaseFileMap.put(fileId, baseFile.get());
        }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
    List<Schema.Field> projectedFields = new ArrayList<>();
    for (String fn : fieldNames) {
      Schema.Field field = schemaFieldsMap.get(fn.toLowerCase());
      if (field == null) {
        throw new HoodieException("Field " + fn + " not found in log schema. Query cannot proceed! "
            + "Derived Schema Fields: " + new ArrayList<>(schemaFieldsMap.keySet()));
      } else {
        projectedFields.add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultVal()));
      }
    }

    Schema projectedSchema = Schema.createRecord(originalSchema.getName(), originalSchema.getDoc(),
        originalSchema.getNamespace(), originalSchema.isError());
    projectedSchema.setFields(projectedFields);
    return projectedSchema;
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/bloom/HoodieDynamicBoundedBloomFilter.java`
#### Snippet
```java
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    DataOutputStream dos = new DataOutputStream(baos);
    try {
      internalDynamicBloomFilter.write(dos);
      byte[] bytes = baos.toByteArray();
      dos.close();
      return Base64CodecUtil.encode(bytes);
    } catch (IOException e) {
      throw new HoodieIndexException("Could not serialize BloomFilter instance", e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java
    try (FSDataOutputStream outputStream = fs.create(propertyPath)) {
      if (!hoodieConfig.contains(NAME)) {
        throw new IllegalArgumentException(NAME.key() + " property needs to be specified");
      }
      hoodieConfig.setDefaultValue(TYPE);
      if (hoodieConfig.getString(TYPE).equals(HoodieTableType.MERGE_ON_READ.name())) {
        hoodieConfig.setDefaultValue(PAYLOAD_CLASS_NAME);
        hoodieConfig.setDefaultValue(RECORD_MERGER_STRATEGY);
      }
      hoodieConfig.setDefaultValue(ARCHIVELOG_FOLDER);
      if (!hoodieConfig.contains(TIMELINE_LAYOUT_VERSION)) {
        // Use latest Version as default unless forced by client
        hoodieConfig.setValue(TIMELINE_LAYOUT_VERSION, TimelineLayoutVersion.CURR_VERSION.toString());
      }
      if (hoodieConfig.contains(BOOTSTRAP_BASE_PATH)) {
        // Use the default bootstrap index class.
        hoodieConfig.setDefaultValue(BOOTSTRAP_INDEX_CLASS_NAME, getDefaultBootstrapIndexClass(properties));
      }
      if (hoodieConfig.contains(TIMELINE_TIMEZONE)) {
        HoodieInstantTimeGenerator.setCommitTimeZone(HoodieTimelineTimeZone.valueOf(hoodieConfig.getString(TIMELINE_TIMEZONE)));
      }
      hoodieConfig.setDefaultValue(DROP_PARTITION_COLUMNS);

      storeProperties(hoodieConfig.getProps(), outputStream);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
      if (instantAndCommitMetadata.isPresent()) {
        HoodieInstant instant = instantAndCommitMetadata.get().getLeft();
        HoodieCommitMetadata metadata = instantAndCommitMetadata.get().getRight();
        synchronized (this) {
          if (latestCommitWithValidSchema == null) {
            latestCommitWithValidSchema = instant;
          }
          commitMetadataCache.get().putIfAbsent(instant, metadata);
        }
      }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
    currentInstantLogBlocks = new ArrayDeque<>();
    progress = 0.0f;
    totalLogFiles = new AtomicLong(0);
    totalRollbacks = new AtomicLong(0);
    totalCorruptBlocks = new AtomicLong(0);
    totalLogBlocks = new AtomicLong(0);
    totalLogRecords = new AtomicLong(0);
    HoodieLogFormatReader logFormatReaderWrapper = null;
    HoodieTimeline commitsTimeline = this.hoodieTableMetaClient.getCommitsTimeline();
    HoodieTimeline completedInstantsTimeline = commitsTimeline.filterCompletedInstants();
    HoodieTimeline inflightInstantsTimeline = commitsTimeline.filterInflights();
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
        HoodieLogFile logFile = logFormatReaderWrapper.getLogFile();
        LOG.info("Scanning log file " + logFile);
        scannedLogFiles.add(logFile);
        totalLogFiles.set(scannedLogFiles.size());
        // Use the HoodieLogFileReader to iterate through the blocks in the log file
        HoodieLogBlock logBlock = logFormatReaderWrapper.next();
        final String instantTime = logBlock.getLogBlockHeader().get(INSTANT_TIME);
        totalLogBlocks.incrementAndGet();
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
    Map<String, HoodieBaseFile> viewDataFiles = fileGroups.stream().flatMap(HoodieFileGroup::getAllRawFileSlices)
        .map(FileSlice::getBaseFile).filter(Option::isPresent).map(Option::get)
        .map(df -> Pair.of(Path.getPathWithoutSchemeAndAuthority(new Path(df.getPath())).toString(), df))
        .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    try {
      List<BaseFileDTO> dataFiles = executeRequest(LATEST_DATA_FILE_ON_INSTANT_URL, paramsMap,
          new TypeReference<List<BaseFileDTO>>() {
          }, RequestMethod.GET);
      return Option.fromJavaOptional(dataFiles.stream().map(BaseFileDTO::toHoodieBaseFile).findFirst());
    } catch (IOException e) {
      throw new HoodieRemoteException(e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    Map<String, String> paramsMap = getParamsWithPartitionPath(partitionPath);
    try {
      List<FileSliceDTO> dataFiles = executeRequest(LATEST_PARTITION_SLICES_URL, paramsMap,
          new TypeReference<List<FileSliceDTO>>() {}, RequestMethod.GET);
      return dataFiles.stream().map(FileSliceDTO::toFileSlice);
    } catch (IOException e) {
      throw new HoodieRemoteException(e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    try {
      List<FileSliceDTO> dataFiles = executeRequest(LATEST_SLICES_BEFORE_ON_INSTANT_URL, paramsMap,
          new TypeReference<List<FileSliceDTO>>() {}, RequestMethod.GET);
      return dataFiles.stream().map(FileSliceDTO::toFileSlice);
    } catch (IOException e) {
      throw new HoodieRemoteException(e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    Map<String, String> paramsMap = getParamsWithPartitionPath(partitionPath);
    try {
      List<FileGroupDTO> fileGroups = executeRequest(ALL_FILEGROUPS_FOR_PARTITION_URL, paramsMap,
          new TypeReference<List<FileGroupDTO>>() {}, RequestMethod.GET);
      return DTOUtils.fileGroupDTOsToFileGroups(fileGroups, metaClient);
    } catch (IOException e) {
      throw new HoodieRemoteException(e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    Map<String, String> paramsMap = getParamsWithAdditionalParam(partitionPath, MAX_INSTANT_PARAM, maxCommitTime);
    try {
      List<FileGroupDTO> fileGroups = executeRequest(ALL_REPLACED_FILEGROUPS_BEFORE_OR_ON, paramsMap,
          new TypeReference<List<FileGroupDTO>>() {}, RequestMethod.GET);
      return DTOUtils.fileGroupDTOsToFileGroups(fileGroups, metaClient);
    } catch (IOException e) {
      throw new HoodieRemoteException(e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    Map<String, String> paramsMap = getParams();
    try {
      List<CompactionOpDTO> dtos = executeRequest(PENDING_COMPACTION_OPS, paramsMap,
          new TypeReference<List<CompactionOpDTO>>() {}, RequestMethod.GET);
      return dtos.stream().map(CompactionOpDTO::toCompactionOperation);
    } catch (IOException e) {
      throw new HoodieRemoteException(e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java`
#### Snippet
```java
      new File(baseStoreDir).mkdirs();
      Map<HoodieFileGroupId, Pair<String, CompactionOperation>> pendingMap = new ExternalSpillableMap<>(
          maxMemoryForPendingCompaction, baseStoreDir, new DefaultSizeEstimator(), new DefaultSizeEstimator<>(),
          diskMapType, isBitCaskDiskMapCompressionEnabled);
      pendingMap.putAll(fgIdToPendingCompaction);
      return pendingMap;
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java`
#### Snippet
```java
    try {
      Map<String, String> checkpointMap = OBJECT_MAPPER.readValue(value, Map.class);
      if (!checkpointMap.containsKey(id)) {
        return Option.empty();
      }
      String checkpointVal = checkpointMap.get(id);
      return Option.of(checkpointVal);
    } catch (IOException e) {
      throw new HoodieIOException("Failed to parse checkpoint as map", e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java`
#### Snippet
```java
    HoodieTimeline deltaCommits = activeTimeline.getDeltaCommitTimeline();

    HoodieInstant latestInstant;
    if (lastCompaction.isPresent()) {
      latestInstant = lastCompaction.get();
      // timeline containing the delta commits after the latest completed compaction commit,
      // and the completed compaction commit instant
      return Option.of(Pair.of(deltaCommits.findInstantsAfter(
          latestInstant.getTimestamp(), Integer.MAX_VALUE), lastCompaction.get()));
    } else {
      if (deltaCommits.countInstants() > 0) {
        latestInstant = deltaCommits.firstInstant().get();
        // timeline containing all the delta commits, and the first delta commit instant
        return Option.of(Pair.of(deltaCommits.findInstantsAfterOrEquals(
            latestInstant.getTimestamp(), Integer.MAX_VALUE), latestInstant));
      } else {
        return Option.empty();
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java
    StringBuilder sb = new StringBuilder();
    for (int i = 0; i < path.length(); i++) {
      char c = path.charAt(i);
      if (needsEscaping(c)) {
        sb.append('%');
        sb.append(String.format("%1$02X", (int) c));
      } else {
        sb.append(c);
      }
    }
    return sb.toString();
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
    final Map<MetadataPartitionType, HoodieData<HoodieRecord>> partitionToRecordsMap = new HashMap<>();
    final HoodieData<HoodieRecord> filesPartitionRecordsRDD = context.parallelize(
        convertMetadataToFilesPartitionRecords(commitMetadata, instantTime), 1);
    partitionToRecordsMap.put(MetadataPartitionType.FILES, filesPartitionRecordsRDD);

    if (recordsGenerationParams.getEnabledPartitionTypes().contains(MetadataPartitionType.BLOOM_FILTERS)) {
      final HoodieData<HoodieRecord> metadataBloomFilterRecords = convertMetadataToBloomFilterRecords(context, commitMetadata, instantTime, recordsGenerationParams);
      partitionToRecordsMap.put(MetadataPartitionType.BLOOM_FILTERS, metadataBloomFilterRecords);
    }

    if (recordsGenerationParams.getEnabledPartitionTypes().contains(MetadataPartitionType.COLUMN_STATS)) {
      final HoodieData<HoodieRecord> metadataColumnStatsRDD = convertMetadataToColumnStatsRecords(commitMetadata, context, recordsGenerationParams);
      partitionToRecordsMap.put(MetadataPartitionType.COLUMN_STATS, metadataColumnStatsRDD);
    }
    return partitionToRecordsMap;
```

### DuplicatedCode
Duplicated code
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
    partitionToRecordsMap.put(MetadataPartitionType.FILES, filesPartitionRecordsRDD);

    if (recordsGenerationParams.getEnabledPartitionTypes().contains(MetadataPartitionType.BLOOM_FILTERS)) {
      final HoodieData<HoodieRecord> metadataBloomFilterRecordsRDD =
          convertFilesToBloomFilterRecords(engineContext, partitionToDeletedFiles, partitionToAppendedFiles, recordsGenerationParams, instantTime);
      partitionToRecordsMap.put(MetadataPartitionType.BLOOM_FILTERS, metadataBloomFilterRecordsRDD);
    }

    if (recordsGenerationParams.getEnabledPartitionTypes().contains(MetadataPartitionType.COLUMN_STATS)) {
      final HoodieData<HoodieRecord> metadataColumnStatsRDD =
          convertFilesToColumnStatsRecords(engineContext, partitionToDeletedFiles, partitionToAppendedFiles, recordsGenerationParams);
      partitionToRecordsMap.put(MetadataPartitionType.COLUMN_STATS, metadataColumnStatsRDD);
    }
    return partitionToRecordsMap;
```

### DuplicatedCode
Duplicated code
in `hudi-examples/hudi-examples-common/src/main/java/org/apache/hudi/examples/common/HoodieExampleDataGenerator.java`
#### Snippet
```java
      HoodieKey key = new HoodieKey(UUID.randomUUID().toString(), partitionPath);
      KeyPartition kp = new KeyPartition();
      kp.key = key;
      kp.partitionPath = partitionPath;
      existingKeys.put(currSize + i, kp);
      numExistingKeys++;
      return new HoodieAvroRecord<>(key, generateRandomValue(key, commitTime));
```

### DuplicatedCode
Duplicated code
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/utils/QuickstartConfigurations.java`
#### Snippet
```java
    StringBuilder builder = new StringBuilder();
    builder.append("create table ").append(tableName).append("(\n");
    for (String field : fields) {
      builder.append("  ").append(field).append(",\n");
    }
    builder.append("  PRIMARY KEY(").append(pkField).append(") NOT ENFORCED\n")
        .append(")\n");
```

### DuplicatedCode
Duplicated code
in `hudi-examples/hudi-examples-java/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java`
#### Snippet
```java
    String newCommitTime = client.startCommit();
    LOG.info("Starting commit " + newCommitTime);

    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);
    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);
```

### DuplicatedCode
Duplicated code
in `hudi-examples/hudi-examples-java/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java`
#### Snippet
```java
    client.insert(writeRecords, newCommitTime);

    // updates
    newCommitTime = client.startCommit();
    LOG.info("Starting commit " + newCommitTime);
    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);
    records.addAll(toBeUpdated);
    recordsSoFar.addAll(toBeUpdated);
```

### DuplicatedCode
Duplicated code
in `hudi-examples/hudi-examples-java/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java`
#### Snippet
```java
    client.upsert(writeRecords, newCommitTime);

    // Delete
    newCommitTime = client.startCommit();
    LOG.info("Starting commit " + newCommitTime);
    // just delete half of the records
    int numToDelete = recordsSoFar.size() / 2;
    List<HoodieKey> toBeDeleted =
        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());
```

### DuplicatedCode
Duplicated code
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java
    Dataset<Row> df = spark.read().json(jsc.parallelize(inserts, 1));

    df.write().format("hudi")
        .options(QuickstartUtils.getQuickstartWriteConfigs())
        .option(HoodieWriteConfig.PRECOMBINE_FIELD_NAME.key(), "ts")
        .option(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), "uuid")
        .option(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key(), "partitionpath")
        .option(TBL_NAME.key(), tableName)
        .mode(Overwrite)
        .save(tablePath);
    return df;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/FlinkClusteringConfig.java`
#### Snippet
```java
    TypedProperties properties = DFSPropertiesConfiguration.getGlobalProps();
    props.forEach(x -> {
      String[] kv = x.split("=");
      ValidationUtils.checkArgument(kv.length == 2);
      properties.setProperty(kv[0], kv[1]);
    });
    return properties;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/FlinkClusteringConfig.java`
#### Snippet
```java
    conf.setString(FlinkOptions.CLUSTERING_PLAN_STRATEGY_CLASS, config.planStrategyClass);
    conf.setString(FlinkOptions.CLUSTERING_PLAN_PARTITION_FILTER_MODE_NAME, config.planPartitionFilterMode);
    conf.setLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_TARGET_FILE_MAX_BYTES, config.targetFileMaxBytes);
    conf.setLong(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SMALL_FILE_LIMIT, config.smallFileLimit);
    conf.setInteger(FlinkOptions.CLUSTERING_PLAN_STRATEGY_SKIP_PARTITIONS_FROM_LATEST, config.skipFromLatestPartitions);
    conf.setString(FlinkOptions.CLUSTERING_PLAN_STRATEGY_CLUSTER_BEGIN_PARTITION, config.clusterBeginPartition);
    conf.setString(FlinkOptions.CLUSTERING_PLAN_STRATEGY_CLUSTER_END_PARTITION, config.clusterEndPartition);
    conf.setString(FlinkOptions.CLUSTERING_PLAN_STRATEGY_PARTITION_REGEX_PATTERN, config.partitionRegexPattern);
    conf.setString(FlinkOptions.CLUSTERING_PLAN_STRATEGY_PARTITION_SELECTED, config.partitionSelected);
    conf.setString(FlinkOptions.CLUSTERING_SORT_COLUMNS, config.sortColumns);
    conf.setInteger(FlinkOptions.WRITE_SORT_MEMORY, config.sortMemory);
    conf.setInteger(FlinkOptions.CLUSTERING_MAX_NUM_GROUPS, config.maxNumGroups);
    conf.setInteger(FlinkOptions.CLUSTERING_TARGET_PARTITIONS, config.targetPartitions);
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/IncrementalInputSplits.java`
#### Snippet
```java
              Option<List<String>> logPaths = Option.ofNullable(fileSlice.getLogFiles()
                  .sorted(HoodieLogFile.getLogFileComparator())
                  .map(logFile -> logFile.getPath().toString())
                  .filter(logPath -> !logPath.endsWith(HoodieCDCUtils.CDC_LOGFILE_SUFFIX))
                  .collect(Collectors.toList()));
              String basePath = fileSlice.getBaseFile().map(BaseFile::getPath).orElse(null);
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableFactory.java`
#### Snippet
```java
      if (OptionsResolver.isDefaultHoodieRecordPayloadClazz(conf)) {
        throw new HoodieValidationException("Option '" + FlinkOptions.PRECOMBINE_FIELD.key()
            + "' is required for payload class: " + DefaultHoodieRecordPayload.class.getName());
      }
      if (preCombineField.equals(FlinkOptions.PRECOMBINE_FIELD.defaultValue())) {
        conf.setString(FlinkOptions.PRECOMBINE_FIELD, FlinkOptions.NO_PRE_COMBINE);
      } else if (!preCombineField.equals(FlinkOptions.NO_PRE_COMBINE)) {
        throw new HoodieValidationException("Field " + preCombineField + " does not exist in the table schema."
            + "Please check '" + FlinkOptions.PRECOMBINE_FIELD.key() + "' option.");
      }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
          String curAvroKey = logRecordsKeyIterator.next();
          Option<IndexedRecord> curAvroRecord = null;
          final HoodieAvroRecord<?> hoodieRecord = (HoodieAvroRecord) scanner.getRecords().get(curAvroKey);
          try {
            curAvroRecord = hoodieRecord.getData().getInsertValue(tableSchema);
          } catch (IOException e) {
            throw new HoodieException("Get avro insert value error for key: " + curAvroKey, e);
          }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/StreamerUtil.java`
#### Snippet
```java
    DFSPropertiesConfiguration conf = new DFSPropertiesConfiguration(hadoopConfig, cfgPath);
    try {
      if (!overriddenProps.isEmpty()) {
        LOG.info("Adding overridden properties to file properties.");
        conf.addPropsFromStream(new BufferedReader(new StringReader(String.join("\n", overriddenProps))), cfgPath);
      }
    } catch (IOException ioe) {
      throw new HoodieIOException("Unexpected error adding config overrides", ioe);
    }

    return conf;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
    List<String> selNonPartNames = Arrays.stream(selectedFields)
        .mapToObj(i -> fullFieldNames[i])
        .filter(n -> !partitionSpec.containsKey(n))
        .collect(Collectors.toList());

    int[] selParquetFields = Arrays.stream(selectedFields)
        .filter(i -> !partitionSpec.containsKey(fullFieldNames[i]))
        .toArray();

    ParquetColumnarRowSplitReader.ColumnBatchGenerator gen = readVectors -> {
      // create and initialize the row batch
      ColumnVector[] vectors = new ColumnVector[selectedFields.length];
      for (int i = 0; i < vectors.length; i++) {
        String name = fullFieldNames[selectedFields[i]];
        LogicalType type = fullFieldTypes[selectedFields[i]].getLogicalType();
        vectors[i] = createVector(readVectors, selNonPartNames, name, type, partitionSpec, batchSize);
      }
      return new VectorizedColumnBatch(vectors);
    };

    return new ParquetColumnarRowSplitReader(
        utcTimestamp,
        caseSensitive,
        conf,
        Arrays.stream(selParquetFields)
            .mapToObj(i -> fullFieldTypes[i].getLogicalType())
            .toArray(LogicalType[]::new),
        selNonPartNames.toArray(new String[0]),
        gen,
        batchSize,
        new org.apache.hadoop.fs.Path(path.toUri()),
        splitStart,
        splitLength);
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
    if (partitionSpec.containsKey(name)) {
      return createVectorFromConstant(type, partitionSpec.get(name), batchSize);
    }
    ColumnVector readVector = readVectors[selNonPartNames.indexOf(name)];
    if (readVector == null) {
      // when the read vector is null, use a constant null vector instead
      readVector = createVectorFromConstant(type, null, batchSize);
    }
    return readVector;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
switch (type.getTypeRoot()) {
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        HeapBytesVector bsv = new HeapBytesVector(batchSize);
        if (value == null) {
          bsv.fillWithNulls();
        } else {
          bsv.fill(value instanceof byte[]
              ? (byte[]) value
              : value.toString().getBytes(StandardCharsets.UTF_8));
        }
        return bsv;
      case BOOLEAN:
        HeapBooleanVector bv = new HeapBooleanVector(batchSize);
        if (value == null) {
          bv.fillWithNulls();
        } else {
          bv.fill((boolean) value);
        }
        return bv;
      case TINYINT:
        HeapByteVector byteVector = new HeapByteVector(batchSize);
        if (value == null) {
          byteVector.fillWithNulls();
        } else {
          byteVector.fill(((Number) value).byteValue());
        }
        return byteVector;
      case SMALLINT:
        HeapShortVector sv = new HeapShortVector(batchSize);
        if (value == null) {
          sv.fillWithNulls();
        } else {
          sv.fill(((Number) value).shortValue());
        }
        return sv;
      case INTEGER:
        HeapIntVector iv = new HeapIntVector(batchSize);
        if (value == null) {
          iv.fillWithNulls();
        } else {
          iv.fill(((Number) value).intValue());
        }
        return iv;
      case BIGINT:
        HeapLongVector lv = new HeapLongVector(batchSize);
        if (value == null) {
          lv.fillWithNulls();
        } else {
          lv.fill(((Number) value).longValue());
        }
        return lv;
      case DECIMAL:
        DecimalType decimalType = (DecimalType) type;
        int precision = decimalType.getPrecision();
        int scale = decimalType.getScale();
        DecimalData decimal = value == null
            ? null
            : Preconditions.checkNotNull(DecimalData.fromBigDecimal((BigDecimal) value, precision, scale));
        ColumnVector internalVector = createVectorFromConstant(
            new VarBinaryType(),
            decimal == null ? null : decimal.toUnscaledBytes(),
            batchSize);
        return new ParquetDecimalVector(internalVector);
      case FLOAT:
        HeapFloatVector fv = new HeapFloatVector(batchSize);
        if (value == null) {
          fv.fillWithNulls();
        } else {
          fv.fill(((Number) value).floatValue());
        }
        return fv;
      case DOUBLE:
        HeapDoubleVector dv = new HeapDoubleVector(batchSize);
        if (value == null) {
          dv.fillWithNulls();
        } else {
          dv.fill(((Number) value).doubleValue());
        }
        return dv;
      case DATE:
        if (value instanceof LocalDate) {
          value = Date.valueOf((LocalDate) value);
        }
        return createVectorFromConstant(
            new IntType(),
            value == null ? null : dateToInternal((Date) value),
            batchSize);
      case TIMESTAMP_WITHOUT_TIME_ZONE:
        HeapTimestampVector tv = new HeapTimestampVector(batchSize);
        if (value == null) {
          tv.fillWithNulls();
        } else {
          tv.fill(TimestampData.fromLocalDateTime((LocalDateTime) value));
        }
        return tv;
      default:
        throw new UnsupportedOperationException("Unsupported type: " + type);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
    List<ColumnDescriptor> filtered = new ArrayList<>();
    for (ColumnDescriptor descriptor : columns) {
      if (depth >= descriptor.getPath().length) {
        throw new InvalidSchemaException("Expect depth " + depth + " for schema: " + descriptor);
      }
      if (type.getName().equals(descriptor.getPath()[depth])) {
        filtered.add(descriptor);
      }
    }
    ValidationUtils.checkState(filtered.size() > 0, "Corrupted Parquet schema");
    return filtered;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
List<ColumnDescriptor> descriptors = filterDescriptors(depth, physicalType, columns);
    ColumnDescriptor descriptor = descriptors.get(0);
    PageReader pageReader = pages.getPageReader(descriptor);
    switch (fieldType.getTypeRoot()) {
      case BOOLEAN:
        return new BooleanColumnReader(descriptor, pageReader);
      case TINYINT:
        return new ByteColumnReader(descriptor, pageReader);
      case DOUBLE:
        return new DoubleColumnReader(descriptor, pageReader);
      case FLOAT:
        return new FloatColumnReader(descriptor, pageReader);
      case INTEGER:
      case DATE:
      case TIME_WITHOUT_TIME_ZONE:
        return new IntColumnReader(descriptor, pageReader);
      case BIGINT:
        return new LongColumnReader(descriptor, pageReader);
      case SMALLINT:
        return new ShortColumnReader(descriptor, pageReader);
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        return new BytesColumnReader(descriptor, pageReader);
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        switch (descriptor.getPrimitiveType().getPrimitiveTypeName()) {
          case INT64:
            int precision = fieldType instanceof TimestampType
                ? ((TimestampType) fieldType).getPrecision()
                : ((LocalZonedTimestampType) fieldType).getPrecision();
            return new Int64TimestampColumnReader(utcTimestamp, descriptor, pageReader, precision);
          case INT96:
            return new TimestampColumnReader(utcTimestamp, descriptor, pageReader);
          default:
            throw new AssertionError();
        }
      case DECIMAL:
        switch (descriptor.getPrimitiveType().getPrimitiveTypeName()) {
          case INT32:
            return new IntColumnReader(descriptor, pageReader);
          case INT64:
            return new LongColumnReader(descriptor, pageReader);
          case BINARY:
            return new BytesColumnReader(descriptor, pageReader);
          case FIXED_LEN_BYTE_ARRAY:
            return new FixedLenBytesColumnReader(
                descriptor, pageReader);
          default:
            throw new AssertionError();
        }
      case ARRAY:
        return new ArrayColumnReader(
            descriptor,
            pageReader,
            utcTimestamp,
            descriptor.getPrimitiveType(),
            fieldType);
      case MAP:
        MapType mapType = (MapType) fieldType;
        ArrayColumnReader keyReader =
            new ArrayColumnReader(
                descriptor,
                pageReader,
                utcTimestamp,
                descriptor.getPrimitiveType(),
                new ArrayType(mapType.getKeyType()));
        ArrayColumnReader valueReader =
            new ArrayColumnReader(
                descriptors.get(1),
                pages.getPageReader(descriptors.get(1)),
                utcTimestamp,
                descriptors.get(1).getPrimitiveType(),
                new ArrayType(mapType.getValueType()));
        return new MapColumnReader(keyReader, valueReader, fieldType);
      case ROW:
        RowType rowType = (RowType) fieldType;
        GroupType groupType = physicalType.asGroupType();
        List<ColumnReader> fieldReaders = new ArrayList<>();
        for (int i = 0; i < rowType.getFieldCount(); i++) {
          // schema evolution: read the parquet file with a new extended field name.
          int fieldIndex = getFieldIndexInPhysicalType(rowType.getFields().get(i).getName(), groupType);
          if (fieldIndex < 0) {
            fieldReaders.add(new EmptyColumnReader());
          } else {
            fieldReaders.add(i,
                createColumnReader(
                    utcTimestamp,
                    rowType.getTypeAt(i),
                    groupType.getType(fieldIndex),
                    descriptors,
                    pages,
                    depth + 1));
          }
        }
        return new RowColumnReader(fieldReaders);
      default:
        throw new UnsupportedOperationException(fieldType + " is not supported now.");
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
List<ColumnDescriptor> descriptors = filterDescriptors(depth, physicalType, columns);
    PrimitiveType primitiveType = descriptors.get(0).getPrimitiveType();
    PrimitiveType.PrimitiveTypeName typeName = primitiveType.getPrimitiveTypeName();
    switch (fieldType.getTypeRoot()) {
      case BOOLEAN:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.BOOLEAN,
            "Unexpected type: %s", typeName);
        return new HeapBooleanVector(batchSize);
      case TINYINT:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.INT32,
            "Unexpected type: %s", typeName);
        return new HeapByteVector(batchSize);
      case DOUBLE:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.DOUBLE,
            "Unexpected type: %s", typeName);
        return new HeapDoubleVector(batchSize);
      case FLOAT:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.FLOAT,
            "Unexpected type: %s", typeName);
        return new HeapFloatVector(batchSize);
      case INTEGER:
      case DATE:
      case TIME_WITHOUT_TIME_ZONE:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.INT32,
            "Unexpected type: %s", typeName);
        return new HeapIntVector(batchSize);
      case BIGINT:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.INT64,
            "Unexpected type: %s", typeName);
        return new HeapLongVector(batchSize);
      case SMALLINT:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.INT32,
            "Unexpected type: %s", typeName);
        return new HeapShortVector(batchSize);
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        checkArgument(
            typeName == PrimitiveType.PrimitiveTypeName.BINARY,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        checkArgument(primitiveType.getOriginalType() != OriginalType.TIME_MICROS,
            "TIME_MICROS original type is not ");
        return new HeapTimestampVector(batchSize);
      case DECIMAL:
        checkArgument(
            (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY
                || typeName == PrimitiveType.PrimitiveTypeName.BINARY)
                && primitiveType.getOriginalType() == OriginalType.DECIMAL,
            "Unexpected type: %s", typeName);
        return new HeapBytesVector(batchSize);
      case ARRAY:
        ArrayType arrayType = (ArrayType) fieldType;
        return new HeapArrayVector(
            batchSize,
            createWritableColumnVector(
                batchSize,
                arrayType.getElementType(),
                physicalType,
                descriptors,
                depth));
      case MAP:
        MapType mapType = (MapType) fieldType;
        GroupType repeatedType = physicalType.asGroupType().getType(0).asGroupType();
        // the map column has three level paths.
        return new HeapMapColumnVector(
            batchSize,
            createWritableColumnVector(
                batchSize,
                mapType.getKeyType(),
                repeatedType.getType(0),
                descriptors,
                depth + 2),
            createWritableColumnVector(
                batchSize,
                mapType.getValueType(),
                repeatedType.getType(1),
                descriptors,
                depth + 2));
      case ROW:
        RowType rowType = (RowType) fieldType;
        GroupType groupType = physicalType.asGroupType();
        WritableColumnVector[] columnVectors = new WritableColumnVector[rowType.getFieldCount()];
        for (int i = 0; i < columnVectors.length; i++) {
          // schema evolution: read the file with a new extended field name.
          int fieldIndex = getFieldIndexInPhysicalType(rowType.getFields().get(i).getName(), groupType);
          if (fieldIndex < 0) {
            columnVectors[i] = (WritableColumnVector) createVectorFromConstant(rowType.getTypeAt(i), null, batchSize);
          } else {
            columnVectors[i] =
                createWritableColumnVector(
                    batchSize,
                    rowType.getTypeAt(i),
                    groupType.getType(fieldIndex),
                    descriptors,
                    depth + 1);
          }
        }
        return new HeapRowColumnVector(batchSize, columnVectors);
      default:
        throw new UnsupportedOperationException(fieldType + " is not supported now.");
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java`
#### Snippet
```java
int rowId = 0;
    WritableIntVector dictionaryIds = null;
    if (dictionary != null) {
      dictionaryIds = vector.reserveDictionaryIds(readNumber);
    }
    while (readNumber > 0) {
      // Compute the number of values we want to read in this page.
      int leftInPage = (int) (endOfPageValueCount - valuesRead);
      if (leftInPage == 0) {
        DataPage page = pageReader.readPage();
        if (page instanceof DataPageV1) {
          readPageV1((DataPageV1) page);
        } else if (page instanceof DataPageV2) {
          readPageV2((DataPageV2) page);
        } else {
          throw new RuntimeException("Unsupported page type: " + page.getClass());
        }
        leftInPage = (int) (endOfPageValueCount - valuesRead);
      }
      int num = Math.min(readNumber, leftInPage);
      if (isCurrentPageDictionaryEncoded) {
        // Read and decode dictionary ids.
        runLenDecoder.readDictionaryIds(
            num, dictionaryIds, vector, rowId, maxDefLevel, this.dictionaryIdsDecoder);

        if (vector.hasDictionary() || (rowId == 0 && supportLazyDecode())) {
          // Column vector supports lazy decoding of dictionary values so just set the dictionary.
          // We can't do this if rowId != 0 AND the column doesn't have a dictionary (i.e. some
          // non-dictionary encoded values have already been added).
          vector.setDictionary(new ParquetDictionary(dictionary));
        } else {
          readBatchFromDictionaryIds(rowId, num, vector, dictionaryIds);
        }
      } else {
        if (vector.hasDictionary() && rowId != 0) {
          // This batch already has dictionary encoded values but this new page is not. The batch
          // does not support a mix of dictionary and not so we will decode the dictionary.
          readBatchFromDictionaryIds(0, rowId, vector, vector.getDictionaryIds());
        }
        vector.setDictionary(null);
        readBatch(rowId, num, vector);
      }

      valuesRead += num;
      rowId += num;
      readNumber -= num;
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java`
#### Snippet
```java
    this.pageValueCount = page.getValueCount();
    ValuesReader rlReader = page.getRlEncoding().getValuesReader(descriptor, REPETITION_LEVEL);

    // Initialize the decoders.
    if (page.getDlEncoding() != Encoding.RLE && descriptor.getMaxDefinitionLevel() != 0) {
      throw new UnsupportedOperationException("Unsupported encoding: " + page.getDlEncoding());
    }
    int bitWidth = BytesUtils.getWidthFromMaxInt(descriptor.getMaxDefinitionLevel());
    this.runLenDecoder = new RunLengthDecoder(bitWidth);
    try {
      BytesInput bytes = page.getBytes();
      ByteBufferInputStream in = bytes.toInputStream();
      rlReader.initFromPage(pageValueCount, in);
      this.runLenDecoder.initFromStream(pageValueCount, in);
      prepareNewPage(page.getValueEncoding(), in);
    } catch (IOException e) {
      throw new IOException("could not read page " + page + " in col " + descriptor, e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java`
#### Snippet
```java
    this.pageValueCount = page.getValueCount();

    int bitWidth = BytesUtils.getWidthFromMaxInt(descriptor.getMaxDefinitionLevel());
    // do not read the length from the stream. v2 pages handle dividing the page bytes.
    this.runLenDecoder = new RunLengthDecoder(bitWidth, false);
    this.runLenDecoder.initFromStream(
        this.pageValueCount, page.getDefinitionLevels().toInputStream());
    try {
      prepareNewPage(page.getDataEncoding(), page.getData().toInputStream());
    } catch (IOException e) {
      throw new IOException("could not read page " + page + " in col " + descriptor, e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/AbstractColumnReader.java`
#### Snippet
```java
    this.endOfPageValueCount = valuesRead + pageValueCount;
    if (dataEncoding.usesDictionary()) {
      if (dictionary == null) {
        throw new IOException("Could not read page in col "
            + descriptor
            + " as the dictionary was missing for encoding "
            + dataEncoding);
      }
      @SuppressWarnings("deprecation")
      Encoding plainDict = Encoding.PLAIN_DICTIONARY; // var to allow warning suppression
      if (dataEncoding != plainDict && dataEncoding != Encoding.RLE_DICTIONARY) {
        throw new UnsupportedOperationException("Unsupported encoding: " + dataEncoding);
      }
      this.dataInputStream = null;
      this.dictionaryIdsDecoder = new RunLengthDecoder();
      try {
        this.dictionaryIdsDecoder.initFromStream(pageValueCount, in);
      } catch (IOException e) {
        throw new IOException("could not read dictionary in col " + descriptor, e);
      }
      this.isCurrentPageDictionaryEncoded = true;
    } else {
      if (dataEncoding != Encoding.PLAIN) {
        throw new UnsupportedOperationException("Unsupported encoding: " + dataEncoding);
      }
      this.dictionaryIdsDecoder = null;
      LOG.debug("init from page at offset {} for length {}", in.position(), in.available());
      this.dataInputStream = in.remainingStream();
      this.isCurrentPageDictionaryEncoded = false;
    }

    afterReadPage();
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
    HeapArrayVector lcv = (HeapArrayVector) vector;
    // before readBatch, initial the size of offsets & lengths as the default value,
    // the actual size will be assigned in setChildrenInfo() after reading complete.
    lcv.offsets = new long[VectorizedColumnBatch.DEFAULT_SIZE];
    lcv.lengths = new long[VectorizedColumnBatch.DEFAULT_SIZE];
    // Because the length of ListColumnVector.child can't be known now,
    // the valueList will save all data for ListColumnVector temporary.
    List<Object> valueList = new ArrayList<>();

    LogicalType category = ((ArrayType) logicalType).getElementType();

    // read the first row in parquet data page, this will be only happened once for this
    // instance
    if (isFirstRow) {
      if (!fetchNextValue(category)) {
        return;
      }
      isFirstRow = false;
    }

    int index = collectDataFromParquetPage(readNumber, lcv, valueList, category);

    // Convert valueList to array for the ListColumnVector.child
    fillColumnVector(category, lcv, valueList, index);
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
    int left = readPageIfNeed();
    if (left > 0) {
      // get the values of repetition and definitionLevel
      readRepetitionAndDefinitionLevels();
      // read the data if it isn't null
      if (definitionLevel == maxDefLevel) {
        if (isCurrentPageDictionaryEncoded) {
          lastValue = dataColumn.readValueDictionaryId();
        } else {
          lastValue = readPrimitiveTypedRow(category);
        }
      } else {
        lastValue = null;
      }
      return true;
    } else {
      eof = true;
      return false;
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
    switch (category.getTypeRoot()) {
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        return dataColumn.readString();
      case BOOLEAN:
        return dataColumn.readBoolean();
      case TIME_WITHOUT_TIME_ZONE:
      case DATE:
      case INTEGER:
        return dataColumn.readInteger();
      case TINYINT:
        return dataColumn.readTinyInt();
      case SMALLINT:
        return dataColumn.readSmallInt();
      case BIGINT:
        return dataColumn.readLong();
      case FLOAT:
        return dataColumn.readFloat();
      case DOUBLE:
        return dataColumn.readDouble();
      case DECIMAL:
        switch (descriptor.getPrimitiveType().getPrimitiveTypeName()) {
          case INT32:
            return dataColumn.readInteger();
          case INT64:
            return dataColumn.readLong();
          case BINARY:
          case FIXED_LEN_BYTE_ARRAY:
            return dataColumn.readString();
          default:
            throw new AssertionError();
        }
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        return dataColumn.readTimestamp();
      default:
        throw new RuntimeException("Unsupported type in the list: " + type);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
    if (dictionaryValue == null) {
      return null;
    }

    switch (category.getTypeRoot()) {
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        return dictionary.readString(dictionaryValue);
      case DATE:
      case TIME_WITHOUT_TIME_ZONE:
      case INTEGER:
        return dictionary.readInteger(dictionaryValue);
      case BOOLEAN:
        return dictionary.readBoolean(dictionaryValue) ? 1 : 0;
      case DOUBLE:
        return dictionary.readDouble(dictionaryValue);
      case FLOAT:
        return dictionary.readFloat(dictionaryValue);
      case TINYINT:
        return dictionary.readTinyInt(dictionaryValue);
      case SMALLINT:
        return dictionary.readSmallInt(dictionaryValue);
      case BIGINT:
        return dictionary.readLong(dictionaryValue);
      case DECIMAL:
        switch (descriptor.getPrimitiveType().getPrimitiveTypeName()) {
          case INT32:
            return dictionary.readInteger(dictionaryValue);
          case INT64:
            return dictionary.readLong(dictionaryValue);
          case FIXED_LEN_BYTE_ARRAY:
          case BINARY:
            return dictionary.readString(dictionaryValue);
          default:
            throw new AssertionError();
        }
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        return dictionary.readTimestamp(dictionaryValue);
      default:
        throw new RuntimeException("Unsupported type in the list: " + type);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
    int index = 0;
    /*
     * Here is a nested loop for collecting all values from a parquet page.
     * A column of array type can be considered as a list of lists, so the two loops are as below:
     * 1. The outer loop iterates on rows (index is a row index, so points to a row in the batch), e.g.:
     * [0, 2, 3]    <- index: 0
     * [NULL, 3, 4] <- index: 1
     *
     * 2. The inner loop iterates on values within a row (sets all data from parquet data page
     * for an element in ListColumnVector), so fetchNextValue returns values one-by-one:
     * 0, 2, 3, NULL, 3, 4
     *
     * As described below, the repetition level (repetitionLevel != 0)
     * can be used to decide when we'll start to read values for the next list.
     */
    while (!eof && index < total) {
      // add element to ListColumnVector one by one
      lcv.offsets[index] = valueList.size();
      /*
       * Let's collect all values for a single list.
       * Repetition level = 0 means that a new list started there in the parquet page,
       * in that case, let's exit from the loop, and start to collect value for a new list.
       */
      do {
        /*
         * Definition level = 0 when a NULL value was returned instead of a list
         * (this is not the same as a NULL value in of a list).
         */
        if (definitionLevel == 0) {
          lcv.setNullAt(index);
        }
        valueList.add(
            isCurrentPageDictionaryEncoded
                ? dictionaryDecodeValue(category, (Integer) lastValue)
                : lastValue);
      } while (fetchNextValue(category) && (repetitionLevel != 0));

      lcv.lengths[index] = valueList.size() - lcv.offsets[index];
      index++;
    }
    return index;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
    lcv.setSize(itemNum);
    long[] lcvLength = new long[elementNum];
    long[] lcvOffset = new long[elementNum];
    System.arraycopy(lcv.lengths, 0, lcvLength, 0, elementNum);
    System.arraycopy(lcv.offsets, 0, lcvOffset, 0, elementNum);
    lcv.lengths = lcvLength;
    lcv.offsets = lcvOffset;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
int total = valueList.size();
    setChildrenInfo(lcv, total, elementNum);
    switch (category.getTypeRoot()) {
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        lcv.child = new HeapBytesVector(total);
        ((HeapBytesVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          byte[] src = ((List<byte[]>) valueList).get(i);
          if (src == null) {
            ((HeapBytesVector) lcv.child).setNullAt(i);
          } else {
            ((HeapBytesVector) lcv.child).appendBytes(i, src, 0, src.length);
          }
        }
        break;
      case BOOLEAN:
        lcv.child = new HeapBooleanVector(total);
        ((HeapBooleanVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapBooleanVector) lcv.child).setNullAt(i);
          } else {
            ((HeapBooleanVector) lcv.child).vector[i] =
                ((List<Boolean>) valueList).get(i);
          }
        }
        break;
      case TINYINT:
        lcv.child = new HeapByteVector(total);
        ((HeapByteVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapByteVector) lcv.child).setNullAt(i);
          } else {
            ((HeapByteVector) lcv.child).vector[i] =
                (byte) ((List<Integer>) valueList).get(i).intValue();
          }
        }
        break;
      case SMALLINT:
        lcv.child = new HeapShortVector(total);
        ((HeapShortVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapShortVector) lcv.child).setNullAt(i);
          } else {
            ((HeapShortVector) lcv.child).vector[i] =
                (short) ((List<Integer>) valueList).get(i).intValue();
          }
        }
        break;
      case INTEGER:
      case DATE:
      case TIME_WITHOUT_TIME_ZONE:
        lcv.child = new HeapIntVector(total);
        ((HeapIntVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapIntVector) lcv.child).setNullAt(i);
          } else {
            ((HeapIntVector) lcv.child).vector[i] = ((List<Integer>) valueList).get(i);
          }
        }
        break;
      case FLOAT:
        lcv.child = new HeapFloatVector(total);
        ((HeapFloatVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapFloatVector) lcv.child).setNullAt(i);
          } else {
            ((HeapFloatVector) lcv.child).vector[i] = ((List<Float>) valueList).get(i);
          }
        }
        break;
      case BIGINT:
        lcv.child = new HeapLongVector(total);
        ((HeapLongVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapLongVector) lcv.child).setNullAt(i);
          } else {
            ((HeapLongVector) lcv.child).vector[i] = ((List<Long>) valueList).get(i);
          }
        }
        break;
      case DOUBLE:
        lcv.child = new HeapDoubleVector(total);
        ((HeapDoubleVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapDoubleVector) lcv.child).setNullAt(i);
          } else {
            ((HeapDoubleVector) lcv.child).vector[i] =
                ((List<Double>) valueList).get(i);
          }
        }
        break;
      case TIMESTAMP_WITHOUT_TIME_ZONE:
      case TIMESTAMP_WITH_LOCAL_TIME_ZONE:
        lcv.child = new HeapTimestampVector(total);
        ((HeapTimestampVector) lcv.child).reset();
        for (int i = 0; i < valueList.size(); i++) {
          if (valueList.get(i) == null) {
            ((HeapTimestampVector) lcv.child).setNullAt(i);
          } else {
            ((HeapTimestampVector) lcv.child)
                .setTimestamp(i, ((List<TimestampData>) valueList).get(i));
          }
        }
        break;
      case DECIMAL:
        PrimitiveType.PrimitiveTypeName primitiveTypeName =
            descriptor.getPrimitiveType().getPrimitiveTypeName();
        switch (primitiveTypeName) {
          case INT32:
            lcv.child = new ParquetDecimalVector(new HeapIntVector(total));
            ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              if (valueList.get(i) == null) {
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .setNullAt(i);
              } else {
                ((HeapIntVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Integer>) valueList).get(i);
              }
            }
            break;
          case INT64:
            lcv.child = new ParquetDecimalVector(new HeapLongVector(total));
            ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              if (valueList.get(i) == null) {
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .setNullAt(i);
              } else {
                ((HeapLongVector) ((ParquetDecimalVector) lcv.child).vector)
                    .vector[i] =
                    ((List<Long>) valueList).get(i);
              }
            }
            break;
          default:
            lcv.child = new ParquetDecimalVector(new HeapBytesVector(total));
            ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector).reset();
            for (int i = 0; i < valueList.size(); i++) {
              byte[] src = ((List<byte[]>) valueList).get(i);
              if (valueList.get(i) == null) {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
                    .setNullAt(i);
              } else {
                ((HeapBytesVector) ((ParquetDecimalVector) lcv.child).vector)
                    .appendBytes(i, src, 0, src.length);
              }
            }
            break;
        }
        break;
      default:
        throw new RuntimeException("Unsupported type in the list: " + type);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/BaseVectorizedColumnReader.java`
#### Snippet
```java
    DataPage page = pageReader.readPage();

    if (page == null) {
      return;
    }

    page.accept(
        new DataPage.Visitor<Void>() {
          @Override
          public Void visit(DataPageV1 dataPageV1) {
            readPageV1(dataPageV1);
            return null;
          }

          @Override
          public Void visit(DataPageV2 dataPageV2) {
            readPageV2(dataPageV2);
            return null;
          }
        });
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/BaseVectorizedColumnReader.java`
#### Snippet
```java
    this.pageValueCount = valueCount;
    this.endOfPageValueCount = valuesRead + pageValueCount;
    if (dataEncoding.usesDictionary()) {
      this.dataColumn = null;
      if (dictionary == null) {
        throw new IOException(
            "could not read page in col "
                + descriptor
                + " as the dictionary was missing for encoding "
                + dataEncoding);
      }
      dataColumn =
          ParquetDataColumnReaderFactory.getDataColumnReaderByType(
              type.asPrimitiveType(),
              dataEncoding.getDictionaryBasedValuesReader(
                  descriptor, VALUES, dictionary.getDictionary()),
              isUtcTimestamp);
      this.isCurrentPageDictionaryEncoded = true;
    } else {
      dataColumn =
          ParquetDataColumnReaderFactory.getDataColumnReaderByType(
              type.asPrimitiveType(),
              dataEncoding.getValuesReader(descriptor, VALUES),
              isUtcTimestamp);
      this.isCurrentPageDictionaryEncoded = false;
    }

    try {
      dataColumn.initFromPage(pageValueCount, in);
    } catch (IOException e) {
      throw new IOException("could not read page in col " + descriptor, e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/BaseVectorizedColumnReader.java`
#### Snippet
```java
    ValuesReader rlReader = page.getRlEncoding().getValuesReader(descriptor, REPETITION_LEVEL);
    ValuesReader dlReader = page.getDlEncoding().getValuesReader(descriptor, DEFINITION_LEVEL);
    this.repetitionLevelColumn = new ValuesReaderIntIterator(rlReader);
    this.definitionLevelColumn = new ValuesReaderIntIterator(dlReader);
    try {
      BytesInput bytes = page.getBytes();
      LOG.debug("page size " + bytes.size() + " bytes and " + pageValueCount + " records");
      ByteBufferInputStream in = bytes.toInputStream();
      LOG.debug("reading repetition levels at " + in.position());
      rlReader.initFromPage(pageValueCount, in);
      LOG.debug("reading definition levels at " + in.position());
      dlReader.initFromPage(pageValueCount, in);
      LOG.debug("reading data at " + in.position());
      initDataReader(page.getValueEncoding(), in, page.getValueCount());
    } catch (IOException e) {
      throw new ParquetDecodingException(
          "could not read page " + page + " in col " + descriptor, e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/BaseVectorizedColumnReader.java`
#### Snippet
```java
    this.pageValueCount = page.getValueCount();
    this.repetitionLevelColumn =
        newRLEIterator(descriptor.getMaxRepetitionLevel(), page.getRepetitionLevels());
    this.definitionLevelColumn =
        newRLEIterator(descriptor.getMaxDefinitionLevel(), page.getDefinitionLevels());
    try {
      LOG.debug(
          "page data size "
              + page.getData().size()
              + " bytes and "
              + pageValueCount
              + " records");
      initDataReader(
          page.getDataEncoding(), page.getData().toInputStream(), page.getValueCount());
    } catch (IOException e) {
      throw new ParquetDecodingException(
          "could not read page " + page + " in col " + descriptor, e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java`
#### Snippet
```java
    int bytesLen = descriptor.getPrimitiveType().getTypeLength();
    WritableBytesVector bytesVector = (WritableBytesVector) column;
    for (int i = 0; i < num; i++) {
      if (runLenDecoder.readInteger() == maxDefLevel) {
        byte[] bytes = readDataBinary(bytesLen).getBytes();
        bytesVector.appendBytes(rowId + i, bytes, 0, bytes.length);
      } else {
        bytesVector.setNullAt(rowId + i);
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java`
#### Snippet
```java
    WritableBytesVector bytesVector = (WritableBytesVector) column;
    for (int i = rowId; i < rowId + num; ++i) {
      if (!bytesVector.isNullAt(i)) {
        byte[] v = dictionary.decodeToBinary(dictionaryIds.getInt(i)).getBytes();
        bytesVector.appendBytes(i, v, 0, v.length);
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/FixedLenBytesColumnReader.java`
#### Snippet
```java
    ByteBuffer buffer = readDataBuffer(len);
    if (buffer.hasArray()) {
      return Binary.fromConstantByteArray(
          buffer.array(), buffer.arrayOffset() + buffer.position(), len);
    } else {
      byte[] bytes = new byte[len];
      buffer.get(bytes);
      return Binary.fromConstantByteArray(bytes);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/Int64TimestampColumnReader.java`
#### Snippet
```java
    for (int i = 0; i < num; i++) {
      if (runLenDecoder.readInteger() == maxDefLevel) {
        ByteBuffer buffer = readDataBuffer(8);
        column.setTimestamp(rowId + i, int64ToTimestamp(utcTimestamp, buffer.getLong(), chronoUnit));
      } else {
        column.setNullAt(rowId + i);
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/MapColumnReader.java`
#### Snippet
```java
    HeapMapColumnVector mapColumnVector = (HeapMapColumnVector) column;
    MapType mapType = (MapType) logicalType;
    // initialize 2 ListColumnVector for keys and values
    HeapArrayVector keyArrayColumnVector = new HeapArrayVector(total);
    HeapArrayVector valueArrayColumnVector = new HeapArrayVector(total);
    // read the keys and values
    keyReader.readToVector(total, keyArrayColumnVector);
    valueReader.readToVector(total, valueArrayColumnVector);

    // set the related attributes according to the keys and values
    mapColumnVector.setKeys(keyArrayColumnVector.child);
    mapColumnVector.setValues(valueArrayColumnVector.child);
    mapColumnVector.setOffsets(keyArrayColumnVector.offsets);
    mapColumnVector.setLengths(keyArrayColumnVector.lengths);
    mapColumnVector.setSize(keyArrayColumnVector.getSize());
    for (int i = 0; i < keyArrayColumnVector.getLen(); i++) {
      if (keyArrayColumnVector.isNullAt(i)) {
        mapColumnVector.setNullAt(i);
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    Type[] types = new Type[fieldNames.length];
    if (caseSensitive) {
      for (int i = 0; i < fieldNames.length; ++i) {
        String fieldName = fieldNames[i];
        types[i] = parquetSchema.containsField(fieldName) ? parquetSchema.getType(fieldName) : null;
      }
    } else {
      Map<String, Type> caseInsensitiveFieldMap = new HashMap<>();
      for (Type type : parquetSchema.getFields()) {
        caseInsensitiveFieldMap.compute(type.getName().toLowerCase(Locale.ROOT),
            (key, previousType) -> {
              if (previousType != null) {
                throw new FlinkRuntimeException(
                    "Parquet with case insensitive mode should have no duplicate key: " + key);
              }
              return type;
            });
      }
      for (int i = 0; i < fieldNames.length; ++i) {
        Type type = caseInsensitiveFieldMap.get(fieldNames[i].toLowerCase(Locale.ROOT));
        // TODO clip for array,map,row types.
        types[i] = type;
      }
    }

    return types;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    WritableColumnVector[] columns = new WritableColumnVector[requestedTypes.length];
    List<Type> types = requestedSchema.getFields();
    List<ColumnDescriptor> descriptors = requestedSchema.getColumns();
    for (int i = 0; i < requestedTypes.length; i++) {
      columns[i] = createWritableColumnVector(
          batchSize,
          requestedTypes[i],
          types.get(i),
          descriptors);
    }
    return columns;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    ColumnVector[] vectors = new ColumnVector[writableVectors.length];
    for (int i = 0; i < writableVectors.length; i++) {
      vectors[i] = requestedTypes[i].getTypeRoot() == LogicalTypeRoot.DECIMAL
          ? new ParquetDecimalVector(writableVectors[i])
          : writableVectors[i];
    }
    return vectors;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    for (int i = 0; i < requestedSchema.getFieldCount(); ++i) {
      String[] colPath = requestedSchema.getPaths().get(i);
      if (fileSchema.containsPath(colPath)) {
        ColumnDescriptor fd = fileSchema.getColumnDescription(colPath);
        if (!fd.equals(requestedSchema.getColumns().get(i))) {
          throw new UnsupportedOperationException("Schema evolution not supported.");
        }
      } else {
        if (requestedSchema.getColumns().get(i).getMaxDefinitionLevel() == 0) {
          // Column is missing in data but the required data is non-nullable. This file is invalid.
          throw new IOException("Required column is missing in data file. Col: " + Arrays.toString(colPath));
        }
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    for (WritableColumnVector v : writableVectors) {
      v.reset();
    }
    columnarBatch.setNumRows(0);
    if (rowsReturned >= totalRowCount) {
      return false;
    }
    if (rowsReturned == totalCountLoadedSoFar) {
      readNextRowGroup();
    }

    int num = (int) Math.min(batchSize, totalCountLoadedSoFar - rowsReturned);
    for (int i = 0; i < columnReaders.length; ++i) {
      //noinspection unchecked
      columnReaders[i].readToVector(num, writableVectors[i]);
    }
    rowsReturned += num;
    columnarBatch.setNumRows(num);
    rowsInBatch = num;
    return true;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    PageReadStore pages = reader.readNextRowGroup();
    if (pages == null) {
      throw new IOException("expecting more rows but reached last block. Read "
          + rowsReturned + " out of " + totalRowCount);
    }
    List<Type> types = requestedSchema.getFields();
    List<ColumnDescriptor> columns = requestedSchema.getColumns();
    columnReaders = new ColumnReader[types.size()];
    for (int i = 0; i < types.size(); ++i) {
      columnReaders[i] = createColumnReader(
          utcTimestamp,
          requestedTypes[i],
          types.get(i),
          columns,
          pages);
    }
    totalCountLoadedSoFar += pages.getRowCount();
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetColumnarRowSplitReader.java`
#### Snippet
```java
    if (totalCountLoadedSoFar != 0) {
      throw new UnsupportedOperationException("Only support seek at first.");
    }

    List<BlockMetaData> blockMetaData = reader.getRowGroups();

    for (BlockMetaData metaData : blockMetaData) {
      if (metaData.getRowCount() > rowCount) {
        break;
      } else {
        reader.skipNextRowGroup();
        rowsReturned += metaData.getRowCount();
        totalCountLoadedSoFar += metaData.getRowCount();
        rowsInBatch = (int) metaData.getRowCount();
        nextRow = (int) metaData.getRowCount();
        rowCount -= metaData.getRowCount();
      }
    }
    for (int i = 0; i < rowCount; i++) {
      boolean end = reachedEnd();
      if (end) {
        throw new RuntimeException("Seek to many rows.");
      }
      nextRecord();
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetDataColumnReaderFactory.java`
#### Snippet
```java
    long millisecond = julianDayToMillis(julianDay) + (nanosOfDay / NANOS_PER_MILLISECOND);

    if (utcTimestamp) {
      int nanoOfMillisecond = (int) (nanosOfDay % NANOS_PER_MILLISECOND);
      return TimestampData.fromEpochMillis(millisecond, nanoOfMillisecond);
    } else {
      Timestamp timestamp = new Timestamp(millisecond);
      timestamp.setNanos((int) (nanosOfDay % NANOS_PER_SECOND));
      return TimestampData.fromTimestamp(timestamp);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RowColumnReader.java`
#### Snippet
```java
    HeapRowColumnVector rowColumnVector = (HeapRowColumnVector) vector;
    WritableColumnVector[] vectors = rowColumnVector.vectors;
    // row vector null array
    boolean[] isNulls = new boolean[readNumber];
    for (int i = 0; i < vectors.length; i++) {
      fieldReaders.get(i).readToVector(readNumber, vectors[i]);

      for (int j = 0; j < readNumber; j++) {
        if (i == 0) {
          isNulls[j] = vectors[i].isNullAt(j);
        } else {
          isNulls[j] = isNulls[j] && vectors[i].isNullAt(j);
        }
        if (i == vectors.length - 1 && isNulls[j]) {
          // rowColumnVector[j] is null only when all fields[j] of rowColumnVector[j] is
          // null
          rowColumnVector.setNullAt(j);
        }
      }
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    this.in = in;
    if (fixedWidth) {
      // initialize for repetition and definition levels
      if (readLength) {
        int length = readIntLittleEndian();
        this.in = in.sliceStream(length);
      }
    } else {
      // initialize for values
      if (in.available() > 0) {
        initWidthAndPacker(in.read());
      }
    }
    if (bitWidth == 0) {
      // 0 bit width, treat this as an RLE run of valueCount number of 0's.
      this.mode = MODE.RLE;
      this.currentCount = valueCount;
      this.currentValue = 0;
    } else {
      this.currentCount = 0;
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    if (this.currentCount == 0) {
      this.readNextGroup();
    }

    this.currentCount--;
    switch (mode) {
      case RLE:
        return this.currentValue;
      case PACKED:
        return this.currentBuffer[currentBufferIdx++];
      default:
        throw new AssertionError();
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    int left = total;
    while (left > 0) {
      if (this.currentCount == 0) {
        this.readNextGroup();
      }
      int n = Math.min(left, this.currentCount);
      switch (mode) {
        case RLE:
          if (currentValue == level) {
            data.readDictionaryIdData(n, values, rowId);
          } else {
            nulls.setNulls(rowId, n);
          }
          break;
        case PACKED:
          for (int i = 0; i < n; ++i) {
            if (currentBuffer[currentBufferIdx++] == level) {
              values.setInt(rowId + i, data.readInteger());
            } else {
              nulls.setNullAt(rowId + i);
            }
          }
          break;
        default:
          throw new AssertionError();
      }
      rowId += n;
      left -= n;
      currentCount -= n;
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    int left = total;
    while (left > 0) {
      if (this.currentCount == 0) {
        this.readNextGroup();
      }
      int n = Math.min(left, this.currentCount);
      switch (mode) {
        case RLE:
          c.setInts(rowId, n, currentValue);
          break;
        case PACKED:
          c.setInts(rowId, n, currentBuffer, currentBufferIdx);
          currentBufferIdx += n;
          break;
        default:
          throw new AssertionError();
      }
      rowId += n;
      left -= n;
      currentCount -= n;
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    int value = 0;
    int shift = 0;
    int b;
    do {
      b = in.read();
      value |= (b & 0x7F) << shift;
      shift += 7;
    } while ((b & 0x80) != 0);
    return value;
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    switch (bytesWidth) {
      case 0:
        return 0;
      case 1:
        return in.read();
      case 2: {
        int ch2 = in.read();
        int ch1 = in.read();
        return (ch1 << 8) + ch2;
      }
      case 3: {
        int ch3 = in.read();
        int ch2 = in.read();
        int ch1 = in.read();
        return (ch1 << 16) + (ch2 << 8) + ch3;
      }
      case 4: {
        return readIntLittleEndian();
      }
      default:
        throw new RuntimeException("Unreachable");
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/RunLengthDecoder.java`
#### Snippet
```java
    try {
      int header = readUnsignedVarInt();
      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;
      switch (mode) {
        case RLE:
          this.currentCount = header >>> 1;
          this.currentValue = readIntLittleEndianPaddedOnBitWidth();
          return;
        case PACKED:
          int numGroups = header >>> 1;
          this.currentCount = numGroups * 8;

          if (this.currentBuffer.length < this.currentCount) {
            this.currentBuffer = new int[this.currentCount];
          }
          currentBufferIdx = 0;
          int valueIndex = 0;
          while (valueIndex < this.currentCount) {
            // values are bit packed 8 at a time, so reading bitWidth will always work
            ByteBuffer buffer = in.slice(bitWidth);
            this.packer.unpack8Values(buffer, buffer.position(), this.currentBuffer, valueIndex);
            valueIndex += 8;
          }
          return;
        default:
          throw new ParquetDecodingException("not a valid mode " + this.mode);
      }
    } catch (IOException e) {
      throw new ParquetDecodingException("Failed to read from input stream", e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/ParquetSplitReaderUtil.java`
#### Snippet
```java
switch (type.getTypeRoot()) {
      case CHAR:
      case VARCHAR:
      case BINARY:
      case VARBINARY:
        HeapBytesVector bsv = new HeapBytesVector(batchSize);
        if (value == null) {
          bsv.fillWithNulls();
        } else {
          bsv.fill(value instanceof byte[]
              ? (byte[]) value
              : value.toString().getBytes(StandardCharsets.UTF_8));
        }
        return bsv;
      case BOOLEAN:
        HeapBooleanVector bv = new HeapBooleanVector(batchSize);
        if (value == null) {
          bv.fillWithNulls();
        } else {
          bv.fill((boolean) value);
        }
        return bv;
      case TINYINT:
        HeapByteVector byteVector = new HeapByteVector(batchSize);
        if (value == null) {
          byteVector.fillWithNulls();
        } else {
          byteVector.fill(((Number) value).byteValue());
        }
        return byteVector;
      case SMALLINT:
        HeapShortVector sv = new HeapShortVector(batchSize);
        if (value == null) {
          sv.fillWithNulls();
        } else {
          sv.fill(((Number) value).shortValue());
        }
        return sv;
      case INTEGER:
        HeapIntVector iv = new HeapIntVector(batchSize);
        if (value == null) {
          iv.fillWithNulls();
        } else {
          iv.fill(((Number) value).intValue());
        }
        return iv;
      case BIGINT:
        HeapLongVector lv = new HeapLongVector(batchSize);
        if (value == null) {
          lv.fillWithNulls();
        } else {
          lv.fill(((Number) value).longValue());
        }
        return lv;
      case DECIMAL:
        DecimalType decimalType = (DecimalType) type;
        int precision = decimalType.getPrecision();
        int scale = decimalType.getScale();
        DecimalData decimal = value == null
            ? null
            : Preconditions.checkNotNull(DecimalData.fromBigDecimal((BigDecimal) value, precision, scale));
        ColumnVector internalVector = createVectorFromConstant(
            new VarBinaryType(),
            decimal == null ? null : decimal.toUnscaledBytes(),
            batchSize);
        return new ParquetDecimalVector(internalVector);
      case FLOAT:
        HeapFloatVector fv = new HeapFloatVector(batchSize);
        if (value == null) {
          fv.fillWithNulls();
        } else {
          fv.fill(((Number) value).floatValue());
        }
        return fv;
      case DOUBLE:
        HeapDoubleVector dv = new HeapDoubleVector(batchSize);
        if (value == null) {
          dv.fillWithNulls();
        } else {
          dv.fill(((Number) value).doubleValue());
        }
        return dv;
      case DATE:
        if (value instanceof LocalDate) {
          value = Date.valueOf((LocalDate) value);
        }
        return createVectorFromConstant(
            new IntType(),
            value == null ? null : toInternal((Date) value),
            batchSize);
      case TIMESTAMP_WITHOUT_TIME_ZONE:
        HeapTimestampVector tv = new HeapTimestampVector(batchSize);
        if (value == null) {
          tv.fillWithNulls();
        } else {
          tv.fill(TimestampData.fromLocalDateTime((LocalDateTime) value));
        }
        return tv;
      default:
        throw new UnsupportedOperationException("Unsupported type: " + type);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
    Job job = Job.getInstance(jsc.hadoopConfiguration());
    // Allow recursive directories to be found
    job.getConfiguration().set(FileInputFormat.INPUT_DIR_RECURSIVE, "true");
    // To parallelize reading file status.
    job.getConfiguration().set(FileInputFormat.LIST_STATUS_NUM_THREADS, "1024");
    AvroReadSupport.setAvroReadSchema(jsc.hadoopConfiguration(), (new Schema.Parser().parse(schemaStr)));
    ParquetInputFormat.setReadSupportClass(job, (AvroReadSupport.class));

    HoodieEngineContext context = new HoodieSparkEngineContext(jsc);
```

### DuplicatedCode
Duplicated code
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
          String partitionPath = partitionField.toString();
          LOG.debug("Row Key : " + rowField + ", Partition Path is (" + partitionPath + ")");
          if (partitionField instanceof Number) {
            try {
              long ts = (long) (Double.parseDouble(partitionField.toString()) * 1000L);
              partitionPath = PARTITION_FORMATTER.format(Instant.ofEpochMilli(ts));
            } catch (NumberFormatException nfe) {
              LOG.warn("Unable to parse date from partition field. Assuming partition as (" + partitionField + ")");
            }
          }
          return new HoodieAvroRecord<>(new HoodieKey(rowField.toString(), partitionPath),
              new HoodieJsonPayload(genericRecord.toString()));
```

### DuplicatedCode
Duplicated code
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
    HoodieCompactionConfig compactionConfig = compactionStrategyClass
        .map(strategy -> HoodieCompactionConfig.newBuilder().withInlineCompaction(false)
            .withCompactionStrategy(ReflectionUtils.loadClass(strategy)).build())
        .orElse(HoodieCompactionConfig.newBuilder().withInlineCompaction(false).build());
    HoodieWriteConfig config =
        HoodieWriteConfig.newBuilder().withPath(basePath)
            .withParallelism(parallelism, parallelism)
            .withBulkInsertParallelism(parallelism)
            .withDeleteParallelism(parallelism)
            .withSchema(schemaStr).combineInput(true, true).withCompactionConfig(compactionConfig)
            .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())
            .withProps(properties).build();
    return new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), config);
```

### DuplicatedCode
Duplicated code
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
    Path p = new Path(schemaFile);
    if (!fs.exists(p)) {
      throw new Exception(String.format("Could not find - %s - schema file.", schemaFile));
    }
    long len = fs.getFileStatus(p).getLen();
    ByteBuffer buf = ByteBuffer.allocate((int) len);
    try (FSDataInputStream inputStream = fs.open(p)) {
      inputStream.readFully(0, buf.array(), 0, buf.array().length);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
    LongAccumulator errors = jsc.sc().longAccumulator();
    writeResponse.foreach(writeStatus -> {
      if (writeStatus.hasErrors()) {
        errors.add(1);
        LOG.error(String.format("Error processing records :writeStatus:%s", writeStatus.getStat().toString()));
      }
    });
    if (errors.value() == 0) {
      LOG.info(String.format("Table imported into hoodie with %s instant time.", instantTime));
      return 0;
    }
    LOG.error(String.format("Import failed with %d errors.", errors.value()));
    return -1;
```

### DuplicatedCode
Duplicated code
in `hudi-sync/hudi-adb-sync/src/main/java/org/apache/hudi/sync/adb/HoodieAdbJdbcClient.java`
#### Snippet
```java
      if (!storagePartitionValues.isEmpty()) {
        String storageValue = String.join(", ", storagePartitionValues);
        if (!paths.containsKey(storageValue)) {
          events.add(PartitionEvent.newPartitionAddEvent(storagePartition));
        } else if (!paths.get(storageValue).equals(fullStoragePartitionPath)) {
          events.add(PartitionEvent.newPartitionUpdateEvent(storagePartition));
        }
      }
```

### DuplicatedCode
Duplicated code
in `hudi-sync/hudi-adb-sync/src/main/java/org/apache/hudi/sync/adb/HoodieAdbJdbcClient.java`
#### Snippet
```java
    try {
      if (stmt != null) {
        stmt.close();
      }
    } catch (SQLException e) {
      LOG.warn("Could not close the statement opened ", e);
    }

    try {
      if (resultSet != null) {
        resultSet.close();
      }
    } catch (SQLException e) {
      LOG.warn("Could not close the resultset opened ", e);
    }
```

### DuplicatedCode
Duplicated code
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/SlashEncodedDayPartitionValueExtractor.java`
#### Snippet
```java
    int year = Integer.parseInt(splits[0].contains("=") ? splits[0].split("=")[1] : splits[0]);
    int mm = Integer.parseInt(splits[1].contains("=") ? splits[1].split("=")[1] : splits[1]);
    int dd = Integer.parseInt(splits[2].contains("=") ? splits[2].split("=")[1] : splits[2]);
```

### DuplicatedCode
Duplicated code
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/HMSDDLExecutor.java`
#### Snippet
```java
      Map<String, String> partitionKeysMap =
          table.getPartitionKeys().stream().collect(Collectors.toMap(FieldSchema::getName, f -> f.getType().toUpperCase()));

      Map<String, String> columnsMap =
          table.getSd().getCols().stream().collect(Collectors.toMap(FieldSchema::getName, f -> f.getType().toUpperCase()));

      Map<String, String> schema = new HashMap<>();
      schema.putAll(columnsMap);
      schema.putAll(partitionKeysMap);
      final long end = System.currentTimeMillis();
      LOG.info(String.format("Time taken to getTableSchema: %s ms", (end - start)));
      return schema;
```

### DuplicatedCode
Duplicated code
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/HMSDDLExecutor.java`
#### Snippet
```java
      for (String dropPartition : partitionsToDrop) {
        if (HivePartitionUtil.partitionExists(client, tableName, dropPartition, partitionValueExtractor, syncConfig)) {
          String partitionClause =
              HivePartitionUtil.getPartitionClauseForDrop(dropPartition, partitionValueExtractor, syncConfig);
          client.dropPartition(databaseName, tableName, partitionClause, false);
        }
        LOG.info("Drop partition " + dropPartition + " on " + tableName);
      }
```

### DuplicatedCode
Duplicated code
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java`
#### Snippet
```java
          LOG.info("Running Mode: [" + SCHEDULE + "]; Do schedule");
          Option<String> instantTime = doSchedule(jsc);
          int result = instantTime.isPresent() ? 0 : -1;
          if (result == 0) {
            LOG.info("The schedule instant time is " + instantTime.get());
          }
          return result;
```

### DuplicatedCode
Duplicated code
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
    sparkConf.set("spark.ui.port", "8090");
    sparkConf.setIfMissing("spark.driver.maxResultSize", "2g");
    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
    sparkConf.set("spark.kryo.registrator", "org.apache.spark.HoodieSparkKryoRegistrar");
    sparkConf.set("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension");
    sparkConf.set("spark.hadoop.mapred.output.compress", "true");
    sparkConf.set("spark.hadoop.mapred.output.compression.codec", "true");
    sparkConf.set("spark.hadoop.mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
    sparkConf.set("spark.hadoop.mapred.output.compression.type", "BLOCK");
```

### DuplicatedCode
Duplicated code
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/postprocessor/DeleteSupportSchemaPostProcessor.java`
#### Snippet
```java
    List<Schema.Field> sourceFields = schema.getFields();
    List<Schema.Field> targetFields = new ArrayList<>(sourceFields.size() + 1);
    // copy existing columns
    for (Schema.Field sourceField : sourceFields) {
      targetFields.add(new Schema.Field(sourceField.name(), sourceField.schema(), sourceField.doc(), sourceField.defaultVal()));
    }
```

### DuplicatedCode
Duplicated code
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java
    Option<String> beginInstant =
        lastCkptStr.isPresent() ? lastCkptStr.get().isEmpty() ? Option.empty() : lastCkptStr : Option.empty();

    Pair<String, Pair<String, String>> queryTypeAndInstantEndpts = IncrSourceHelper.calculateBeginAndEndInstants(sparkContext, srcPath,
        numInstantsPerFetch, beginInstant, missingCheckpointStrategy);

    if (queryTypeAndInstantEndpts.getValue().getKey().equals(queryTypeAndInstantEndpts.getValue().getValue())) {
      LOG.warn("Already caught up. Begin Checkpoint was :" + queryTypeAndInstantEndpts.getValue().getKey());
      return Pair.of(Option.empty(), queryTypeAndInstantEndpts.getValue().getKey());
    }

    Dataset<Row> source = null;
```

### DuplicatedCode
Duplicated code
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DFSPathSelector.java`
#### Snippet
```java
      long currentBytes = 0;
      long newCheckpointTime = lastCheckpointTime;
      List<FileStatus> filteredFiles = new ArrayList<>();
      for (FileStatus f : eligibleFiles) {
        if (currentBytes + f.getLen() >= sourceLimit && f.getModificationTime() > newCheckpointTime) {
          // we have enough data, we are done
          // Also, we've read up to a file with a newer modification time
          // so that some files with the same modification time won't be skipped in next read
          break;
        }

        newCheckpointTime = f.getModificationTime();
        currentBytes += f.getLen();
        filteredFiles.add(f);
      }

      // no data to read
      if (filteredFiles.isEmpty()) {
        return new ImmutablePair<>(Option.empty(), String.valueOf(newCheckpointTime));
      }

      // read the files out.
      String pathStr = filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(","));

      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(newCheckpointTime));
```

## RuleId[id=MismatchedJavadocCode]
### MismatchedJavadocCode
Method is specified to return map but the return type is list
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   *
   * @param datasetMetaClient data set meta client instance.
   * @return Map of partition names to a list of FileStatus for all the files in the partition
   */
  private List<DirectoryInfo> listAllPartitions(HoodieTableMetaClient datasetMetaClient) {
```

## RuleId[id=IOStreamConstructor]
### IOStreamConstructor
'OutputStream' can be constructed using 'Files.newOutputStream()'
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/TableCommand.java`
#### Snippet
```java
    OutputStream os = null;
    try {
      os = new FileOutputStream(outFile);
      os.write(data.getBytes(), 0, data.length());
    } finally {
```

### IOStreamConstructor
'InputStream' can be constructed using 'Files.newInputStream()'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/replication/HiveSyncGlobalCommitParams.java`
#### Snippet
```java
    }
    finalize = true;
    try (InputStream configStream = new FileInputStream(configFile)) {
      loadedProps.loadFromXML(configStream);
    }
```

### IOStreamConstructor
'OutputStream' can be constructed using 'Files.newOutputStream()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java`
#### Snippet
```java

  public static void writeStringToFile(String str, String filePath) throws IOException {
    PrintStream out = new PrintStream(new FileOutputStream(filePath));
    out.println(str);
    out.flush();
```

## RuleId[id=ProtectedMemberInFinalClass]
### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private Long timestamp;

    protected ValueMetadata(String filePath, int sizeOfValue, long offsetOfValue, long timestamp) {
      this.filePath = filePath;
      this.sizeOfValue = sizeOfValue;
```

## RuleId[id=CollectionAddAllCanBeReplacedWithConstructor]
### CollectionAddAllCanBeReplacedWithConstructor
'addAll()' call can be replaced with parametrized constructor call
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RestoresCommand.java`
#### Snippet
```java
  private List<HoodieInstant> getRestoreInstants(HoodieActiveTimeline activeTimeline, boolean includeInFlight) {
    List<HoodieInstant> restores = new ArrayList<>();
    restores.addAll(activeTimeline.getRestoreTimeline().filterCompletedInstants().getInstants());

    if (includeInFlight) {
```

### CollectionAddAllCanBeReplacedWithConstructor
'addAll()' call can be replaced with parametrized constructor call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
    // In this scenario, we will assume that once replaced a file group automatically becomes eligible for cleaning completely
    // In other words, the file versions only apply to the active file groups.
    deletePaths.addAll(getReplacedFilesEligibleToClean(savepointedFiles, partitionPath, Option.empty()));
    boolean toDeletePartition = false;
    List<HoodieFileGroup> fileGroups = fileSystemView.getAllFileGroups(partitionPath).collect(Collectors.toList());
```

## RuleId[id=UnnecessaryToStringCall]
### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
    } catch (FileNotFoundException e) {
      // Metadata directory does not exist
      throw new RuntimeException("Metadata directory (" + metadataPath.toString() + ") does not exist.");
    }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
      FileStatus[] statuses = HoodieCLI.fs.listStatus(metadataPath);
      if (statuses.length > 0) {
        throw new RuntimeException("Metadata directory (" + metadataPath.toString() + ") not empty.");
      }
    } catch (FileNotFoundException e) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java`
#### Snippet
```java

  private static String getTmpSerializerFile() {
    return TMP_DIR + UUID.randomUUID().toString() + ".ser";
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
      } else {
        throw new UnsupportedOperationException(
            "Unsupported SqlTimeTypeInfo " + typeInfo.toString());
      }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                        String.format(
                            "List field [%s] in List [%s] has to be required. ",
                            type.toString(), fieldType.getName()));
                  }
                }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/parquet/ParquetSchemaConverter.java`
#### Snippet
```java
                            "Unrecgonized List schema [%s] according to Parquet"
                                + " standard",
                            parquetGroupType.toString()));
                  }
                }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bucket/HoodieSparkConsistentBucketIndex.java`
#### Snippet
```java

      LOG.error("Consistent hashing node has no file group, partition: " + partitionPath + ", meta: "
              + partitionToIdentifier.get(partitionPath).getMetadata().getFilename() + ", record_key: " + key.toString());
      throw new HoodieIndexException("Failed to getBucket as hashing node has no file group");
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/DirectWriteMarkers.java`
#### Snippet
```java
      throw new HoodieException("Failed to create marker file " + markerPath, e);
    }
    LOG.info("[direct] Created marker file " + markerPath.toString()
        + " in " + timer.endTimer() + " ms");
    return Option.of(markerPath);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
          ALL_MARKERS_URL, paramsMap, new TypeReference<Set<String>>() {}, RequestMethod.GET);
    } catch (IOException e) {
      throw new HoodieRemoteException("Failed to get all markers in " + markerDirPath.toString(), e);
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
    } catch (IOException e) {
      throw new HoodieRemoteException("Failed to get CREATE and MERGE data file paths in "
          + markerDirPath.toString(), e);
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
          DELETE_MARKER_DIR_URL, paramsMap, new TypeReference<Boolean>() {}, RequestMethod.POST);
    } catch (IOException e) {
      throw new HoodieRemoteException("Failed to delete marker directory " + markerDirPath.toString(), e);
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
          MARKERS_DIR_EXISTS_URL, paramsMap, new TypeReference<Boolean>() {}, RequestMethod.GET);
    } catch (IOException e) {
      throw new HoodieRemoteException("Failed to check marker directory " + markerDirPath.toString(), e);
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGenUtils.java`
#### Snippet
```java
    if (keyIsNullEmpty) {
      throw new HoodieKeyException("recordKey values: \"" + recordKey + "\" for fields: "
          + recordKeyFields.toString() + " cannot be entirely null or empty.");
    }
    return recordKey.toString();
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload.java`
#### Snippet
```java
    JsonNode node = new ObjectMapper().readTree(getJsonData());
    if (!node.has(field)) {
      throw new HoodieException("Field :" + field + " not found in payload => " + node.toString());
    }
    return node.get(field).textValue();
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
          return Pair.of(true, new GenericData.EnumSymbol(schema, value.toString()));
        }
        throw new HoodieJsonToAvroConversionException(String.format("Symbol %s not in enum", value.toString()),
            schema.getFullName(), schema, shouldSanitize, invalidCharMask);
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
    // When the file is first written, we do not have a track of it
    throw new IllegalArgumentException(
        file.toString() + " does not have a open stream. Cannot get the bytes written on the stream");
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              type.getCategory().getName(),
              value.getClass().getName(),
              value.toString()
          ));
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              "Unrecognized type for Avro DATE field value, which has type %s, value %s",
              value.getClass().getName(),
              value.toString()
          ));
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              "Unrecognized type for Avro TIMESTAMP field value, which has type %s, value %s",
              value.getClass().getName(),
              value.toString()
          ));
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              "Unrecognized type for Avro BINARY field value, which has type %s, value %s",
              value.getClass().getName(),
              value.toString()
          ));
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
              "Failed to add value %s to union with type %s",
              value == null ? "null" : value.toString(),
              type.toString()
          ));
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        break;
      default:
        throw new IllegalArgumentException("Invalid TypeDescription " + type.toString() + ".");
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        return readFromVector(type.getChildren().get(tag), fieldVector, avroSchema.getTypes().get(tag), vectorPos);
      default:
        throw new HoodieIOException("Unrecognized TypeDescription " + type.toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
      bufferedWriter.write(markerType.toString());
    } catch (IOException e) {
      throw new HoodieException("Failed to create marker type file " + markerTypeFilePath.toString()
          + "; " + e.getMessage(), e);
    } finally {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
      content = Option.of(MarkerType.valueOf(FileIOUtils.readAsUTFString(fsDataInputStream)));
    } catch (IOException e) {
      throw new HoodieIOException("Cannot read marker type file " + markerTypeFilePath.toString()
          + "; " + e.getMessage(), e);
    } finally {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
      fileSystem.delete(markerTypeFilePath, false);
    } catch (IOException e) {
      throw new HoodieIOException("Cannot delete marker type file " + markerTypeFilePath.toString()
          + "; " + e.getMessage(), e);
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java`
#### Snippet
```java
      } catch (Exception e1) {
        // swallow this exception.
        LOG.warn(String.format("Cannot find internal schema from commit file %s. Falling back to parsing historical internal schema", candidateCommitFile.toString()));
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/DiskMap.java`
#### Snippet
```java
  public DiskMap(String basePath, String prefix) throws IOException {
    this.diskMapPath =
        String.format("%s/%s-%s-%s", basePath, SUBFOLDER_PREFIX, prefix, UUID.randomUUID().toString());
    diskMapPathFile = new File(diskMapPath);
    FileIOUtils.deleteDirectory(diskMapPathFile);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBDAO.java`
#### Snippet
```java
  public RocksDBDAO(String basePath, String rocksDBBasePath) {
    this.rocksDBBasePath =
        String.format("%s/%s/%s", rocksDBBasePath, URI.create(basePath).getPath().replace(":","").replace("/", "_"), UUID.randomUUID().toString());
    init();
    totalBytesWritten = 0L;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java`
#### Snippet
```java
  public HoodieInstant transitionRestoreRequestedToInflight(HoodieInstant requestedInstant) {
    ValidationUtils.checkArgument(requestedInstant.getAction().equals(HoodieTimeline.RESTORE_ACTION), "Transition to inflight requested for a restore instant with diff action "
        + requestedInstant.toString());
    ValidationUtils.checkArgument(requestedInstant.isRequested(), "Transition to inflight requested for an instant not in requested state " + requestedInstant.toString());
    HoodieInstant inflight = new HoodieInstant(State.INFLIGHT, RESTORE_ACTION, requestedInstant.getTimestamp());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java`
#### Snippet
```java
    ValidationUtils.checkArgument(requestedInstant.getAction().equals(HoodieTimeline.RESTORE_ACTION), "Transition to inflight requested for a restore instant with diff action "
        + requestedInstant.toString());
    ValidationUtils.checkArgument(requestedInstant.isRequested(), "Transition to inflight requested for an instant not in requested state " + requestedInstant.toString());
    HoodieInstant inflight = new HoodieInstant(State.INFLIGHT, RESTORE_ACTION, requestedInstant.getTimestamp());
    transitionState(requestedInstant, inflight, Option.empty());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java`
#### Snippet
```java
          newFields.add(f);
        } else {
          throw new HoodieSchemaException(String.format("cannot find pruned id %s in currentSchema %s", id, schema.toString()));
        }
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java`
#### Snippet
```java
                  instant.getTimestamp())).get(), HoodieRollbackPlan.class);
          commitsToRollback = Collections.singletonList(rollbackPlan.getInstantToRollback().getCommitTime());
          LOG.warn("Had to fetch rollback info from requested instant since completed file is empty " + instant.toString());
        }
        return commitsToRollback;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java`
#### Snippet
```java
            hoodiePathCache.put(folder.toString(), new HashSet<>());
          }
          LOG.info("Based on hoodie metadata from base path: " + baseDir.toString() + ", caching " + latestFiles.size()
              + " files under " + folder);
          for (HoodieBaseFile lfile : latestFiles) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java`
#### Snippet
```java
          // Non-hoodie path, accept it.
          if (LOG.isDebugEnabled()) {
            LOG.debug(String.format("(1) Caching non-hoodie path under %s with basePath %s \n", folder.toString(), baseDir.toString()));
          }
          nonHoodiePathCache.add(folder.toString());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java`
#### Snippet
```java
          // Non-hoodie path, accept it.
          if (LOG.isDebugEnabled()) {
            LOG.debug(String.format("(1) Caching non-hoodie path under %s with basePath %s \n", folder.toString(), baseDir.toString()));
          }
          nonHoodiePathCache.add(folder.toString());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java`
#### Snippet
```java
        // files is at < 3 level depth in FS tree, can't be hoodie dataset
        if (LOG.isDebugEnabled()) {
          LOG.debug(String.format("(2) Caching non-hoodie path under %s \n", folder.toString()));
        }
        nonHoodiePathCache.add(folder.toString());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
      tool.run();
    } catch (Throwable throwable) {
      LOG.error("Fail to run deleting table partitions for " + cfg.toString(), throwable);
    } finally {
      jsc.stop();
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
        + hiveSyncConfig.getStringOrDefault(HiveSyncConfigHolder.HIVE_URL)
        + ", basePath :" + cfg.basePath);
    LOG.info("Hive Sync Conf => " + hiveSyncConfig.toString());
    FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());
    HiveConf hiveConf = new HiveConf();
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/DebeziumSource.java`
#### Snippet
```java
          }).map(Field::name).collect(Collectors.toList());

      LOG.info("Date fields: " + dateFields.toString());

      for (String dateCol : dateFields) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/kafka/KafkaConnectControlAgent.java`
#### Snippet
```java
    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
    // Todo fetch the worker id or name instead of a uuid.
    props.put(ConsumerConfig.GROUP_ID_CONFIG, "hudi-control-group" + UUID.randomUUID().toString());
    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/DataSourceUtils.java`
#### Snippet
```java
        return client.insertOverwriteTable(hoodieRecords, instantTime);
      default:
        throw new HoodieException("Not a valid operation type for doWriteOperation: " + operation.toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerDirState.java`
#### Snippet
```java
      closeQuietly(fsDataOutputStream);
    }
    LOG.debug(markersFilePath.toString() + " written in " + timer.endTimer() + " ms");
  }
}
```

## RuleId[id=InnerClassMayBeStatic]
### InnerClassMayBeStatic
Inner class `Heartbeat` may be 'static'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  }

  class Heartbeat {

    private String instantTime;
```

### InnerClassMayBeStatic
Inner class `IteratorWrapper` may be 'static'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
   * in-memory map 2) diskLazyFileIterator - Iterates over all the data spilled to disk.
   */
  private class IteratorWrapper<R> implements Iterator<R> {

    private final Iterator<R> inMemoryIterator;
```

## RuleId[id=SuspiciousMethodCalls]
### SuspiciousMethodCalls
'Map' may not contain keys of type ''
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java`
#### Snippet
```java
    // Return PartitionCleanStat for each partition passed.
    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {
      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)
          ? partitionCleanStatsMap.get(partitionPath)
          : new PartitionCleanStat(partitionPath);
```

### SuspiciousMethodCalls
'Map' may not contain keys of type ''
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java`
#### Snippet
```java
    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {
      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)
          ? partitionCleanStatsMap.get(partitionPath)
          : new PartitionCleanStat(partitionPath);
      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();
```

### SuspiciousMethodCalls
'List' may not contain objects of type ''
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java`
#### Snippet
```java
          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())
          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())
          .isPartitionDeleted(partitionsToBeDeleted.contains(partitionPath))
          .build();
    }).collect(Collectors.toList());
```

### SuspiciousMethodCalls
'Set\>' may not contain objects of type 'Entry'
in `hudi-common/src/main/java/org/apache/hudi/common/util/MapUtils.java`
#### Snippet
```java

  public static boolean containsAll(Map<?, ?> m1, Map<?, ?> m2) {
    return m1.entrySet().containsAll(m2.entrySet());
  }
}
```

### SuspiciousMethodCalls
Suspicious call to 'Map.containsKey()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
  @Override
  public R get(Object key) {
    if (inMemoryMap.containsKey(key)) {
      return inMemoryMap.get(key);
    } else if (getDiskBasedMap().containsKey(key)) {
```

### SuspiciousMethodCalls
Suspicious call to 'Map.containsKey()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
    if (inMemoryMap.containsKey(key)) {
      return inMemoryMap.get(key);
    } else if (getDiskBasedMap().containsKey(key)) {
      return getDiskBasedMap().get(key);
    }
```

### SuspiciousMethodCalls
Suspicious call to 'Map.containsKey()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java

  public boolean inDiskContainsKey(Object key) {
    return getDiskBasedMap().containsKey(key);
  }

```

### SuspiciousMethodCalls
Suspicious call to 'Map.containsKey()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java

  public boolean inMemoryContainsKey(Object key) {
    return inMemoryMap.containsKey(key);
  }

```

### SuspiciousMethodCalls
Suspicious call to 'Map.containsKey()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
  public R remove(Object key) {
    // NOTE : getDiskBasedMap().remove does not delete the data from disk
    if (inMemoryMap.containsKey(key)) {
      currentInMemoryMapSize -= estimatedPayloadSize;
      return inMemoryMap.remove(key);
```

### SuspiciousMethodCalls
Suspicious call to 'Map.containsKey()'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/ExternalSpillableMap.java`
#### Snippet
```java
      currentInMemoryMapSize -= estimatedPayloadSize;
      return inMemoryMap.remove(key);
    } else if (getDiskBasedMap().containsKey(key)) {
      return getDiskBasedMap().remove(key);
    }
```

## RuleId[id=DanglingJavadoc]
### DanglingJavadoc
Dangling Javadoc comment
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bucket/HoodieSparkConsistentBucketIndex.java`
#### Snippet
```java
      ConsistentHashingNode node = partitionToIdentifier.get(partitionPath).getBucket(key, indexKeyFields);
      if (!StringUtils.isNullOrEmpty(node.getFileIdPrefix())) {
        /**
         * Dynamic Bucket Index doesn't need the instant time of the latest file group.
         * We add suffix 0 here to the file uuid, following the naming convention, i.e., fileId = [uuid]_[numWrites]
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkConsistentBucketClusteringPlanStrategy.java`
#### Snippet
```java
          int nextIdx = forward ? (rangeIdx[k] + 1 < fileSlices.size() ? rangeIdx[k] + 1 : 0) : (rangeIdx[k] >= 1 ? rangeIdx[k] - 1 : fileSlices.size() - 1);
          boolean isNeighbour = identifier.getBucketByFileId(fileSlices.get(nextIdx).getFileId()) == identifier.getFormerBucket(fileSlices.get(rangeIdx[k]).getFileId());
          /**
           * Merge condition:
           * 1. there is still slot to merge bucket
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java`
#### Snippet
```java

@SuppressWarnings("Duplicates")
/**
 * Handle to merge incoming records to those in storage.
 * <p>
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java`
#### Snippet
```java
          + "This controls the parallelism for deduplication during indexing where more than 1 record could be tagged due to partition update.");

  /**
   * ***** Bucket Index Configs *****
   * Bucket Index is targeted to locate the record fast by hash in big data scenarios.
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java
    }

    /**
     * ASCII 01-1F are HTTP control characters that need to be escaped.
     * \u000A and \u000D are \n and \r, respectively.
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-common/src/main/java/org/apache/hudi/common/util/HoodieRecordSizeEstimator.java`
#### Snippet
```java
    // all records in the JVM. Calculate and print the size of the Schema and of the Record to
    // note the sizes and differences. A correct estimation in such cases is handled in
    /** {@link ExternalSpillableMap} **/
    long sizeOfRecord = ObjectSizeCalculator.getObjectSize(hoodieRecord);
    LOG.debug("SizeOfRecord => " + sizeOfRecord + " SizeOfSchema => " + sizeOfSchema);
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieWriteStat.java`
#### Snippet
```java
  private String tempPath;

  /**
   * Following properties are associated only with the result of a Compaction Operation
   */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java`
#### Snippet
```java

  @Override
  /**
   * Note : In lazy mode, clients must ensure close() should be called only after processing all log-blocks as the
   * underlying inputstream will be closed. TODO: We can introduce invalidate() API at HoodieLogBlock and this object
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
          readerSchema, readBlocksLazily, reverseReader, bufferSize, shouldLookupRecords(), recordKeyField, internalSchema);

      /**
       * Scanning log blocks and placing the compacted blocks at the right place require two traversals.
       * First traversal to identify the rollback blocks and valid data and compacted blocks.
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java

    List<HoodieFileGroup> fileGroups = fetchAllStoredFileGroups(partition).collect(Collectors.toList());
    /**
     * Note that while finding the new data/log files added/removed, the path stored in metadata will be missing the
     * base-path,scheme and authority. Ensure the matching process takes care of this discrepancy.
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java`
#### Snippet
```java
  public static Schema generateProjectionSchema(Schema writeSchema, Map<String, Schema.Field> schemaFieldsMap,
                                                List<String> fieldNames) {
    /**
     * Avro & Presto field names seems to be case sensitive (support fields differing only in case) whereas
     * Hive/Impala/SparkSQL(default) are case-insensitive. Spark allows this to be configurable using
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/PubsubMessagesFetcher.java`
#### Snippet
```java

    try {
      /** For details of timeout and retry configs,
       * see {@link com.google.cloud.pubsub.v1.stub.SubscriberStubSettings#initDefaults()},
       * and the static code block in SubscriberStubSettings */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    fs.delete(new Path(tablePath), true);

    /**
     * Commit with only inserts
     */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    LOG.info("First commit at instant time :" + commitInstantTime1);

    /**
     * Commit that updates records
     */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    LOG.info("Second commit at instant time :" + commitInstantTime2);

    /**
     * Commit that Deletes some records
     */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    LOG.info("Third commit at instant time :" + commitInstantTime3);

    /**
     * Read & do some queries
     */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

    if (tableType.equals(HoodieTableType.COPY_ON_WRITE.name())) {
      /**
       * Consume incrementally, only changes in commit 2 above. Currently only supported for COPY_ON_WRITE TABLE
       */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
    }

    /**
     * Read & do some queries
     */
```

### DanglingJavadoc
Dangling Javadoc comment
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

    if (tableType.equals(HoodieTableType.COPY_ON_WRITE.name())) {
      /**
       * Consume incrementally, only changes in commit 2 above. Currently only supported for COPY_ON_WRITE TABLE
       */
```

## RuleId[id=SwitchStatementWithConfusingDeclaration]
### SwitchStatementWithConfusingDeclaration
Local variable `propsFilePath` declared in one 'switch' branch and used in another
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
        case UPSERT:
          assert (args.length >= 13);
          String propsFilePath = null;
          if (!StringUtils.isNullOrEmpty(args[12])) {
            propsFilePath = args[12];
```

### SwitchStatementWithConfusingDeclaration
Local variable `configs` declared in one 'switch' branch and used in another
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
            propsFilePath = args[12];
          }
          List<String> configs = new ArrayList<>();
          if (args.length > 13) {
            configs.addAll(Arrays.asList(args).subList(13, args.length));
```

## RuleId[id=TrivialIf]
### TrivialIf
`if` statement can be simplified
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  public static Boolean heartbeatExists(FileSystem fs, String basePath, String instantTime) throws IOException {
    Path heartbeatFilePath = new Path(HoodieTableMetaClient.getHeartbeatFolderPath(basePath) + Path.SEPARATOR + instantTime);
    if (fs.exists(heartbeatFilePath)) {
      return true;
    }
```

### TrivialIf
`if` statement can be simplified
in `hudi-common/src/main/java/org/apache/hudi/common/util/HoodieRecordUtils.java`
#### Snippet
```java

  public static boolean recordTypeCompatibleEngine(HoodieRecordType recordType, EngineType engineType) {
    if (engineType == EngineType.SPARK && recordType == HoodieRecordType.SPARK) {
      return true;
    } else {
```

### TrivialIf
`if` statement can be simplified
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/InternalSchemaUtils.java`
#### Snippet
```java
          if (fieldIds.contains(f.fieldId())) {
            newTypes.add(f.type());
          } else if (newType != null) {
            newTypes.add(newType);
          } else {
```

## RuleId[id=SynchronizeOnNonFinalField]
### SynchronizeOnNonFinalField
Synchronization on a non-final field `checkpointLock`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/StreamReadMonitoringFunction.java`
#### Snippet
```java
    checkpointLock = context.getCheckpointLock();
    while (isRunning) {
      synchronized (checkpointLock) {
        monitorDirAndForwardSplits(context);
      }
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `checkpointLock`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/StreamReadMonitoringFunction.java`
#### Snippet
```java
    if (checkpointLock != null) {
      // this is to cover the case where cancel() is called before the run()
      synchronized (checkpointLock) {
        isRunning = false;
      }
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `checkpointLock`
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/StreamReadMonitoringFunction.java`
#### Snippet
```java

    if (checkpointLock != null) {
      synchronized (checkpointLock) {
        issuedInstant = null;
        isRunning = false;
```

## RuleId[id=NonStrictComparisonCanBeEquality]
### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java`
#### Snippet
```java
    checkArgument(batchSize > 0, "batch size must be positive.");
    int total = list.size();
    if (total <= 0) {
      return Stream.empty();
    }
```

## RuleId[id=OptionalUsedAsFieldOrParameterType]
### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'v'
in `hudi-common/src/main/java/org/apache/hudi/common/util/Option.java`
#### Snippet
```java
   * @return Option
   */
  public static <T> Option<T> fromJavaOptional(Optional<T> v) {
    return Option.ofNullable(v.orElse(null));
  }
```

## RuleId[id=CharsetObjectCanBeUsed]
### CharsetObjectCanBeUsed
StandardCharsets.UTF_8 can be used instead
in `hudi-common/src/main/java/org/apache/hudi/common/util/BinaryUtil.java`
#### Snippet
```java

  public static byte[] utf8To8Byte(String a) {
    return paddingTo8Byte(a.getBytes(Charset.forName("utf-8")));
  }

```

## RuleId[id=ImplicitArrayToString]
### ImplicitArrayToString
Implicit call to 'toString()' on array returned by call to `record.value()`
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/kafka/KafkaConnectControlAgent.java`
#### Snippet
```java
          try {
            LOG.debug(String.format("Kafka consumerGroupId = %s topic = %s, partition = %s, offset = %s, customer = %s, country = %s",
                "", record.topic(), record.partition(), record.offset(), record.key(), record.value()));
            ControlMessage message = ControlMessage.parseFrom(record.value());
            String senderTopic = message.getTopicName();
```

## RuleId[id=AutoCloseableResource]
### AutoCloseableResource
'SparkSession' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java
  public void close() {
    if (sqlContext != null) {
      sqlContext.sparkSession().stop();
    }
  }
```

### AutoCloseableResource
'HoodieBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
    initJavaSparkContext(Option.of(master));
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metadata = new HoodieBackedTableMetadata(new HoodieSparkEngineContext(jsc), config,
        HoodieCLI.basePath, "/tmp");

```

### AutoCloseableResource
'HoodieTableMetadataWriter' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
      HoodieWriteConfig writeConfig = getWriteConfig();
      initJavaSparkContext(Option.of(master));
      SparkHoodieBackedTableMetadataWriter.create(HoodieCLI.conf, writeConfig, new HoodieSparkEngineContext(jsc));
    }

```

### AutoCloseableResource
'HoodieBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
    HoodieCLI.getTableMetaClient();
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metaReader = new HoodieBackedTableMetadata(
        new HoodieLocalEngineContext(HoodieCLI.conf), config, HoodieCLI.basePath, "/tmp");

```

### AutoCloseableResource
'HoodieBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
    HoodieCLI.getTableMetaClient();
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metadataReader = new HoodieBackedTableMetadata(
        new HoodieLocalEngineContext(HoodieCLI.conf), config, HoodieCLI.basePath, "/tmp");

```

### AutoCloseableResource
'HoodieBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java

    HoodieMetadataConfig fsConfig = HoodieMetadataConfig.newBuilder().enable(false).build();
    HoodieBackedTableMetadata fsMetaReader = new HoodieBackedTableMetadata(
        new HoodieLocalEngineContext(HoodieCLI.conf), fsConfig, HoodieCLI.basePath, "/tmp");

```

### AutoCloseableResource
'HoodieBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
    HoodieCLI.getTableMetaClient();
    HoodieMetadataConfig config = HoodieMetadataConfig.newBuilder().enable(true).build();
    HoodieBackedTableMetadata metadata = new HoodieBackedTableMetadata(new HoodieLocalEngineContext(HoodieCLI.conf),
        config, HoodieCLI.basePath, "/tmp");
    Map<String, String> stats = metadata.stats();
```

### AutoCloseableResource
'HoodieTableMetadataWriter' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/MetadataCommand.java`
#### Snippet
```java
    HoodieWriteConfig writeConfig = getWriteConfig();
    initJavaSparkContext(Option.of(master));
    SparkHoodieBackedTableMetadataWriter.create(HoodieCLI.conf, writeConfig, new HoodieSparkEngineContext(jsc));
    return String.format("Created Metadata Table in %s (duration=%.2f secs)", metadataPath, timer.endTimer() / 1000.0);
  }
```

### AutoCloseableResource
'IndexReader' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/BootstrapCommand.java`
#### Snippet
```java
  public String showBootstrapIndexPartitions() {

    BootstrapIndex.IndexReader indexReader = createBootstrapIndexReader();
    List<String> indexedPartitions = indexReader.getIndexedPartitionPaths();

```

### AutoCloseableResource
'IndexReader' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/BootstrapCommand.java`
#### Snippet
```java
    }

    BootstrapIndex.IndexReader indexReader = createBootstrapIndexReader();
    List<String> indexedPartitions = indexReader.getIndexedPartitionPaths();

```

### AutoCloseableResource
'BigQuerySyncTool' used without 'try'-with-resources statement
in `hudi-gcp/src/main/java/org/apache/hudi/gcp/bigquery/BigQuerySyncTool.java`
#### Snippet
```java
      System.exit(0);
    }
    new BigQuerySyncTool(params.toProps()).syncHoodieTable();
  }
}
```

### AutoCloseableResource
'HoodieMergedLogRecordScanner' used without 'try'-with-resources statement
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java`
#### Snippet
```java
              .withRecordMerger(HoodieRecordUtils.loadRecordMerger(HoodieAvroRecordMerger.class.getName()))
              .withOptimizedLogBlocksScan(Boolean.parseBoolean(HoodieCompactionConfig.ENABLE_OPTIMIZED_LOG_BLOCKS_SCAN.defaultValue()))
              .build();
      for (HoodieRecord hoodieRecord : scanner) {
        Option<HoodieAvroIndexedRecord> record = hoodieRecord.toIndexedRecord(readerSchema, new Properties());
```

### AutoCloseableResource
'AdbSyncTool' used without 'try'-with-resources statement
in `hudi-sync/hudi-adb-sync/src/main/java/org/apache/hudi/sync/adb/AdbSyncTool.java`
#### Snippet
```java
      System.exit(0);
    }
    new AdbSyncTool(params.toProps()).syncHoodieTable();
  }
}
```

### AutoCloseableResource
'HiveSyncTool' used without 'try'-with-resources statement
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java`
#### Snippet
```java
      System.exit(0);
    }
    new HiveSyncTool(params.toProps(), new Configuration()).syncHoodieTable();
  }
}
```

### AutoCloseableResource
'DataHubSyncTool' used without 'try'-with-resources statement
in `hudi-sync/hudi-datahub-sync/src/main/java/org/apache/hudi/sync/datahub/DataHubSyncTool.java`
#### Snippet
```java
      System.exit(0);
    }
    new DataHubSyncTool(params.toProps()).syncHoodieTable();
  }
}
```

### AutoCloseableResource
'SparkSession' used without 'try'-with-resources statement
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/sort/SpaceCurveSortingHelper.java`
#### Snippet
```java
    StructType newStructType = composeOrderedRDDStructType(df.schema());

    return df.sparkSession()
        .createDataFrame(sortedRDD, newStructType)
        .drop("Index");
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/OrcBootstrapMetadataHandler.java`
#### Snippet
```java
  @Override
  Schema getAvroSchema(Path sourceFilePath) throws IOException {
    Reader orcReader = OrcFile.createReader(sourceFilePath, OrcFile.readerOptions(table.getHadoopConf()));
    TypeDescription orcSchema = orcReader.getSchema();
    return AvroOrcUtils.createAvroSchema(orcSchema);
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/bootstrap/HoodieSparkBootstrapSchemaProvider.java`
#### Snippet
```java
    Reader orcReader = null;
    try {
      orcReader = OrcFile.createReader(filePath, OrcFile.readerOptions(context.getHadoopConf().get()));
    } catch (IOException e) {
      throw new HoodieException("Could not determine schema from the ORC data files.");
```

### AutoCloseableResource
'HoodieTableMetadataWriter' used without 'try'-with-resources statement
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDTableServiceClient.java`
#### Snippet
```java
  protected void initializeMetadataTable(Option<String> inFlightInstantTimestamp) {
    if (config.isMetadataTableEnabled()) {
      SparkHoodieBackedTableMetadataWriter.create(context.getHadoopConf().get(), config,
          context, Option.empty(), inFlightInstantTimestamp);
    }
```

### AutoCloseableResource
'SparkSession' used without 'try'-with-resources statement
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BucketBulkInsertPartitionerWithRows.java`
#### Snippet
```java
        .partitionBy(partitioner)
        .values();
    return rows.sparkSession().createDataFrame(rddRows, rows.schema());
  }

```

### AutoCloseableResource
'HoodieTableMetadataWriter' used without 'try'-with-resources statement
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/ScheduleIndexActionExecutor.java`
#### Snippet
```java
        // in case FILES partition itself was not initialized before (i.e. metadata was never enabled), this will initialize synchronously
        HoodieTableMetadataWriter metadataWriter = table.getMetadataWriter(instantTime)
            .orElseThrow(() -> new HoodieIndexException(String.format("Could not get metadata writer to initialize filegroups for indexing for instant: %s", instantTime)));
        if (!finalPartitionsToIndex.get(0).getPartitionPath().equals(MetadataPartitionType.FILES.getPartitionPath())) {
          // initialize metadata partition only if not for FILES partition.
```

### AutoCloseableResource
'Writer' used without 'try'-with-resources statement
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java`
#### Snippet
```java
    Path planPath = new Path(metaClient.getArchivePath(), HoodieArchivedTimeline.MERGE_ARCHIVE_PLAN_NAME);
    // Flush remained content if existed and open a new write
    reOpenWriter();
    // List all archive files
    FileStatus[] fsStatuses = metaClient.getFs().globStatus(
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroOrcReader.java`
#### Snippet
```java

    try {
      Reader reader = OrcFile.createReader(path, OrcFile.readerOptions(conf));
      TypeDescription orcSchema = AvroOrcUtils.createOrcSchema(readerSchema);
      RecordReader recordReader = reader.rows(new Options(conf).schema(orcSchema));
```

### AutoCloseableResource
'Stream' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java`
#### Snippet
```java
  public static void deleteDirectory(File directory) throws IOException {
    if (directory.exists()) {
      Files.walk(directory.toPath()).sorted(Comparator.reverseOrder()).map(Path::toFile).forEach(File::delete);
      directory.delete();
      if (directory.exists()) {
```

### AutoCloseableResource
'SizeAwareFSDataOutputStream' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

    SizeAwareFSDataOutputStream os = new SizeAwareFSDataOutputStream(path, fsDataOutputStream, consistencyGuard,
        () -> openStreams.remove(path.getName()));
    openStreams.put(path.getName(), os);
    return os;
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/util/OrcUtils.java`
#### Snippet
```java
      Configuration conf = new Configuration(configuration);
      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());
      Reader reader = OrcFile.createReader(filePath, OrcFile.readerOptions(conf));

      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();
```

### AutoCloseableResource
'ColumnFamilyHandle' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBDAO.java`
#### Snippet
```java
        throw new HoodieException(e);
      }
      managedHandlesMap.remove(columnFamilyName);
      return null;
    });
```

### AutoCloseableResource
'HoodieAvroHFileReader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
    FileSystem fs = metaClient.getRawFs();
    CacheConfig cacheConfig = new CacheConfig(fs.getConf());
    HoodieAvroHFileReader hFileReader = new HoodieAvroHFileReader(fs.getConf(), hFilePath, cacheConfig);
    return convertAvroSchemaToParquet(hFileReader.getSchema());
  }
```

### AutoCloseableResource
'HoodieAvroOrcReader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java

    FileSystem fs = metaClient.getRawFs();
    HoodieAvroOrcReader orcReader = new HoodieAvroOrcReader(fs.getConf(), orcFilePath);
    return convertAvroSchemaToParquet(orcReader.getSchema());
  }
```

### AutoCloseableResource
'DataFileWriter' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/TimelineMetadataUtils.java`
#### Snippet
```java
      throws IOException {
    DatumWriter<T> datumWriter = new SpecificDatumWriter<>(clazz);
    DataFileWriter<T> fileWriter = new DataFileWriter<>(datumWriter);
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    fileWriter.create(metadata.getSchema(), baos);
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
    private HoodieBootstrapIndexInfo fetchBootstrapIndexInfo() throws IOException {
      return TimelineMetadataUtils.deserializeAvroMetadata(
          partitionIndexReader().getHFileInfo().get(INDEX_INFO_KEY),
          HoodieBootstrapIndexInfo.class);
    }
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
    @Override
    public List<String> getIndexedPartitionPaths() {
      try (HFileScanner scanner = partitionIndexReader().getScanner(true, false)) {
        return getAllKeys(scanner, HFileBootstrapIndex::getPartitionFromKey);
      }
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
    @Override
    public List<HoodieFileGroupId> getIndexedFileGroupIds() {
      try (HFileScanner scanner = fileIdIndexReader().getScanner(true, false)) {
        return getAllKeys(scanner, HFileBootstrapIndex::getFileGroupFromKey);
      }
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
      Collections.sort(fileGroupIds);
      try {
        HFileScanner scanner = fileIdIndexReader().getScanner(true, false);
        for (HoodieFileGroupId fileGroupId : fileGroupIds) {
          KeyValue keyValue = new KeyValue(Bytes.toBytes(getFileGroupKey(fileGroupId)), new byte[0], new byte[0],
```

### AutoCloseableResource
'Reader' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
    public List<BootstrapFileMapping> getSourceFileMappingForPartition(String partition) {
      try {
        HFileScanner scanner = partitionIndexReader().getScanner(true, false);
        KeyValue keyValue = new KeyValue(Bytes.toBytes(getPartitionKey(partition)), new byte[0], new byte[0],
            HConstants.LATEST_TIMESTAMP, KeyValue.Type.Put, new byte[0]);
```

### AutoCloseableResource
'FileSystemBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java`
#### Snippet
```java

    FileSystemBackedTableMetadata fileSystemBackedTableMetadata =
        createFileSystemBackedTableMetadata();
    return fileSystemBackedTableMetadata.getAllFilesInPartitions(partitions);
  }
```

### AutoCloseableResource
'FileSystemBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java`
#### Snippet
```java

    FileSystemBackedTableMetadata fileSystemBackedTableMetadata =
        createFileSystemBackedTableMetadata();
    return fileSystemBackedTableMetadata.getAllFilesInPartition(partitionPath);
  }
```

### AutoCloseableResource
'FileSystemBackedTableMetadata' used without 'try'-with-resources statement
in `hudi-common/src/main/java/org/apache/hudi/metadata/BaseTableMetadata.java`
#### Snippet
```java

    FileSystemBackedTableMetadata fileSystemBackedTableMetadata =
        createFileSystemBackedTableMetadata();
    return fileSystemBackedTableMetadata.getAllPartitionPaths();
  }
```

### AutoCloseableResource
'HiveHoodieTableFileIndex' used without 'try'-with-resources statement
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieCopyOnWriteTableInputFormat.java`
#### Snippet
```java

      HiveHoodieTableFileIndex fileIndex =
          new HiveHoodieTableFileIndex(
              engineContext,
              tableMetaClient,
```

### AutoCloseableResource
'HoodieTableMetadata' used without 'try'-with-resources statement
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
        .build();
    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);
    HoodieTableMetadata tableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, basePath,
        FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue());
    SerializableConfiguration serializableConfiguration = new SerializableConfiguration(jsc.hadoopConfiguration());
```

### AutoCloseableResource
'Javalin' used without 'try'-with-resources statement
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java`
#### Snippet
```java

  public void startService() {
    Javalin app = Javalin.create().start(cfg.serverPort);
    app.get("/", ctx -> ctx.result("Hello World"));
  }
```

### AutoCloseableResource
'AdminClient' used without 'try'-with-resources statement
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java`
#### Snippet
```java
    props.put("bootstrap.servers", bootstrapServers);
    try {
      AdminClient client = AdminClient.create(props);
      DescribeTopicsResult result = client.describeTopics(Arrays.asList(topicName));
      Map<String, KafkaFuture<TopicDescription>> values = result.values();
```

### AutoCloseableResource
'Stream' used without 'try'-with-resources statement
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java`
#### Snippet
```java
      return new ArrayList<>();
    }
    return Files.walk(basePath, FileVisitOption.FOLLOW_LINKS)
            .filter(path -> path.toFile().isFile())
            .filter(path -> path.toString().endsWith(".xml"))
```

### AutoCloseableResource
'HoodieMergedLogRecordScanner' used without 'try'-with-resources statement
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
    final AvroToRowDataConverters.AvroToRowDataConverter avroToRowDataConverter =
        AvroToRowDataConverters.createRowConverter(tableState.getRequiredRowType());
    final HoodieMergedLogRecordScanner scanner = FormatUtils.logScanner(split, tableSchema, internalSchemaManager.getQuerySchema(), conf, hadoopConf);
    final Iterator<String> logRecordsKeyIterator = scanner.getRecords().keySet().iterator();
    final int[] pkOffset = tableState.getPkOffsetsInRequired();
```

### AutoCloseableResource
'HoodieMergedLogRecordScanner' used without 'try'-with-resources statement
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/mor/MergeOnReadInputFormat.java`
#### Snippet
```java
    final AvroToRowDataConverters.AvroToRowDataConverter avroToRowDataConverter =
        AvroToRowDataConverters.createRowConverter(tableState.getRowType());
    final HoodieMergedLogRecordScanner scanner = FormatUtils.logScanner(split, tableSchema, InternalSchema.getEmptyInternalSchema(), conf, hadoopConf);
    final Iterator<String> logRecordsKeyIterator = scanner.getRecords().keySet().iterator();

```

### AutoCloseableResource
'HoodieTableMetadata' used without 'try'-with-resources statement
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/source/stats/ColumnStatsIndices.java`
#### Snippet
```java
    //    - Fetching the records from CSI by key-prefixes (encoded column names)
    //    - Deserializing fetched records into [[RowData]]s
    HoodieTableMetadata metadataTable = HoodieTableMetadata.create(
        HoodieFlinkEngineContext.DEFAULT,
        metadataConfig, basePath,
```

### AutoCloseableResource
'JavaSparkContext' used without 'try'-with-resources statement
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
    HoodieTestDataGenerator dataGen = getDataGenerate();

    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());

    // Generate some input..
```

### AutoCloseableResource
'JavaSparkContext' used without 'try'-with-resources statement
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
        .getOrCreate();

    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());
    spark.sparkContext().setLogLevel("WARN");
    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());
```

### AutoCloseableResource
'HoodieTestDataGenerator' used without 'try'-with-resources statement
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
    if (nonPartitionedTable) {
      // All data goes to base-path
      dataGen = new HoodieTestDataGenerator(new String[]{""});
    } else {
      dataGen = new HoodieTestDataGenerator();
```

### AutoCloseableResource
'HoodieTestDataGenerator' used without 'try'-with-resources statement
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java
      dataGen = new HoodieTestDataGenerator(new String[]{""});
    } else {
      dataGen = new HoodieTestDataGenerator();
    }

```

## RuleId[id=ConditionCoveredByFurtherCondition]
### ConditionCoveredByFurtherCondition
Condition 'schemaProvider == null' covered by subsequent condition '!(schemaProvider instanceof SchemaRegistryProvider)'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/debezium/DebeziumSource.java`
#### Snippet
```java

    // Currently, debezium source requires Confluent/Kafka schema-registry to fetch the latest schema.
    if (schemaProvider == null || !(schemaProvider instanceof SchemaRegistryProvider)) {
      schemaRegistryProvider = new SchemaRegistryProvider(props, sparkContext);
    } else {
```

## RuleId[id=SuspiciousToArrayCall]
### SuspiciousToArrayCall
Array of type 'java.lang.Object\[\]' expected, 'java.lang.Comparable\[\]' found
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/ArchivedCommitsCommand.java`
#### Snippet
```java
      commitDetails.add(Option.ofNullable(record.get(metadataName)).orElse("{}").toString());
    }
    return commitDetails.toArray(new Comparable[commitDetails.size()]);
  }

```

### SuspiciousToArrayCall
Array of type 'org.apache.hadoop.mapred.SplitLocationInfo\[\]' expected, 'java.lang.String\[\]' found
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
    try {
      String[] hosts = split.getLocationInfo() != null ? Arrays.stream(split.getLocationInfo())
          .filter(x -> !x.isInMemory()).toArray(String[]::new) : new String[0];
      String[] inMemoryHosts = split.getLocationInfo() != null ? Arrays.stream(split.getLocationInfo())
          .filter(SplitLocationInfo::isInMemory).toArray(String[]::new) : new String[0];
```

### SuspiciousToArrayCall
Array of type 'org.apache.hadoop.mapred.SplitLocationInfo\[\]' expected, 'java.lang.String\[\]' found
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieMergeOnReadTableInputFormat.java`
#### Snippet
```java
          .filter(x -> !x.isInMemory()).toArray(String[]::new) : new String[0];
      String[] inMemoryHosts = split.getLocationInfo() != null ? Arrays.stream(split.getLocationInfo())
          .filter(SplitLocationInfo::isInMemory).toArray(String[]::new) : new String[0];
      FileSplit baseSplit = new FileSplit(split.getPath(), split.getStart(), split.getLength(),
          hosts, inMemoryHosts);
```

### SuspiciousToArrayCall
Array of type 'org.apache.hadoop.mapred.InputSplit\[\]' expected, 'org.apache.hudi.hadoop.hive.HoodieCombineHiveInputFormat.CombineHiveInputSplit\[\]' found
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java

    LOG.info("number of splits " + result.size());
    return result.toArray(new CombineHiveInputSplit[result.size()]);
  }

```

## RuleId[id=RedundantCast]
### RedundantCast
Casting `args` to `Object[]` is redundant
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieSyncException.java`
#### Snippet
```java

  protected static String format(String message, Object... args) {
    return String.format(String.valueOf(message), (Object[]) args);
  }
}
```

### RedundantCast
Casting `context` to `HoodieJavaEngineContext` is redundant
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/HoodieJavaTable.java`
#### Snippet
```java
            .setLoadActiveTimelineOnLoad(true).setConsistencyGuardConfig(config.getConsistencyGuardConfig())
            .setLayoutVersion(Option.of(new TimelineLayoutVersion(config.getTimelineLayoutVersion()))).build();
    return HoodieJavaTable.create(config, (HoodieJavaEngineContext) context, metaClient);
  }

```

### RedundantCast
Casting `context` to `HoodieJavaEngineContext` is redundant
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java`
#### Snippet
```java
  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {
    // Create a Hoodie table which encapsulated the commits and files visible
    HoodieJavaTable<T> table = HoodieJavaTable.create(config, (HoodieJavaEngineContext) context);
    Timer.Context indexTimer = metrics.getIndexCtx();
    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(HoodieListData.eager(hoodieRecords), context, table).collectAsList();
```

### RedundantCast
Casting `Math.floorMod(...)` to `int` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java`
#### Snippet
```java
    final long hashOfKey = NumericUtils.getMessageDigestHash("MD5", parts.getRight());
    final List<Integer> candidatePartitions = fileGroupToPartitions.get(parts.getLeft());
    final int idx = (int) Math.floorMod((int) hashOfKey, candidatePartitions.size());
    assert idx >= 0;
    return candidatePartitions.get(idx);
```

### RedundantCast
Casting `row.getFloat(...)` to `double` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/sort/SpaceCurveSortingHelper.java`
#### Snippet
```java
      return row.isNullAt(index) ? Long.MAX_VALUE : (long) row.getInt(index);
    } else if (dataType instanceof FloatType) {
      return row.isNullAt(index) ? Long.MAX_VALUE : Double.doubleToLongBits((double) row.getFloat(index));
    } else if (dataType instanceof StringType) {
      return row.isNullAt(index) ? Long.MAX_VALUE : BinaryUtil.convertStringToLong(row.getString(index));
```

### RedundantCast
Casting `(maxBlockSize / averageRecordSize)` to `long` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java

    // Append if max number of records reached to achieve block size
    if (numberOfRecords >= (long) (maxBlockSize / averageRecordSize)) {
      // Recompute averageRecordSize before writing a new block and update existing value with
      // avg of new and old
```

### RedundantCast
Casting `mapper.readValue(...)` to `T` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/TimelineServerBasedWriteMarkers.java`
#### Snippet
```java
    }
    String content = response.returnContent().asString();
    return (T) mapper.readValue(content, reference);
  }

```

### RedundantCast
Casting `oldValue` to `byte[]` is redundant
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
        }
        if (oldSchema.getType() == Schema.Type.BYTES) {
          return String.valueOf(((byte[]) oldValue));
        }
        if (oldSchema.getLogicalType() == LogicalTypes.date()) {
```

### RedundantCast
Casting `value` to `GenericData.EnumSymbol` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
          bytes = utf8.getBytes();
        } else if (value instanceof GenericData.EnumSymbol) {
          bytes = ((GenericData.EnumSymbol) value).toString().getBytes(StandardCharsets.UTF_8);
        } else {
          throw new IllegalStateException(String.format(
```

### RedundantCast
Casting `metadata.getBlocks().stream().sequential().flatMap(...).collect(...)` to `Map`>> is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java`
#### Snippet
```java
    // Collect stats from all individual Parquet blocks
    Map<String, List<HoodieColumnRangeMetadata<Comparable>>> columnToStatsListMap =
        (Map<String, List<HoodieColumnRangeMetadata<Comparable>>>) metadata.getBlocks().stream().sequential()
          .flatMap(blockMetaData ->
              blockMetaData.getColumns().stream()
```

### RedundantCast
Casting `get(...)` to `R` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
  public Stream<R> valueStream() {
    final BufferedRandomAccessFile file = getRandomAccessFile();
    return valueMetadataMap.values().stream().sorted().sequential().map(valueMetaData -> (R) get(valueMetaData, file, isCompressionEnabled));
  }

```

### RedundantCast
Casting `HoodieAvroUtils.bytesToAvro(...)` to `IndexedRecord` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/model/OverwriteWithLatestAvroPayload.java`
#### Snippet
```java
    }

    return Option.of((IndexedRecord) HoodieAvroUtils.bytesToAvro(recordBytes, schema));
  }

```

### RedundantCast
Casting `HoodieAvroUtils.bytesToAvro(...)` to `IndexedRecord` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
    }

    return Option.of((IndexedRecord) HoodieAvroUtils.bytesToAvro(recordBytes, schema));
  }

```

### RedundantCast
Casting `mapper.readValue(...)` to `T` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java`
#### Snippet
```java
    Response response = retryHelper != null ? retryHelper.start(() -> get(timeoutMs, url, method)) : get(timeoutMs, url, method);
    String content = response.returnContent().asString(Consts.UTF_8);
    return (T) mapper.readValue(content, reference);
  }

```

### RedundantCast
Casting `p` to `HoodieRealtimeFileSplit` is redundant
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineRealtimeFileSplit.java`
#### Snippet
```java
  public HoodieCombineRealtimeFileSplit(JobConf jobConf, List<FileSplit> realtimeFileSplits) {
    super(jobConf, realtimeFileSplits.stream().map(p ->
            ((HoodieRealtimeFileSplit) p).getPath()).collect(Collectors.toList()).toArray(new
            Path[realtimeFileSplits.size()]),
        ArrayUtils.toPrimitive(realtimeFileSplits.stream().map(p -> ((HoodieRealtimeFileSplit) p).getStart())
```

### RedundantCast
Casting `p` to `HoodieRealtimeFileSplit` is redundant
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineRealtimeFileSplit.java`
#### Snippet
```java
            ((HoodieRealtimeFileSplit) p).getPath()).collect(Collectors.toList()).toArray(new
            Path[realtimeFileSplits.size()]),
        ArrayUtils.toPrimitive(realtimeFileSplits.stream().map(p -> ((HoodieRealtimeFileSplit) p).getStart())
            .collect(Collectors.toList()).toArray(new Long[realtimeFileSplits.size()])),
        ArrayUtils.toPrimitive(realtimeFileSplits.stream().map(p -> ((HoodieRealtimeFileSplit) p).getLength())
```

### RedundantCast
Casting `p` to `HoodieRealtimeFileSplit` is redundant
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineRealtimeFileSplit.java`
#### Snippet
```java
        ArrayUtils.toPrimitive(realtimeFileSplits.stream().map(p -> ((HoodieRealtimeFileSplit) p).getStart())
            .collect(Collectors.toList()).toArray(new Long[realtimeFileSplits.size()])),
        ArrayUtils.toPrimitive(realtimeFileSplits.stream().map(p -> ((HoodieRealtimeFileSplit) p).getLength())
            .collect(Collectors.toList()).toArray(new Long[realtimeFileSplits.size()])),
        realtimeFileSplits.stream().map(p -> {
```

### RedundantCast
Casting `Stream.of(...).filter(...).min(...).orElse(...)` to `Comparable` is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java

    Comparable minValue =
        (Comparable) Stream.of(
            (Comparable) unwrapStatisticValueWrapper(prevColumnStats.getMinValue()),
            (Comparable) unwrapStatisticValueWrapper(newColumnStats.getMinValue()))
```

### RedundantCast
Casting `unwrapStatisticValueWrapper(...)` to `Comparable` is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    Comparable minValue =
        (Comparable) Stream.of(
            (Comparable) unwrapStatisticValueWrapper(prevColumnStats.getMinValue()),
            (Comparable) unwrapStatisticValueWrapper(newColumnStats.getMinValue()))
        .filter(Objects::nonNull)
```

### RedundantCast
Casting `Stream.of(...).filter(...).max(...).orElse(...)` to `Comparable` is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java

    Comparable maxValue =
        (Comparable) Stream.of(
            (Comparable) unwrapStatisticValueWrapper(prevColumnStats.getMaxValue()),
            (Comparable) unwrapStatisticValueWrapper(newColumnStats.getMaxValue()))
```

### RedundantCast
Casting `unwrapStatisticValueWrapper(...)` to `Comparable` is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java`
#### Snippet
```java
    Comparable maxValue =
        (Comparable) Stream.of(
            (Comparable) unwrapStatisticValueWrapper(prevColumnStats.getMaxValue()),
            (Comparable) unwrapStatisticValueWrapper(newColumnStats.getMaxValue()))
        .filter(Objects::nonNull)
```

### RedundantCast
Casting `targetFields.stream().map(...).collect(...)` to `Map`> is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
        Collectors.toMap(colRangeMetadata -> colRangeMetadata.getColumnName(), Function.identity());

    return (Map<String, HoodieColumnRangeMetadata<Comparable>>) targetFields.stream()
        .map(field -> {
          ColumnStats colStats = allColumnStats.get(field.name());
```

### RedundantCast
Casting `entry` to `Tuple2` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java`
#### Snippet
```java
        // To reduce large number of tasks.
        .coalesce(16 * cfg.parallelism).map(entry -> {
          GenericRecord genericRecord = ((Tuple2<Void, GenericRecord>) entry)._2();
          Object partitionField = genericRecord.get(cfg.partitionKey);
          if (partitionField == null) {
```

### RedundantCast
Casting `entry` to `Tuple2` is redundant
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/HDFSParquetImporterUtils.java`
#### Snippet
```java
        // To reduce large number of tasks.
        .coalesce(16 * this.parallelism).map(entry -> {
          GenericRecord genericRecord = ((Tuple2<Void, GenericRecord>) entry)._2();
          Object partitionField = genericRecord.get(this.partitionKey);
          if (partitionField == null) {
```

## RuleId[id=JavadocDeclaration]
### JavadocDeclaration
Wrong tag `Experimental`
in `hudi-aws/src/main/java/org/apache/hudi/aws/sync/AwsGlueCatalogSyncTool.java`
#### Snippet
```java
 * the necessary functionality using Glue APIs.
 *
 * @Experimental
 */
public class AwsGlueCatalogSyncTool extends HiveSyncTool {
```

### JavadocDeclaration
Wrong tag `Experimental`
in `hudi-aws/src/main/java/org/apache/hudi/aws/sync/AWSGlueCatalogSyncClient.java`
#### Snippet
```java
 * AWS Glue Data Catalog (https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html).
 *
 * @Experimental
 */
public class AWSGlueCatalogSyncClient extends HoodieSyncClient {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-cli/src/main/java/org/apache/hudi/cli/Table.java`
#### Snippet
```java
   * 
   * @param rows Rows to be added
   * @return
   */
  public Table addAllRows(List<Comparable[]> rows) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-cli/src/main/java/org/apache/hudi/cli/Table.java`
#### Snippet
```java
   * 
   * @param rows Rows to be added
   * @return
   */
  public Table addAll(List<List<Comparable>> rows) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-cli/src/main/java/org/apache/hudi/cli/Table.java`
#### Snippet
```java
   * Sorting of rows by a specified field.
   * 
   * @return
   */
  private List<List<Comparable>> orderRows() {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java`
#### Snippet
```java
   * @param includeInflight   Include inflight instants
   * @param excludeCompaction Exclude Compaction instants
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/FileSystemViewCommand.java`
#### Snippet
```java
   * @param excludeCompaction Exclude Compaction instants
   * @return
   * @throws IOException
   */
  private HoodieTableFileSystemView buildFileSystemView(String globRegex, String maxInstant, boolean basefileOnly,
```

### JavadocDeclaration
Wrong tag `Experimental`
in `hudi-gcp/src/main/java/org/apache/hudi/gcp/bigquery/BigQuerySyncTool.java`
#### Snippet
```java
 * This utility will get the schema from the latest commit and will sync big query table schema.
 *
 * @Experimental
 */
public class BigQuerySyncTool extends HoodieSyncTool {
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * Adds partition to table.
   *
   * @param tableName
   * @param partitionsToAdd
   */
```

### JavadocDeclaration
`@param partitionsToAdd` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   *
   * @param tableName
   * @param partitionsToAdd
   */
  void addPartitionsToTable(String tableName, List<String> partitionsToAdd);
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * Fetches tableSchema for a table.
   *
   * @param tableName
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   *
   * @param tableName
   * @return
   */
  Map<String, String> getTableSchema(String tableName);
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * Updates partitions for a given table.
   *
   * @param tableName
   * @param changedPartitions
   */
```

### JavadocDeclaration
`@param changedPartitions` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   *
   * @param tableName
   * @param changedPartitions
   */
  void updatePartitionsToTable(String tableName, List<String> changedPartitions);
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * Updates the table with the newSchema.
   *
   * @param tableName
   * @param newSchema
   */
```

### JavadocDeclaration
`@param newSchema` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   *
   * @param tableName
   * @param newSchema
   */
  void updateTableDefinition(String tableName, MessageType newSchema);
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * update table comments
   *
   * @param tableName
   * @param newSchema Map key: field name, Map value: [field type, field comment]
   */
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * Drop partitions for a given table.
   *
   * @param tableName
   * @param partitionsToDrop
   */
```

### JavadocDeclaration
`@param partitionsToDrop` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   *
   * @param tableName
   * @param partitionsToDrop
   */
  void dropPartitionsToTable(String tableName, List<String> partitionsToDrop);
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * Creates a table with the following properties.
   *
   * @param tableName
   * @param storageSchema
   * @param inputFormatClass
```

### JavadocDeclaration
`@param storageSchema` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   *
   * @param tableName
   * @param storageSchema
   * @param inputFormatClass
   * @param outputFormatClass
```

### JavadocDeclaration
`@param inputFormatClass` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * @param tableName
   * @param storageSchema
   * @param inputFormatClass
   * @param outputFormatClass
   * @param serdeClass
```

### JavadocDeclaration
`@param outputFormatClass` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * @param storageSchema
   * @param inputFormatClass
   * @param outputFormatClass
   * @param serdeClass
   * @param serdeProperties
```

### JavadocDeclaration
`@param serdeClass` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * @param inputFormatClass
   * @param outputFormatClass
   * @param serdeClass
   * @param serdeProperties
   * @param tableProperties
```

### JavadocDeclaration
`@param serdeProperties` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * @param outputFormatClass
   * @param serdeClass
   * @param serdeProperties
   * @param tableProperties
   */
```

### JavadocDeclaration
`@param tableProperties` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/ddl/DDLExecutor.java`
#### Snippet
```java
   * @param serdeClass
   * @param serdeProperties
   * @param tableProperties
   */
  void createTable(String tableName, MessageType storageSchema, String inputFormatClass,
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieMetaSyncOperations.java`
#### Snippet
```java
  /**
   * Get the metadata of partitions that belong to the specified table
   * @param tableName
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieMetaSyncOperations.java`
#### Snippet
```java
   * Get the metadata of partitions that belong to the specified table
   * @param tableName
   * @return
   */
  default List<Partition> getPartitionsByFilter(String tableName, String filter) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/HoodieMetaSyncOperations.java`
#### Snippet
```java
   * Update the field comments for table in metastore, by using the ones from storage.
   *
   * @return
   */
  default boolean updateTableComments(String tableName, List<FieldSchema> fromMetastore, List<FieldSchema> fromStorage) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
   * @param toVersion version to which upgrade/downgrade to be done.
   * @return 0 if success, else -1.
   * @throws Exception
   */
  protected static int upgradeOrDowngradeTable(JavaSparkContext jsc, String basePath, String toVersion) {
```

### JavadocDeclaration
Exception is not declared to be thrown by method upgradeOrDowngradeTable
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java`
#### Snippet
```java
   * @param toVersion version to which upgrade/downgrade to be done.
   * @return 0 if success, else -1.
   * @throws Exception
   */
  protected static int upgradeOrDowngradeTable(JavaSparkContext jsc, String basePath, String toVersion) {
```

### JavadocDeclaration
`@param keyValueConfig` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/ConfigUtils.java`
#### Snippet
```java
   * is a key-value pair just like "k1=v1\nk2=v2\nk3=v3".
   *
   * @param keyValueConfig
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/ConfigUtils.java`
#### Snippet
```java
   *
   * @param keyValueConfig
   * @return
   */
  public static Map<String, String> toMap(String keyValueConfig) {
```

### JavadocDeclaration
`@param config` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/ConfigUtils.java`
#### Snippet
```java
   * is a key-value pair just like "k1=v1\nk2=v2\nk3=v3".
   *
   * @param config
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/util/ConfigUtils.java`
#### Snippet
```java
   *
   * @param config
   * @return
   */
  public static String configToString(Map<String, String> config) {
```

### JavadocDeclaration
`@param syncConfig` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java
  /**
   * @param schema Intermediate schema in the form of Map<String,String>
   * @param syncConfig
   * @return List of FieldSchema objects derived from schema without the partition fields as the HMS api expects them as different arguments for alter table commands.
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java
   * @param syncConfig
   * @return List of FieldSchema objects derived from schema without the partition fields as the HMS api expects them as different arguments for alter table commands.
   * @throws IOException
   */
  public static List<FieldSchema> convertMapSchemaToHiveFieldSchema(LinkedHashMap<String, String> schema, HiveSyncConfig syncConfig) throws IOException {
```

### JavadocDeclaration
`@param supportTimestamp` tag description is missing
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/HiveSchemaUtil.java`
#### Snippet
```java
   *
   * @param messageType : parquet Schema
   * @param supportTimestamp
   * @param doFormat : This option controls whether schema will have spaces in the value part of the schema map. This is required because spaces in complex schema trips the HMS create table calls.
   *                 This value will be false for HMS but true for QueryBasedDDLExecutors
```

### JavadocDeclaration
Wrong tag `Experimental`
in `hudi-sync/hudi-datahub-sync/src/main/java/org/apache/hudi/sync/datahub/DataHubSyncTool.java`
#### Snippet
```java
 * To sync with DataHub via REST APIs.
 *
 * @Experimental
 * @see <a href="https://datahubproject.io/">https://datahubproject.io/</a>
 */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java`
#### Snippet
```java
   * @param partitionPath The partition path
   * @param record        instance of {@link RowData} that needs to be written to the fileWriter.
   * @throws IOException
   */
  public void write(String recordKey, String partitionPath, RowData record) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowDataCreateHandle.java`
#### Snippet
```java
   *
   * @return the {@link HoodieInternalWriteStatus} containing the stats and status of the writes to this handle.
   * @throws IOException
   */
  public HoodieInternalWriteStatus close() throws IOException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java`
#### Snippet
```java
   * @param instantTime   Instant Time for scheduling cleaning
   * @param extraMetadata additional metadata to write into plan
   * @return
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/row/HoodieRowCreateHandle.java`
#### Snippet
```java
   *
   * @param row instance of {@link InternalRow} that needs to be written to the fileWriter.
   * @throws IOException
   */
  public void write(InternalRow row) throws IOException {
```

### JavadocDeclaration
Illegal character
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/data/HoodieJavaPairRDD.java`
#### Snippet
```java
   * @param <K>         type of key.
   * @param <V>         type of value.
   * @return a new instance containing the {@link JavaPairRDD<K, V>} reference.
   */
  public static <K, V> HoodieJavaPairRDD<K, V> of(JavaPairRDD<K, V> pairRDDData) {
```

### JavadocDeclaration
Illegal character
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/data/HoodieJavaPairRDD.java`
#### Snippet
```java
   * @param <K>         type of key.
   * @param <V>         type of value.
   * @return a new instance containing the {@link JavaPairRDD<K, V>} reference.
   */
  public static <K, V> HoodieJavaPairRDD<K, V> of(JavaPairRDD<K, V> pairRDDData) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bucket/HoodieSparkConsistentBucketIndex.java`
#### Snippet
```java
   * @param fileStatus file for which commit marker should be created
   * @param partitionPath partition path the file belongs to
   * @throws IOException
   */
  private static void createCommitMarker(HoodieTable table, Path fileStatus, Path partitionPath) throws IOException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
   * Return all pending compactions with instant time for clients to decide what to compact next.
   *
   * @return
   */
  public List<Pair<String, HoodieCompactionPlan>> getPendingCompactions() {
```

### JavadocDeclaration
`@param context` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java

  /**
   * @param context
   * @param basePath
   * @param sqlContext
```

### JavadocDeclaration
`@param basePath` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
  /**
   * @param context
   * @param basePath
   * @param sqlContext
   */
```

### JavadocDeclaration
`@param sqlContext` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
   * @param context
   * @param basePath
   * @param sqlContext
   */
  public SparkRDDReadClient(HoodieSparkEngineContext context, String basePath, SQLContext sqlContext) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
  /**
   * Return Bootstrap Mode selections for partitions listed and figure out bootstrap Schema.
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
   * Return Bootstrap Mode selections for partitions listed and figure out bootstrap Schema.
   * @return
   * @throws IOException
   */
  private Map<BootstrapMode, List<Pair<String, List<HoodieFileStatus>>>> listAndProcessSourcePartitions() throws IOException {
```

### JavadocDeclaration
`@param ` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/ExecutionStrategyUtil.java`
#### Snippet
```java
   * @param writeConfig writeConfig.
   * @return hoodieRecord.
   * @param <T>
   */
  public static <T> HoodieRecord<T> transform(IndexedRecord indexedRecord,
```

### JavadocDeclaration
`@param conf` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   * table will end up bootstrapping at this time.
   *
   * @param conf
   * @param writeConfig
   * @param context
```

### JavadocDeclaration
`@param writeConfig` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   *
   * @param conf
   * @param writeConfig
   * @param context
   * @param actionMetadata
```

### JavadocDeclaration
`@param context` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   * @param conf
   * @param writeConfig
   * @param context
   * @param actionMetadata
   * @param inflightInstantTimestamp Timestamp of an instant which is in-progress. This instant is ignored while
```

### JavadocDeclaration
`@param actionMetadata` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metadata/SparkHoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   * @param writeConfig
   * @param context
   * @param actionMetadata
   * @param inflightInstantTimestamp Timestamp of an instant which is in-progress. This instant is ignored while
   *                                 attempting to bootstrap the table.
```

### JavadocDeclaration
`@param records` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   * Sort by record key within each partition. The behaviour is the same as BulkInsertSortMode.PARTITION_SORT.
   *
   * @param records
   * @param partitioner
   * @return
```

### JavadocDeclaration
`@param partitioner` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   *
   * @param records
   * @param partitioner
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   * @param records
   * @param partitioner
   * @return
   */
  private JavaRDD<HoodieRecord<T>> doPartitionAndSortByRecordKey(JavaRDD<HoodieRecord<T>> records, Partitioner partitioner) {
```

### JavadocDeclaration
`@param records` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   * Sort by specified column value. The behaviour is the same as `RDDCustomColumnsSortPartitioner`
   *
   * @param records
   * @param partitioner
   * @return
```

### JavadocDeclaration
`@param partitioner` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   *
   * @param records
   * @param partitioner
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   * @param records
   * @param partitioner
   * @return
   */
  private JavaRDD<HoodieRecord<T>> doPartitionAndCustomColumnSort(JavaRDD<HoodieRecord<T>> records, Partitioner partitioner) {
```

### JavadocDeclaration
`@param records` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   * By default, do partition only.
   *
   * @param records
   * @param partitioner a default partition that accepts `HoodieKey` as the partition key
   * @return
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDBucketIndexPartitioner.java`
#### Snippet
```java
   * @param records
   * @param partitioner a default partition that accepts `HoodieKey` as the partition key
   * @return
   */

```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/AsyncCompactService.java`
#### Snippet
```java
   * Check whether compactor thread needs to be stopped.
   *
   * @return
   */
  protected boolean shouldStopCompactor() {
```

### JavadocDeclaration
`@param onShutdownCallback` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
   * run-callbacks in case of shutdown
   * 
   * @param onShutdownCallback
   */
  public void start(Function<Boolean, Boolean> onShutdownCallback) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
   * Wait till the service shutdown. If the service shutdown with exception, it will be thrown
   *
   * @throws ExecutionException
   * @throws InterruptedException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
   *
   * @throws ExecutionException
   * @throws InterruptedException
   */
  public void waitForShutdown() throws ExecutionException, InterruptedException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
   *
   * @param numPending Maximum pending compactions/clustering allowed
   * @throws InterruptedException
   */
  public void waitTillPendingAsyncServiceInstantsReducesTo(int numPending) throws InterruptedException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
   *
   * @return {@link HoodieInstant} corresponding to the next pending compaction/clustering.
   * @throws InterruptedException
   */
  HoodieInstant fetchNextAsyncServiceInstant() throws InterruptedException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/BulkInsertPartitioner.java`
#### Snippet
```java
   *
   * @param partitionId data partition
   * @return
   */
  default String getFileIdPfx(int partitionId) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/BulkInsertPartitioner.java`
#### Snippet
```java
   * @param records          Input Hoodie records.
   * @param outputPartitions Expected number of output partitions as a hint.
   * @return
   */
  I repartitionRecords(I records, int outputPartitions);
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/BulkInsertPartitioner.java`
#### Snippet
```java
   *
   * @param partitionId data partition
   * @return
   */
  default Option<WriteHandleFactory> getWriteHandleFactory(int partitionId) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
   * @param instantTime Instant Time for scheduling log compaction
   * @param extraMetadata additional metadata to write into plan
   * @return
   */
  public Option<HoodieCompactionPlan> scheduleLogCompaction(HoodieEngineContext context,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
   * @param stats Hoodie Write Stat
   * @param consistencyCheckEnabled Consistency Check Enabled
   * @throws HoodieIOException
   */
  protected void reconcileAgainstMarkers(HoodieEngineContext context,
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
   * @param instantTime Instant Time for scheduling compaction
   * @param extraMetadata additional metadata to write into plan
   * @return
   */
  public abstract Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
   * @param instantToRetain Earliest Instant to retain
   * @return list of partitions
   * @throws IOException
   */
  private List<String> getPartitionPathsForCleanByCommits(Option<HoodieInstant> instantToRetain) throws IOException {
```

### JavadocDeclaration
`@param cleanMetadata` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
  /**
   * Use Incremental Mode for finding partition paths.
   * @param cleanMetadata
   * @param newInstantToRetain
   * @return
```

### JavadocDeclaration
`@param newInstantToRetain` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
   * Use Incremental Mode for finding partition paths.
   * @param cleanMetadata
   * @param newInstantToRetain
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
   * @param cleanMetadata
   * @param newInstantToRetain
   * @return
   */
  private List<String> getPartitionPathsForIncrementalCleaning(HoodieCleanMetadata cleanMetadata,
```

### JavadocDeclaration
`@param writeConfig` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java`
#### Snippet
```java
   * Filter the partition paths based on compaction strategy.
   * 
   * @param writeConfig
   * @param allPartitionPaths
   * @return
```

### JavadocDeclaration
`@param allPartitionPaths` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java`
#### Snippet
```java
   * 
   * @param writeConfig
   * @param allPartitionPaths
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java`
#### Snippet
```java
   * @param writeConfig
   * @param allPartitionPaths
   * @return
   */
  public List<String> filterPartitionPaths(HoodieWriteConfig writeConfig, List<String> allPartitionPaths) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/RestoreUtils.java`
#### Snippet
```java
   * @param restoreInstant Instant referring to restore action
   * @return Rollback plan corresponding to rollback instant
   * @throws IOException
   */
  public static HoodieRestorePlan getRestorePlan(HoodieTableMetaClient metaClient, HoodieInstant restoreInstant)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/restore/RestoreUtils.java`
#### Snippet
```java
   * @param restoreInstant Instant referring to restore action
   * @return timestamp of the savepoint we are restoring
   * @throws IOException
   *
   * */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java`
#### Snippet
```java
   * @param rollbackInstant Instant referring to rollback action
   * @return Rollback plan corresponding to rollback instant
   * @throws IOException
   */
  public static HoodieRollbackPlan getRollbackPlan(HoodieTableMetaClient metaClient, HoodieInstant rollbackInstant)
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackPlanActionExecutor.java`
#### Snippet
```java
   * Fetch the Rollback strategy used.
   *
   * @return
   */
  private BaseRollbackPlanActionExecutor.RollbackStrategy getRollbackStrategy() {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/bootstrap/BootstrapUtils.java`
#### Snippet
```java
   * @param context JHoodieEngineContext
   * @return list of partition paths with files under them.
   * @throws IOException
   */
  public static List<Pair<String, List<HoodieFileStatus>>> getAllLeafFoldersWithFiles(HoodieTableMetaClient metaClient,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/MarkerBasedRollbackUtils.java`
#### Snippet
```java
   * @param parallelism parallelism to use
   * @return a list of all markers
   * @throws IOException
   */
  public static List<String> getAllMarkerPaths(HoodieTable table, HoodieEngineContext context,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java`
#### Snippet
```java
   * @param hoodieRollbackPlan instance of {@link HoodieRollbackPlan} that needs to be executed.
   * @return a list of {@link HoodieRollbackStat}s.
   * @throws IOException
   */
  protected abstract List<HoodieRollbackStat> executeRollback(HoodieRollbackPlan hoodieRollbackPlan) throws IOException;
```

### JavadocDeclaration
Wrong tag `true`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java

  /**
   * @return {@true} if the marker directory exists in the file system; {@false} otherwise.
   * @throws IOException
   */
```

### JavadocDeclaration
Wrong tag `false`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java

  /**
   * @return {@true} if the marker directory exists in the file system; {@false} otherwise.
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java
  /**
   * @return {@true} if the marker directory exists in the file system; {@false} otherwise.
   * @throws IOException
   */
  public abstract boolean doesMarkerDirExist() throws IOException;
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java
  /**
   * @return all the marker paths
   * @throws IOException
   */
  public abstract Set<String> allMarkerFilePaths() throws IOException;
```

### JavadocDeclaration
Wrong tag `true`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java
   * @param context {@code HoodieEngineContext} instance.
   * @param parallelism parallelism for deleting the marker files in the directory.
   * @return {@true} if successful; {@false} otherwise.
   */
  public abstract boolean deleteMarkerDir(HoodieEngineContext context, int parallelism);
```

### JavadocDeclaration
Wrong tag `false`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java
   * @param context {@code HoodieEngineContext} instance.
   * @param parallelism parallelism for deleting the marker files in the directory.
   * @return {@true} if successful; {@false} otherwise.
   */
  public abstract boolean deleteMarkerDir(HoodieEngineContext context, int parallelism);
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/WriteMarkers.java`
#### Snippet
```java
   * @param parallelism parallelism for reading the marker files in the directory.
   * @return all the data file paths of write IO type "CREATE" and "MERGE"
   * @throws IOException
   */
  public abstract Set<String> createdAndMergedDataPaths(HoodieEngineContext context, int parallelism) throws IOException;
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/marker/DirectWriteMarkers.java`
#### Snippet
```java
  /**
   * @return {@code true} if marker directory exists; {@code false} otherwise.
   * @throws IOException
   */
  public boolean doesMarkerDirExist() throws IOException {
```

### JavadocDeclaration
`@param instant` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseClusterer.java`
#### Snippet
```java
  /**
   * Run clustering for the instant.
   * @param instant
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseClusterer.java`
#### Snippet
```java
   * Run clustering for the instant.
   * @param instant
   * @throws IOException
   */
  public abstract void cluster(HoodieInstant instant) throws IOException;
```

### JavadocDeclaration
`@param writeClient` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseClusterer.java`
#### Snippet
```java
  /**
   * Update the write client used by async clustering.
   * @param writeClient
   */
  public void updateWriteClient(BaseHoodieWriteClient<T, I, K, O> writeClient) {
```

### JavadocDeclaration
Class/method reference, quoted text, or HTML link are expected after @see tag
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieClient.java`
#### Snippet
```java
   * @param pendingInflightAndRequestedInstants Pending instants on the timeline
   *
   * @see {@link BaseHoodieWriteClient#preCommit}
   * @see {@link BaseHoodieTableServiceClient#preCommit}
   */
```

### JavadocDeclaration
Class/method reference, quoted text, or HTML link are expected after @see tag
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieClient.java`
#### Snippet
```java
   *
   * @see {@link BaseHoodieWriteClient#preCommit}
   * @see {@link BaseHoodieTableServiceClient#preCommit}
   */
  protected void resolveWriteConflict(HoodieTable table, HoodieCommitMetadata metadata, Set<String> pendingInflightAndRequestedInstants) {
```

### JavadocDeclaration
`@param metaClient` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * Get InflightAndRequest instants.
   *
   * @param metaClient
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   *
   * @param metaClient
   * @return
   */
  public static Set<String> getInflightAndRequestedInstants(HoodieTableMetaClient metaClient) {
```

### JavadocDeclaration
`@param table` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * Resolve any write conflicts when committing data.
   *
   * @param table
   * @param currentTxnOwnerInstant
   * @param thisCommitMetadata
```

### JavadocDeclaration
`@param currentTxnOwnerInstant` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   *
   * @param table
   * @param currentTxnOwnerInstant
   * @param thisCommitMetadata
   * @param config
```

### JavadocDeclaration
`@param thisCommitMetadata` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * @param table
   * @param currentTxnOwnerInstant
   * @param thisCommitMetadata
   * @param config
   * @param lastCompletedTxnOwnerInstant
```

### JavadocDeclaration
`@param config` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * @param currentTxnOwnerInstant
   * @param thisCommitMetadata
   * @param config
   * @param lastCompletedTxnOwnerInstant
   * @param pendingInstants
```

### JavadocDeclaration
`@param lastCompletedTxnOwnerInstant` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * @param thisCommitMetadata
   * @param config
   * @param lastCompletedTxnOwnerInstant
   * @param pendingInstants
   *
```

### JavadocDeclaration
`@param pendingInstants` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * @param config
   * @param lastCompletedTxnOwnerInstant
   * @param pendingInstants
   *
   * @return
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * @param pendingInstants
   *
   * @return
   * @throws HoodieWriteConflictException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   *
   * @return
   * @throws HoodieWriteConflictException
   */
  public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(
```

### JavadocDeclaration
`@param metaClient` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   * Get the last completed transaction hoodie instant and {@link HoodieCommitMetadata#getExtraMetadata()}.
   *
   * @param metaClient
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java`
#### Snippet
```java
   *
   * @param metaClient
   * @return
   */
  public static Option<Pair<HoodieInstant, Map<String, String>>> getLastCompletedTxnInstantAndMetadata(
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineServerHelper.java`
#### Snippet
```java
   * @param config     Hoodie Write Config
   * @return TimelineServer if configured to run
   * @throws IOException
   */
  public static synchronized Option<EmbeddedTimelineService> createEmbeddedTimelineService(
```

### JavadocDeclaration
`@param instantTime` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
  /**
   * Check if the heartbeat corresponding to instantTime has expired. If yes, abort by throwing an exception.
   * @param instantTime
   * @param table
   * @param heartbeatClient
```

### JavadocDeclaration
`@param table` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
   * Check if the heartbeat corresponding to instantTime has expired. If yes, abort by throwing an exception.
   * @param instantTime
   * @param table
   * @param heartbeatClient
   * @param config
```

### JavadocDeclaration
`@param heartbeatClient` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
   * @param instantTime
   * @param table
   * @param heartbeatClient
   * @param config
   */
```

### JavadocDeclaration
`@param config` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
   * @param table
   * @param heartbeatClient
   * @param config
   */
  public static void abortIfHeartbeatExpired(String instantTime, HoodieTable table,
```

### JavadocDeclaration
`@param fs` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
  /**
   * Deletes the heartbeat file for the specified instant.
   * @param fs
   * @param basePath
   * @param instantTime
```

### JavadocDeclaration
`@param basePath` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
   * Deletes the heartbeat file for the specified instant.
   * @param fs
   * @param basePath
   * @param instantTime
   * @return
```

### JavadocDeclaration
`@param instantTime` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
   * @param fs
   * @param basePath
   * @param instantTime
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HeartbeatUtils.java`
#### Snippet
```java
   * @param basePath
   * @param instantTime
   * @return
   */
  public static boolean deleteHeartbeatFile(FileSystem fs, String basePath, String instantTime) {
```

### JavadocDeclaration
`@param thisOperation` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
  /**
   * Implementations of this method will determine how to resolve a conflict between 2 commits.
   * @param thisOperation
   * @param otherOperation
   * @return
```

### JavadocDeclaration
`@param otherOperation` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
   * Implementations of this method will determine how to resolve a conflict between 2 commits.
   * @param thisOperation
   * @param otherOperation
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
   * @param thisOperation
   * @param otherOperation
   * @return
   */
  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
  /**
   * Stream of instants to check conflicts against.
   * @return
   */
  Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);
```

### JavadocDeclaration
`@param thisOperation` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
  /**
   * Implementations of this method will determine whether a conflict exists between 2 commits.
   * @param thisOperation
   * @param otherOperation
   * @return
```

### JavadocDeclaration
`@param otherOperation` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
   * Implementations of this method will determine whether a conflict exists between 2 commits.
   * @param thisOperation
   * @param otherOperation
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/ConflictResolutionStrategy.java`
#### Snippet
```java
   * @param thisOperation
   * @param otherOperation
   * @return
   */
  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  /**
   * Stops all heartbeats started via this instance of the client.
   * @throws HoodieException
   */
  public void stop() throws HoodieException {
```

### JavadocDeclaration
`@param instantTime` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  /**
   * Stops the heartbeat for the specified instant.
   * @param instantTime
   * @throws HoodieException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
   * Stops the heartbeat for the specified instant.
   * @param instantTime
   * @throws HoodieException
   */
  public void stop(String instantTime) throws HoodieException {
```

### JavadocDeclaration
`@param instantTime` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  /**
   * Start a new heartbeat for the specified instant. If there is already one running, this will be a NO_OP
   * @param instantTime
   */
  public void start(String instantTime) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java`
#### Snippet
```java
   *
   * @param context HoodieEngineContext used for parallelize to delete small archive files if necessary.
   * @throws IOException
   */
  private void verifyLastMergeArchiveFilesIfNecessary(HoodieEngineContext context) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/HoodieTimelineArchiver.java`
#### Snippet
```java
   *
   * @param context HoodieEngineContext
   * @throws IOException
   */
  private void mergeArchiveFilesIfNecessary(HoodieEngineContext context) throws IOException {
```

### JavadocDeclaration
`@param instantTime` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
  /**
   * Common method containing steps to be performed before write (upsert/insert/...
   * @param instantTime
   * @param writeOperationType
   * @param metaClient
```

### JavadocDeclaration
`@param writeOperationType` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
   * Common method containing steps to be performed before write (upsert/insert/...
   * @param instantTime
   * @param writeOperationType
   * @param metaClient
   */
```

### JavadocDeclaration
`@param metaClient` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieWriteClient.java`
#### Snippet
```java
   * @param instantTime
   * @param writeOperationType
   * @param metaClient
   */
  public void preWrite(String instantTime, WriteOperationType writeOperationType,
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
   * @param extraMetadata    Metadata to pass onto the scheduled service instant
   * @param tableServiceType Type of table service to schedule
   * @return
   */
  public Option<String> scheduleTableService(String instantTime, Option<Map<String, String>> extraMetadata,
```

### JavadocDeclaration
`@param metaClient` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
   * Get inflight timeline excluding compaction and clustering.
   *
   * @param metaClient
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
   *
   * @param metaClient
   * @return
   */
  private HoodieTimeline getInflightTimelineExcludeCompactionAndClustering(HoodieTableMetaClient metaClient) {
```

### JavadocDeclaration
Wrong tag `Deprecated`
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java

  /**
   * @Deprecated
   * Rollback the inflight record changes with the given commit time. This
   * will be removed in future in favor of {@link BaseHoodieWriteClient#restoreToInstant(String, boolean)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java`
#### Snippet
```java
   * @param metadataPartitions - metadata partitions for which file groups needs to be initialized
   * @param instantTime        - instant time of the index action
   * @throws IOException
   */
  void initializeMetadataPartitions(HoodieTableMetaClient dataMetaClient, List<MetadataPartitionType> metadataPartitions, String instantTime) throws IOException;
```

### JavadocDeclaration
`@param metadataPartitions` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java`
#### Snippet
```java
   * Drop the given metadata partitions.
   *
   * @param metadataPartitions
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java`
#### Snippet
```java
   *
   * @param metadataPartitions
   * @throws IOException
   */
  void dropMetadataPartitions(List<MetadataPartitionType> metadataPartitions) throws IOException;
```

### JavadocDeclaration
`@param engineContext` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java`
#### Snippet
```java
   * Builds the given metadata partitions to create index.
   *
   * @param engineContext
   * @param indexPartitionInfos - information about partitions to build such as partition type and base instant time
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java
   * Hoodie Client Lock Configs.
   *
   * @return
   */
  public boolean isAutoAdjustLockConfigs() {
```

### JavadocDeclaration
`@param filePath` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuard.java`
#### Snippet
```java
   * Wait for file to be listable based on configurable timeout.
   * 
   * @param filePath
   * @throws IOException when having trouble listing the path
   * @throws TimeoutException when retries exhausted
```

### JavadocDeclaration
`@param filePath` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuard.java`
#### Snippet
```java
   * Wait for file to be listable based on configurable timeout.
   * 
   * @param filePath
   * @throws IOException when having trouble listing the path
   * @throws TimeoutException when retries exhausted
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuard.java`
#### Snippet
```java
   * @param files Files
   * @param targetVisibility Target Visibility
   * @throws IOException
   * @throws TimeoutException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuard.java`
#### Snippet
```java
   * @param targetVisibility Target Visibility
   * @throws IOException
   * @throws TimeoutException
   */
  default void waitTill(String dirPath, List<String> files, FileVisibility targetVisibility)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/fs/FailSafeConsistencyGuard.java`
#### Snippet
```java
   * @param files Files to appear/disappear
   * @param event Appear/Disappear
   * @throws TimeoutException
   */
  public void waitForFilesVisibility(String dirPath, List<String> files, FileVisibility event) throws TimeoutException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   * @param inflightInstantTimestamp - timestamp of an instant in progress on the dataset
   * @param <T>                      - action metadata types extending Avro generated SpecificRecordBase
   * @throws IOException
   */
  protected <T extends SpecificRecordBase> void initializeIfNeeded(HoodieTableMetaClient dataMetaClient,
```

### JavadocDeclaration
`@param engineContext` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   * If the metadata table does not exist, then file and partition listing is used to initialize the table.
   *
   * @param engineContext
   * @param actionMetadata           Action metadata types extending Avro generated SpecificRecordBase
   * @param inflightInstantTimestamp Timestamp of an instant in progress on the dataset. This instant is ignored
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java
   * @param dataMetaClient    - Meta client for the data table
   * @param createInstantTime - Metadata table create instant time
   * @throws IOException
   */
  private void initializeEnabledFileGroups(HoodieTableMetaClient dataMetaClient, String createInstantTime, List<MetadataPartitionType> partitionTypes) throws IOException {
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodiePairData.java`
#### Snippet
```java

  /**
   * Collects results of the underlying collection into a {@link List<Pair<K, V>>}
   *
   * This is a terminal operation
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodiePairData.java`
#### Snippet
```java

  /**
   * Collects results of the underlying collection into a {@link List<Pair<K, V>>}
   *
   * This is a terminal operation
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodiePairData.java`
#### Snippet
```java

  /**
   * Collects results of the underlying collection into a {@link List<Pair<K, V>>}
   *
   * This is a terminal operation
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodieData.java`
#### Snippet
```java

  /**
   * Maps every element in the collection using provided mapping {@code func} into a {@link Pair<K, V>}
   * of elements {@code K} and {@code V}
   * <p>
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodieData.java`
#### Snippet
```java

  /**
   * Maps every element in the collection using provided mapping {@code func} into a {@link Pair<K, V>}
   * of elements {@code K} and {@code V}
   * <p>
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodieData.java`
#### Snippet
```java
   * @param <K>  key type of the pair
   * @param <V>  value type of the pair
   * @return {@link HoodiePairData<K, V>} holding mapped elements
   */
  <K, V> HoodiePairData<K, V> mapToPair(SerializablePairFunction<T, K, V> func);
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/common/data/HoodieData.java`
#### Snippet
```java
   * @param <K>  key type of the pair
   * @param <V>  value type of the pair
   * @return {@link HoodiePairData<K, V>} holding mapped elements
   */
  <K, V> HoodiePairData<K, V> mapToPair(SerializablePairFunction<T, K, V> func);
```

### JavadocDeclaration
Wrong tag `VisibleForTesting`
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
   * NOTE: This method could only be used in tests
   *
   * @VisibleForTesting
   */
  public static int fromJavaDate(Date date) {
```

### JavadocDeclaration
Wrong tag `VisibleForTesting`
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
   * NOTE: This method could only be used in tests
   *
   * @VisibleForTesting
   */
  public static java.sql.Date toJavaDate(int days) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
   * @param writeSchema - write schema of the record
   * @param fieldName   -  name of the field
   * @return
   */
  public static Schema getNestedFieldSchemaFromWriteSchema(Schema writeSchema, String fieldName) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
   * @param record    - record containing the value of the given field
   * @param fieldName - name of the field
   * @return
   */
  public static Schema getNestedFieldSchemaFromRecord(GenericRecord record, String fieldName) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/OrcReaderIterator.java`
#### Snippet
```java
   * If the current batch is empty, get a new one.
   * @return true if we have rows available.
   * @throws IOException
   */
  private boolean ensureBatch() throws IOException {
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java`
#### Snippet
```java
   * Returns difference b/w {@code one} {@link List} of elements and {@code another}
   *
   * NOTE: This is less optimal counterpart to {@link #diff(Collection, Collection)}, accepting {@link List}
   *       as a holding collection to support duplicate elements use-cases
   */
```

### JavadocDeclaration
`@param clazz` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java`
#### Snippet
```java
  /**
   * Invoke a static method of a class.
   * @param clazz
   * @param methodName
   * @param args
```

### JavadocDeclaration
`@param methodName` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java`
#### Snippet
```java
   * Invoke a static method of a class.
   * @param clazz
   * @param methodName
   * @param args
   * @param parametersType
```

### JavadocDeclaration
`@param args` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java`
#### Snippet
```java
   * @param clazz
   * @param methodName
   * @param args
   * @param parametersType
   * @return the return value of the method
```

### JavadocDeclaration
`@param parametersType` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java`
#### Snippet
```java
   * @param methodName
   * @param args
   * @param parametersType
   * @return the return value of the method
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ReflectionUtils.java`
#### Snippet
```java
   * @param clazz               Class name.
   * @param constructorArgTypes Argument types of the constructor.
   * @return
   */
  public static boolean hasConstructor(String clazz, Class<?>[] constructorArgTypes) {
```

### JavadocDeclaration
`@param metaClient` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
  /**
   * Get requested replace metadata from timeline.
   * @param metaClient
   * @param pendingReplaceInstant
   * @return
```

### JavadocDeclaration
`@param pendingReplaceInstant` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
   * Get requested replace metadata from timeline.
   * @param metaClient
   * @param pendingReplaceInstant
   * @return
   * @throws IOException
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
   * @param metaClient
   * @param pendingReplaceInstant
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
   * @param pendingReplaceInstant
   * @return
   * @throws IOException
   */
  private static Option<HoodieRequestedReplaceMetadata> getRequestedReplaceMetadata(HoodieTableMetaClient metaClient, HoodieInstant pendingReplaceInstant) throws IOException {
```

### JavadocDeclaration
`@param metaClient` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
  /**
   * Get Clustering plan from timeline.
   * @param metaClient
   * @param pendingReplaceInstant
   * @return
```

### JavadocDeclaration
`@param pendingReplaceInstant` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
   * Get Clustering plan from timeline.
   * @param metaClient
   * @param pendingReplaceInstant
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java`
#### Snippet
```java
   * @param metaClient
   * @param pendingReplaceInstant
   * @return
   */
  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant pendingReplaceInstant) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param mode - "r" for read only; "rw" for read write
   * @param size - size/capacity of the buffer.
   * @throws IOException
   */
  public BufferedRandomAccessFile(File file, String mode, int size) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param off - starting offset.
   * @param len - length of bytes to be written
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * If space is available at the end of the buffer, start using it. Otherwise,
   * flush the unwritten data into the file and load the buffer corresponding to startPosition.
   * @throws IOException
   */
  private void expandBufferToCapacityIfNeeded() throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param len  - length of bytes to be read
   * @return - number of bytes read.
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * Returns the length of the file, depending on whether buffer has more data (to be flushed).
   * @return - length of the file (including data yet to be flushed to the file).
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * write a byte to the buffer/file.
   * @param v - value to be written
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param file  - file name
   * @param mode - "r" for read only; "rw" for read write
   * @throws IOException
   */
  public BufferedRandomAccessFile(File file, String mode) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   *
   * @param pos  - position in the file to be loaded to the buffer.
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
  /**
   * Close the file, after flushing data in the buffer.
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * write an array of bytes to the buffer/file.
   * @param b - byte array with data to be written
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param b - byte array into which to read data.
   * @return  - returns number of bytes read.
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
  /**
   * If the file is writable, flush any bytes in the buffer that have not yet been written to disk.
   * @throws IOException
   */
  public void flush() throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * If the diskPosition differs from the startPosition, flush the data in the buffer
   * and realign/fill the buffer at startPosition.
   * @throws IOException
   */
  private void alignDiskPositionToBufferStartIfNeeded() throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param mode - "r" for read only; "rw" for read write
   * @param size - size/capacity of the buffer
   * @throws FileNotFoundException
   */
  public BufferedRandomAccessFile(String name, String mode, int size) throws FileNotFoundException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param len - length of bytes to be written
   * @return - number of bytes written
   * @throws IOException
   */
  private int writeAtMost(byte[] b, int off, int len) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * Load a new data block.  Returns false, when EOF is reached.
   * @return - whether new data block was loaded or not
   * @throws IOException
   */
  private boolean loadNewBlockToBuffer() throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
  /**
   * @return - returns a byte as an integer.
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * read ahead file contents to buffer.
   * @return number of bytes filled
   * @throws IOException
   */
  private int fillBuffer() throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
  /**
   * Flush any dirty bytes in the buffer to disk.
   * @throws IOException
   */
  private void flushBuffer() throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/BufferedRandomAccessFile.java`
#### Snippet
```java
   * @param name - name of the file
   * @param mode - "r" for read only; "rw" for read write
   * @throws IOException
   */
  public BufferedRandomAccessFile(String name, String mode) throws IOException {
```

### JavadocDeclaration
`@param cleaningPolicy` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
  /**
   * Execute {@link HoodieFailedWritesCleaningPolicy} to rollback failed writes for different actions.
   * @param cleaningPolicy
   * @param actionType
   * @param rollbackFailedWritesFunc
```

### JavadocDeclaration
`@param actionType` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
   * Execute {@link HoodieFailedWritesCleaningPolicy} to rollback failed writes for different actions.
   * @param cleaningPolicy
   * @param actionType
   * @param rollbackFailedWritesFunc
   */
```

### JavadocDeclaration
`@param rollbackFailedWritesFunc` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
   * @param cleaningPolicy
   * @param actionType
   * @param rollbackFailedWritesFunc
   */
  public static void rollbackFailedWrites(HoodieFailedWritesCleaningPolicy cleaningPolicy, String actionType,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
   * @param cleanInstant Instant referring to clean action
   * @return Cleaner plan corresponding to clean instant
   * @throws IOException
   */
  public static HoodieCleanerPlan getCleanerPlan(HoodieTableMetaClient metaClient, HoodieInstant cleanInstant)
```

### JavadocDeclaration
`@param cleanFileInfoList` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
  /**
   * Convert list of cleanFileInfo instances to list of avro-generated HoodieCleanFileInfo instances.
   * @param cleanFileInfoList
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
   * Convert list of cleanFileInfo instances to list of avro-generated HoodieCleanFileInfo instances.
   * @param cleanFileInfoList
   * @return
   */
  public static List<HoodieCleanFileInfo> convertToHoodieCleanFileInfoList(List<CleanFileInfo> cleanFileInfoList) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java`
#### Snippet
```java
   * @param cleanInstant Instant referring to clean action
   * @return Latest version of Clean metadata corresponding to clean instant
   * @throws IOException
   */
  public static HoodieCleanMetadata getCleanerMetadata(HoodieTableMetaClient metaClient, HoodieInstant cleanInstant)
```

### JavadocDeclaration
`@param instants` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
   * 4. Skip pending compaction instant (For now we don' do early conflict check with compact action)
   *      Because we don't want to let pending compaction block common writer.
   * @param instants
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
   *      Because we don't want to let pending compaction block common writer.
   * @param instants
   * @return
   */
  public static List<String> getCandidateInstants(HoodieActiveTimeline activeTimeline, List<Path> instants, String currentInstantTime,
```

### JavadocDeclaration
`@param marker` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
   * 20210623/0/20210825/932a86d9-5c1d-44c7-ac99-cb88b8ef8478-0_85-15-1390_20220620181735781.parquet.marker.MERGE
   *    ==> get 20210623/0/20210825/932a86d9-5c1d-44c7-ac99-cb88b8ef8478-0
   * @param marker
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
   *    ==> get 20210623/0/20210825/932a86d9-5c1d-44c7-ac99-cb88b8ef8478-0
   * @param marker
   * @return
   */
  public static String makerToPartitionAndFileID(String marker) {
```

### JavadocDeclaration
`@param marker` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
   * /var/folders/t3/th1dw75d0yz2x2k2qt6ys9zh0000gp/T/junit6502909693741900820/dataset/.hoodie/.temp/003
   *    ==> 003
   * @param marker
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/MarkerUtils.java`
#### Snippet
```java
   *    ==> 003
   * @param marker
   * @return
   */
  public static String markerDirToInstantTime(String marker) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java`
#### Snippet
```java
   * Return all pending compaction instant times.
   *
   * @return
   */
  public static List<HoodieInstant> getPendingCompactionInstantTimes(HoodieTableMetaClient metaClient) {
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/DiskMap.java`
#### Snippet
```java

  /**
   * @returns a stream of the values stored in the disk.
   */
  abstract Stream<R> valueStream();
```

### JavadocDeclaration
Tag `return` is not allowed here
in `hudi-common/src/main/java/org/apache/hudi/common/bloom/HoodieDynamicBoundedBloomFilter.java`
#### Snippet
```java
   * @param errorRate  maximum allowable error rate.
   * @param hashType   type of the hashing function (see {@link org.apache.hadoop.util.hash.Hash}).
   * @return the {@link HoodieDynamicBoundedBloomFilter} thus created
   */
  HoodieDynamicBoundedBloomFilter(int numEntries, double errorRate, int hashType, int maxNoOfEntries) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
   * RandomAccessFile is not thread-safe. This API opens a new file handle per thread and returns.
   *
   * @return
   */
  private BufferedRandomAccessFile getRandomAccessFile() {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/FileSlice.java`
#### Snippet
```java
   * Returns true if there is no data file and no log files. Happens as part of pending compaction
   * 
   * @return
   */
  public boolean isEmpty() {
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/model/DefaultHoodieRecordPayload.java`
#### Snippet
```java
   * @param genericRecord instance of {@link GenericRecord} of interest.
   * @param properties payload related properties
   * @returns {@code true} if record represents a delete record. {@code false} otherwise.
   */
  protected boolean isDeleteRecord(GenericRecord genericRecord, Properties properties) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/CompactionOperation.java`
#### Snippet
```java
   * 
   * @param operation Hoodie Compaction Operation
   * @return
   */
  public static CompactionOperation convertFromAvroRecordInstance(HoodieCompactionOperation operation) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodiePartitionMetadata.java`
#### Snippet
```java
   *
   * @param filePath Path of the file to write
   * @throws IOException
   */
  private void writeMetafile(Path filePath) throws IOException {
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/model/BaseAvroPayload.java`
#### Snippet
```java
  /**
   * @param genericRecord instance of {@link GenericRecord} of interest.
   * @returns {@code true} if record represents a delete record. {@code false} otherwise.
   */
  protected boolean isDeleteRecord(GenericRecord genericRecord) {
```

### JavadocDeclaration
`@param schema` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
  /**
   * return itself as long as it called by preCombine
   * @param schema
   * @param isPreCombining
   * @return
```

### JavadocDeclaration
`@param isPreCombining` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   * return itself as long as it called by preCombine
   * @param schema
   * @param isPreCombining
   * @return
   * @throws IOException
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   * @param schema
   * @param isPreCombining
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   * @param isPreCombining
   * @return
   * @throws IOException
   */
  public Option<IndexedRecord> getInsertValue(Schema schema, boolean isPreCombining) throws IOException {
```

### JavadocDeclaration
`@param oldRecord` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   * Merge old record with new record.
   *
   * @param oldRecord
   * @param schema
   * @param isOldRecordNewer
```

### JavadocDeclaration
`@param schema` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   *
   * @param oldRecord
   * @param schema
   * @param isOldRecordNewer
   * @param isPreCombining   flag for deleted record combine logic
```

### JavadocDeclaration
`@param isOldRecordNewer` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   * @param oldRecord
   * @param schema
   * @param isOldRecordNewer
   * @param isPreCombining   flag for deleted record combine logic
   *                         1 preCombine: if delete record is newer, return merged record with _hoodie_is_deleted = true
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   *                         1 preCombine: if delete record is newer, return merged record with _hoodie_is_deleted = true
   *                         2 combineAndGetUpdateValue:  if delete record is newer, return empty since we don't need to store deleted data to storage
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   *                         2 combineAndGetUpdateValue:  if delete record is newer, return empty since we don't need to store deleted data to storage
   * @return
   * @throws IOException
   */
  private Option<IndexedRecord> mergeOldRecord(IndexedRecord oldRecord,
```

### JavadocDeclaration
`@param orderingVal` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/PartialUpdateAvroPayload.java`
#### Snippet
```java
   * Returns whether the given record is newer than the record of this payload.
   *
   * @param orderingVal
   * @param record      The record
   * @param prop        The payload properties
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/debezium/PostgresDebeziumAvroPayload.java`
#### Snippet
```java
   * @param incomingRecord The incoming avro record
   * @param field          the column of interest
   * @return
   */
  private boolean containsStringToastedValues(IndexedRecord incomingRecord, Schema.Field field) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/debezium/PostgresDebeziumAvroPayload.java`
#### Snippet
```java
   * @param incomingRecord The incoming avro record
   * @param field          the column of interest
   * @return
   */
  private boolean containsBytesToastedValues(IndexedRecord incomingRecord, Schema.Field field) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java`
#### Snippet
```java
  /**
   * Get column in record to support RDDCustomColumnsSortPartitioner
   * @return
   */
  public abstract Object[] getColumnValues(Schema recordSchema, String[] columns, boolean consistentLogicalTimestampEnabled);
```

### JavadocDeclaration
`@param hadoopConf` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java
   * from the latest commit.
   *
   * @param hadoopConf
   * @param basePath The base path
   * @return the file full path to file status mapping
```

### JavadocDeclaration
`@param hadoopConf` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java`
#### Snippet
```java
   * this is an optimization for COPY_ON_WRITE table to eliminate legacy files for filesystem view.
   *
   * @param hadoopConf
   * @param basePath The base path
   * @return the file ID to file status mapping
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormat.java`
#### Snippet
```java
     * Read log file in reverse order and check if prev block is present.
     * 
     * @return
     */
    boolean hasPrev();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormat.java`
#### Snippet
```java
     * Read log file in reverse order and return prev block if present.
     * 
     * @return
     * @throws IOException
     */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormat.java`
#### Snippet
```java
     * 
     * @return
     * @throws IOException
     */
    HoodieLogBlock prev() throws IOException;
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java
   * @param outputStream - output stream to which properties will be written
   * @return return the table checksum
   * @throws IOException
   */
  private static String storeProperties(Properties props, FSDataOutputStream outputStream) throws IOException {
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java

  /**
   * @returns the partition field prop.
   * @deprecated please use {@link #getPartitionFields()} instead
   */
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java

  /**
   * @returns true is meta fields need to be populated. else returns false.
   */
  public boolean populateMetaFields() {
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java

  /**
   * @returns the record key field prop.
   */
  public String getRawRecordKeyFieldProp() {
```

### JavadocDeclaration
Wrong tag `returns`
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java`
#### Snippet
```java

  /**
   * @returns the record key field prop.
   */
  public String getRecordKeyFieldProp() {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
   * @param includeMetadataFields choice if include metadata fields
   * @return Avro schema for this table
   * @throws Exception
   */
  public Schema getTableAvroSchema(boolean includeMetadataFields) throws Exception {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
   * Read the schema from the log file on path.
   *
   * @return
   */
  public static MessageType readSchemaFromLogFile(FileSystem fs, Path path) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
   *
   * @return  Avro user data schema
   * @throws Exception
   *
   * @deprecated use {@link #getTableAvroSchema(boolean)} instead
```

### JavadocDeclaration
Wrong tag `VisibleForTesting`
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
   * NOTE: This method could only be used in tests
   *
   * @VisibleForTesting
   */
  public boolean hasOperationField() {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java`
#### Snippet
```java
   *
   * @return Avro schema for this table
   * @throws Exception
   */
  public Schema getTableAvroSchema() throws Exception {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java`
#### Snippet
```java
   * Lazily opens the output stream if needed for writing.
   * @return OutputStream for writing to current log file.
   * @throws IOException
   * @throws InterruptedException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java`
#### Snippet
```java
   * @return OutputStream for writing to current log file.
   * @throws IOException
   * @throws InterruptedException
   */
  private FSDataOutputStream getOutputStream() throws IOException, InterruptedException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java`
#### Snippet
```java
   * 
   * @param instantTs Instant Timestamp
   * @return
   */
  public String getMarkerFolderPath(String instantTs) {
```

### JavadocDeclaration
`@param basePath` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   * Main API to get the file-system view for the base-path.
   *
   * @param basePath
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   *
   * @param basePath
   * @return
   */
  public SyncableFileSystemView getFileSystemView(String basePath) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   * @param viewConf View Storage Configuration
   * @param metaClient HoodieTableMetaClient
   * @return
   */
  private static RocksDbBasedFileSystemView createRocksDBBasedFileSystemView(SerializableConfiguration conf,
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   *
   * @param metaClient HoodieTableMetaClient
   * @return
   */
  public SyncableFileSystemView getFileSystemView(HoodieTableMetaClient metaClient) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   * @param viewConf View Storage Configuration
   * @param metaClient HoodieTableMetaClient
   * @return
   */
  private static SpillableMapBasedFileSystemView createSpillableMapBasedFileSystemView(SerializableConfiguration conf,
```

### JavadocDeclaration
`@param basePath` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   * Drops reference to File-System Views. Future calls to view results in creating a new view
   *
   * @param basePath
   */
  public void clearFileSystemView(String basePath) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewManager.java`
#### Snippet
```java
   * @param viewConf View Storage Configuration
   * @param metaClient Hoodie Table MetaClient for the table.
   * @return
   */
  private static RemoteHoodieTableFileSystemView createRemoteFileSystemView(SerializableConfiguration conf,
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java`
#### Snippet
```java
     * @param partitionPath Partition Path
     * @param maxInstantTime Max Instant Time
     * @return
     */
    Stream<FileSlice> getLatestMergedFileSlicesBeforeOrOn(String partitionPath, String maxInstantTime);
```

### JavadocDeclaration
`@param partitionPath` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java`
#### Snippet
```java
   * Get the latest file slices for a given partition including the inflight ones.
   *
   * @param partitionPath
   * @return Stream of latest {@link FileSlice} in the partition path.
   */
```

### JavadocDeclaration
ParseException is not declared to be thrown by method getInstantForDateString
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java`
#### Snippet
```java
   * @param dateString A date-time string in the format yyyy-MM-dd HH:mm:ss[:SSS]
   * @return A timeline instant
   * @throws ParseException If we cannot parse the date string
   */
  public static String getInstantForDateString(String dateString) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Timeline to just include replace instants that have valid (commit/deltacommit) actions.
   *
   * @return
   */
  HoodieTimeline getCompletedReplaceTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Filter this timeline to just include requested and inflight log compaction instants.
   *
   * @return
   */
  HoodieTimeline filterPendingLogCompactionTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Then, a timeline of [C0.completed, C1.completed, C2.completed] will be returned.
   *
   * @return
   */
  HoodieTimeline getContiguousCompletedWriteTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Get hash of timeline.
   * 
   * @return
   */
  String getTimelineHash();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Timeline to just include commits (commit/deltacommit), compaction and replace actions.
   * 
   * @return
   */
  HoodieTimeline getWriteTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Filter this timeline to just include requested and inflight from both major and minor compaction instants.
   *
   * @return
   */
  HoodieTimeline filterPendingMajorOrMinorCompactionTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Filter this timeline to just include requested and inflight compaction instants.
   * 
   * @return
   */
  HoodieTimeline filterPendingCompactionTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java`
#### Snippet
```java
   * Timeline to just include completed commits or all rewrites like compaction, logcompaction and replace actions.
   *
   * @return
   */
  HoodieTimeline filterCompletedInstantsOrRewriteTimeline();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/VersionMigrator.java`
#### Snippet
```java
   * Version of Metadata that this class will handle.
   * 
   * @return
   */
  Integer getManagedVersion();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/TimelineLayoutVersion.java`
#### Snippet
```java
   * For Pre 0.5.1 release, there was no metadata version. This method is used to detect
   * this case.
   * @return
   */
  public boolean isNullVersion() {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/config/DFSPropertiesConfiguration.java`
#### Snippet
```java
   *
   * @param reader Buffered Reader
   * @throws IOException
   */
  public void addPropsFromStream(BufferedReader reader, Path cfgFilePath) throws IOException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/metrics/Registry.java`
#### Snippet
```java
   * @param flush clear all metrics after this operation.
   * @param prefixWithRegistryName prefix each metric name with the registry name.
   * @return
   */
  static Map<String, Long> getAllMetrics(boolean flush, boolean prefixWithRegistryName) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/heartbeat/HoodieHeartbeatUtils.java`
#### Snippet
```java
   * @param instantTime Instant time.
   * @return Last heartbeat timestamp.
   * @throws IOException
   */
  public static Long getLastHeartbeatTime(FileSystem fs, String basePath, String instantTime) throws IOException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/BootstrapIndex.java`
#### Snippet
```java
     * Lookup bootstrap index by partition.
     * @param partition Partition to lookup
     * @return
     */
    public abstract List<BootstrapFileMapping> getSourceFileMappingForPartition(String partition);
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/BootstrapIndex.java`
#### Snippet
```java
    /**
     * Return list of partitions indexed.
     * @return
     */
    public abstract List<String> getIndexedPartitionPaths();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/BootstrapIndex.java`
#### Snippet
```java
    /**
     * Return Source base path.
     * @return
     */
    public abstract String getBootstrapBasePath();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/BootstrapIndex.java`
#### Snippet
```java
  /**
   * Returns true if valid metadata bootstrap is present.
   * @return
   */
  public final boolean useIndex() {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/BootstrapIndex.java`
#### Snippet
```java
     * Lookup Bootstrap index by file group ids.
     * @param ids File Group Ids
     * @return
     */
    public abstract Map<HoodieFileGroupId, BootstrapFileMapping> getSourceFileMappingForFileIds(
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/BootstrapIndex.java`
#### Snippet
```java
    /**
     * Return list file-ids indexed.
     * @return
     */
    public abstract List<HoodieFileGroupId> getIndexedFileGroupIds();
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java`
#### Snippet
```java
   *
   * @param partitionPath The absolute path of the partition
   * @throws IOException
   */
  protected FileStatus[] listPartition(Path partitionPath) throws IOException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
   * This API returns only the user key part from it.
   * @param cellKey HFIle Cell Key
   * @return
   */
  private static String getUserKeyFromCellKey(String cellKey) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
   * Returns file group key to be used in HFile.
   * @param fileGroupId File Group Id.
   * @return
   */
  private static String getFileGroupKey(HoodieFileGroupId fileGroupId) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/common/bootstrap/index/HFileBootstrapIndex.java`
#### Snippet
```java
   * Returns partition-key to be used in HFile.
   * @param partition Partition-Path
   * @return
   */
  private static String getPartitionKey(String partition) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java
     * @param newName new name for the column
     * @return this
     * @throws IllegalArgumentException
     */
    public ColumnUpdateChange renameColumn(String name, String newName) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java
     * @param newDoc new documentation for the column
     * @return this
     * @throws IllegalArgumentException
     */
    public ColumnUpdateChange updateColumnComment(String name, String newDoc) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java
     * @param newType new type for the column
     * @return this
     * @throws IllegalArgumentException
     */
    public ColumnUpdateChange updateColumnType(String name, Type newType) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChanges.java`
#### Snippet
```java
     * @param nullable nullable for updated name
     * @return this
     * @throws IllegalArgumentException
     */
    public ColumnUpdateChange updateColumnNullability(String name, boolean nullable) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadata.java`
#### Snippet
```java
   * @param fileName      - File name for which bloom filter needs to be retrieved
   * @return BloomFilter if available, otherwise empty
   * @throws HoodieMetadataException
   */
  Option<BloomFilter> getBloomFilter(final String partitionName, final String fileName)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadata.java`
#### Snippet
```java
   * @param columnName                - Column name for which stats are needed
   * @return Map of partition and file name pair to its column stats
   * @throws HoodieMetadataException
   */
  Map<Pair<String, String>, HoodieMetadataColumnStats> getColumnStats(final List<Pair<String, String>> partitionNameFileNameList, final String columnName)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadata.java`
#### Snippet
```java
   * @param partitionNameFileNameList - List of partition and file name pair for which bloom filters need to be retrieved
   * @return Map of partition file name pair to its bloom filter
   * @throws HoodieMetadataException
   */
  Map<Pair<String, String>, BloomFilter> getBloomFilters(final List<Pair<String, String>> partitionNameFileNameList)
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataFileSystemView.java`
#### Snippet
```java
   *
   * @param partitionPath The absolute path of the partition
   * @throws IOException
   */
  @Override
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/InputPathHandler.java`
#### Snippet
```java
   * @param incrementalTables - List of all incremental tables extracted from the config
   * `hoodie.&lt;table-name&gt;.consume.mode=INCREMENTAL`
   * @throws IOException
   */
  private void parseInputPaths(Path[] inputPaths, List<String> incrementalTables)
```

### JavadocDeclaration
Tag `return` is not allowed here
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
   *
   * @param realtimeRecordReader recordReader for RealtimeInputFormat.
   * @return
   */
  public void doEvolutionForRealtimeInputFormat(AbstractRealtimeRecordReader realtimeRecordReader) throws Exception {
```

### JavadocDeclaration
`@param job` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
   * Returns a list of tableNames for which hoodie.<tableName>.consume.mode is set to incremental else returns empty List
   *
   * @param job
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
   *
   * @param job
   * @return
   */
  public static List<String> getIncrementalTableNames(JobContext job) {
```

### JavadocDeclaration
`@param path` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
   * Gets the n'th parent for the Path. Assumes the path has at-least n components
   *
   * @param path
   * @param n
   * @return
```

### JavadocDeclaration
`@param n` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
   *
   * @param path
   * @param n
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieHiveUtils.java`
#### Snippet
```java
   * @param path
   * @param n
   * @return
   */
  public static Path getNthParent(Path path, int n) {
```

### JavadocDeclaration
Wrong tag `VisibleInTesting`
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieRealtimeFileSplit.java`
#### Snippet
```java

  /**
   * @VisibleInTesting
   */
  public HoodieRealtimeFileSplit(FileSplit baseSplit,
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeSplit.java`
#### Snippet
```java
  /**
   * Return Base Path of the dataset.
   * @return
   */
  String getBasePath();
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeSplit.java`
#### Snippet
```java
  /**
   * Return Log File Paths.
   * @return
   */
  default List<String> getDeltaLogPaths() {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeSplit.java`
#### Snippet
```java
  /**
   * Returns Virtual key info if meta fields are disabled.
   * @return
   */
  Option<HoodieVirtualKeyInfo> getVirtualKeyInfo();
```

### JavadocDeclaration
`@param basePath` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeSplit.java`
#### Snippet
```java
  /**
   * Set Base Path.
   * @param basePath
   */
  void setBasePath(String basePath);
```

### JavadocDeclaration
`@param maxCommitTime` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeSplit.java`
#### Snippet
```java
  /**
   * Update Maximum valid instant time.
   * @param maxCommitTime
   */
  void setMaxCommitTime(String maxCommitTime);
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeSplit.java`
#### Snippet
```java
  /**
   * Return Max Instant Time.
   * @return
   */
  String getMaxCommitTime();
```

### JavadocDeclaration
`@param conf` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.
   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file
   * @param conf
   * @param dataFile
   * @return
```

### JavadocDeclaration
`@param dataFile` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file
   * @param conf
   * @param dataFile
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param conf
   * @param dataFile
   * @return
   */
  private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFile dataFile) {
```

### JavadocDeclaration
`@param job` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * Get HoodieTimeline for incremental query from Hive map reduce configuration.
   *
   * @param job
   * @param tableName
   * @param timeline
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   *
   * @param job
   * @param tableName
   * @param timeline
   * @return
```

### JavadocDeclaration
`@param timeline` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param job
   * @param tableName
   * @param timeline
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param tableName
   * @param timeline
   * @return
   */
  public static HoodieTimeline getHoodieTimelineForIncrementalQuery(JobContext job, String tableName, HoodieTimeline timeline) {
```

### JavadocDeclaration
`@param job` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
  /**
   * Get commits for incremental query from Hive map reduce configuration.
   * @param job
   * @param tableName
   * @param timeline
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * Get commits for incremental query from Hive map reduce configuration.
   * @param job
   * @param tableName
   * @param timeline
   * @return
```

### JavadocDeclaration
`@param timeline` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param job
   * @param tableName
   * @param timeline
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param tableName
   * @param timeline
   * @return
   */
  public static Option<List<HoodieInstant>> getCommitsForIncrementalQuery(Job job, String tableName, HoodieTimeline timeline) {
```

### JavadocDeclaration
`@param timeline` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * To workaround this problem, we want to stop returning data belonging to commits > t2.
   * After compaction is complete, incremental reader would see updates in t2, t3, so on.
   * @param timeline
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * After compaction is complete, incremental reader would see updates in t2, t3, so on.
   * @param timeline
   * @return
   */
  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {
```

### JavadocDeclaration
`@param fileStatuses` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * based on given table metadata.
   *
   * @param fileStatuses
   * @param fileExtension
   * @param metaClientList
```

### JavadocDeclaration
`@param fileExtension` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   *
   * @param fileStatuses
   * @param fileExtension
   * @param metaClientList
   * @return
```

### JavadocDeclaration
`@param metaClientList` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param fileStatuses
   * @param fileExtension
   * @param metaClientList
   * @return
   * @throws IOException
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param fileExtension
   * @param metaClientList
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param metaClientList
   * @return
   * @throws IOException
   */
  public static Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(
```

### JavadocDeclaration
IOException is not declared to be thrown by method groupFileStatusForSnapshotPaths
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param metaClientList
   * @return
   * @throws IOException
   */
  public static Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(
```

### JavadocDeclaration
`@param commitsToCheck` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
  /**
   * Extract partitions touched by the commitsToCheck.
   * @param commitsToCheck
   * @param tableMetaClient
   * @param timeline
```

### JavadocDeclaration
`@param tableMetaClient` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * Extract partitions touched by the commitsToCheck.
   * @param commitsToCheck
   * @param tableMetaClient
   * @param timeline
   * @param inputPaths
```

### JavadocDeclaration
`@param timeline` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param commitsToCheck
   * @param tableMetaClient
   * @param timeline
   * @param inputPaths
   * @return
```

### JavadocDeclaration
`@param inputPaths` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param tableMetaClient
   * @param timeline
   * @param inputPaths
   * @return
   * @throws IOException
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param timeline
   * @param inputPaths
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param inputPaths
   * @return
   * @throws IOException
   */
  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,
```

### JavadocDeclaration
`@param job` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
  /**
   * Filter a list of FileStatus based on commitsToCheck for incremental view.
   * @param job
   * @param tableMetaClient
   * @param timeline
```

### JavadocDeclaration
`@param tableMetaClient` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * Filter a list of FileStatus based on commitsToCheck for incremental view.
   * @param job
   * @param tableMetaClient
   * @param timeline
   * @param fileStatuses
```

### JavadocDeclaration
`@param timeline` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param job
   * @param tableMetaClient
   * @param timeline
   * @param fileStatuses
   * @param commitsToCheck
```

### JavadocDeclaration
`@param fileStatuses` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param tableMetaClient
   * @param timeline
   * @param fileStatuses
   * @param commitsToCheck
   * @return
```

### JavadocDeclaration
`@param commitsToCheck` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param timeline
   * @param fileStatuses
   * @param commitsToCheck
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param fileStatuses
   * @param commitsToCheck
   * @return
   */
  public static List<FileStatus> filterIncrementalFileStatus(Job job, HoodieTableMetaClient tableMetaClient,
```

### JavadocDeclaration
`@param job` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
  /**
   * Extract HoodieTimeline based on HoodieTableMetaClient.
   * @param job
   * @param tableMetaClient
   * @return
```

### JavadocDeclaration
`@param tableMetaClient` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * Extract HoodieTimeline based on HoodieTableMetaClient.
   * @param job
   * @param tableMetaClient
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java`
#### Snippet
```java
   * @param job
   * @param tableMetaClient
   * @return
   */
  public static Option<HoodieTimeline> getFilteredCommitsTimeline(JobContext job, HoodieTableMetaClient tableMetaClient) {
```

### JavadocDeclaration
`@param pathToPartitionInfo` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
   * HiveFileFormatUtils.getPartitionDescFromPathRecursively is no longer available since Hive 3.
   * This method is to make it compatible with both Hive 2 and Hive 3.
   * @param pathToPartitionInfo
   * @param dir
   * @param cacheMap
```

### JavadocDeclaration
`@param dir` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
   * This method is to make it compatible with both Hive 2 and Hive 3.
   * @param pathToPartitionInfo
   * @param dir
   * @param cacheMap
   * @return
```

### JavadocDeclaration
`@param cacheMap` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
   * @param pathToPartitionInfo
   * @param dir
   * @param cacheMap
   * @return
   * @throws IOException
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
   * @param dir
   * @param cacheMap
   * @return
   * @throws IOException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java`
#### Snippet
```java
   * @param cacheMap
   * @return
   * @throws IOException
   */
  private static PartitionDesc getPartitionFromPath(Map<Path, PartitionDesc> pathToPartitionInfo, Path dir,
```

### JavadocDeclaration
IOException is not declared to be thrown by method listFilesFromBasePath
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
   * @param parallelism   Parallelism for the file listing.
   * @return A list of absolute file paths of all Hoodie files.
   * @throws IOException upon errors.
   */
  static List<String> listFilesFromBasePath(
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
   * @param fileSystemView - hoodie table file system view, which will be fetched from meta client if not already present
   * @param partition      - name of the partition whose file groups are to be loaded
   * @return
   */
  public static List<FileSlice> getPartitionLatestFileSlicesIncludingInflight(HoodieTableMetaClient metaClient,
```

### JavadocDeclaration
`@param cleanMetadata` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
   * Finds all files that were deleted as part of a clean and creates metadata table records for them.
   *
   * @param cleanMetadata
   * @param instantTime
   * @return a list of metadata table records
```

### JavadocDeclaration
`@param instantTime` tag description is missing
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
   *
   * @param cleanMetadata
   * @param instantTime
   * @return a list of metadata table records
   */
```

### JavadocDeclaration
Illegal character
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java

  /**
   * Given a schema, coerces provided value to instance of {@link Comparable<?>} such that
   * it could subsequently used in column stats
   *
```

### JavadocDeclaration
Tag `return` is not allowed here
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
   *
   * @param restoreMetadata - Restore action metadata
   * @return a list of metadata table records
   */
  private static void processRestoreMetadata(HoodieActiveTimeline metadataTableTimeline,
```

### JavadocDeclaration
`@param includeSchemaAndVersion` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   * We need to inject sourceSchema instead of reader schema during deserialization or later stages of the pipeline.
   *
   * @param includeSchemaAndVersion
   * @param topic
   * @param isKey
```

### JavadocDeclaration
`@param topic` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   *
   * @param includeSchemaAndVersion
   * @param topic
   * @param isKey
   * @param payload
```

### JavadocDeclaration
`@param isKey` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   * @param includeSchemaAndVersion
   * @param topic
   * @param isKey
   * @param payload
   * @param readerSchema
```

### JavadocDeclaration
`@param payload` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   * @param topic
   * @param isKey
   * @param payload
   * @param readerSchema
   * @return
```

### JavadocDeclaration
`@param readerSchema` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   * @param isKey
   * @param payload
   * @param readerSchema
   * @return
   * @throws SerializationException
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   * @param payload
   * @param readerSchema
   * @return
   * @throws SerializationException
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
   * @param readerSchema
   * @return
   * @throws SerializationException
   */
  @Override
```

### JavadocDeclaration
`@param partitionToReplaceFileIds` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
   * Prints the delete data files info.
   *
   * @param partitionToReplaceFileIds
   */
  private void printDeleteFilesInfo(Map<String, List<String>> partitionToReplaceFileIds) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
   *
   * @param options - JDBC options that contains url, table and other information.
   * @return
   * @throws SQLException if the driver could not open a JDBC connection.
   */
```

### JavadocDeclaration
`@param options` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
   * call spark function get the schema through jdbc.
   * The code logic implementation refers to spark 2.4.x and spark 3.x.
   * @param options
   * @return
   * @throws Exception
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
   * The code logic implementation refers to spark 2.4.x and spark 3.x.
   * @param options
   * @return
   * @throws Exception
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
   * @param options
   * @return
   * @throws Exception
   */
  public static Schema getJDBCSchema(Map<String, String> options) throws Exception {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
   * Build Spark Context for ingestion/compaction.
   *
   * @return
   */
  public static JavaSparkContext buildSparkContext(String appName, String sparkMaster, String sparkMemory) {
```

### JavadocDeclaration
`@param registryUrl` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java`
#### Snippet
```java
   * header.
   *
   * @param registryUrl
   * @return the Schema in String form.
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java`
#### Snippet
```java
   * @param registryUrl
   * @return the Schema in String form.
   * @throws IOException
   */
  public String fetchSchemaFromRegistry(String registryUrl) throws IOException {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java`
#### Snippet
```java
   * @param lastCkptStr Last Checkpoint
   * @param sourceLimit Source Limit
   * @return
   */
  public final InputBatch<T> fetchNext(Option<String> lastCkptStr, long sourceLimit) {
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link  #MISSING_CHECKPOINT_STRATEGY} allows delta-streamer to decide the checkpoint to consume from when checkpoint is not set.
     * instant when checkpoint is not provided.
     */
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link #HOODIE_SRC_BASE_PATH} is the base-path for the source Hoodie table.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link  #READ_LATEST_INSTANT_ON_MISSING_CKPT} allows delta-streamer to incrementally fetch from latest committed
     * instant when checkpoint is not provided. This config is deprecated. Please refer to {@link #MISSING_CHECKPOINT_STRATEGY}.
     */
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link #HOODIE_SRC_PARTITION_EXTRACTORCLASS} PartitionValueExtractor class to extract partition fields from
     * _hoodie_partition_path.
     */
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link #NUM_INSTANTS_PER_FETCH} allows the max number of instants whose changes can be incrementally fetched.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link #HOODIE_SRC_PARTITION_FIELDS} specifies partition fields that needs to be added to source table after
     * parsing _hoodie_partition_path.
     */
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java`
#### Snippet
```java

    /**
     * {@link  #SOURCE_FILE_FORMAT} is passed to the reader while loading dataset. Default value is parquet.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/S3EventsHoodieIncrSource.java`
#### Snippet
```java
    static final String S3_IGNORE_KEY_SUBSTRING = S3EventsHoodieIncrSourceConfig.S3_IGNORE_KEY_SUBSTRING.key();
    /**
     *{@link #SPARK_DATASOURCE_OPTIONS} is json string, passed to the reader while loading dataset.
     * Example delta streamer conf
     * - --hoodie-conf hoodie.deltastreamer.source.s3incr.spark.datasource.options={"header":"true","encoding":"UTF-8"}
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java`
#### Snippet
```java
    private static final String USER_PROP = "user";
    /**
     * {@value #PASSWORD_PROP} used internally to build jdbc params.
     */
    private static final String PASSWORD_PROP = "password";
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java`
#### Snippet
```java
    private static final String DRIVER_PROP = "driver";
    /**
     * {@value #RDBMS_TABLE_PROP} used internally for jdbc.
     */
    private static final String RDBMS_TABLE_PROP = "dbtable";
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java`
#### Snippet
```java
   * @param properties The JDBC connection properties and data source options.
   * @return The {@link DataFrameReader} to read from RDBMS
   * @throws HoodieException
   */
  private static DataFrameReader validatePropsAndGetDataFrameReader(final SparkSession session,
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java`
#### Snippet
```java
    private static final String URL_PROP = "url";
    /**
     * {@value #USER_PROP} used internally to build jdbc params.
     */
    private static final String USER_PROP = "user";
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/JdbcSource.java`
#### Snippet
```java
    private static final String PASSWORD_PROP = "password";
    /**
     * {@value #DRIVER_PROP} used internally to build jdbc params.
     */
    private static final String DRIVER_PROP = "driver";
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java`
#### Snippet
```java
   * during a run of a {@link HoodieIncrSource}.
   * @param props the usual Hudi props object
   * @return
   */
  public static MissingCheckpointStrategy getMissingCheckpointStrategy(TypedProperties props) {
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java
  public static class Config {
    /**
     * {@link  #S3_SOURCE_QUEUE_URL} is the queue url for cloud object events.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #SOURCE_INPUT_SELECTOR} source input selector.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #S3_SOURCE_QUEUE_VISIBILITY_TIMEOUT} is visibility timeout for messages in queue. After we
     * consume the message, queue will move the consumed messages to in-flight state, these messages
     * can't be consumed again by source for this timeout period.
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #S3_SOURCE_QUEUE_MAX_MESSAGES_PER_REQUEST} is max messages for each request.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #S3_SOURCE_QUEUE_FS} is file system corresponding to queue. For example, for AWS SQS it is s3/s3a.
     */
    @Deprecated
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #S3_QUEUE_LONG_POLL_WAIT} is the long poll wait time in seconds If set as 0 then
     * client will fetch on short poll basis.
     */
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #S3_SOURCE_QUEUE_MAX_MESSAGES_PER_BATCH} is max messages for each batch of delta streamer
     * run. Source will process these maximum number of message at a time.
     */
```

### JavadocDeclaration
Javadoc pointing to itself
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelector.java`
#### Snippet
```java

    /**
     * {@link  #S3_SOURCE_QUEUE_REGION} is the case-sensitive region name of the cloud provider for the queue. For example, "us-east-1".
     */
    @Deprecated
```

### JavadocDeclaration
`@param serializableHadoopConf` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelectorCommon.java`
#### Snippet
```java
   * Here Row is assumed to have the schema [bucket_name, filepath_relative_to_bucket, object_size]
   * @param storageUrlSchemePrefix    Eg: s3:// or gs://. The storage-provider-specific prefix to use within the URL.
   * @param serializableHadoopConf
   * @param checkIfExists             check if each file exists, before adding it to the returned list
   * @return
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/CloudObjectsSelectorCommon.java`
#### Snippet
```java
   * @param serializableHadoopConf
   * @param checkIfExists             check if each file exists, before adding it to the returned list
   * @return
   */
  public static MapPartitionsFunction<Row, CloudObjectMetadata> getCloudObjectMetadataPerPartition(
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   * Check if topic exists.
   * @param consumer kafka consumer
   * @return
   */
  public boolean checkTopicExists(KafkaConsumer consumer)  {
```

### JavadocDeclaration
`@param lastCheckpointStr` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
  /**
   * Check if the checkpoint is a timestamp.
   * @param lastCheckpointStr
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   * Check if the checkpoint is a timestamp.
   * @param lastCheckpointStr
   * @return
   */
  private Boolean isValidTimestampCheckpointType(Option<String> lastCheckpointStr) {
```

### JavadocDeclaration
`@param consumer` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   * Fetch partition infos for given topic.
   *
   * @param consumer
   * @param topicName
   */
```

### JavadocDeclaration
`@param topicName` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   *
   * @param consumer
   * @param topicName
   */
  private List<PartitionInfo> fetchPartitionInfos(KafkaConsumer consumer, String topicName) {
```

### JavadocDeclaration
`@param consumer` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   * 2. output: topicName,partition_num_0:100,partition_num_1:101,partition_num_2:102.
   *
   * @param consumer
   * @param topicName
   * @param timestamp
```

### JavadocDeclaration
`@param topicName` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   *
   * @param consumer
   * @param topicName
   * @param timestamp
   * @return
```

### JavadocDeclaration
`@param timestamp` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   * @param consumer
   * @param topicName
   * @param timestamp
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java`
#### Snippet
```java
   * @param topicName
   * @param timestamp
   * @return
   */
  private Option<String> getOffsetsByTimestamp(KafkaConsumer consumer, List<PartitionInfo> partitionInfoList, Set<TopicPartition> topicPartitions,
```

### JavadocDeclaration
`@param dataset` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/ErrorTableUtils.java`
#### Snippet
```java
  /**
   * validates for constraints on ErrorRecordColumn when ErrorTable enabled configs are set.
   * @param dataset
   */
  public static void validate(Dataset<Row> dataset) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
   * @param filter  PathFilter
   * @return All file status match kafka connect naming convention
   * @throws IOException
   */
  private ArrayList<FileStatus> listAllFileStatus(Path curPath,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
   * @param fs         File System
   * @param properties Bootstrap Writer Properties
   * @throws IOException
   */
  public BootstrapExecutor(HoodieDeltaStreamer.Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration conf,
```

### JavadocDeclaration
`@param eventsRow` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
  /**
   * transform datasets with error events when error table is enabled
   * @param eventsRow
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
   * transform datasets with error events when error table is enabled
   * @param eventsRow
   * @return
   */
  public Option<Dataset<Row>> processErrorEvents(Option<Dataset<Row>> eventsRow,
```

### JavadocDeclaration
`@param srcBatch` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
  /**
   * Sanitize all columns including nested ones as per Avro conventions.
   * @param srcBatch
   * @return sanitized batch.
   */
```

### JavadocDeclaration
`@param inputBatch` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
  /**
   * transform input rdd of json string to generic records with support for adding error events to error table
   * @param inputBatch
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java`
#### Snippet
```java
   * transform input rdd of json string to generic records with support for adding error events to error table
   * @param inputBatch
   * @return
   */
  private JavaRDD<GenericRecord> transformJsonToGenericRdd(InputBatch<JavaRDD<String>> inputBatch) {
```

### JavadocDeclaration
`@param configuration` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
   * Resets target table name and target path using base-path-prefix.
   *
   * @param configuration
   * @param database
   * @param tableName
```

### JavadocDeclaration
`@param database` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
   *
   * @param configuration
   * @param database
   * @param tableName
   * @return
```

### JavadocDeclaration
`@param tableName` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
   * @param configuration
   * @param database
   * @param tableName
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
   * @param database
   * @param tableName
   * @return
   */
  private static String resetTarget(Config configuration, String database, String tableName) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java`
#### Snippet
```java
   * Returns the default Hadoop Configuration.
   *
   * @return
   */
  public static Configuration getDefaultHadoopConf(KafkaConnectConfigs connectConfigs) {
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
     *
     * @param writeClient HoodieWriteClient
     * @return
     */
    protected Boolean onInitializingWriteClient(SparkRDDWriteClient writeClient) {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * @param commitsTimelineOpt commits timeline of interest, including .commit and .deltacommit.
   * @return the checkpoint to resume from if applicable.
   * @throws IOException
   */
  private Option<String> getCheckpointToResume(Option<HoodieTimeline> commitsTimelineOpt) throws IOException {
```

### JavadocDeclaration
Exception is not declared to be thrown by method readFromSource
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * @return Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> Input data read from upstream source, consists
   * of schemaProvider, checkpointStr and hoodieRecord
   * @throws Exception in case of any Exception
   */
  public Pair<SchemaProvider, Pair<String, JavaRDD<HoodieRecord>>> readFromSource(Option<HoodieTimeline> commitsTimelineOpt, String instantTime) throws IOException {
```

### JavadocDeclaration
`@param source` tag description is missing
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/transform/Transformer.java`
#### Snippet
```java
   * Transform source DataStream to target DataStream.
   *
   * @param source
   */
  DataStream<RowData> apply(DataStream<RowData> source);
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java`
#### Snippet
```java
     * A clustering plan will be generated if `schedule` is true.
     *
     * @throws Exception
     * @see HoodieFlinkCompactor
     */
```

### JavadocDeclaration
`@param registryUrl` tag description is missing
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/schema/SchemaRegistryProvider.java`
#### Snippet
```java
   * header.
   *
   * @param registryUrl
   * @return the Schema in String form.
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/schema/SchemaRegistryProvider.java`
#### Snippet
```java
   * @param registryUrl
   * @return the Schema in String form.
   * @throws IOException
   */
  public String fetchSchemaFromRegistry(String registryUrl) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetDataColumnReader.java`
#### Snippet
```java
   * @param valueCount value count
   * @param in         page data
   * @throws IOException
   */
  void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException;
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param lcv       column vector to do initial setup in data collection time
   * @param valueList collection of values that will be fed into the vector later
   * @param category
   * @return int
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return int
   * @throws IOException
   */
  private int collectDataFromParquetPage(
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * if there is more values to read (true).
   *
   * @param category
   * @return boolean
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.13.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return boolean
   * @throws IOException
   */
  private boolean fetchNextValue(LogicalType category) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetDataColumnReader.java`
#### Snippet
```java
   * @param valueCount value count
   * @param in         page data
   * @throws IOException
   */
  void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException;
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param lcv       column vector to do initial setup in data collection time
   * @param valueList collection of values that will be fed into the vector later
   * @param category
   * @return int
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return int
   * @throws IOException
   */
  private int collectDataFromParquetPage(
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * if there is more values to read (true).
   *
   * @param category
   * @return boolean
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.14.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return boolean
   * @throws IOException
   */
  private boolean fetchNextValue(LogicalType category) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetDataColumnReader.java`
#### Snippet
```java
   * @param valueCount value count
   * @param in         page data
   * @throws IOException
   */
  void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException;
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param lcv       column vector to do initial setup in data collection time
   * @param valueList collection of values that will be fed into the vector later
   * @param category
   * @return int
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return int
   * @throws IOException
   */
  private int collectDataFromParquetPage(
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * if there is more values to read (true).
   *
   * @param category
   * @return boolean
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.15.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return boolean
   * @throws IOException
   */
  private boolean fetchNextValue(LogicalType category) throws IOException {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetDataColumnReader.java`
#### Snippet
```java
   * @param valueCount value count
   * @param in         page data
   * @throws IOException
   */
  void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException;
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ParquetDataColumnReader.java`
#### Snippet
```java
   * @param valueCount value count
   * @param in         page data
   * @throws IOException
   */
  void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException;
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param lcv       column vector to do initial setup in data collection time
   * @param valueList collection of values that will be fed into the vector later
   * @param category
   * @return int
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return int
   * @throws IOException
   */
  private int collectDataFromParquetPage(
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * if there is more values to read (true).
   *
   * @param category
   * @return boolean
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.16.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return boolean
   * @throws IOException
   */
  private boolean fetchNextValue(LogicalType category) throws IOException {
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * if there is more values to read (true).
   *
   * @param category
   * @return boolean
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return boolean
   * @throws IOException
   */
  private boolean fetchNextValue(LogicalType category) throws IOException {
```

### JavadocDeclaration
`@param category` tag description is missing
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param lcv       column vector to do initial setup in data collection time
   * @param valueList collection of values that will be fed into the vector later
   * @param category
   * @return int
   * @throws IOException
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-flink-datasource/hudi-flink1.17.x/src/main/java/org/apache/hudi/table/format/cow/vector/reader/ArrayColumnReader.java`
#### Snippet
```java
   * @param category
   * @return int
   * @throws IOException
   */
  private int collectDataFromParquetPage(
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/cli/BootstrapExecutorUtils.java`
#### Snippet
```java
   * @param fs         File System
   * @param properties Bootstrap Writer Properties
   * @throws IOException
   */
  public BootstrapExecutorUtils(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration conf,
```

### JavadocDeclaration
`@param streamingInput` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * Hoodie spark streaming job.
   * 
   * @param streamingInput
   * @throws Exception
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * 
   * @param streamingInput
   * @throws Exception
   */
  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
  /**
   *
   * @throws Exception
   */
  public void run() throws Exception {
```

### JavadocDeclaration
`@param writer` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * Setup configs for syncing to hive.
   * 
   * @param writer
   * @return
   */
```

### JavadocDeclaration
`@return` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * 
   * @param writer
   * @return
   */
  private DataStreamWriter<Row> updateHiveSyncConfig(DataStreamWriter<Row> writer) {
```

### JavadocDeclaration
`@param spark` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * Adding data to the streaming source and showing results over time.
   * 
   * @param spark
   * @param fs
   * @param inputDF1
```

### JavadocDeclaration
`@param fs` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * 
   * @param spark
   * @param fs
   * @param inputDF1
   * @param inputDF2
```

### JavadocDeclaration
`@param inputDF1` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * @param spark
   * @param fs
   * @param inputDF1
   * @param inputDF2
   * @throws Exception
```

### JavadocDeclaration
`@param inputDF2` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * @param fs
   * @param inputDF1
   * @param inputDF2
   * @throws Exception
   */
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
   * @param inputDF1
   * @param inputDF2
   * @throws Exception
   */
  public int addInputAndValidateIngestion(SparkSession spark, FileSystem fs, String srcPath,
```

### JavadocDeclaration
`@throws` tag description is missing
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/RequestHandler.java`
#### Snippet
```java
   * @param logger          {@code Logger} instance
   * @return JSON String from the input object
   * @throws JsonProcessingException
   */
  public static String jsonifyResult(
```

## RuleId[id=FieldMayBeFinal]
### FieldMayBeFinal
Field `is` may be 'final'
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/InputStreamConsumer.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(InputStreamConsumer.class);
  private InputStream is;

  public InputStreamConsumer(InputStream is) {
```

### FieldMayBeFinal
Field `jsc` may be 'final'
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SparkTempViewProvider.class);

  private JavaSparkContext jsc;
  private SQLContext sqlContext;

```

### FieldMayBeFinal
Field `sqlContext` may be 'final'
in `hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkTempViewProvider.java`
#### Snippet
```java

  private JavaSparkContext jsc;
  private SQLContext sqlContext;

  public SparkTempViewProvider(String appName) {
```

### FieldMayBeFinal
Field `updateColumnTypes` may be 'final'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/SchemaDifference.java`
#### Snippet
```java
    private final Map<String, String> tableSchema;
    private List<String> deleteColumns;
    private Map<String, String> updateColumnTypes;
    private Map<String, String> addColumnTypes;

```

### FieldMayBeFinal
Field `deleteColumns` may be 'final'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/SchemaDifference.java`
#### Snippet
```java
    private final MessageType storageSchema;
    private final Map<String, String> tableSchema;
    private List<String> deleteColumns;
    private Map<String, String> updateColumnTypes;
    private Map<String, String> addColumnTypes;
```

### FieldMayBeFinal
Field `addColumnTypes` may be 'final'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/SchemaDifference.java`
#### Snippet
```java
    private List<String> deleteColumns;
    private Map<String, String> updateColumnTypes;
    private Map<String, String> addColumnTypes;

    public Builder(MessageType storageSchema, Map<String, String> tableSchema) {
```

### FieldMayBeFinal
Field `xformMap` may be 'final'
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/util/ColumnNameXLator.java`
#### Snippet
```java
public class ColumnNameXLator {

  private static Map<String, String> xformMap = new HashMap<>();

  public static String translateNestedColumn(String colName) {
```

### FieldMayBeFinal
Field `inputRecords` may be 'final'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertCommitActionExecutor.java`
#### Snippet
```java
public class JavaInsertCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {

  private List<HoodieRecord<T>> inputRecords;

  public JavaInsertCommitActionExecutor(HoodieEngineContext context,
```

### FieldMayBeFinal
Field `inputRecords` may be 'final'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertCommitActionExecutor.java`
#### Snippet
```java
public class JavaUpsertCommitActionExecutor<T> extends BaseJavaCommitActionExecutor<T> {

  private List<HoodieRecord<T>> inputRecords;

  public JavaUpsertCommitActionExecutor(HoodieEngineContext context,
```

### FieldMayBeFinal
Field `updateLocationToBucket` may be 'final'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
   * Helps decide which bucket an incoming update should go to.
   */
  private HashMap<String, Integer> updateLocationToBucket;
  /**
   * Helps us pack inserts into 1 or more buckets depending on number of incoming records.
```

### FieldMayBeFinal
Field `partitionPathToInsertBucketInfos` may be 'final'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
   * Helps us pack inserts into 1 or more buckets depending on number of incoming records.
   */
  private HashMap<String, List<InsertBucketCumulativeWeightPair>> partitionPathToInsertBucketInfos;
  /**
   * Remembers what type each bucket is for later.
```

### FieldMayBeFinal
Field `workloadProfile` may be 'final'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
   * Stat for the input and output workload. Describe the workload before and after being assigned buckets.
   */
  private WorkloadProfile workloadProfile;
  /**
   * Helps decide which bucket an incoming update should go to.
```

### FieldMayBeFinal
Field `bucketInfoMap` may be 'final'
in `hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java`
#### Snippet
```java
   * Remembers what type each bucket is for later.
   */
  private HashMap<Integer, BucketInfo> bucketInfoMap;

  protected final HoodieTable table;
```

### FieldMayBeFinal
Field `inputRecords` may be 'final'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkInsertCommitActionExecutor.java`
#### Snippet
```java
public class FlinkInsertCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {

  private List<HoodieRecord<T>> inputRecords;

  public FlinkInsertCommitActionExecutor(HoodieEngineContext context,
```

### FieldMayBeFinal
Field `inputRecords` may be 'final'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/FlinkUpsertCommitActionExecutor.java`
#### Snippet
```java
public class FlinkUpsertCommitActionExecutor<T> extends BaseFlinkCommitActionExecutor<T> {

  private List<HoodieRecord<T>> inputRecords;

  public FlinkUpsertCommitActionExecutor(HoodieEngineContext context,
```

### FieldMayBeFinal
Field `flinkRuntimeContext` may be 'final'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java`
#### Snippet
```java
 */
public class FlinkTaskContextSupplier extends TaskContextSupplier {
  private RuntimeContext flinkRuntimeContext;

  public FlinkTaskContextSupplier(RuntimeContext flinkRuntimeContext) {
```

### FieldMayBeFinal
Field `handle` may be 'final'
in `hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/execution/ExplicitWriteHandler.java`
#### Snippet
```java
  private final List<WriteStatus> statuses = new ArrayList<>();

  private HoodieWriteHandle handle;

  public ExplicitWriteHandler(HoodieWriteHandle handle) {
```

### FieldMayBeFinal
Field `readerIterators` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkParquetReader.java`
#### Snippet
```java
  private final Configuration conf;
  private final BaseFileUtils parquetUtils;
  private List<ParquetReaderIterator> readerIterators = new ArrayList<>();

  public HoodieSparkParquetReader(Configuration conf, Path path) {
```

### FieldMayBeFinal
Field `partitions` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(BucketizedBloomCheckPartitioner.class);

  private int partitions;

  /**
```

### FieldMayBeFinal
Field `fileGroupToPartitions` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java`
#### Snippet
```java
   * Stores the final mapping of a file group to a list of partitions for its keys.
   */
  private Map<HoodieFileGroupId, List<Integer>> fileGroupToPartitions;

  /**
```

### FieldMayBeFinal
Field `partitions` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java`
#### Snippet
```java
    extends SparkInsertOverwriteCommitActionExecutor<T> {

  private List<String> partitions;
  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,
                                                  HoodieWriteConfig config, HoodieTable table,
```

### FieldMayBeFinal
Field `bucketInfoMap` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java`
#### Snippet
```java
   * Remembers what type each bucket is for later.
   */
  private HashMap<Integer, BucketInfo> bucketInfoMap;

  protected final HoodieWriteConfig config;
```

### FieldMayBeFinal
Field `partitionPathToInsertBucketInfos` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java`
#### Snippet
```java
   * Helps us pack inserts into 1 or more buckets depending on number of incoming records.
   */
  private HashMap<String, List<InsertBucketCumulativeWeightPair>> partitionPathToInsertBucketInfos;
  /**
   * Remembers what type each bucket is for later.
```

### FieldMayBeFinal
Field `updateLocationToBucket` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java`
#### Snippet
```java
   * Helps decide which bucket an incoming update should go to.
   */
  private HashMap<String, Integer> updateLocationToBucket;
  /**
   * Helps us pack inserts into 1 or more buckets depending on number of incoming records.
```

### FieldMayBeFinal
Field `context` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkCompactor.java`
#### Snippet
```java
    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {
  private static final Logger LOG = LoggerFactory.getLogger(HoodieSparkCompactor.class);
  private transient HoodieEngineContext context;

  public HoodieSparkCompactor(BaseHoodieWriteClient<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> compactionClient,
```

### FieldMayBeFinal
Field `totalPartitions` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
   */
  public static class WriteStatusPartitioner extends Partitioner {
    private int totalPartitions;
    final Map<String, Integer> fileIdPartitionMap;

```

### FieldMayBeFinal
Field `hoodieTable` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDReadClient.java`
#### Snippet
```java
   */
  private final transient HoodieIndex<?, ?> index;
  private HoodieTable hoodieTable;
  private transient Option<SQLContext> sqlContextOpt;
  private final transient HoodieSparkEngineContext context;
```

### FieldMayBeFinal
Field `bootstrapSourceFileSystem` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SparkBootstrapCommitActionExecutor.class);
  protected String bootstrapSchema = null;
  private transient FileSystem bootstrapSourceFileSystem;

  public SparkBootstrapCommitActionExecutor(HoodieSparkEngineContext context,
```

### FieldMayBeFinal
Field `engineContext` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java`
#### Snippet
```java

  private HoodieSparkTable<T> table;
  private HoodieEngineContext engineContext;
  private HoodieWriteConfig writeConfig;

```

### FieldMayBeFinal
Field `writeConfig` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java`
#### Snippet
```java
  private HoodieSparkTable<T> table;
  private HoodieEngineContext engineContext;
  private HoodieWriteConfig writeConfig;

  protected SparkPreCommitValidator(HoodieSparkTable<T> table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {
```

### FieldMayBeFinal
Field `table` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/validator/SparkPreCommitValidator.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SparkPreCommitValidator.class);

  private HoodieSparkTable<T> table;
  private HoodieEngineContext engineContext;
  private HoodieWriteConfig writeConfig;
```

### FieldMayBeFinal
Field `name` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/metrics/DistributedRegistry.java`
#### Snippet
```java
public class DistributedRegistry extends AccumulatorV2<Map<String, Long>, Map<String, Long>>
    implements Registry, Serializable {
  private String name;
  ConcurrentHashMap<String, Long> counters = new ConcurrentHashMap<>();

```

### FieldMayBeFinal
Field `useWriterSchema` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
  private HoodieWriteConfig config;
  private HoodieTable hoodieTable;
  private boolean useWriterSchema;
  private BulkInsertPartitioner partitioner;
  private WriteHandleFactory writeHandleFactory;
```

### FieldMayBeFinal
Field `config` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
  private String instantTime;
  private boolean areRecordsSorted;
  private HoodieWriteConfig config;
  private HoodieTable hoodieTable;
  private boolean useWriterSchema;
```

### FieldMayBeFinal
Field `instantTime` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
    implements Function2<Integer, Iterator<HoodieRecord<T>>, Iterator<List<WriteStatus>>> {

  private String instantTime;
  private boolean areRecordsSorted;
  private HoodieWriteConfig config;
```

### FieldMayBeFinal
Field `areRecordsSorted` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java

  private String instantTime;
  private boolean areRecordsSorted;
  private HoodieWriteConfig config;
  private HoodieTable hoodieTable;
```

### FieldMayBeFinal
Field `partitioner` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
  private HoodieTable hoodieTable;
  private boolean useWriterSchema;
  private BulkInsertPartitioner partitioner;
  private WriteHandleFactory writeHandleFactory;

```

### FieldMayBeFinal
Field `writeHandleFactory` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
  private boolean useWriterSchema;
  private BulkInsertPartitioner partitioner;
  private WriteHandleFactory writeHandleFactory;

  public BulkInsertMapFunction(String instantTime, boolean areRecordsSorted,
```

### FieldMayBeFinal
Field `hoodieTable` may be 'final'
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java`
#### Snippet
```java
  private boolean areRecordsSorted;
  private HoodieWriteConfig config;
  private HoodieTable hoodieTable;
  private boolean useWriterSchema;
  private BulkInsertPartitioner partitioner;
```

### FieldMayBeFinal
Field `compactor` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/AsyncCompactService.java`
#### Snippet
```java
  private final int maxConcurrentCompaction;
  protected transient HoodieEngineContext context;
  private transient BaseCompactor compactor;

  public AsyncCompactService(HoodieEngineContext context, BaseHoodieWriteClient client) {
```

### FieldMayBeFinal
Field `consumed` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
  private transient ReentrantLock queueLock = new ReentrantLock();
  // Condition instance to use with the queueLock
  private transient Condition consumed = queueLock.newCondition();

  protected HoodieAsyncService() {
```

### FieldMayBeFinal
Field `pendingInstants` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
  private final boolean runInDaemonMode;
  // Queue to hold pending compaction/clustering instants
  private transient BlockingQueue<HoodieInstant> pendingInstants = new LinkedBlockingQueue<>();
  // Mutex lock for synchronized access to pendingInstants queue
  private transient ReentrantLock queueLock = new ReentrantLock();
```

### FieldMayBeFinal
Field `clusteringClient` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/AsyncClusteringService.java`
#### Snippet
```java
  private final int maxConcurrentClustering;
  protected transient HoodieEngineContext context;
  private transient BaseClusterer clusteringClient;

  public AsyncClusteringService(HoodieEngineContext context, BaseHoodieWriteClient writeClient) {
```

### FieldMayBeFinal
Field `queueLock` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/async/HoodieAsyncService.java`
#### Snippet
```java
  private transient BlockingQueue<HoodieInstant> pendingInstants = new LinkedBlockingQueue<>();
  // Mutex lock for synchronized access to pendingInstants queue
  private transient ReentrantLock queueLock = new ReentrantLock();
  // Condition instance to use with the queueLock
  private transient Condition consumed = queueLock.newCondition();
```

### FieldMayBeFinal
Field `hoodieWriteConfig` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/hbase/DefaultHBaseQPSResourceAllocator.java`
#### Snippet
```java

public class DefaultHBaseQPSResourceAllocator implements HBaseIndexQPSResourceAllocator {
  private HoodieWriteConfig hoodieWriteConfig;
  private static final Logger LOG = LoggerFactory.getLogger(DefaultHBaseQPSResourceAllocator.class);

```

### FieldMayBeFinal
Field `recordProperties` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
  private boolean useWriterSchema = false;

  private Properties recordProperties = new Properties();

  /**
```

### FieldMayBeFinal
Field `sizeEstimator` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java`
#### Snippet
```java
  // Header metadata for a log block
  protected final Map<HeaderMetadataType, String> header = new HashMap<>();
  private SizeEstimator<HoodieRecord> sizeEstimator;
  // Instant time of the basefile on which append operation is performed.
  private String baseInstantTime;
```

### FieldMayBeFinal
Field `insertLocationToCount` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/WorkloadStat.java`
#### Snippet
```java
  private long numUpdates = 0L;

  private HashMap<String, Pair<String, Long>> insertLocationToCount;

  private HashMap<String, Pair<String, Long>> updateLocationToCount;
```

### FieldMayBeFinal
Field `updateLocationToCount` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/WorkloadStat.java`
#### Snippet
```java
  private HashMap<String, Pair<String, Long>> insertLocationToCount;

  private HashMap<String, Pair<String, Long>> updateLocationToCount;

  public WorkloadStat() {
```

### FieldMayBeFinal
Field `hadoopConfiguration` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java`
#### Snippet
```java
  protected final HoodieTableMetaClient metaClient;
  protected final HoodieIndex<?, ?> index;
  private SerializableConfiguration hadoopConfiguration;
  protected final TaskContextSupplier taskContextSupplier;
  private final HoodieTableMetadata metadata;
```

### FieldMayBeFinal
Field `context` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
  private HoodieTable<T, I, K, O> hoodieTable;
  private HoodieWriteConfig config;
  private transient HoodieEngineContext context;

  public CleanPlanner(HoodieEngineContext context, HoodieTable<T, I, K, O> hoodieTable, HoodieWriteConfig config) {
```

### FieldMayBeFinal
Field `hoodieTable` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
  private final Map<HoodieFileGroupId, CompactionOperation> fgIdToPendingCompactionOperations;
  private final Map<HoodieFileGroupId, CompactionOperation> fgIdToPendingLogCompactionOperations;
  private HoodieTable<T, I, K, O> hoodieTable;
  private HoodieWriteConfig config;
  private transient HoodieEngineContext context;
```

### FieldMayBeFinal
Field `config` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/CleanPlanner.java`
#### Snippet
```java
  private final Map<HoodieFileGroupId, CompactionOperation> fgIdToPendingLogCompactionOperations;
  private HoodieTable<T, I, K, O> hoodieTable;
  private HoodieWriteConfig config;
  private transient HoodieEngineContext context;

```

### FieldMayBeFinal
Field `operationType` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/RunCompactionActionExecutor.java`
#### Snippet
```java
  private final HoodieCompactor compactor;
  private final HoodieCompactionHandler compactionHandler;
  private WriteOperationType operationType;

  public RunCompactionActionExecutor(HoodieEngineContext context,
```

### FieldMayBeFinal
Field `operationType` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/ScheduleCompactionActionExecutor.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(ScheduleCompactionActionExecutor.class);
  private WriteOperationType operationType;
  private final Option<Map<String, String>> extraMetadata;
  private BaseHoodieCompactionPlanGenerator planGenerator;
```

### FieldMayBeFinal
Field `propsFilePath` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
  private transient FileSystem fs;
  private Path updatedPropsFilePath;
  private Path propsFilePath;

  public UpgradeDowngrade(
```

### FieldMayBeFinal
Field `fs` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
  protected HoodieWriteConfig config;
  protected HoodieEngineContext context;
  private transient FileSystem fs;
  private Path updatedPropsFilePath;
  private Path propsFilePath;
```

### FieldMayBeFinal
Field `updatedPropsFilePath` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java
  protected HoodieEngineContext context;
  private transient FileSystem fs;
  private Path updatedPropsFilePath;
  private Path propsFilePath;

```

### FieldMayBeFinal
Field `metaClient` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java`
#### Snippet
```java

  private final SupportsUpgradeDowngrade upgradeDowngradeHelper;
  private HoodieTableMetaClient metaClient;
  protected HoodieWriteConfig config;
  protected HoodieEngineContext context;
```

### FieldMayBeFinal
Field `context` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/embedded/EmbeddedTimelineService.java`
#### Snippet
```java
  private int serverPort;
  private String hostAddr;
  private HoodieEngineContext context;
  private final SerializableConfiguration hadoopConf;
  private final HoodieWriteConfig writeConfig;
```

### FieldMayBeFinal
Field `instantToHeartbeatMap` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  private Integer numTolerableHeartbeatMisses;
  private final Long maxAllowableHeartbeatIntervalInMs;
  private Map<String, Heartbeat> instantToHeartbeatMap;

  public HoodieHeartbeatClient(FileSystem fs, String basePath, Long heartbeatIntervalInMs,
```

### FieldMayBeFinal
Field `numTolerableHeartbeatMisses` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  // heartbeat interval in millis
  private final Long heartbeatIntervalInMs;
  private Integer numTolerableHeartbeatMisses;
  private final Long maxAllowableHeartbeatIntervalInMs;
  private Map<String, Heartbeat> instantToHeartbeatMap;
```

### FieldMayBeFinal
Field `heartbeatFolderPath` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/heartbeat/HoodieHeartbeatClient.java`
#### Snippet
```java
  private final String basePath;
  // path to the heartbeat folder where all writers are updating their heartbeats
  private String heartbeatFolderPath;
  // heartbeat interval in millis
  private final Long heartbeatIntervalInMs;
```

### FieldMayBeFinal
Field `metrics` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/LockManager.java`
#### Snippet
```java
  private final int maxRetries;
  private final long maxWaitTimeInMs;
  private transient HoodieLockMetrics metrics;
  private volatile LockProvider lockProvider;

```

### FieldMayBeFinal
Field `lockInfo` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/FileSystemBasedLockProvider.java`
#### Snippet
```java
  protected LockConfiguration lockConfiguration;
  private SimpleDateFormat sdf;
  private LockInfo lockInfo;
  private String currentOwnerLockInfo;

```

### FieldMayBeFinal
Field `sdf` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/FileSystemBasedLockProvider.java`
#### Snippet
```java
  private final transient Path lockFile;
  protected LockConfiguration lockConfiguration;
  private SimpleDateFormat sdf;
  private LockInfo lockInfo;
  private String currentOwnerLockInfo;
```

### FieldMayBeFinal
Field `engineType` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java`
#### Snippet
```java
  public static final String DEFAULT_SIMPLE_INDEX_UPDATE_PARTITION_PATH = SIMPLE_INDEX_UPDATE_PARTITION_PATH_ENABLE.defaultValue();

  private EngineType engineType;

  /**
```

### FieldMayBeFinal
Field `hoodieMetricsPrometheusConfig` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/metrics/HoodieMetricsPrometheusConfig.java`
#### Snippet
```java
  public static class Builder {

    private HoodieMetricsPrometheusConfig hoodieMetricsPrometheusConfig = new HoodieMetricsPrometheusConfig();

    public Builder fromProperties(Properties props) {
```

### FieldMayBeFinal
Field `serverHost` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/MetricsGraphiteReporter.java`
#### Snippet
```java
  private final GraphiteReporter graphiteReporter;
  private final HoodieWriteConfig config;
  private String serverHost;
  private int serverPort;
  private final int periodSeconds;
```

### FieldMayBeFinal
Field `serverPort` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/MetricsGraphiteReporter.java`
#### Snippet
```java
  private final HoodieWriteConfig config;
  private String serverHost;
  private int serverPort;
  private final int periodSeconds;

```

### FieldMayBeFinal
Field `registry` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/custom/CustomizableMetricsReporter.java`
#### Snippet
```java
public abstract class CustomizableMetricsReporter extends MetricsReporter {
  private Properties props;
  private MetricRegistry registry;

  public CustomizableMetricsReporter(Properties props, MetricRegistry registry) {
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/custom/CustomizableMetricsReporter.java`
#### Snippet
```java
 */
public abstract class CustomizableMetricsReporter extends MetricsReporter {
  private Properties props;
  private MetricRegistry registry;

```

### FieldMayBeFinal
Field `mapper` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/callback/util/HoodieWriteCommitCallbackUtil.java`
#### Snippet
```java
public class HoodieWriteCommitCallbackUtil {

  private static ObjectMapper mapper = new ObjectMapper();

  /**
```

### FieldMayBeFinal
Field `httpServer` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/prometheus/PrometheusReporter.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(PrometheusReporter.class);

  private HTTPServer httpServer;
  private final DropwizardExports metricExports;
  private final CollectorRegistry collectorRegistry;
```

### FieldMayBeFinal
Field `config` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java`
#### Snippet
```java
  private String conflictResolutionSuccessCounterName = null;
  private String conflictResolutionFailureCounterName = null;
  private HoodieWriteConfig config;
  private String tableName;
  private Timer rollbackTimer = null;
```

### FieldMayBeFinal
Field `tableName` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java`
#### Snippet
```java
  private String conflictResolutionFailureCounterName = null;
  private HoodieWriteConfig config;
  private String tableName;
  private Timer rollbackTimer = null;
  private Timer cleanTimer = null;
```

### FieldMayBeFinal
Field `orcConfig` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroOrcWriter.java`
#### Snippet
```java
  private final TaskContextSupplier taskContextSupplier;

  private HoodieOrcConfig orcConfig;
  private String minRecordKey;
  private String maxRecordKey;
```

### FieldMayBeFinal
Field `DROP_BEHIND_CACHE_COMPACTION_KEY` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java

  // This is private in CacheConfig so have been copied here.
  private static String DROP_BEHIND_CACHE_COMPACTION_KEY = "hbase.hfile.drop.behind.compaction";

  public HoodieAvroHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileConfig, Schema schema,
```

### FieldMayBeFinal
Field `hfileConfig` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java

  private final Path file;
  private HoodieHFileConfig hfileConfig;
  private final HoodieWrapperFileSystem fs;
  private final long maxFileSize;
```

### FieldMayBeFinal
Field `recordIndex` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroHFileWriter.java`
#### Snippet
```java
public class HoodieAvroHFileWriter
    implements HoodieAvroFileWriter {
  private static AtomicLong recordIndex = new AtomicLong(1);

  private final Path file;
```

### FieldMayBeFinal
Field `isOptimizeConfigSet` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java
    private boolean isArchivalConfigSet = false;
    private boolean isClusteringConfigSet = false;
    private boolean isOptimizeConfigSet = false;
    private boolean isMetricsConfigSet = false;
    private boolean isBootstrapConfigSet = false;
```

### FieldMayBeFinal
Field `engineType` may be 'final'
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java`
#### Snippet
```java
  private HoodieCommonConfig commonConfig;
  private HoodieStorageConfig storageConfig;
  private EngineType engineType;

  /**
```

### FieldMayBeFinal
Field `cachedAllInputFileSlices` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java`
#### Snippet
```java
  // NOTE: Individual partitions are always cached in full: meaning that if partition is cached
  //       it will hold all the file-slices residing w/in the partition
  private transient volatile Map<PartitionPath, List<FileSlice>> cachedAllInputFileSlices = new HashMap<>();

  // NOTE: It always contains either all partition paths, or null if it is not initialized yet
```

### FieldMayBeFinal
Field `outputStream` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareDataOutputStream.java`
#### Snippet
```java

  // Actual outputStream
  private DataOutputStream outputStream;
  // Counter to keep track of number of bytes written
  private AtomicLong size;
```

### FieldMayBeFinal
Field `size` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/SizeAwareDataOutputStream.java`
#### Snippet
```java
  private DataOutputStream outputStream;
  // Counter to keep track of number of bytes written
  private AtomicLong size;

  public SizeAwareDataOutputStream(FileOutputStream fileOutputStream, int cacheSize) {
```

### FieldMayBeFinal
Field `DEFAULT_OPTIMISTIC_CONSISTENCY_GUARD_SLEEP_TIME_MS_PROP` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java`
#### Snippet
```java
   */
  @Deprecated
  private static long DEFAULT_OPTIMISTIC_CONSISTENCY_GUARD_SLEEP_TIME_MS_PROP = OPTIMISTIC_CONSISTENCY_GUARD_SLEEP_TIME_MS.defaultValue();
  /**
   * @deprecated use {@link #OPTIMISTIC_CONSISTENCY_GUARD_ENABLE} and its methods.
```

### FieldMayBeFinal
Field `DEFAULT_INITIAL_CONSISTENCY_CHECK_INTERVAL_MS` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java`
#### Snippet
```java
   */
  @Deprecated
  private static long DEFAULT_INITIAL_CONSISTENCY_CHECK_INTERVAL_MS = INITIAL_CHECK_INTERVAL_MS.defaultValue();
  /**
   * @deprecated use {@link #MAX_CHECK_INTERVAL_MS} and its methods.
```

### FieldMayBeFinal
Field `DEFAULT_ENABLE_OPTIMISTIC_CONSISTENCY_GUARD` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java`
#### Snippet
```java
   */
  @Deprecated
  private static boolean DEFAULT_ENABLE_OPTIMISTIC_CONSISTENCY_GUARD = OPTIMISTIC_CONSISTENCY_GUARD_ENABLE.defaultValue();
}

```

### FieldMayBeFinal
Field `DEFAULT_MAX_CONSISTENCY_CHECKS` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java`
#### Snippet
```java
   */
  @Deprecated
  private static int DEFAULT_MAX_CONSISTENCY_CHECKS = MAX_CHECKS.defaultValue();
  /**
   * @deprecated use {@link #OPTIMISTIC_CONSISTENCY_GUARD_SLEEP_TIME_MS} and its methods.
```

### FieldMayBeFinal
Field `DEFAULT_MAX_CONSISTENCY_CHECK_INTERVAL_MS` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/ConsistencyGuardConfig.java`
#### Snippet
```java
   */
  @Deprecated
  private static long DEFAULT_MAX_CONSISTENCY_CHECK_INTERVAL_MS = MAX_CHECK_INTERVAL_MS.defaultValue();
  /**
   * @deprecated use {@link #MAX_CHECKS} and its methods.
```

### FieldMayBeFinal
Field `supportAtomicCreation` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java
  private Boolean isWriteTransactional;
  // null for uncertain if dfs support atomic create&delete, please update this for each FS
  private Boolean supportAtomicCreation;

  StorageSchemes(String scheme, boolean supportsAppend, Boolean isWriteTransactional, Boolean supportAtomicCreation) {
```

### FieldMayBeFinal
Field `isWriteTransactional` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java
  private boolean supportsAppend;
  // null for uncertain if write is transactional, please update this for each FS
  private Boolean isWriteTransactional;
  // null for uncertain if dfs support atomic create&delete, please update this for each FS
  private Boolean supportAtomicCreation;
```

### FieldMayBeFinal
Field `supportsAppend` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java

  private String scheme;
  private boolean supportsAppend;
  // null for uncertain if write is transactional, please update this for each FS
  private Boolean isWriteTransactional;
```

### FieldMayBeFinal
Field `scheme` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/StorageSchemes.java`
#### Snippet
```java
  CFS("cfs", true, null, null);

  private String scheme;
  private boolean supportsAppend;
  // null for uncertain if write is transactional, please update this for each FS
```

### FieldMayBeFinal
Field `fs` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/BoundedFsDataInputStream.java`
#### Snippet
```java
 */
public class BoundedFsDataInputStream extends FSDataInputStream {
  private FileSystem fs;
  private Path file;
  private long fileLen = -1L;
```

### FieldMayBeFinal
Field `file` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/BoundedFsDataInputStream.java`
#### Snippet
```java
public class BoundedFsDataInputStream extends FSDataInputStream {
  private FileSystem fs;
  private Path file;
  private long fileLen = -1L;

```

### FieldMayBeFinal
Field `invalidCharMask` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java

    private boolean shouldSanitize;
    private String invalidCharMask;

    public HoodieJsonToAvroConversionException(Object value, String fieldName, Schema schema, boolean shouldSanitize, String invalidCharMask) {
```

### FieldMayBeFinal
Field `shouldSanitize` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
    private Schema schema;

    private boolean shouldSanitize;
    private String invalidCharMask;

```

### FieldMayBeFinal
Field `fieldName` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java

    private Object value;
    private String fieldName;
    private Schema schema;

```

### FieldMayBeFinal
Field `value` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
  public static class HoodieJsonToAvroConversionException extends HoodieException {

    private Object value;
    private String fieldName;
    private Schema schema;
```

### FieldMayBeFinal
Field `schema` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java`
#### Snippet
```java
    private Object value;
    private String fieldName;
    private Schema schema;

    private boolean shouldSanitize;
```

### FieldMayBeFinal
Field `maxRetryNumbers` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java
  private FileSystem fileSystem;
  private long maxRetryIntervalMs;
  private int maxRetryNumbers;
  private long initialRetryIntervalMs;
  private String retryExceptionsList;
```

### FieldMayBeFinal
Field `retryExceptionsList` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java
  private int maxRetryNumbers;
  private long initialRetryIntervalMs;
  private String retryExceptionsList;

  public HoodieRetryWrapperFileSystem(FileSystem fs, long maxRetryIntervalMs, int maxRetryNumbers, long initialRetryIntervalMs, String retryExceptions) {
```

### FieldMayBeFinal
Field `initialRetryIntervalMs` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java
  private long maxRetryIntervalMs;
  private int maxRetryNumbers;
  private long initialRetryIntervalMs;
  private String retryExceptionsList;

```

### FieldMayBeFinal
Field `maxRetryIntervalMs` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java

  private FileSystem fileSystem;
  private long maxRetryIntervalMs;
  private int maxRetryNumbers;
  private long initialRetryIntervalMs;
```

### FieldMayBeFinal
Field `fileSystem` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieRetryWrapperFileSystem.java`
#### Snippet
```java
public class HoodieRetryWrapperFileSystem extends FileSystem {

  private FileSystem fileSystem;
  private long maxRetryIntervalMs;
  private int maxRetryNumbers;
```

### FieldMayBeFinal
Field `openStreams` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java


  private ConcurrentMap<String, SizeAwareFSDataOutputStream> openStreams = new ConcurrentHashMap<>();
  private FileSystem fileSystem;
  private URI uri;
```

### FieldMayBeFinal
Field `lockList` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java`
#### Snippet
```java
  // Use segment lock to reduce competition.
  // the lock size should be powers of 2 for better hash.
  private static Object[] lockList = new Object[16];

  static {
```

### FieldMayBeFinal
Field `throwable` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/queue/DisruptorMessageQueue.java`
#### Snippet
```java
  private final Function<I, O> transformFunction;
  private final RingBuffer<HoodieDisruptorEvent> ringBuffer;
  private AtomicReference<Throwable> throwable = new AtomicReference<>(null);

  private boolean isShutdown = false;
```

### FieldMayBeFinal
Field `internalDynamicBloomFilter` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/bloom/HoodieDynamicBoundedBloomFilter.java`
#### Snippet
```java

  public static final String TYPE_CODE_PREFIX = "DYNAMIC";
  private InternalDynamicBloomFilter internalDynamicBloomFilter;

  /**
```

### FieldMayBeFinal
Field `filePath` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java

    // FilePath to store the spilled data
    private String filePath;
    // Size (numberOfBytes) of the value written to disk
    private Integer sizeOfValue;
```

### FieldMayBeFinal
Field `sizeOfKey` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private Long crc;
    // Size (numberOfBytes) of the key written to disk
    private Integer sizeOfKey;
    // Size (numberOfBytes) of the value written to disk
    private Integer sizeOfValue;
```

### FieldMayBeFinal
Field `offsetOfValue` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private Integer sizeOfValue;
    // FilePosition of the value written to disk
    private Long offsetOfValue;
    // Current timestamp when the value was written to disk
    private Long timestamp;
```

### FieldMayBeFinal
Field `key` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private Integer sizeOfValue;
    // Actual key
    private byte[] key;
    // Actual value
    private byte[] value;
```

### FieldMayBeFinal
Field `crc` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java

    // Checksum of the value written to disk, compared during every readFromDisk to make sure no corruption
    private Long crc;
    // Size (numberOfBytes) of the key written to disk
    private Integer sizeOfKey;
```

### FieldMayBeFinal
Field `timestamp` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private byte[] value;
    // Current timestamp when the value was written to disk
    private Long timestamp;

    public FileEntry(long crc, int sizeOfKey, int sizeOfValue, byte[] key, byte[] value, long timestamp) {
```

### FieldMayBeFinal
Field `sizeOfValue` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private String filePath;
    // Size (numberOfBytes) of the value written to disk
    private Integer sizeOfValue;
    // FilePosition of the value written to disk
    private Long offsetOfValue;
```

### FieldMayBeFinal
Field `sizeOfValue` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private Integer sizeOfKey;
    // Size (numberOfBytes) of the value written to disk
    private Integer sizeOfValue;
    // Actual key
    private byte[] key;
```

### FieldMayBeFinal
Field `timestamp` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private Long offsetOfValue;
    // Current timestamp when the value was written to disk
    private Long timestamp;

    protected ValueMetadata(String filePath, int sizeOfValue, long offsetOfValue, long timestamp) {
```

### FieldMayBeFinal
Field `value` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/util/collection/BitCaskDiskMap.java`
#### Snippet
```java
    private byte[] key;
    // Actual value
    private byte[] value;
    // Current timestamp when the value was written to disk
    private Long timestamp;
```

### FieldMayBeFinal
Field `record` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/model/RewriteAvroPayload.java`
#### Snippet
```java
public class RewriteAvroPayload implements HoodieRecordPayload<RewriteAvroPayload> {

  private GenericRecord record;
  
  public RewriteAvroPayload(GenericRecord record) {
```

### FieldMayBeFinal
Field `fileStatus` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/model/BaseFile.java`
#### Snippet
```java
  private static final long serialVersionUID = 1L;

  private transient FileStatus fileStatus;
  private final String fullPath;
  private final String fileName;
```

### FieldMayBeFinal
Field `bufferSize` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatReader.java`
#### Snippet
```java
  private final String recordKeyField;
  private final boolean enableInlineReading;
  private int bufferSize;

  private static final Logger LOG = LoggerFactory.getLogger(HoodieLogFormatReader.class);
```

### FieldMayBeFinal
Field `others` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java`
#### Snippet
```java
     * Like KeyGenerator's configs.
     */
    private Properties others = new Properties();

    private PropertyBuilder() {
```

### FieldMayBeFinal
Field `schemaMap` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java`
#### Snippet
```java

  //  Map of string schema to parsed schema.
  private static ConcurrentHashMap<String, Schema> schemaMap = new ConcurrentHashMap<>();

  /**
```

### FieldMayBeFinal
Field `completedCommitsFileSystemView` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
  private List<HoodieWriteStat> filesWritten;
  private String preCommitInstantTime;
  private SyncableFileSystemView completedCommitsFileSystemView;
  private HoodieTableMetaClient tableMetaClient;

```

### FieldMayBeFinal
Field `partitionToReplaceFileIds` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
public class HoodieTablePreCommitFileSystemView {
  
  private Map<String, List<String>> partitionToReplaceFileIds;
  private List<HoodieWriteStat> filesWritten;
  private String preCommitInstantTime;
```

### FieldMayBeFinal
Field `filesWritten` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
  
  private Map<String, List<String>> partitionToReplaceFileIds;
  private List<HoodieWriteStat> filesWritten;
  private String preCommitInstantTime;
  private SyncableFileSystemView completedCommitsFileSystemView;
```

### FieldMayBeFinal
Field `tableMetaClient` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
  private String preCommitInstantTime;
  private SyncableFileSystemView completedCommitsFileSystemView;
  private HoodieTableMetaClient tableMetaClient;

  /**
```

### FieldMayBeFinal
Field `preCommitInstantTime` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTablePreCommitFileSystemView.java`
#### Snippet
```java
  private Map<String, List<String>> partitionToReplaceFileIds;
  private List<HoodieWriteStat> filesWritten;
  private String preCommitInstantTime;
  private SyncableFileSystemView completedCommitsFileSystemView;
  private HoodieTableMetaClient tableMetaClient;
```

### FieldMayBeFinal
Field `MILLIS_GRANULARITY_DATE_FORMATTER` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java`
#### Snippet
```java
      .appendValue(ChronoField.MILLI_OF_SECOND, 3).toFormatter();
  private static final String MILLIS_GRANULARITY_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss.SSS";
  private static DateTimeFormatter MILLIS_GRANULARITY_DATE_FORMATTER = DateTimeFormatter.ofPattern(MILLIS_GRANULARITY_DATE_FORMAT);

  // The last Instant timestamp generated
```

### FieldMayBeFinal
Field `MILLIS_INSTANT_TIME_FORMATTER` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java`
#### Snippet
```java
  // Formatter to generate Instant timestamps
  // Unfortunately millisecond format is not parsable as is https://bugs.openjdk.java.net/browse/JDK-8031085. hence have to do appendValue()
  private static DateTimeFormatter MILLIS_INSTANT_TIME_FORMATTER = new DateTimeFormatterBuilder().appendPattern(SECS_INSTANT_TIMESTAMP_FORMAT)
      .appendValue(ChronoField.MILLI_OF_SECOND, 3).toFormatter();
  private static final String MILLIS_GRANULARITY_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss.SSS";
```

### FieldMayBeFinal
Field `lastInstantTime` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstantTimeGenerator.java`
#### Snippet
```java

  // The last Instant timestamp generated
  private static AtomicReference<String> lastInstantTime = new AtomicReference<>(String.valueOf(Integer.MIN_VALUE));

  // The default number of milliseconds that we add if they are not present
```

### FieldMayBeFinal
Field `hadoopConf` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/engine/HoodieEngineContext.java`
#### Snippet
```java
   * A wrapped hadoop configuration which can be serialized.
   */
  private SerializableConfiguration hadoopConf;

  protected TaskContextSupplier taskContextSupplier;
```

### FieldMayBeFinal
Field `mainFilePath` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/common/config/DFSPropertiesConfiguration.java`
#### Snippet
```java
  private final Configuration hadoopConfig;

  private Path mainFilePath;

  // props read from user defined configuration file or input stream
```

### FieldMayBeFinal
Field `name` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/Type.java`
#### Snippet
```java
    RECORD, ARRAY, MAP, FIXED, STRING, BINARY,
    INT, LONG, FLOAT, DOUBLE, DATE, BOOLEAN, TIME, TIMESTAMP, DECIMAL, UUID;
    private String name;

    TypeID() {
```

### FieldMayBeFinal
Field `latestSchema` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/InternalSchemaChangeApplier.java`
#### Snippet
```java
 */
public class InternalSchemaChangeApplier {
  private InternalSchema latestSchema;

  public InternalSchemaChangeApplier(InternalSchema latestSchema) {
```

### FieldMayBeFinal
Field `name` may be 'final'
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/action/TableChange.java`
#### Snippet
```java
  enum ColumnChangeID {
    ADD, UPDATE, DELETE, PROPERTY_CHANGE, REPLACE;
    private String name;

    private ColumnChangeID() {
```

### FieldMayBeFinal
Field `tableType` may be 'final'
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/spark/HoodieSparkBootstrapExample.java`
#### Snippet
```java
public class HoodieSparkBootstrapExample {

  private static String tableType = HoodieTableType.MERGE_ON_READ.name();

  public static void main(String[] args) throws Exception {
```

### FieldMayBeFinal
Field `tableType` may be 'final'
in `hudi-examples/hudi-examples-java/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HoodieJavaWriteClientExample.class);

  private static String tableType = HoodieTableType.COPY_ON_WRITE.name();

  public static void main(String[] args) throws Exception {
```

### FieldMayBeFinal
Field `tableType` may be 'final'
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HoodieWriteClientExample.class);

  private static String tableType = HoodieTableType.COPY_ON_WRITE.name();

  public static void main(String[] args) throws Exception {
```

### FieldMayBeFinal
Field `columns` may be 'final'
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/utils/SchemaBuilder.java`
#### Snippet
```java
 */
public class SchemaBuilder {
  private List<Column> columns;
  private List<WatermarkSpec> watermarkSpecs;
  private UniqueConstraint constraint;
```

### FieldMayBeFinal
Field `watermarkSpecs` may be 'final'
in `hudi-examples/hudi-examples-flink/src/main/java/org/apache/hudi/examples/quickstart/utils/SchemaBuilder.java`
#### Snippet
```java
public class SchemaBuilder {
  private List<Column> columns;
  private List<WatermarkSpec> watermarkSpecs;
  private UniqueConstraint constraint;

```

### FieldMayBeFinal
Field `dataGen` may be 'final'
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/common/RandomJsonSource.java`
#### Snippet
```java

public class RandomJsonSource extends JsonSource {
  private HoodieExampleDataGenerator<HoodieAvroPayload> dataGen;

  public RandomJsonSource(TypedProperties props, JavaSparkContext sparkContext, SparkSession sparkSession, SchemaProvider schemaProvider) {
```

### FieldMayBeFinal
Field `schema` may be 'final'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileRecordReader.java`
#### Snippet
```java
  private HoodieAvroHFileReader reader;
  private Iterator<HoodieRecord<IndexedRecord>> recordIterator;
  private Schema schema;

  public HoodieHFileRecordReader(Configuration conf, InputSplit split, JobConf job) throws IOException {
```

### FieldMayBeFinal
Field `valueObj` may be 'final'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileRecordReader.java`
#### Snippet
```java

  private long count = 0;
  private ArrayWritable valueObj;
  private HoodieAvroHFileReader reader;
  private Iterator<HoodieRecord<IndexedRecord>> recordIterator;
```

### FieldMayBeFinal
Field `isIncrementalUseDatabase` may be 'final'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/InputPathHandler.java`
#### Snippet
```java
  private final List<Path> snapshotPaths;
  private final List<Path> nonHoodieInputPaths;
  private boolean isIncrementalUseDatabase;

  public InputPathHandler(Configuration conf, Path[] inputPaths, List<String> incrementalTables) throws IOException {
```

### FieldMayBeFinal
Field `hoodiePathCache` may be 'final'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java`
#### Snippet
```java
   * metadata for known partition paths and the latest versions of files.
   */
  private Map<String, HashSet<Path>> hoodiePathCache;

  /**
```

### FieldMayBeFinal
Field `metaClient` may be 'final'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/SchemaEvolutionContext.java`
#### Snippet
```java
  private final InputSplit split;
  private final JobConf job;
  private HoodieTableMetaClient metaClient;
  public Option<InternalSchema> internalSchemaOption;

```

### FieldMayBeFinal
Field `metaClient` may be 'final'
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java`
#### Snippet
```java
  private Schema writerSchema;
  private Schema hiveSchema;
  private HoodieTableMetaClient metaClient;
  protected SchemaEvolutionContext schemaEvolutionContext;
  // support merge operation
```

### FieldMayBeFinal
Field `jssc` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java`
#### Snippet
```java
   * Spark context.
   */
  private transient JavaSparkContext jssc;

  /**
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java`
#### Snippet
```java
   * Bag of properties with source, hoodie client, key generator etc.
   */
  private TypedProperties props;

  public HoodieCleaner(Config cfg, JavaSparkContext jssc) {
```

### FieldMayBeFinal
Field `metaClient` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
  private TypedProperties props;

  private HoodieTableMetaClient metaClient;

  protected transient Option<AsyncDataTableValidateService> asyncDataTableValidateService;
```

### FieldMayBeFinal
Field `jsc` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java

  // Spark context
  private transient JavaSparkContext jsc;
  // config
  private Config cfg;
```

### FieldMayBeFinal
Field `cfg` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
  private transient JavaSparkContext jsc;
  // config
  private Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/TableSizeStats.java`
#### Snippet
```java
  private Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;

  public TableSizeStats(JavaSparkContext jsc, Config cfg) {
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieRepairTool.java`
#### Snippet
```java
  private final Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;
  // Spark context
  private final HoodieEngineContext context;
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
  private final Config cfg;
  private transient FileSystem fs;
  private TypedProperties props;
  private final JavaSparkContext jsc;
  private final HoodieTableMetaClient metaClient;
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieIndexer.java`
#### Snippet
```java

  private final HoodieIndexer.Config cfg;
  private TypedProperties props;
  private final JavaSparkContext jsc;
  private final HoodieTableMetaClient metaClient;
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDropPartitionsTool.java`
#### Snippet
```java
  private final Config cfg;
  // Properties with source, hoodie client, key generator etc.
  private TypedProperties props;

  private final HoodieTableMetaClient metaClient;
```

### FieldMayBeFinal
Field `timelineServer` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(TimelineServerPerf.class);
  private final Config cfg;
  private transient TimelineService timelineServer;
  private final boolean useExternalTimelineServer;
  private String hostAddr;
```

### FieldMayBeFinal
Field `options` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/JdbcbasedSchemaProvider.java`
#### Snippet
```java
public class JdbcbasedSchemaProvider extends SchemaProvider {
  private Schema sourceSchema;
  private Map<String, String> options = new HashMap<>();

  public JdbcbasedSchemaProvider(TypedProperties props, JavaSparkContext jssc) {
```

### FieldMayBeFinal
Field `overriddenSchemaProvider` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/Source.java`
#### Snippet
```java
  protected transient JavaSparkContext sparkContext;
  protected transient SparkSession sparkSession;
  private transient SchemaProvider overriddenSchemaProvider;

  private final SourceType sourceType;
```

### FieldMayBeFinal
Field `props` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/gcs/GcsObjectDataFetcher.java`
#### Snippet
```java

  private final String fileFormat;
  private TypedProperties props;

  private static final Logger LOG = LoggerFactory.getLogger(GcsObjectDataFetcher.class);
```

### FieldMayBeFinal
Field `LOG` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java`
#### Snippet
```java
public class DatePartitionPathSelector extends DFSPathSelector {

  private static volatile Logger LOG = LoggerFactory.getLogger(DatePartitionPathSelector.class);

  private final String dateFormat;
```

### FieldMayBeFinal
Field `bootstrapServers` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/callback/kafka/HoodieWriteCommitKafkaCallback.java`
#### Snippet
```java

  private HoodieConfig hoodieConfig;
  private String bootstrapServers;
  private String topic;

```

### FieldMayBeFinal
Field `topic` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/callback/kafka/HoodieWriteCommitKafkaCallback.java`
#### Snippet
```java
  private HoodieConfig hoodieConfig;
  private String bootstrapServers;
  private String topic;

  public HoodieWriteCommitKafkaCallback(HoodieWriteConfig config) {
```

### FieldMayBeFinal
Field `hoodieConfig` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/callback/kafka/HoodieWriteCommitKafkaCallback.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HoodieWriteCommitKafkaCallback.class);

  private HoodieConfig hoodieConfig;
  private String bootstrapServers;
  private String topic;
```

### FieldMayBeFinal
Field `FILENAME_SEPARATOR` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/KafkaConnectHdfsProvider.java`
#### Snippet
```java
 */
public class KafkaConnectHdfsProvider extends InitialCheckPointProvider {
  private static String FILENAME_SEPARATOR = "[\\+\\.]";

  public KafkaConnectHdfsProvider(TypedProperties props) {
```

### FieldMayBeFinal
Field `schemaProvider` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
   * Schema provider that supplies the command for reading the input and writing out the target table.
   */
  private transient SchemaProvider schemaProvider;

  /**
```

### FieldMayBeFinal
Field `jssc` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
   * Spark context.
   */
  private transient JavaSparkContext jssc;

  /**
```

### FieldMayBeFinal
Field `bootstrapBasePath` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
  private transient FileSystem fs;

  private String bootstrapBasePath;

  /**
```

### FieldMayBeFinal
Field `fs` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/BootstrapExecutor.java`
#### Snippet
```java
   * FileSystem instance.
   */
  private transient FileSystem fs;

  private String bootstrapBasePath;
```

### FieldMayBeFinal
Field `jssc` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java

  private List<TableExecutionContext> tableExecutionContexts;
  private transient JavaSparkContext jssc;
  private Set<String> successTables;
  private Set<String> failedTables;
```

### FieldMayBeFinal
Field `tableExecutionContexts` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
  private static Logger logger = LoggerFactory.getLogger(HoodieMultiTableDeltaStreamer.class);

  private List<TableExecutionContext> tableExecutionContexts;
  private transient JavaSparkContext jssc;
  private Set<String> successTables;
```

### FieldMayBeFinal
Field `logger` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
public class HoodieMultiTableDeltaStreamer {

  private static Logger logger = LoggerFactory.getLogger(HoodieMultiTableDeltaStreamer.class);

  private List<TableExecutionContext> tableExecutionContexts;
```

### FieldMayBeFinal
Field `failedTables` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
  private transient JavaSparkContext jssc;
  private Set<String> successTables;
  private Set<String> failedTables;

  public HoodieMultiTableDeltaStreamer(Config config, JavaSparkContext jssc) throws IOException {
```

### FieldMayBeFinal
Field `successTables` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
  private List<TableExecutionContext> tableExecutionContexts;
  private transient JavaSparkContext jssc;
  private Set<String> successTables;
  private Set<String> failedTables;

```

### FieldMayBeFinal
Field `conf` may be 'final'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/bulk/sort/SortOperator.java`
#### Snippet
```java
  private GeneratedRecordComparator gComparator;

  private Configuration conf;

  private transient BinaryExternalSorter sorter;
```

### FieldMayBeFinal
Field `deltaSync` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
     * Delta Sync.
     */
    private transient DeltaSync deltaSync;

    private final Option<PostWriteTerminationStrategy> postWriteTerminationStrategy;
```

### FieldMayBeFinal
Field `jssc` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
     * Spark context.
     */
    private transient JavaSparkContext jssc;

    /**
```

### FieldMayBeFinal
Field `schemaProvider` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
     * Schema provider that supplies the command for reading the input and writing out the target table.
     */
    private transient SchemaProvider schemaProvider;

    /**
```

### FieldMayBeFinal
Field `sparkSession` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java`
#### Snippet
```java
     * Spark Session.
     */
    private transient SparkSession sparkSession;

    /**
```

### FieldMayBeFinal
Field `userProvidedSchemaProvider` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * User Provided Schema Provider.
   */
  private transient SchemaProvider userProvidedSchemaProvider;

  /**
```

### FieldMayBeFinal
Field `conf` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Hive Config.
   */
  private transient Configuration conf;

  /**
```

### FieldMayBeFinal
Field `metrics` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
  private HoodieErrorTableConfig.ErrorWriteFailureStrategy errorWriteFailureStrategy;

  private transient HoodieIngestionMetrics metrics;
  private transient HoodieMetrics hoodieMetrics;

```

### FieldMayBeFinal
Field `formatAdapter` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Source to pull deltas from.
   */
  private transient SourceFormatAdapter formatAdapter;

  /**
```

### FieldMayBeFinal
Field `sparkSession` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Spark Session.
   */
  private transient SparkSession sparkSession;

  /**
```

### FieldMayBeFinal
Field `multiwriterIdentifier` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Unique identifier of the deltastreamer
   * */
  private transient Option<String> multiwriterIdentifier;

  /**
```

### FieldMayBeFinal
Field `jssc` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Spark context.
   */
  private transient JavaSparkContext jssc;

  /**
```

### FieldMayBeFinal
Field `onInitializingHoodieWriteClient` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Callback when write client is instantiated.
   */
  private transient Function<SparkRDDWriteClient, Boolean> onInitializingHoodieWriteClient;

  /**
```

### FieldMayBeFinal
Field `hoodieMetrics` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java

  private transient HoodieIngestionMetrics metrics;
  private transient HoodieMetrics hoodieMetrics;

  /**
```

### FieldMayBeFinal
Field `fs` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Filesystem used.
   */
  private transient FileSystem fs;

  /**
```

### FieldMayBeFinal
Field `keyGenClassName` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
  private transient Option<Transformer> transformer;

  private String keyGenClassName;

  /**
```

### FieldMayBeFinal
Field `transformer` may be 'final'
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java`
#### Snippet
```java
   * Allows transforming source to target table before writing.
   */
  private transient Option<Transformer> transformer;

  private String keyGenClassName;
```

### FieldMayBeFinal
Field `transformers` may be 'final'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/transform/ChainedTransformer.java`
#### Snippet
```java
public class ChainedTransformer implements Transformer {

  private List<Transformer> transformers;

  public ChainedTransformer(List<Transformer> transformers) {
```

### FieldMayBeFinal
Field `numTolerableHeartbeatMisses` may be 'final'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/ClientIds.java`
#### Snippet
```java
    private String clientId = INIT_CLIENT_ID;
    private long heartbeatIntervalInMs = DEFAULT_HEARTBEAT_INTERVAL_IN_MS;
    private int numTolerableHeartbeatMisses = DEFAULT_NUM_TOLERABLE_HEARTBEAT_MISSES;

    public Builder fs(FileSystem fs) {
```

### FieldMayBeFinal
Field `decimalConversion` may be 'final'
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/util/RowDataToAvroConverters.java`
#### Snippet
```java
public class RowDataToAvroConverters {

  private static Conversions.DecimalConversion decimalConversion = new Conversions.DecimalConversion();

  // --------------------------------------------------------------------------------
```

### FieldMayBeFinal
Field `rand` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java`
#### Snippet
```java
    static Schema avroSchema = new Schema.Parser().parse(TRIP_EXAMPLE_SCHEMA);

    private static Random rand = new Random(46474747);

    private final Map<Integer, HoodieKey> existingKeys;
```

### FieldMayBeFinal
Field `tableType` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
  private String tableType = HoodieTableType.COPY_ON_WRITE.name();

  @Parameter(names = {"--hive-sync", "-hs"}, description = "Enable syncing to hive")
```

### FieldMayBeFinal
Field `hiveUser` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-user", "-hu"}, description = "Hive username")
  private String hiveUser = "hive";

  @Parameter(names = {"--hive-password", "-hp"}, description = "Hive password")
```

### FieldMayBeFinal
Field `nonPartitionedTable` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--non-partitioned", "-np"}, description = "Use non-partitioned Table")
  private Boolean nonPartitionedTable = false;

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
```

### FieldMayBeFinal
Field `enableHiveSync` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-sync", "-hs"}, description = "Enable syncing to hive")
  private Boolean enableHiveSync = false;

  @Parameter(names = {"--hive-db", "-hd"}, description = "Hive database")
```

### FieldMayBeFinal
Field `tableName` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-name", "-n"}, description = "Table name for Hoodie sample table")
  private String tableName = "hoodie_test";

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
```

### FieldMayBeFinal
Field `hiveTable` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-table", "-ht"}, description = "Hive table")
  private String hiveTable = "hoodie_sample_test";

  @Parameter(names = {"--hive-user", "-hu"}, description = "Hive username")
```

### FieldMayBeFinal
Field `commitType` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--commit-type", "-ct"}, description = "How may commits will run")
  private String commitType = "overwrite";

  @Parameter(names = {"--help", "-h"}, help = true)
```

### FieldMayBeFinal
Field `useMultiPartitionKeys` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
  private Boolean useMultiPartitionKeys = false;

  @Parameter(names = {"--commit-type", "-ct"}, description = "How may commits will run")
```

### FieldMayBeFinal
Field `hivePass` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-password", "-hp"}, description = "Hive password")
  private String hivePass = "hive";

  @Parameter(names = {"--hive-url", "-hl"}, description = "Hive JDBC URL")
```

### FieldMayBeFinal
Field `hiveDB` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-db", "-hd"}, description = "Hive database")
  private String hiveDB = "default";

  @Parameter(names = {"--hive-table", "-ht"}, description = "Hive table")
```

### FieldMayBeFinal
Field `hiveJdbcUrl` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-url", "-hl"}, description = "Hive JDBC URL")
  private String hiveJdbcUrl = "jdbc:hive2://localhost:10000";

  @Parameter(names = {"--non-partitioned", "-np"}, description = "Use non-partitioned Table")
```

### FieldMayBeFinal
Field `tablePath` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java`
#### Snippet
```java
public class HoodieJavaGenerateApp {
  @Parameter(names = {"--table-path", "-p"}, description = "Path for Hoodie sample table")
  private String tablePath = "file:///tmp/hoodie/sample-table";

  @Parameter(names = {"--table-name", "-n"}, description = "Table name for Hoodie sample table")
```

### FieldMayBeFinal
Field `tableType` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
  private String tableType = HoodieTableType.COPY_ON_WRITE.name();

  @Parameter(names = {"--hive-sync", "-hv"}, description = "Enable syncing to hive")
```

### FieldMayBeFinal
Field `nonPartitionedTable` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--non-partitioned", "-np"}, description = "Use non-partitioned Table")
  private Boolean nonPartitionedTable = false;

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
```

### FieldMayBeFinal
Field `enableHiveSync` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-sync", "-hv"}, description = "Enable syncing to hive")
  private Boolean enableHiveSync = false;

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
```

### FieldMayBeFinal
Field `hiveTable` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
  private String hiveTable = "hoodie_sample_test";

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
```

### FieldMayBeFinal
Field `hiveDB` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
  private String hiveDB = "default";

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
```

### FieldMayBeFinal
Field `hiveJdbcUrl` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
  private String hiveJdbcUrl = "jdbc:hive2://localhost:10000";

  @Parameter(names = {"--non-partitioned", "-np"}, description = "Use non-partitioned Table")
```

### FieldMayBeFinal
Field `hivePass` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
  private String hivePass = "hive";

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
```

### FieldMayBeFinal
Field `tablePath` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-path", "-p"}, description = "path for Hoodie sample table")
  private String tablePath = "file:///tmp/hoodie/sample-table";

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
```

### FieldMayBeFinal
Field `useMultiPartitionKeys` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
  private Boolean useMultiPartitionKeys = false;

  @Parameter(names = {"--help", "-h"}, help = true)
```

### FieldMayBeFinal
Field `tableName` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
  private String tableName = "hoodie_test";

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
```

### FieldMayBeFinal
Field `hiveUser` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
  private String hiveUser = "hive";

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
```

### FieldMayBeFinal
Field `capacity` may be 'final'
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/spark/sql/execution/datasources/parquet/Spark24HoodieVectorizedParquetRecordReader.java`
#### Snippet
```java

  // The capacity of vectorized batch.
  private int capacity;

  // If true, this class returns batches instead of rows.
```

### FieldMayBeFinal
Field `typeChangeInfos` may be 'final'
in `hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/spark/sql/execution/datasources/parquet/Spark24HoodieVectorizedParquetRecordReader.java`
#### Snippet
```java

  // save the col type change info.
  private Map<Integer, Pair<DataType, DataType>> typeChangeInfos;

  private ColumnarBatch columnarBatch;
```

### FieldMayBeFinal
Field `value` may be 'final'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/sql/InsertMode.java`
#### Snippet
```java
  NON_STRICT("non-strict");

  private String value;

  InsertMode(String value) {
```

### FieldMayBeFinal
Field `writeStatuses` may be 'final'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java`
#### Snippet
```java
public class BaseWriterCommitMessage {

  private List<HoodieInternalWriteStatus> writeStatuses;

  public BaseWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {
```

### FieldMayBeFinal
Field `tableType` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
  private String tableType = HoodieTableType.MERGE_ON_READ.name();

  @Parameter(names = {"--hive-sync", "-hv"}, description = "Enable syncing to hive")
```

### FieldMayBeFinal
Field `hivePass` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
  private String hivePass = "hive";

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
```

### FieldMayBeFinal
Field `streamingCheckpointingPath` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
  @Parameter(names = {"--streaming-checkpointing-path", "-scp"},
      description = "path for streaming checking pointing folder")
  private String streamingCheckpointingPath = "/tmp/hoodie/streaming/checkpoint";

  @Parameter(names = {"--streaming-duration-in-ms", "-sdm"},
```

### FieldMayBeFinal
Field `tablePath` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-path", "-p"}, description = "path for Hoodie sample table")
  private String tablePath = "/tmp/hoodie/streaming/sample-table";

  @Parameter(names = {"--streaming-source-path", "-ssp"}, description = "path for streaming source file folder")
```

### FieldMayBeFinal
Field `hiveJdbcUrl` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-url", "-hl"}, description = "hive JDBC URL")
  private String hiveJdbcUrl = "jdbc:hive2://localhost:10000";

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
```

### FieldMayBeFinal
Field `enableHiveSync` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-sync", "-hv"}, description = "Enable syncing to hive")
  private Boolean enableHiveSync = false;

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
```

### FieldMayBeFinal
Field `streamingDurationInMs` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java
  @Parameter(names = {"--streaming-duration-in-ms", "-sdm"},
      description = "time in millisecond for the streaming duration")
  private Long streamingDurationInMs = 15000L;

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
```

### FieldMayBeFinal
Field `hiveTable` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
  private String hiveTable = "hoodie_sample_test";

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
```

### FieldMayBeFinal
Field `tableName` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--table-name", "-n"}, description = "table name for Hoodie sample table")
  private String tableName = "hoodie_test";

  @Parameter(names = {"--table-type", "-t"}, description = "One of COPY_ON_WRITE or MERGE_ON_READ")
```

### FieldMayBeFinal
Field `useMultiPartitionKeys` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--use-multi-partition-keys", "-mp"}, description = "Use Multiple Partition Keys")
  private Boolean useMultiPartitionKeys = false;

  @Parameter(names = {"--help", "-h"}, help = true)
```

### FieldMayBeFinal
Field `streamingSourcePath` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--streaming-source-path", "-ssp"}, description = "path for streaming source file folder")
  private String streamingSourcePath = "/tmp/hoodie/streaming/source";

  @Parameter(names = {"--streaming-checkpointing-path", "-scp"},
```

### FieldMayBeFinal
Field `hiveDB` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-db", "-hd"}, description = "hive database")
  private String hiveDB = "default";

  @Parameter(names = {"--hive-table", "-ht"}, description = "hive table")
```

### FieldMayBeFinal
Field `hiveUser` may be 'final'
in `hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java`
#### Snippet
```java

  @Parameter(names = {"--hive-user", "-hu"}, description = "hive username")
  private String hiveUser = "hive";

  @Parameter(names = {"--hive-password", "-hp"}, description = "hive password")
```

### FieldMayBeFinal
Field `extraMetadata` may be 'final'
in `hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/DataSourceInternalWriterHelper.java`
#### Snippet
```java
  private final HoodieTable hoodieTable;
  private final WriteOperationType operationType;
  private Map<String, String> extraMetadata;

  public DataSourceInternalWriterHelper(String instantTime, HoodieWriteConfig writeConfig, StructType structType,
```

### FieldMayBeFinal
Field `hasConflict` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/AsyncTimelineServerBasedDetectionStrategy.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(AsyncTimelineServerBasedDetectionStrategy.class);

  private AtomicBoolean hasConflict = new AtomicBoolean(false);
  private ScheduledExecutorService asyncDetectorExecutor;

```

### FieldMayBeFinal
Field `fs` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java
  private String markerDir;
  private String basePath;
  private FileSystem fs;
  private AtomicBoolean hasConflict;
  private long maxAllowableHeartbeatIntervalInMs;
```

### FieldMayBeFinal
Field `hasConflict` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java
  private String basePath;
  private FileSystem fs;
  private AtomicBoolean hasConflict;
  private long maxAllowableHeartbeatIntervalInMs;
  private Set<HoodieInstant> completedCommits;
```

### FieldMayBeFinal
Field `basePath` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java
  private MarkerHandler markerHandler;
  private String markerDir;
  private String basePath;
  private FileSystem fs;
  private AtomicBoolean hasConflict;
```

### FieldMayBeFinal
Field `maxAllowableHeartbeatIntervalInMs` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java
  private FileSystem fs;
  private AtomicBoolean hasConflict;
  private long maxAllowableHeartbeatIntervalInMs;
  private Set<HoodieInstant> completedCommits;
  private final boolean checkCommitConflict;
```

### FieldMayBeFinal
Field `markerDir` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java

  private MarkerHandler markerHandler;
  private String markerDir;
  private String basePath;
  private FileSystem fs;
```

### FieldMayBeFinal
Field `markerHandler` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MarkerBasedEarlyConflictDetectionRunnable.class);

  private MarkerHandler markerHandler;
  private String markerDir;
  private String basePath;
```

### FieldMayBeFinal
Field `completedCommits` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerBasedEarlyConflictDetectionRunnable.java`
#### Snippet
```java
  private AtomicBoolean hasConflict;
  private long maxAllowableHeartbeatIntervalInMs;
  private Set<HoodieInstant> completedCommits;
  private final boolean checkCommitConflict;

```

### FieldMayBeFinal
Field `hoodieEngineContext` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/MarkerHandler.java`
#### Snippet
```java
  private final Object firstCreationRequestSeenLock = new Object();
  private final Object earlyConflictDetectionLock = new Object();
  private transient HoodieEngineContext hoodieEngineContext;
  private ScheduledFuture<?> dispatchingThreadFuture;
  private boolean firstCreationRequestSeen;
```

### FieldMayBeFinal
Field `hoodieEngineContext` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/marker/MarkerDirState.java`
#### Snippet
```java
  // Early conflict detection strategy if enabled
  private final Option<TimelineServerBasedDetectionStrategy> conflictDetectionStrategy;
  private transient HoodieEngineContext hoodieEngineContext;
  // Last underlying file index used, for finding the next file index
  // in a round-robin fashion
```

### FieldMayBeFinal
Field `conf` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java`
#### Snippet
```java
  private int serverPort;
  private Config timelineServerConf;
  private Configuration conf;
  private transient HoodieEngineContext context;
  private transient FileSystem fs;
```

### FieldMayBeFinal
Field `fs` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java`
#### Snippet
```java
  private Configuration conf;
  private transient HoodieEngineContext context;
  private transient FileSystem fs;
  private transient Javalin app = null;
  private transient FileSystemViewManager fsViewsManager;
```

### FieldMayBeFinal
Field `timelineServerConf` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java`
#### Snippet
```java

  private int serverPort;
  private Config timelineServerConf;
  private Configuration conf;
  private transient HoodieEngineContext context;
```

### FieldMayBeFinal
Field `fsViewsManager` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java`
#### Snippet
```java
  private transient FileSystem fs;
  private transient Javalin app = null;
  private transient FileSystemViewManager fsViewsManager;
  private transient RequestHandler requestHandler;

```

### FieldMayBeFinal
Field `context` may be 'final'
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/TimelineService.java`
#### Snippet
```java
  private Config timelineServerConf;
  private Configuration conf;
  private transient HoodieEngineContext context;
  private transient FileSystem fs;
  private transient Javalin app = null;
```

## RuleId[id=CaughtExceptionImmediatelyRethrown]
### CaughtExceptionImmediatelyRethrown
Caught exception `e` is immediately rethrown
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java
      String scheme = FSUtils.getFs(file.toString(), conf).getScheme();
      return convertPathWithScheme(file, getHoodieScheme(scheme));
    } catch (HoodieIOException e) {
      throw e;
    }
```

### CaughtExceptionImmediatelyRethrown
Caught exception `e` is immediately rethrown
in `hudi-common/src/main/java/org/apache/hudi/common/util/InternalSchemaCache.java`
#### Snippet
```java
        try (FSDataInputStream is = fs.open(candidateCommitFile)) {
          data = FileIOUtils.readAsByteArray(is);
        } catch (IOException e) {
          throw e;
        }
```

## RuleId[id=PrimitiveArrayArgumentToVariableArgMethod]
### PrimitiveArrayArgumentToVariableArgMethod
Confusing primitive array argument to varargs method
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/ProtoKafkaSource.java`
#### Snippet
```java
    public Message parse(byte[] bytes) {
      try {
        return (Message) getParseMethod().invoke(getClass(), bytes);
      } catch (IllegalAccessException | InvocationTargetException ex) {
        throw new HoodieException("Failed to parse proto message from kafka", ex);
```

## RuleId[id=UnnecessaryUnicodeEscape]
### UnnecessaryUnicodeEscape
Unicode escape sequence `\u000A` can be replaced with a line feed character
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java
    /**
     * ASCII 01-1F are HTTP control characters that need to be escaped.
     * \u000A and \u000D are \n and \r, respectively.
     */
    char[] clist = new char[] {'\u0001', '\u0002', '\u0003', '\u0004',
```

### UnnecessaryUnicodeEscape
Unicode escape sequence `\u0009` can be replaced with a tab character
in `hudi-common/src/main/java/org/apache/hudi/common/util/PartitionPathEncodeUtils.java`
#### Snippet
```java
     */
    char[] clist = new char[] {'\u0001', '\u0002', '\u0003', '\u0004',
      '\u0005', '\u0006', '\u0007', '\u0008', '\u0009', '\n', '\u000B',
      '\u000C', '\r', '\u000E', '\u000F', '\u0010', '\u0011', '\u0012',
      '\u0013', '\u0014', '\u0015', '\u0016', '\u0017', '\u0018', '\u0019',
```

### UnnecessaryUnicodeEscape
Unicode escape sequence `\u000A` can be replaced with a line feed character
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/FilePathUtils.java`
#### Snippet
```java
    /*
     * ASCII 01-1F are HTTP control characters that need to be escaped.
     * \u000A and \u000D are \n and \r, respectively.
     */
    char[] clist = new char[] {'\u0001', '\u0002', '\u0003', '\u0004',
```

### UnnecessaryUnicodeEscape
Unicode escape sequence `\u0009` can be replaced with a tab character
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/table/format/FilePathUtils.java`
#### Snippet
```java
     */
    char[] clist = new char[] {'\u0001', '\u0002', '\u0003', '\u0004',
        '\u0005', '\u0006', '\u0007', '\u0008', '\u0009', '\n', '\u000B',
        '\u000C', '\r', '\u000E', '\u000F', '\u0010', '\u0011', '\u0012',
        '\u0013', '\u0014', '\u0015', '\u0016', '\u0017', '\u0018', '\u0019',
```

## RuleId[id=SynchronizationOnLocalVariableOrMethodParameter]
### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `jobConf`
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java`
#### Snippet
```java
    // actual heavy lifting of reading the parquet files happen.
    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {
      synchronized (jobConf) {
        LOG.info(
            "Before adding Hoodie columns, Projections :" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `jobConf`
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java`
#### Snippet
```java
    // actual heavy lifting of reading the parquet files happen.
    if (HoodieRealtimeInputFormatUtils.canAddProjectionToJobConf(realtimeSplit, jobConf)) {
      synchronized (jobConf) {
        LOG.info(
            "Before adding Hoodie columns, Projections :" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `view`
in `hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/RequestHandler.java`
#### Snippet
```java
    String basePath = ctx.queryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM);
    SyncableFileSystemView view = viewManager.getFileSystemView(basePath);
    synchronized (view) {
      if (isLocalViewBehind(ctx)) {

```

## RuleId[id=UnnecessaryLocalVariable]
### UnnecessaryLocalVariable
Local variable `restorePlan` is redundant
in `hudi-cli/src/main/java/org/apache/hudi/cli/commands/RestoresCommand.java`
#### Snippet
```java
            restoreInstant.getTimestamp());
    Option<byte[]> instantDetails = activeTimeline.getInstantDetails(instantKey);
    HoodieRestorePlan restorePlan = TimelineMetadataUtils
            .deserializeAvroMetadata(instantDetails.get(), HoodieRestorePlan.class);
    return restorePlan;
```

### UnnecessaryLocalVariable
Local variable `state` is redundant
in `hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/replication/HiveSyncGlobalCommitTool.java`
#### Snippet
```java
        forRemote ? REMOTE_HIVE_SITE_URI : LOCAL_HIVE_SITE_URI)));
    // TODO: get clusterId as input parameters
    ReplicationStateSync state = new ReplicationStateSync(params.mkGlobalHiveSyncProps(forRemote),
        hiveConf, forRemote ? "REMOTESYNC" : "LOCALSYNC");
    return state;
```

### UnnecessaryLocalVariable
Local variable `softDeleteUndoProposal` is redundant
in `hudi-sync/hudi-datahub-sync/src/main/java/org/apache/hudi/sync/datahub/DataHubSyncClient.java`
#### Snippet
```java

  private MetadataChangeProposalWrapper createUndoSoftDelete() {
    MetadataChangeProposalWrapper softDeleteUndoProposal = MetadataChangeProposalWrapper.builder()
            .entityType("dataset")
            .entityUrn(datasetUrn)
```

### UnnecessaryLocalVariable
Local variable `enableBloomFilter` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/storage/HoodieSparkFileWriterFactory.java`
#### Snippet
```java
      TaskContextSupplier taskContextSupplier) throws IOException {
    boolean populateMetaFields = config.getBooleanOrDefault(HoodieTableConfig.POPULATE_META_FIELDS);
    boolean enableBloomFilter = populateMetaFields;
    Option<BloomFilter> filter = enableBloomFilter ? Option.of(createBloomFilter(config)) : Option.empty();
    String compressionCodecName = config.getStringOrDefault(HoodieStorageConfig.PARQUET_COMPRESSION_CODEC_NAME);
```

### UnnecessaryLocalVariable
Local variable `partitionPathToInPendingClusteringFileId` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java`
#### Snippet
```java
   */
  private Map<String, Set<String>> getPartitionPathToPendingClusteringFileGroupsId() {
    Map<String, Set<String>>  partitionPathToInPendingClusteringFileId =
        table.getFileSystemView().getFileGroupsInPendingClustering()
            .map(fileGroupIdAndInstantPair ->
```

### UnnecessaryLocalVariable
Local variable `rec` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/OrcBootstrapMetadataHandler.java`
#### Snippet
```java
            gr.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, recKey);
            BootstrapRecordPayload payload = new BootstrapRecordPayload(gr);
            HoodieRecord rec = new HoodieAvroRecord(new HoodieKey(recKey, partitionPath), payload);
            return rec;
          }, table.getPreExecuteRunnable());
```

### UnnecessaryLocalVariable
Local variable `numRSAlive` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
    public int getBatchSize(int numRegionServersForTable, int maxQpsPerRegionServer,
                            int numTasksDuringPut, int maxExecutors, float qpsFraction) {
      int numRSAlive = numRegionServersForTable;
      int maxReqPerSec = getMaxReqPerSec(numRSAlive, maxQpsPerRegionServer, qpsFraction);
      int numTasks = numTasksDuringPut;
```

### UnnecessaryLocalVariable
Local variable `numTasks` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
      int numRSAlive = numRegionServersForTable;
      int maxReqPerSec = getMaxReqPerSec(numRSAlive, maxQpsPerRegionServer, qpsFraction);
      int numTasks = numTasksDuringPut;
      int maxParallelPutsTask = Math.max(1, Math.min(numTasks, maxExecutors));
      int multiPutBatchSizePerSecPerTask = Math.max(1, (int) Math.ceil(maxReqPerSec / maxParallelPutsTask));
```

### UnnecessaryLocalVariable
Local variable `schema` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/bootstrap/HoodieSparkBootstrapSchemaProvider.java`
#### Snippet
```java
  @Override
  protected Schema getBootstrapSourceSchema(HoodieEngineContext context, List<Pair<String, List<HoodieFileStatus>>> partitions) {
    Schema schema = partitions.stream().flatMap(p -> p.getValue().stream()).map(fs -> {
          Path filePath = FileStatusUtils.toPath(fs.getPath());
          String extension = FSUtils.getFileExtension(filePath.getName());
```

### UnnecessaryLocalVariable
Local variable `hoodieRecord` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/ExecutionStrategyUtil.java`
#### Snippet
```java

    HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
    return hoodieRecord;
  }
```

### UnnecessaryLocalVariable
Local variable `mappingIterator` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/strategy/SingleSparkJobExecutionStrategy.java`
#### Snippet
```java
          //       payload pointing into a shared, mutable (underlying) buffer we get a clean copy of
          //       it since these records will be shuffled later.
          CloseableMappingIterator mappingIterator = new CloseableMappingIterator((ClosableIterator<HoodieRecord>) baseFileReader.getRecordIterator(readerSchema),
              rec -> ((HoodieRecord) rec).copy().wrapIntoHoodieRecordPayloadWithKeyGen(readerSchema,
                  getWriteConfig().getProps(), keyGeneratorOp));
```

### UnnecessaryLocalVariable
Local variable `keyPartStr` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/keygen/StringPartitionPathFormatter.java`
#### Snippet
```java
    } else {
      // NOTE: [[toString]] is a no-op if key-part was already a [[String]]
      String keyPartStr = partitionPathPart;
      return keyPartStr.isEmpty() ? HUDI_DEFAULT_PARTITION_PATH : keyPartStr;
    }
```

### UnnecessaryLocalVariable
Local variable `hoodieRecord` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDSpatialCurveSortPartitioner.java`
#### Snippet
```java
            String partition = record.get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();
            HoodieKey hoodieKey = new HoodieKey(key, partition);
            HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, new RewriteAvroPayload(record));
            return hoodieRecord;
          });
```

### UnnecessaryLocalVariable
Local variable `sortColumns` is redundant
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RowCustomColumnsSortPartitioner.java`
#### Snippet
```java
  @Override
  public Dataset<Row> repartitionRecords(Dataset<Row> records, int outputSparkPartitions) {
    final String[] sortColumns = this.sortColumnNames;
    return records.sort(HoodieRecord.PARTITION_PATH_METADATA_FIELD, sortColumns)
        .coalesce(outputSparkPartitions);
```

### UnnecessaryLocalVariable
Local variable `maxInstantTime` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java

  public String getMaxInstantTime(HoodieTableMetaClient metaClient) {
    String maxInstantTime = metaClient
        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,
            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))
```

### UnnecessaryLocalVariable
Local variable `executionStrategy` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/HoodieCompactor.java`
#### Snippet
```java
      return new CompactionExecutionHelper();
    } else {
      CompactionExecutionHelper executionStrategy = ReflectionUtils.loadClass(compactionPlan.getStrategy().getCompactorClassName());
      return executionStrategy;
    }
```

### UnnecessaryLocalVariable
Local variable `inflightTimelineExcludeClusteringCommit` is redundant
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/BaseHoodieTableServiceClient.java`
#### Snippet
```java
  private HoodieTimeline getInflightTimelineExcludeCompactionAndClustering(HoodieTableMetaClient metaClient) {
    HoodieTimeline inflightTimelineWithReplaceCommit = metaClient.getCommitsTimeline().filterPendingExcludingCompaction();
    HoodieTimeline inflightTimelineExcludeClusteringCommit = inflightTimelineWithReplaceCommit.filter(instant -> {
      if (instant.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)) {
        Option<Pair<HoodieInstant, HoodieClusteringPlan>> instantPlan = ClusteringUtils.getClusteringPlan(metaClient, instant);
```

### UnnecessaryLocalVariable
Local variable `enableBloomFilter` is redundant
in `hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieAvroFileWriterFactory.java`
#### Snippet
```java
      TaskContextSupplier taskContextSupplier) throws IOException {
    boolean populateMetaFields = config.getBooleanOrDefault(HoodieTableConfig.POPULATE_META_FIELDS);
    boolean enableBloomFilter = populateMetaFields;
    Option<BloomFilter> filter = enableBloomFilter ? Option.of(createBloomFilter(config)) : Option.empty();
    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(new AvroSchemaConverter(conf).convert(schema), schema, filter);
```

### UnnecessaryLocalVariable
Local variable `newTableMetadata` is redundant
in `hudi-common/src/main/java/org/apache/hudi/BaseHoodieTableFileIndex.java`
#### Snippet
```java
      Path basePath
  ) {
    HoodieTableMetadata newTableMetadata = HoodieTableMetadata.create(engineContext, metadataConfig, basePath.toString(),
        FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue(), true);
    return newTableMetadata;
```

### UnnecessaryLocalVariable
Local variable `toReturn` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/fs/inline/InLineFileSystem.java`
#### Snippet
```java
    FileSystem outerFs = outerPath.getFileSystem(conf);
    FileStatus status = outerFs.getFileStatus(outerPath);
    FileStatus toReturn = new FileStatus(InLineFSUtils.length(inlinePath), status.isDirectory(), status.getReplication(), status.getBlockSize(),
        status.getModificationTime(), status.getAccessTime(), status.getPermission(), status.getOwner(),
        status.getGroup(), inlinePath);
```

### UnnecessaryLocalVariable
Local variable `julianDays` is redundant
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
    long millisUtc = date.getTime();
    long millisLocal = millisUtc + TimeZone.getDefault().getOffset(millisUtc);
    int julianDays = Math.toIntExact(Math.floorDiv(millisLocal, MILLIS_PER_DAY));
    return julianDays;
  }
```

### UnnecessaryLocalVariable
Local variable `nullUnion` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
      case NULL:
        // empty union represents null type
        final TypeDescription nullUnion = TypeDescription.createUnion();
        return nullUnion;
      case LONG:
```

### UnnecessaryLocalVariable
Local variable `hoodieRecord` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/model/HoodieAvroIndexedRecord.java`
#### Snippet
```java

    HoodieRecordPayload avroPayload = new RewriteAvroPayload(record);
    HoodieRecord hoodieRecord = new HoodieAvroRecord(hoodieKey, avroPayload);
    return hoodieRecord;
  }
```

### UnnecessaryLocalVariable
Local variable `iterator` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieParquetDataBlock.java`
#### Snippet
```java
    Schema writerSchema = new Schema.Parser().parse(this.getLogBlockHeader().get(HeaderMetadataType.SCHEMA));

    ClosableIterator<HoodieRecord<T>> iterator = HoodieFileReaderFactory.getReaderFactory(type).getFileReader(inlineConf, inlineLogFilePath, PARQUET)
        .getRecordIterator(writerSchema, readerSchema);
    return iterator;
```

### UnnecessaryLocalVariable
Local variable `reservedForExternalDataFile` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java`
#### Snippet
```java
  public long getMaxMemoryForBootstrapBaseFile() {
    long totalMemory = getLong(SPILLABLE_MEMORY);
    long reservedForExternalDataFile =
        new Double(totalMemory * getDouble(BOOTSTRAP_BASE_FILE_MEM_FRACTION))
            .longValue();
```

### UnnecessaryLocalVariable
Local variable `fgInpendingClustering` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java`
#### Snippet
```java

  protected Map<HoodieFileGroupId, HoodieInstant> createFileIdToPendingClusteringMap(final Map<HoodieFileGroupId, HoodieInstant> fileGroupsInClustering) {
    Map<HoodieFileGroupId, HoodieInstant> fgInpendingClustering = new ConcurrentHashMap<>(fileGroupsInClustering);
    return fgInpendingClustering;
  }
```

### UnnecessaryLocalVariable
Local variable `replacedFileGroupsMap` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java`
#### Snippet
```java

  protected Map<HoodieFileGroupId, HoodieInstant> createFileIdToReplaceInstantMap(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups) {
    Map<HoodieFileGroupId, HoodieInstant> replacedFileGroupsMap = new ConcurrentHashMap<>(replacedFileGroups);
    return replacedFileGroupsMap;
  }
```

### UnnecessaryLocalVariable
Local variable `status` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java`
#### Snippet
```java
        LOG.info("Syncing partition (" + partition + ") of instant (" + instant + ")");
        FileStatus[] statuses = entry.getValue().stream().map(p -> {
          FileStatus status = new FileStatus(p.getFileSizeInBytes(), false, 0, 0, 0, 0, null, null, null,
              new Path(String.format("%s/%s", metaClient.getBasePath(), p.getPath())));
          return status;
```

### UnnecessaryLocalVariable
Local variable `configProperty` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/config/ConfigProperty.java`
#### Snippet
```java
      Objects.requireNonNull(value);
      Objects.requireNonNull(docOnDefaultValue);
      ConfigProperty<T> configProperty = new ConfigProperty<>(key, value, docOnDefaultValue, "", Option.empty(), Option.empty(), Option.empty(), Collections.emptySet(), false);
      return configProperty;
    }
```

### UnnecessaryLocalVariable
Local variable `configProperty` is redundant
in `hudi-common/src/main/java/org/apache/hudi/common/config/ConfigProperty.java`
#### Snippet
```java

    public ConfigProperty<String> noDefaultValue(String docOnDefaultValue) {
      ConfigProperty<String> configProperty = new ConfigProperty<>(key, null, docOnDefaultValue, "", Option.empty(),
          Option.empty(), Option.empty(), Collections.emptySet(), false);
      return configProperty;
```

### UnnecessaryLocalVariable
Local variable `snapshotBeforeDelete` is redundant
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java
    pointInTimeQuery(spark, tablePath, tableName);

    Dataset<Row> snapshotBeforeDelete = snapshotAfterUpdate;
    Dataset<Row> deleteDf = delete(spark, tablePath, tableName);
    queryData(spark, jsc, tablePath, tableName, dataGen);
```

### UnnecessaryLocalVariable
Local variable `snapshotBeforeOverwrite` is redundant
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java
    assert snapshotBeforeDelete.except(deleteDf).except(snapshotAfterDelete).count() == 0;

    Dataset<Row> snapshotBeforeOverwrite = snapshotAfterDelete;
    Dataset<Row> overwriteDf = insertOverwriteData(spark, jsc, tablePath, tableName, dataGen);
    queryData(spark, jsc, tablePath, tableName, dataGen);
```

### UnnecessaryLocalVariable
Local variable `snapshotBeforeDeleteByPartition` is redundant
in `hudi-examples/hudi-examples-spark/src/main/java/org/apache/hudi/examples/quickstart/HoodieSparkQuickstart.java`
#### Snippet
```java


    Dataset<Row> snapshotBeforeDeleteByPartition = snapshotAfterOverwrite;
    deleteByPartition(spark, tablePath, tableName);
    queryData(spark, jsc, tablePath, tableName, dataGen);
```

### UnnecessaryLocalVariable
Local variable `fixed` is redundant
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java`
#### Snippet
```java
          return AvroSerdeUtils.getBufferFromBytes((byte[])fieldOI.getPrimitiveJavaObject(structFieldData));
        } else if (schema.getType() == Schema.Type.FIXED) {
          GenericData.Fixed fixed = new GenericData.Fixed(schema, (byte[])fieldOI.getPrimitiveJavaObject(structFieldData));
          return fixed;
        } else {
```

### UnnecessaryLocalVariable
Local variable `record` is redundant
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java`
#### Snippet
```java

  private GenericRecord convertArrayWritableToHoodieRecord(ArrayWritable arrayWritable) {
    GenericRecord record = serializer.serialize(arrayWritable, getHiveSchema());
    return record;
  }
```

### UnnecessaryLocalVariable
Local variable `ret` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java`
#### Snippet
```java
    LOG.info(cfg.toString());

    int ret = UtilHelpers.retry(retry, () -> {
      switch (cfg.runningMode.toLowerCase()) {
        case SCHEDULE: {
```

### UnnecessaryLocalVariable
Local variable `columnRangeMetadataList` is redundant
in `hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java`
#### Snippet
```java
      if (filePath.endsWith(HoodieFileFormat.PARQUET.getFileExtension())) {
        Path fullFilePath = new Path(datasetMetaClient.getBasePath(), filePath);
        List<HoodieColumnRangeMetadata<Comparable>> columnRangeMetadataList =
            new ParquetUtils().readRangeFromParquetMetadata(datasetMetaClient.getHadoopConf(), fullFilePath, columnsToIndex);

```

### UnnecessaryLocalVariable
Local variable `recordMerger` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java`
#### Snippet
```java
    List<String> recordMergerImplClasses = ConfigUtils.split2List(props.getProperty(HoodieWriteConfig.RECORD_MERGER_IMPLS.key(),
        HoodieWriteConfig.RECORD_MERGER_IMPLS.defaultValue()));
    HoodieRecordMerger recordMerger = HoodieRecordUtils.createRecordMerger(null, EngineType.SPARK, recordMergerImplClasses,
        props.getProperty(HoodieWriteConfig.RECORD_MERGER_STRATEGY.key(), HoodieWriteConfig.RECORD_MERGER_STRATEGY.defaultValue()));

```

### UnnecessaryLocalVariable
Local variable `newSchema` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/KafkaOffsetPostProcessor.java`
#### Snippet
```java
    newFieldList.add(new Schema.Field(KAFKA_SOURCE_PARTITION_COLUMN, Schema.create(Schema.Type.INT), "partition column", 0));
    newFieldList.add(new Schema.Field(KAFKA_SOURCE_TIMESTAMP_COLUMN, Schema.create(Schema.Type.LONG), "timestamp column", 0));
    Schema newSchema = Schema.createRecord(schema.getName() + "_processed", schema.getDoc(), schema.getNamespace(), false, newFieldList);
    return newSchema;
  }
```

### UnnecessaryLocalVariable
Local variable `transformerClassNameOverride` is redundant
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieMultiTableDeltaStreamer.java`
#### Snippet
```java
    String transformerClass = typedProperties.getString(Constants.TRANSFORMER_CLASS, null);
    if (transformerClass != null && !transformerClass.trim().isEmpty()) {
      List<String> transformerClassNameOverride = Arrays.asList(transformerClass.split(","));
      cfg.transformerClassNames = transformerClassNameOverride;
    }
```

## RuleId[id=BusyWait]
### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/index/RunIndexActionExecutor.java`
#### Snippet
```java
            instant = currentInstant.orElse(instant);
            // so that timeline is not reloaded very frequently
            Thread.sleep(TIMELINE_RELOAD_INTERVAL_MILLIS);
          } catch (InterruptedException e) {
            throw new HoodieIndexException(String.format("Thread interrupted while running indexing check for instant: %s", instant), e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java`
#### Snippet
```java
    try {
      while (!semaphore.tryAcquire(numOps)) {
        Thread.sleep(WAIT_BEFORE_NEXT_ACQUIRE_PERMIT_IN_MS);
      }
      LOG.debug(String.format("acquire permits: %s, maxPermits: %s", numOps, maxPermits));
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-common/src/main/java/org/apache/hudi/common/util/RetryHelper.java`
#### Snippet
```java
        LOG.warn("Catch Exception for " + taskInfo + ", will retry after " + waitTime + " ms.", e);
        try {
          Thread.sleep(waitTime);
        } catch (InterruptedException ex) {
          // ignore InterruptedException here
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieDataTableValidator.java`
#### Snippet
```java
              LOG.info("Last validate ran less than min validate interval: " + cfg.minValidateIntervalSeconds + " s, sleep: "
                  + toSleepMs + " ms.");
              Thread.sleep(toSleepMs);
            }
          } catch (HoodieValidationException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java`
#### Snippet
```java
      while (true) {
        try {
          Thread.sleep(60000);
        } catch (InterruptedException e) {
          // skip it
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieMetadataTableValidator.java`
#### Snippet
```java
              LOG.info("Last validate ran less than min validate interval: " + cfg.minValidateIntervalSeconds + " s, sleep: "
                  + toSleepMs + " ms.");
              Thread.sleep(toSleepMs);
            }
          } catch (HoodieValidationException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/compact/HoodieFlinkCompactor.java`
#### Snippet
```java
            try {
              compact();
              Thread.sleep(cfg.minCompactionIntervalSeconds * 1000);
            } catch (ApplicationExecutionException aee) {
              if (aee.getMessage().contains(NO_EXECUTE_KEYWORD)) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hudi-flink-datasource/hudi-flink/src/main/java/org/apache/hudi/sink/clustering/HoodieFlinkClusteringJob.java`
#### Snippet
```java
            try {
              cluster();
              Thread.sleep(cfg.minClusteringIntervalSeconds * 1000);
            } catch (ApplicationExecutionException aee) {
              if (aee.getMessage().contains(NO_EXECUTE_KEYWORD)) {
```

## RuleId[id=ArraysAsListWithZeroOrOneArgument]
### ArraysAsListWithZeroOrOneArgument
Call to `asList()` with only one argument
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/sort/SpaceCurveSortingHelper.java`
#### Snippet
```java
        CollectionUtils.combine(
            Arrays.asList(schema.fields()),
            Arrays.asList(new StructField("Index", BinaryType$.MODULE$, true, Metadata.empty()))
        )
    );
```

### ArraysAsListWithZeroOrOneArgument
Call to `asList()` with only one argument
in `hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java`
#### Snippet
```java

  private HoodieData<HoodieRecord> getFilesPartitionRecords(String createInstantTime, List<DirectoryInfo> partitionInfoList, HoodieRecord allPartitionRecord) {
    HoodieData<HoodieRecord> filesPartitionRecords = engineContext.parallelize(Arrays.asList(allPartitionRecord), 1);
    if (partitionInfoList.isEmpty()) {
      return filesPartitionRecords;
```

### ArraysAsListWithZeroOrOneArgument
Call to `asList()` with only one argument
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/Types.java`
#### Snippet
```java
    @Override
    public List<Field> fields() {
      return Arrays.asList(elementField);
    }

```

### ArraysAsListWithZeroOrOneArgument
Call to `asList()` with only one argument
in `hudi-common/src/main/java/org/apache/hudi/internal/schema/utils/SerDeHelper.java`
#### Snippet
```java
    }
    if (oldSchemas == null || oldSchemas.isEmpty()) {
      return toJson(Arrays.asList(newSchema));
    }
    String checkedString = "{\"schemas\":[";
```

### ArraysAsListWithZeroOrOneArgument
Call to `asList()` with only one argument
in `hudi-kafka-connect/src/main/java/org/apache/hudi/connect/utils/KafkaConnectUtils.java`
#### Snippet
```java
    try {
      AdminClient client = AdminClient.create(props);
      DescribeTopicsResult result = client.describeTopics(Arrays.asList(topicName));
      Map<String, KafkaFuture<TopicDescription>> values = result.values();
      KafkaFuture<TopicDescription> topicDescription = values.get(topicName);
```

## RuleId[id=IntegerDivisionInFloatingPointContext]
### IntegerDivisionInFloatingPointContext
`maxReqPerSec / maxParallelPutsTask`: integer division in floating-point context
in `hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java`
#### Snippet
```java
      int numTasks = numTasksDuringPut;
      int maxParallelPutsTask = Math.max(1, Math.min(numTasks, maxExecutors));
      int multiPutBatchSizePerSecPerTask = Math.max(1, (int) Math.ceil(maxReqPerSec / maxParallelPutsTask));
      LOG.info("HbaseIndexThrottling: qpsFraction :" + qpsFraction);
      LOG.info("HbaseIndexThrottling: numRSAlive :" + numRSAlive);
```

### IntegerDivisionInFloatingPointContext
`bitStep / 8`: integer division in floating-point context
in `hudi-common/src/main/java/org/apache/hudi/common/util/BinaryUtil.java`
#### Snippet
```java
    int totalBits = size * 8;
    for (int bitStep = 0; bitStep < totalBits; bitStep++) {
      int currentBytePos = (int) Math.floor(bitStep / 8);
      int currentBitPos = bitStep % 8;

```

### IntegerDivisionInFloatingPointContext
`resBitPos / 8`: integer division in floating-point context
in `hudi-common/src/main/java/org/apache/hudi/common/util/BinaryUtil.java`
#### Snippet
```java

      for (int i = 0; i < candidateSize; i++) {
        int tempResBytePos = (int) Math.floor(resBitPos / 8);
        int tempResBitPos = resBitPos % 8;
        result[tempResBytePos] = updatePos(result[tempResBytePos], tempResBitPos, buffer[i][currentBytePos], currentBitPos);
```

### IntegerDivisionInFloatingPointContext
`(numLogFilesSeen - 1) / logFilePaths.size()`: integer division in floating-point context
in `hudi-common/src/main/java/org/apache/hudi/common/table/log/AbstractHoodieLogRecordReader.java`
#### Snippet
```java
    }
    // At this step the lastBlocks are consumed. We track approximate progress by number of log-files seen
    progress = (numLogFilesSeen - 1) / logFilePaths.size();
  }

```

## RuleId[id=EqualsWhichDoesntCheckParameterClass]
### EqualsWhichDoesntCheckParameterClass
`equals()` should check the class of its parameter
in `hudi-common/src/main/java/org/apache/hudi/common/fs/HoodieWrapperFileSystem.java`
#### Snippet
```java

  @Override
  public boolean equals(Object obj) {
    return fileSystem.equals(obj);
  }
```

## RuleId[id=UseBulkOperation]
### UseBulkOperation
Iteration can be replaced with bulk 'Map.putAll()' call
in `hudi-utilities/src/main/java/org/apache/hudi/utilities/deser/KafkaAvroSchemaDeserializer.java`
#### Snippet
```java
    TypedProperties typedProperties = new TypedProperties();
    for (Entry<String, ?> entry : configs.entrySet()) {
      typedProperties.put(entry.getKey(), entry.getValue());
    }
    return typedProperties;
```

## RuleId[id=BigDecimalMethodWithoutRoundingCalled]
### BigDecimalMethodWithoutRoundingCalled
'BigDecimal.setScale()' called without a rounding mode argument
in `hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java`
#### Snippet
```java
              bytes = ((GenericFixed) oldValue).bytes();
              Decimal decimal = (Decimal) oldSchema.getLogicalType();
              BigDecimal bd = new BigDecimal(new BigInteger(bytes), decimal.getScale()).setScale(((Decimal) newSchema.getLogicalType()).getScale());
              return DECIMAL_CONVERSION.toFixed(bd, newSchema, newSchema.getLogicalType());
            } else {
```

### BigDecimalMethodWithoutRoundingCalled
'BigDecimal.setScale()' called without a rounding mode argument
in `hudi-common/src/main/java/org/apache/hudi/common/util/AvroOrcUtils.java`
#### Snippet
```java
        BigDecimal bigDecimal = ((DecimalColumnVector) colVector).vector[vectorPos]
            .getHiveDecimal().bigDecimalValue()
            .setScale(((LogicalTypes.Decimal) logicalType).getScale());
        Schema.Type baseType = avroSchema.getType();
        if (baseType.equals(Schema.Type.FIXED)) {
```

### BigDecimalMethodWithoutRoundingCalled
'BigDecimal.setScale()' called without a rounding mode argument
in `hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java`
#### Snippet
```java
        HiveDecimal dec = (HiveDecimal)fieldOI.getPrimitiveJavaObject(structFieldData);
        LogicalTypes.Decimal decimal = (LogicalTypes.Decimal)schema.getLogicalType();
        BigDecimal bd = new BigDecimal(dec.toString()).setScale(decimal.getScale());
        return HoodieAvroUtils.DECIMAL_CONVERSION.toFixed(bd, schema, decimal);
      case CHAR:
```

