# hbase 
 
# Bad smells
I found 9984 bad smells with 1201 repairable:
| ruleID | number | fixable |
| --- | --- | --- |
| RedundantFieldInitialization | 1085 | false |
| ReturnNull | 936 | false |
| BoundedWildcard | 729 | false |
| SystemOutErr | 720 | false |
| MissortedModifiers | 550 | false |
| UnusedAssignment | 454 | false |
| UnnecessaryFullyQualifiedName | 424 | false |
| UnnecessaryModifier | 408 | true |
| AssignmentToMethodParameter | 343 | false |
| ConstantValue | 260 | false |
| PublicFieldAccessedInSynchronizedContext | 237 | false |
| EmptyMethod | 224 | false |
| SizeReplaceableByIsEmpty | 195 | true |
| DataFlowIssue | 185 | false |
| FieldAccessedSynchronizedAndUnsynchronized | 167 | false |
| UnnecessaryToStringCall | 142 | true |
| DuplicateThrows | 130 | false |
| SynchronizeOnThis | 124 | false |
| StringConcatenationInsideStringBufferAppend | 118 | false |
| DeprecatedIsStillUsed | 113 | false |
| NonProtectedConstructorInAbstractClass | 94 | true |
| ZeroLengthArrayInitialization | 89 | false |
| UnnecessaryLocalVariable | 87 | true |
| Convert2MethodRef | 86 | false |
| NestedAssignment | 80 | false |
| InstanceofCatchParameter | 78 | false |
| BusyWait | 76 | false |
| UtilityClassWithoutPrivateConstructor | 73 | true |
| MethodOverridesStaticMethod | 70 | false |
| ToArrayCallWithZeroLengthArrayArgument | 68 | true |
| ReplaceAssignmentWithOperatorAssignment | 68 | false |
| RedundantMethodOverride | 64 | false |
| StringBufferReplaceableByString | 52 | false |
| Convert2Lambda | 46 | false |
| DynamicRegexReplaceableByCompiledPattern | 45 | false |
| UnnecessaryUnboxing | 44 | false |
| StaticCallOnSubclass | 41 | false |
| MismatchedJavadocCode | 41 | false |
| NonShortCircuitBoolean | 39 | false |
| IntegerMultiplicationImplicitCastToLong | 38 | false |
| UnnecessaryReturn | 36 | true |
| UnnecessarySuperQualifier | 34 | false |
| SynchronizationOnLocalVariableOrMethodParameter | 32 | false |
| Java8MapApi | 32 | false |
| AssignmentToForLoopParameter | 31 | false |
| CodeBlock2Expr | 30 | true |
| FinalStaticMethod | 28 | false |
| RedundantSuppression | 27 | false |
| UnnecessaryBoxing | 27 | false |
| PointlessBooleanExpression | 26 | true |
| DuplicateBranchesInSwitch | 26 | false |
| ObsoleteCollection | 25 | false |
| ConditionCoveredByFurtherCondition | 24 | false |
| UnnecessaryContinue | 24 | false |
| CStyleArrayDeclaration | 23 | false |
| StringOperationCanBeSimplified | 23 | false |
| OptionalGetWithoutIsPresent | 23 | false |
| SimplifyStreamApiCallChains | 22 | false |
| OptionalUsedAsFieldOrParameterType | 22 | false |
| ClassNameSameAsAncestorName | 20 | false |
| FinalPrivateMethod | 19 | false |
| OptionalContainsCollection | 19 | false |
| NonStrictComparisonCanBeEquality | 19 | true |
| NullableProblems | 18 | false |
| AbstractMethodCallInConstructor | 18 | false |
| UnnecessaryCallToStringValueOf | 17 | false |
| NonFinalFieldOfException | 16 | false |
| ManualMinMaxCalculation | 15 | false |
| RedundantImplements | 15 | false |
| AssignmentToStaticFieldFromInstanceMethod | 14 | false |
| UseBulkOperation | 14 | false |
| PointlessArithmeticExpression | 13 | false |
| IOResource | 13 | false |
| TrivialStringConcatenation | 12 | false |
| UNUSED_IMPORT | 12 | false |
| AbstractClassNeverImplemented | 11 | false |
| SynchronizeOnNonFinalField | 11 | false |
| WhileCanBeForeach | 10 | false |
| UnnecessarySemicolon | 10 | false |
| AssignmentToCatchBlockParameter | 10 | false |
| ArrayEquality | 10 | false |
| NonSerializableFieldInSerializableClass | 10 | false |
| CatchMayIgnoreException | 10 | false |
| UnnecessaryQualifierForThis | 9 | false |
| FieldMayBeStatic | 9 | false |
| CastCanBeRemovedNarrowingVariableType | 9 | false |
| SuspiciousNameCombination | 8 | false |
| ManualArrayToCollectionCopy | 8 | false |
| AssignmentToLambdaParameter | 8 | false |
| AnonymousHasLambdaAlternative | 8 | false |
| EmptyStatementBody | 7 | false |
| IgnoreResultOfCall | 7 | false |
| ProtectedMemberInFinalClass | 7 | true |
| RedundantArrayCreation | 7 | true |
| IfStatementWithIdenticalBranches | 7 | false |
| StringEqualsEmptyString | 7 | false |
| Anonymous2MethodRef | 7 | false |
| ObjectNotify | 6 | false |
| TypeParameterHidesVisibleType | 6 | false |
| JavaReflectionMemberAccess | 6 | false |
| RedundantStringFormatCall | 6 | false |
| ComparatorCombinators | 6 | false |
| SetReplaceableByEnumSet | 6 | false |
| WrapperTypeMayBePrimitive | 5 | false |
| RegExpRedundantEscape | 5 | false |
| ExplicitArrayFilling | 5 | false |
| ComparatorMethodParameterNotUsed | 5 | false |
| UnnecessaryStringEscape | 5 | true |
| RegExpSimplifiable | 5 | false |
| Java8MapForEach | 5 | false |
| NestedSynchronizedStatement | 5 | false |
| ThreadStartInConstruction | 5 | false |
| CommentedOutCode | 4 | false |
| DuplicateCondition | 4 | false |
| PointlessBitwiseExpression | 4 | false |
| IndexOfReplaceableByContains | 4 | false |
| CharsetObjectCanBeUsed | 4 | false |
| ThrowFromFinallyBlock | 4 | false |
| UseOfPropertiesAsHashtable | 4 | false |
| ExceptionNameDoesntEndWithException | 4 | false |
| OptionalIsPresent | 4 | false |
| RedundantLengthCheck | 4 | false |
| InnerClassMayBeStatic | 4 | true |
| RegExpUnnecessaryNonCapturingGroup | 4 | false |
| OptionalAssignedToNull | 3 | false |
| EqualsAndHashcode | 3 | false |
| CastConflictsWithInstanceof | 3 | false |
| MethodOverloadsParentMethod | 3 | false |
| SwitchStatementWithConfusingDeclaration | 3 | false |
| PrimitiveArrayArgumentToVariableArgMethod | 3 | false |
| LongLiteralsEndingWithLowercaseL | 3 | false |
| WhileLoopSpinsOnField | 3 | false |
| ForLoopReplaceableByWhile | 2 | false |
| ReturnFromFinallyBlock | 2 | false |
| SimplifiableBooleanExpression | 2 | false |
| UseCompareMethod | 2 | false |
| RedundantFileCreation | 2 | false |
| ConditionalBreakInInfiniteLoop | 2 | false |
| ManualArrayCopy | 2 | false |
| MisspelledEquals | 2 | false |
| DuplicateExpressions | 2 | false |
| NegativeIntConstantInLongContext | 2 | false |
| FunctionalExpressionCanBeFolded | 2 | false |
| SlowListContainsAll | 2 | false |
| RedundantOperationOnEmptyContainer | 2 | false |
| EqualsWhichDoesntCheckParameterClass | 2 | false |
| MismatchedArrayReadWrite | 1 | false |
| StaticInitializerReferencesSubClass | 1 | false |
| Java8ListReplaceAll | 1 | false |
| EqualsBetweenInconvertibleTypes | 1 | false |
| ListRemoveInLoop | 1 | false |
| ExcessiveLambdaUsage | 1 | false |
| CallToStringConcatCanBeReplacedByOperator | 1 | false |
| AssignmentUsedAsCondition | 1 | false |
| RefusedBequest | 1 | false |
| SuspiciousListRemoveInLoop | 1 | false |
| IfStatementMissingBreakInLoop | 1 | false |
| DefaultAnnotationParam | 1 | false |
| InfiniteLoopStatement | 1 | false |
| FuseStreamOperations | 1 | false |
| NonExceptionNameEndsWithException | 1 | false |
| RedundantCollectionOperation | 1 | false |
| RegExpDuplicateAlternationBranch | 1 | false |
| MissingDeprecatedAnnotation | 1 | false |
| RegExpSingleCharAlternation | 1 | false |
| CopyConstructorMissesField | 1 | false |
| BigDecimalMethodWithoutRoundingCalled | 1 | false |
## RuleId[id=ToArrayCallWithZeroLengthArrayArgument]
### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new HostAndWeight\[orderedHosts.size()\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java`
#### Snippet
```java
    NavigableSet<HostAndWeight> orderedHosts = new TreeSet<>(new HostAndWeight.WeightComparator());
    orderedHosts.addAll(this.hostAndWeights.values());
    return orderedHosts.descendingSet().toArray(new HostAndWeight[orderedHosts.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[list.size()\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java`
#### Snippet
```java
    assert (version != null);
    List<String> list = Splitter.onPattern("[\\.-]").splitToList(version);
    String[] strComps = list.toArray(new String[list.size()]);
    assert (strComps.length > 0);
    String[] comps = new String[strComps.length];
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new DataType\[fields.size()\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructBuilder.java`
#### Snippet
```java
   */
  public Struct toStruct() {
    return new Struct(fields.toArray(new DataType<?>[fields.size()]));
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[result.size()\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
          try {
            Set<String> result = new LinkedHashSet<>(groups.getGroups(ugi));
            return result.toArray(new String[result.size()]);
          } catch (Exception e) {
            return new String[0];
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[parts.size()\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
  public static ZKClusterKey transformClusterKey(String key) throws IOException {
    List<String> parts = Splitter.on(':').splitToList(key);
    String[] partsArray = parts.toArray(new String[parts.size()]);

    if (partsArray.length == 3) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new HRegionLocation\[locations.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java

  public RegionLocations(Collection<HRegionLocation> locations) {
    this(locations.toArray(new HRegionLocation[locations.size()]));
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new ColumnFamilyDescriptor\[families.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    @Override
    public ColumnFamilyDescriptor[] getColumnFamilies() {
      return families.values().toArray(new ColumnFamilyDescriptor[families.size()]);
    }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Cell\[cells.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      return new Result(null, exists, stale, mayHaveMoreCellsInRow);
    }
    return new Result(cells.toArray(new Cell[cells.size()]), null, stale, mayHaveMoreCellsInRow);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new ReplicationProtos.TableCF\[tableCFList.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      tableCFList.add(tableCFBuilder.build());
    }
    return tableCFList.toArray(new ReplicationProtos.TableCF[tableCFList.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new ReplicationProtos.TableCF\[tableCFList.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      tableCFList.add(tableCFBuilder.build());
    }
    return tableCFList.toArray(new ReplicationProtos.TableCF[tableCFList.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Action\[actions.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/Permission.java`
#### Snippet
```java

    public Permission build() {
      Action[] actionArray = actions.toArray(new Action[actions.size()]);
      if (namespace != null) {
        return new NamespacePermission(namespace, actionArray);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Action\[actions.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/Permission.java`
#### Snippet
```java

  public Action[] getActions() {
    return actions.toArray(new Action[actions.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Procedure\[subProcList.size()\]'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java

      if (subProcList != null && !subProcList.isEmpty()) {
        Procedure[] subProcedures = subProcList.toArray(new Procedure[subProcList.size()]);
        subProcList = null;
        return subProcedures;
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[res.size()\]'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
          }
        }
        return res.toArray(new String[res.size()]);
      }
    }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new ServerName\[hosts.size()\]'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java`
#### Snippet
```java
      hosts.add(ServerName.valueOf(host, port, -1));
    }
    return hosts.toArray(new ServerName[hosts.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[locations.size()\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
        this.locations = new String[0];
      } else {
        this.locations = locations.toArray(new String[locations.size()]);
      }
      try {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[locationsList.size()\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
      this.regionInfo = ProtobufUtil.toRegionInfo(split.getRegion());
      List<String> locationsList = split.getLocationsList();
      this.locations = locationsList.toArray(new String[locationsList.size()]);

      this.scan = Bytes.toString(Bytes.readByteArray(in));
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[filterArgs.size()\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    List<String> filterArgs) throws IOException {
    conf.set(Import.FILTER_CLASS_CONF_KEY, clazz.getName());
    conf.setStrings(Import.FILTER_ARGS_CONF_KEY, filterArgs.toArray(new String[filterArgs.size()]));
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[hosts.size()\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java

      List<String> hosts = hdfsBlocksDistribution.getTopHosts();
      return hosts.toArray(new String[hosts.size()]);
    }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[a.size()\]'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
          for (Map.Entry<String, List<String>> entry : m.entrySet()) {
            final List<String> a = entry.getValue();
            parameters.put(entry.getKey(), a.toArray(new String[a.size()]));
          }
        }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[scanStrings.size()\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
    }
    job.getConfiguration().setStrings(MultiTableInputFormat.SCANS,
      scanStrings.toArray(new String[scanStrings.size()]));

    if (addDependencyJars) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[jars.size()\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
    if (jars.isEmpty()) return;

    conf.set("tmpjars", StringUtils.arrayToString(jars.toArray(new String[jars.size()])));
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[secureProtocols.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.java`
#### Snippet
```java
      }
    }
    socket.setEnabledProtocols(secureProtocols.toArray(new String[secureProtocols.size()]));
    return socket;
  }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[secureProtocols.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.java`
#### Snippet
```java
          }
        }
        sslSocket.setEnabledProtocols(secureProtocols.toArray(new String[secureProtocols.size()]));

        return sslSocket;
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[execScript.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java`
#### Snippet
```java
    ArrayList<String> execScript = new ArrayList<>();
    execScript.add(healthCheckScript);
    this.shexec = new ShellCommandExecutor(execScript.toArray(new String[execScript.size()]), null,
      null, scriptTimeout);
    LOG.info("HealthChecker initialized with script at " + this.healthCheckScript + ", timeout="
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Path\[locations.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
   */
  public FileLink(final Collection<Path> locations) {
    this.locations = locations.toArray(new Path[locations.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[splitFutures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
            addListener(
              CompletableFuture
                .allOf(splitFutures.toArray(new CompletableFuture<?>[splitFutures.size()])),
              (ret, exception) -> {
                if (exception != null) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        });
      addListener(
        CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[futures.size()])),
        (result, err2) -> {
          if (err2 != null) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[compactFutures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      }
      addListener(CompletableFuture.allOf(
        compactFutures.toArray(new CompletableFuture<?>[compactFutures.size()])), (ret, err2) -> {
          if (err2 != null) {
            future.completeExceptionally(err2);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          status.getBackupMasterNames().forEach(master -> futures.add(updateConfiguration(master)));
          addListener(
            CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[futures.size()])),
            (result, err2) -> {
              if (err2 != null) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[compactFutures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      }
      addListener(CompletableFuture.allOf(
        compactFutures.toArray(new CompletableFuture<?>[compactFutures.size()])), (ret, err2) -> {
          if (err2 != null) {
            future.completeExceptionally(err2);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
            });
          addListener(
            CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[futures.size()])),
            (ret, err3) -> {
              // If future not completed, check all regions's compaction state
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      });
      addListener(
        CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[futures.size()])),
        (ret, err3) -> {
          if (!future.isCompletedExceptionally()) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
            groupServers.forEach(server -> futures.add(updateConfiguration(server)));
            addListener(
              CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[futures.size()])),
              (result, err3) -> {
                if (err3 != null) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          }));
      }
      addListener(CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()])),
        (ret, err3) -> {
          if (err3 != null) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[tmpTables.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

        if (tmpTables.size() > 0) {
          returnTables = tmpTables.toArray(new String[tmpTables.size()]);
        } else {
          String msg = "No HTable found, tablePattern:" + Arrays.toString(monitorTargets);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[serializedKvps.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConfigurationUtil.java`
#### Snippet
```java
    }

    conf.setStrings(key, serializedKvps.toArray(new String[serializedKvps.size()]));
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new RegionInfo\[check.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
          check.add(regionLocator.getRegionLocation(start).getRegion());
          check.add(regionLocator.getRegionLocation(split).getRegion());
          for (RegionInfo hri : check.toArray(new RegionInfo[check.size()])) {
            byte[] sk = hri.getStartKey();
            if (sk.length == 0) sk = splitAlgo.firstRow();
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new URL\[urlList.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java`
#### Snippet
```java

    List<URL> urlList = buildClasspath(jars);
    URL[] urls = urlList.toArray(new URL[urlList.size()]);

    LOG.debug("Classpath: {}", urlList);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new ServerName\[servers.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java`
#### Snippet
```java
    if (numReplicas > 1) {
      Random rand = ThreadLocalRandom.current();
      ServerName[] serversArr = servers.toArray(new ServerName[servers.size()]);
      for (int i = 1; i < numReplicas; i++) {
        ServerName sn = serversArr[rand.nextInt(serversArr.length)];
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new TableDescriptor\[tds.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      Admin admin = conn.getAdmin()) {
      List<TableDescriptor> tds = admin.listTableDescriptors(tableNames);
      return tds.toArray(new TableDescriptor[tds.size()]);
    } catch (IOException e) {
      LOG.debug("Exception getting table descriptors", e);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[groups.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
        if (!caller.getShortName().equals(userName)) {
          List<String> groups = AccessChecker.getUserGroups(userName);
          caller = new InputUser(userName, groups.toArray(new String[groups.size()]));
        }
        List<Boolean> hasUserPermissions = new ArrayList<>();
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
      // current directory asynchronously.
      FutureUtils.addListener(
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()])),
        (voidObj, e) -> {
          if (e != null) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[cleanerClasses.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
        }
        conf.setStrings(HFileCleaner.HFILE_CLEANER_CUSTOM_PATHS_PLUGINS,
          cleanerClasses.toArray(new String[cleanerClasses.size()]));
        LOG.info("Archive custom cleaner paths: {}, plugins: {}", Arrays.asList(paths),
          cleanerClasses);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[masterCoprocessors.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  public String[] getMasterCoprocessors() {
    Set<String> masterCoprocessors = getMasterCoprocessorHost().getCoprocessors();
    return masterCoprocessors.toArray(new String[masterCoprocessors.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[hfileCleaners.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
      // Set cleaners conf
      conf.setStrings(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS,
        hfileCleaners.toArray(new String[hfileCleaners.size()]));
      conf.setStrings(HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS,
        logCleaners.toArray(new String[logCleaners.size()]));
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[logCleaners.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
        hfileCleaners.toArray(new String[hfileCleaners.size()]));
      conf.setStrings(HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS,
        logCleaners.toArray(new String[logCleaners.size()]));
    } else {
      // There may be restore tables if snapshot is enabled and then disabled, so add
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[hfileCleaners.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
      hfileCleaners.add(HFileLinkCleaner.class.getName());
      conf.setStrings(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS,
        hfileCleaners.toArray(new String[hfileCleaners.size()]));
      // Verify if SnapshotHFileCleaner are present
      snapshotEnabled = hfileCleaners.contains(SnapshotHFileCleaner.class.getName());
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Procedure\[procedures.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java`
#### Snippet
```java
    SplitWALManager splitWALManager = env.getMasterServices().getSplitWALManager();
    List<Procedure> procedures = splitWALManager.splitWALs(serverName, splitMeta);
    return procedures.toArray(new Procedure[procedures.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new RegionInfo\[newRegions.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java`
#### Snippet
```java
    if (cpHost != null) {
      final RegionInfo[] regions =
        (newRegions == null) ? null : newRegions.toArray(new RegionInfo[newRegions.size()]);
      cpHost.postCompletedCreateTableAction(tableDescriptor, regions, getUser());
    }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new RegionInfo\[newRegions.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java`
#### Snippet
```java
    if (cpHost != null) {
      final RegionInfo[] regions =
        newRegions == null ? null : newRegions.toArray(new RegionInfo[newRegions.size()]);
      cpHost.preCreateTableAction(tableDescriptor, regions, getUser());
    }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new RegionInfo\[newRegions.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java`
#### Snippet
```java
        throws IOException {
        RegionInfo[] regions =
          newRegions != null ? newRegions.toArray(new RegionInfo[newRegions.size()]) : null;
        return ModifyRegionUtils.createRegions(env.getMasterConfiguration(), tableRootDir,
          tableDescriptor, regions, null);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new RegionInfo\[newRegions.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java`
#### Snippet
```java
    if (cpHost != null) {
      final RegionInfo[] regions =
        (newRegions == null) ? null : newRegions.toArray(new RegionInfo[newRegions.size()]);
      cpHost.postCompletedCreateTableAction(tableDescriptor, regions, getUser());
    }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[groups.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
      // Initialize user instance for the input user name
      List<String> groups = getUserGroups(inputUserName);
      filterUser = new InputUser(inputUserName, groups.toArray(new String[groups.size()]));
    } else {
      // User don't need ADMIN privilege for self check.
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Permission.Action\[remainingActions.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
          if (!remainingActions.isEmpty()) {
            perm.getPermission()
              .setActions(remainingActions.toArray(new Permission.Action[remainingActions.size()]));
            addUserPermission(conf, perm, t);
          } else {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Mutation\[mutations.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
    throws IOException {
    OperationStatus[] opStatus =
      this.labelsRegion.batchMutate(mutations.toArray(new Mutation[mutations.size()]));
    int i = 0;
    boolean updateZk = false;
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
    }
    CompletableFuture<Void> future =
      CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()]));
    future.get();
  }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new CompletableFuture\[futures.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
    List<CompletableFuture<Void>> futures =
      operations.stream().map(this::handleHDFSAcl).collect(Collectors.toList());
    return CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()]));
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new WALEntryFilter\[rawFilters.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
      }
    }
    this.filters = rawFilters.toArray(new WALEntryFilter[rawFilters.size()]);
    initCellFilters();
  }
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new WALCellFilter\[cellFilters.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
      }
    }
    this.cellFilters = cellFilters.toArray(new WALCellFilter[cellFilters.size()]);
  }

```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Entry\[entries.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java`
#### Snippet
```java
      while (!entries.isEmpty()) {
        Pair<AdminProtos.ReplicateWALEntryRequest, CellScanner> pair = ReplicationProtobufUtil
          .buildReplicateWALEntryRequest(entries.toArray(new Entry[entries.size()]));
        ReplicateWALEntryRequest request = pair.getFirst();
        rs.getReplicationSinkService().replicateLogEntries(request.getEntryList(), pair.getSecond(),
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Entry\[entries.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
    final SinkPeer sinkPeerToUse = sinkPeer;
    FutureUtils.addListener(
      ReplicationProtobufUtil.replicateWALEntry(rsAdmin, entries.toArray(new Entry[entries.size()]),
        replicationClusterId, baseNamespaceDir, hfileArchiveDir, timeout),
      (response, exception) -> {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new MutationReplay\[mutations.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        server.getMemStoreFlusher().reclaimMemStoreMemory();
      }
      return region.batchReplay(mutations.toArray(new MutationReplay[mutations.size()]),
        replaySeqId);
    } finally {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new TableName\[tableList.size()\]'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java

    // there are one or more tables in the table list
    backupInfo = new BackupInfo(backupId, type, tableList.toArray(new TableName[tableList.size()]),
      targetRootDir);
    backupInfo.setBandwidth(bandwidth);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new String\[tNamespaces.size()\]'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java`
#### Snippet
```java
    try {
      List<String> tNamespaces = client.listNamespaces();
      return tNamespaces.toArray(new String[tNamespaces.size()]);
    } catch (TException e) {
      throw new IOException(e);
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Mutation\[mutations.size()\]'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  public void mutateRowsWithLocks(Collection<Mutation> mutations, Collection<byte[]> rowsToLock,
    long nonceGroup, long nonce) throws IOException {
    batchMutate(new MutationBatchOperation(this, mutations.toArray(new Mutation[mutations.size()]),
      true, nonceGroup, nonce) {
      @Override
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new SequenceFile.Writer.Option\[opts.size()\]'
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java
        SequenceFile.Writer sequenceFileWriter =
          user.runAs((PrivilegedExceptionAction<SequenceFile.Writer>) () -> SequenceFile
            .createWriter(conf, opts.toArray(new SequenceFile.Writer.Option[opts.size()])));
        privilegedWriter = new PrivilegedWriter(user, sequenceFileWriter);
      } catch (InterruptedException e) {
```

### ToArrayCallWithZeroLengthArrayArgument
Call to `toArray()` with pre-sized array argument 'new Algorithm\[supportedAlgos.size()\]'
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      }
    }
    return supportedAlgos.toArray(new Algorithm[supportedAlgos.size()]);
  }

```

## RuleId[id=WrapperTypeMayBePrimitive]
### WrapperTypeMayBePrimitive
Type may be primitive
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
      }

      Float multiplier = c.getMultiplier();
      double cost = c.cost();

```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.java`
#### Snippet
```java
        return new PairOfSameType<>(0, 0);
      }
      Integer onlineRegionCount = 0;
      Integer offlineRegionCount = 0;

```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.java`
#### Snippet
```java
      }
      Integer onlineRegionCount = 0;
      Integer offlineRegionCount = 0;

      List<TableDescriptor> descriptors = master.listTableDescriptors(null, null, null, false);
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java

            // To eventually infer start key-end key boundaries
            Integer value = map.containsKey(first) ? (Integer) map.get(first) : 0;
            map.put(first, value + 1);
            value = map.containsKey(last) ? (Integer) map.get(last) : 0;
```

### WrapperTypeMayBePrimitive
Type may be primitive
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);
      S sumVal = null;
      Long rowCountVal = 0L;
      Scan scan = ProtobufUtil.toScan(request.getScan());
      scanner = env.getRegion().getScanner(scan);
```

## RuleId[id=MismatchedArrayReadWrite]
### MismatchedArrayReadWrite
Contents of array `remainArgs` are written to, but never read
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java

    String cmd = null;
    String[] remainArgs = null;
    if (args == null || args.length == 0) {
      printToolUsage();
```

## RuleId[id=UnnecessaryModifier]
### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * @return tag value in a new byte array.
   */
  public static byte[] cloneValue(Tag tag) {
    int tagLength = tag.getValueLength();
    byte[] tagArr = new byte[tagLength];
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * @return True if values of both tags are same.
   */
  public static boolean matchingValue(Tag t1, Tag t2) {
    if (t1.hasArray() && t2.hasArray()) {
      return Bytes.equals(t1.getValueArray(), t1.getValueOffset(), t1.getValueLength(),
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
public interface Tag {

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
public interface Tag {

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
public interface Tag {

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * internal tag types
   */
  public static final byte CUSTOM_TAG_TYPE_RANGE = (byte) 64;

  /** Returns the tag type */
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * internal tag types
   */
  public static final byte CUSTOM_TAG_TYPE_RANGE = (byte) 64;

  /** Returns the tag type */
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * internal tag types
   */
  public static final byte CUSTOM_TAG_TYPE_RANGE = (byte) 64;

  /** Returns the tag type */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * @return value as long
   */
  public static long getValueAsLong(Tag tag) {
    if (tag.hasArray()) {
      return Bytes.toLong(tag.getValueArray(), tag.getValueOffset(), tag.getValueLength());
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * @param offset The offset within 'out' array where to copy the Tag value.
   */
  public static void copyValueTo(Tag tag, byte[] out, int offset) {
    if (tag.hasArray()) {
      Bytes.putBytes(out, offset, tag.getValueArray(), tag.getValueOffset(), tag.getValueLength());
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * @return value as byte
   */
  public static byte getValueAsByte(Tag tag) {
    if (tag.hasArray()) {
      return tag.getValueArray()[tag.getValueOffset()];
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
   * @return value as String
   */
  public static String getValueAsString(Tag tag) {
    if (tag.hasArray()) {
      return Bytes.toString(tag.getValueArray(), tag.getValueOffset(), tag.getValueLength());
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/RawCell.java`
#### Snippet
```java

  /** Returns A new cell which is having the extra tags also added to it. */
  public static Cell createCell(Cell cell, List<Tag> tags) {
    return PrivateCellUtil.createCell(cell, tags);
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/RawCell.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
public interface RawCell extends Cell {
  static final int MAX_TAGS_LENGTH = (2 * Short.MAX_VALUE) + 1;

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/RawCell.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
public interface RawCell extends Cell {
  static final int MAX_TAGS_LENGTH = (2 * Short.MAX_VALUE) + 1;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/RawCell.java`
#### Snippet
```java
   * @throws IllegalArgumentException if tagslength is invalid
   */
  public static void checkForTagsLength(int tagsLength) {
    if (tagsLength > MAX_TAGS_LENGTH) {
      throw new IllegalArgumentException("tagslength " + tagsLength + " > " + MAX_TAGS_LENGTH);
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/Dictionary.java`
#### Snippet
```java
   * @param dict   the dictionary whose contents are to written
   */
  public static void write(OutputStream out, byte[] data, int offset, int length, Dictionary dict)
    throws IOException {
    short dictIdx = Dictionary.NOT_IN_DICTIONARY;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/Dictionary.java`
#### Snippet
```java
   * @param dict   the dictionary whose contents are to written
   */
  public static void write(OutputStream out, ByteBuffer data, int offset, int length,
    Dictionary dict) throws IOException {
    short dictIdx = Dictionary.NOT_IN_DICTIONARY;
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java`
#### Snippet
```java
  private final BlockCategory metricCat;

  private BlockType(String magicStr, BlockCategory metricCat) {
    magic = Bytes.toBytes(magicStr);
    this.metricCat = metricCat;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Decryptor.java`
#### Snippet
```java
   * Set the secret key
   */
  public void setKey(Key key);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Decryptor.java`
#### Snippet
```java
   * Set the initialization vector
   */
  public void setIv(byte[] iv);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Decryptor.java`
#### Snippet
```java
   * Create a stream for decryption
   */
  public InputStream createDecryptionStream(InputStream in);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Decryptor.java`
#### Snippet
```java
   * @return the expected length for the initialization vector
   */
  public int getIvLength();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Decryptor.java`
#### Snippet
```java
   * @return the cipher's internal block size
   */
  public int getBlockSize();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryptor.java`
#### Snippet
```java
   * Create a stream for encryption
   */
  public OutputStream createEncryptionStream(OutputStream out);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryptor.java`
#### Snippet
```java
   * @return the cipher's internal block size
   */
  public int getBlockSize();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryptor.java`
#### Snippet
```java
   * Set the initialization vector
   */
  public void setIv(byte[] iv);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryptor.java`
#### Snippet
```java
   * @return the expected length for the initialization vector
   */
  public int getIvLength();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryptor.java`
#### Snippet
```java
   * Get the initialization vector
   */
  public byte[] getIv();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryptor.java`
#### Snippet
```java
   * Set the secret key
   */
  public void setKey(Key key);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/CipherProvider.java`
#### Snippet
```java
   * Return the provider's name
   */
  public String getName();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/CipherProvider.java`
#### Snippet
```java
   * @return the appropriate Cipher
   */
  public Cipher getCipher(String name);

}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/CipherProvider.java`
#### Snippet
```java
   * Return the set of Ciphers supported by this provider
   */
  public String[] getSupportedCiphers();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java
   * Initialize the key provider
   */
  public void init(String params);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java
   *         is not found
   */
  public Key[] getKeys(String[] aliases);

}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java

  public static final String PASSWORD = "password";
  public static final String PASSWORDFILE = "passwordfile";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java

  public static final String PASSWORD = "password";
  public static final String PASSWORDFILE = "passwordfile";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java

  public static final String PASSWORD = "password";
  public static final String PASSWORDFILE = "passwordfile";

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java
   * @return the keys corresponding to the supplied alias, or null if a key is not found
   */
  public Key getKey(String alias);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java
public interface KeyProvider {

  public static final String PASSWORD = "password";
  public static final String PASSWORDFILE = "passwordfile";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java
public interface KeyProvider {

  public static final String PASSWORD = "password";
  public static final String PASSWORDFILE = "passwordfile";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyProvider.java`
#### Snippet
```java
public interface KeyProvider {

  public static final String PASSWORD = "password";
  public static final String PASSWORDFILE = "passwordfile";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java`
#### Snippet
```java
     * @return -1 is the passed key is smaller than the current key, 0 if equal and 1 if greater
     */
    public int compareKey(CellComparator comparator, Cell key);
  }
}
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
  @SuppressWarnings("ImmutableEnumChecker")
  @InterfaceAudience.Public
  public static enum Algorithm {
    // LZO is GPL and requires extra install to setup. See
    // https://stackoverflow.com/questions/23441142/class-com-hadoop-compression-lzo-lzocodec-not-found-for-spark-on-cdh-5
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/IndexBlockEncoding.java`
#### Snippet
```java
  }

  private IndexBlockEncoding(int id, String encoderClsName) {
    if (id < 0 || id > Byte.MAX_VALUE) {
      throw new AssertionError("Data block encoding algorithm id is out of range: " + id);
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java`
#### Snippet
```java
  }

  private DataBlockEncoding(int id, String encoderClsName) {
    if (id < 0 || id > Byte.MAX_VALUE) {
      throw new AssertionError("Data block encoding algorithm id is out of range: " + id);
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static interface AvlKeyComparator<TNode extends AvlNode> {
    int compareKey(TNode node, Object key);
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static interface AvlNodeVisitor<TNode extends AvlNode> {
    /**
     * Visitor
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ChecksumType.java`
#### Snippet
```java
  public abstract DataChecksum.Type getDataChecksumType();

  private ChecksumType(final byte c) {
    this.code = c;
  }
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/DNS.java`
#### Snippet
```java
    private final String name;

    private ServerType(String name) {
      this.name = name;
    }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param val   the value to store
   */
  public ByteRange putLong(int index, long val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * The underlying byte[].
   */
  public byte[] getBytes();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange get(int index, byte[] dst);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange put(int index, byte val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange put(int index, byte[] val, int offset, int length);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return number of bytes written
   */
  public int putVLong(int index, long val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param val   the value to store
   */
  public ByteRange putShort(int index, short val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return single byte at index.
   */
  public byte get(int index);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java

  /** Returns true when this range is of zero length, false otherwise. */
  public boolean isEmpty();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param capacity the size of a new byte[].
   */
  public ByteRange set(int capacity);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return The newly cloned byte[].
   */
  public byte[] deepCopyToNewArray();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * The length of the range.
   */
  public int getLength();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return the long value at {@code index}
   */
  public long getLong(int index);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param bytes the array to wrap.
   */
  public ByteRange set(byte[] bytes);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange get(int index, byte[] dst, int offset, int length);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return new {@code ByteRange} object referencing this range's byte[].
   */
  public ByteRange shallowCopy();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange put(int index, byte[] val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return new {@code ByteRange} object referencing this range's byte[].
   */
  public ByteRange shallowCopySubRange(int innerOffset, int copyLength);

}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param destinationOffset First index in the destination array.
   */
  public void deepCopyTo(byte[] destination, int destinationOffset);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param destinationOffset First index in the destination array.
   */
  public void deepCopySubRangeTo(int innerOffset, int copyLength, byte[] destination,
    int destinationOffset);

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * Nullifies this ByteRange. That is, it becomes a husk, being a range over no byte[] whatsoever.
   */
  public ByteRange unset();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return Deep copy
   */
  public ByteRange deepCopy();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return the long value at {@code index} which is stored as VLong
   */
  public long getVLong(int index);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange setOffset(int offset);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange set(byte[] bytes, int offset, int length);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public ByteRange setLength(int length);

  /** Returns true when this range is of zero length, false otherwise. */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return the short value at {@code index}
   */
  public short getShort(int index);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @see #getBytes()
   */
  public int getOffset();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @param val   the value to store
   */
  public ByteRange putInt(int index, int val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRange.java`
#### Snippet
```java
   * @return the int value at {@code index}
   */
  public int getInt(int index);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Addressing.java`
#### Snippet
```java
     * @return true to accept this address
     */
    public boolean isAcceptableAddress(InetAddress address);
  }
}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Retrieve the next byte from this range without incrementing position.
   */
  public byte peek();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange putLong(int index, long val);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange put(int index, byte val);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange deepCopy();

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Retrieve the next int value from this range.
   */
  public int getInt();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange put(byte[] val, int offset, int length);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange get(byte[] dst);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Retrieve the next short value from this range.
   */
  public short getShort();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange put(byte val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange putShort(int index, short val);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange unset();

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange put(byte[] val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange put(int index, byte[] val);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Limits the byte range upto a specified value. Limit cannot be greater than capacity
   */
  public PositionedByteRange setLimit(int limit);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange setPosition(int position);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange set(int capacity);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange shallowCopySubRange(int innerOffset, int copyLength);
}

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange putInt(int val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange putInt(int index, int val);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange set(byte[] bytes, int offset, int length);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange get(byte[] dst, int offset, int length);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange setOffset(int offset);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Return the current limit
   */
  public int getLimit();

  // override parent interface declarations to return this interface.
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange put(int index, byte[] val, int offset, int length);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * The number of bytes remaining between position and the end of the range.
   */
  public int getRemaining();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return number of bytes written
   */
  public int putVLong(long val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange shallowCopy();

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange get(int index, byte[] dst);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange setLength(int length);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * range.
   */
  public int getPosition();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange set(byte[] bytes);

  @Override
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Retrieve the next byte from this range.
   */
  public byte get();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * Retrieve the next long value from this range.
   */
  public long getLong();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return the long value which is stored as VLong
   */
  public long getVLong();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange putShort(short val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java
   * @return this.
   */
  public PositionedByteRange putLong(long val);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PositionedByteRange.java`
#### Snippet
```java

  @Override
  public PositionedByteRange get(int index, byte[] dst, int offset, int length);

  @Override
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java`
#### Snippet
```java

  @SuppressWarnings("ImmutableEnumChecker")
  public static enum SingletonStorage {
    INSTANCE;

```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterSourceFactoryImpl.java`
#### Snippet
```java

  @SuppressWarnings("ImmutableEnumChecker")
  private static enum FactoryStorage {
    INSTANCE;

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSource.java`
#### Snippet
```java
   * Reports stochastic load balancer costs to JMX
   */
  public void updateStochasticCost(String tableName, String costFunctionName,
    String costFunctionDesc, Double value);
}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSource.java`
#### Snippet
```java
   * Updates the number of metrics reported to JMX
   */
  public void updateMetricsSize(int size);

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/BaseSourceImpl.java`
#### Snippet
```java

  @SuppressWarnings("ImmutableEnumChecker")
  private static enum DefaultMetricsSystemInitializer {
    INSTANCE;

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceFactory.java`
#### Snippet
```java
  public MetricsReplicationSinkSource getSink();

  public MetricsReplicationSourceSource getSource(String id);

  public MetricsReplicationTableSource getTableSource(String tableName);
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceFactory.java`
#### Snippet
```java
@InterfaceAudience.Private
public interface MetricsReplicationSourceFactory {
  public MetricsReplicationSinkSource getSink();

  public MetricsReplicationSourceSource getSource(String id);
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceFactory.java`
#### Snippet
```java
  public MetricsReplicationTableSource getTableSource(String tableName);

  public MetricsReplicationGlobalSourceSourceImpl getGlobalSource();
}

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceFactory.java`
#### Snippet
```java
  public MetricsReplicationSourceSource getSource(String id);

  public MetricsReplicationTableSource getTableSource(String tableName);

  public MetricsReplicationGlobalSourceSourceImpl getGlobalSource();
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
public interface MetricsReplicationSinkSource {
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
public interface MetricsReplicationSinkSource {
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
public interface MetricsReplicationSinkSource {
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
@InterfaceAudience.Private
public interface MetricsReplicationSinkSource {
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
@InterfaceAudience.Private
public interface MetricsReplicationSinkSource {
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
@InterfaceAudience.Private
public interface MetricsReplicationSinkSource {
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_AGE_OF_LAST_APPLIED_OP = "sink.ageOfLastAppliedOp";
  public static final String SINK_APPLIED_BATCHES = "sink.appliedBatches";
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";

  void setLastAppliedOpAge(long age);
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";

  void setLastAppliedOpAge(long age);
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java`
#### Snippet
```java
  public static final String SINK_FAILED_BATCHES = "sink.failedBatches";
  public static final String SINK_APPLIED_OPS = "sink.appliedOps";
  public static final String SINK_APPLIED_HFILES = "sink.appliedHFiles";

  void setLastAppliedOpAge(long age);
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java`
#### Snippet
```java
public interface MetricsReplicationGlobalSourceSource extends MetricsReplicationSourceSource {

  public static final String SOURCE_WAL_READER_EDITS_BUFFER = "source.walReaderEditsBufferUsage";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java`
#### Snippet
```java
public interface MetricsReplicationGlobalSourceSource extends MetricsReplicationSourceSource {

  public static final String SOURCE_WAL_READER_EDITS_BUFFER = "source.walReaderEditsBufferUsage";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java`
#### Snippet
```java
public interface MetricsReplicationGlobalSourceSource extends MetricsReplicationSourceSource {

  public static final String SOURCE_WAL_READER_EDITS_BUFFER = "source.walReaderEditsBufferUsage";

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
public interface MetricsReplicationSourceSource extends BaseSource {

  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
public interface MetricsReplicationSourceSource extends BaseSource {

  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
public interface MetricsReplicationSourceSource extends BaseSource {

  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
  public static final String SOURCE_INITIALIZING = "source.numInitializing";

  void setLastShippedAge(long age);
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
  public static final String SOURCE_INITIALIZING = "source.numInitializing";

  void setLastShippedAge(long age);
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
  public static final String SOURCE_INITIALIZING = "source.numInitializing";

  void setLastShippedAge(long age);
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SHIPPED_OPS = "source.shippedOps";

  public static final String SOURCE_LOG_READ_IN_BYTES = "source.logReadInBytes";
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SIZE_OF_HFILE_REFS_QUEUE = "source.sizeOfHFileRefsQueue";

  public static final String SOURCE_CLOSED_LOGS_WITH_UNKNOWN_LENGTH =
    "source.closedLogsWithUnknownFileLength";
  public static final String SOURCE_UNCLEANLY_CLOSED_LOGS = "source.uncleanlyClosedLogs";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_SIZE_OF_LOG_QUEUE = "source.sizeOfLogQueue";
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_AGE_OF_LAST_SHIPPED_OP = "source.ageOfLastShippedOp";
  public static final String SOURCE_SHIPPED_BATCHES = "source.shippedBatches";
  public static final String SOURCE_FAILED_BATCHES = "source.failedBatches";

  public static final String SOURCE_SHIPPED_BYTES = "source.shippedBytes";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
  public static final String SOURCE_INITIALIZING = "source.numInitializing";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
  public static final String SOURCE_INITIALIZING = "source.numInitializing";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
  public static final String SOURCE_COMPLETED_RECOVERY_QUEUES = "source.completedRecoverQueues";
  public static final String SOURCE_FAILED_RECOVERY_QUEUES = "source.failedRecoverQueues";
  // This is to track the num of replication sources getting initialized
  public static final String SOURCE_INITIALIZING = "source.numInitializing";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_UNCLEANLY_CLOSED_IGNORED_IN_BYTES =
    "source.ignoredUncleanlyClosedLogContentsInBytes";
  public static final String SOURCE_RESTARTED_LOG_READING = "source.restartedLogReading";
  public static final String SOURCE_REPEATED_LOG_FILE_BYTES = "source.repeatedLogFileBytes";
  public static final String SOURCE_COMPLETED_LOGS = "source.completedLogs";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java`
#### Snippet
```java
  public static final String SOURCE_LOG_READ_IN_EDITS = "source.logEditsRead";

  public static final String SOURCE_LOG_EDITS_FILTERED = "source.logEditsFiltered";

  public static final String SOURCE_SHIPPED_HFILES = "source.shippedHFiles";
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceFactoryImpl.java`
#### Snippet
```java

  @SuppressWarnings("ImmutableEnumChecker")
  private static enum SourceHolder {
    INSTANCE;

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * Description
   */
  static final String METRICS_DESCRIPTION = "Metrics about users connected to the regionserver";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * Description
   */
  static final String METRICS_DESCRIPTION = "Metrics about users connected to the regionserver";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * The name of the metrics
   */
  static final String METRICS_NAME = "Users";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * The name of the metrics
   */
  static final String METRICS_NAME = "Users";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
  static final String METRICS_JMX_CONTEXT = "RegionServer,sub=" + METRICS_NAME;

  static final String NUM_USERS = "numUsers";
  static final String NUMBER_OF_USERS_DESC = "Number of users in the metrics system";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
  static final String METRICS_JMX_CONTEXT = "RegionServer,sub=" + METRICS_NAME;

  static final String NUM_USERS = "numUsers";
  static final String NUMBER_OF_USERS_DESC = "Number of users in the metrics system";

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * The name of the metrics context that metrics will be under in jmx
   */
  static final String METRICS_JMX_CONTEXT = "RegionServer,sub=" + METRICS_NAME;

  static final String NUM_USERS = "numUsers";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * The name of the metrics context that metrics will be under in jmx
   */
  static final String METRICS_JMX_CONTEXT = "RegionServer,sub=" + METRICS_NAME;

  static final String NUM_USERS = "numUsers";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java

  static final String NUM_USERS = "numUsers";
  static final String NUMBER_OF_USERS_DESC = "Number of users in the metrics system";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java

  static final String NUM_USERS = "numUsers";
  static final String NUMBER_OF_USERS_DESC = "Number of users in the metrics system";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * The name of the metrics context that metrics will be under.
   */
  static final String METRICS_CONTEXT = "regionserver";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSource.java`
#### Snippet
```java
   * The name of the metrics context that metrics will be under.
   */
  static final String METRICS_CONTEXT = "regionserver";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java

  @SuppressWarnings("ImmutableEnumChecker")
  public static enum FactoryStorage {
    INSTANCE;

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregate.java`
#### Snippet
```java
@InterfaceAudience.Private
public interface MetricsTableWrapperAggregate {
  public String HASH = "#";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
   * change if item is removed or moved. Do our own codes.
   */
  public static enum Type {
    Minimum((byte) 0),
    Put((byte) 4),
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionMetrics.java`
#### Snippet
```java

  /** Returns the number of coprocessor service requests made to region */
  public long getCpRequestCount();

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScanResultCache.java`
#### Snippet
```java
interface ScanResultCache {

  static final Result[] EMPTY_RESULT_ARRAY = new Result[0];

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScanResultCache.java`
#### Snippet
```java
interface ScanResultCache {

  static final Result[] EMPTY_RESULT_ARRAY = new Result[0];

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java

  /** A lambda for dispatching to the appropriate metric factory method */
  private static interface NewMetric<T> {
    T newMetric(Class<?> clazz, String name, String scope);
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableState.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @InterfaceStability.Evolving
  public static enum State {
    ENABLED,
    DISABLED,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutator.java`
#### Snippet
```java
  @InterfaceAudience.Public
  interface ExceptionListener {
    public void onException(RetriesExhaustedWithDetailsException exception, BufferedMutator mutator)
      throws RetriesExhaustedWithDetailsException;
  }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private // For use by internals only.
  public static boolean isEncodedRegionName(byte[] regionName) {
    // If not parseable as region name, presume encoded. TODO: add stringency; e.g. if hex.
    if (parseRegionNameOrReturnNull(regionName) == null) {
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/NonceGenerator.java`
#### Snippet
```java
public interface NonceGenerator {

  static final String CLIENT_NONCES_ENABLED_KEY = "hbase.client.nonces.enabled";

  /** Returns the nonce group (client ID) of this client manager. */
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/NonceGenerator.java`
#### Snippet
```java
public interface NonceGenerator {

  static final String CLIENT_NONCES_ENABLED_KEY = "hbase.client.nonces.enabled";

  /** Returns the nonce group (client ID) of this client manager. */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestController.java`
#### Snippet
```java

  @InterfaceAudience.Public
  public enum ReturnCode {
    /**
     * Accept current row.
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestController.java`
#### Snippet
```java
   */
  @InterfaceAudience.Public
  public interface Checker {
    /**
     * Checks the data whether it is valid to submit.
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptor.java`
#### Snippet
```java
    };

  static final Bytes REPLICATION_SCOPE_BYTES =
    new Bytes(Bytes.toBytes(ColumnFamilyDescriptorBuilder.REPLICATION_SCOPE));

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptor.java`
#### Snippet
```java
    };

  static final Bytes REPLICATION_SCOPE_BYTES =
    new Bytes(Bytes.toBytes(ColumnFamilyDescriptorBuilder.REPLICATION_SCOPE));

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptor.java`
#### Snippet
```java

  @InterfaceAudience.Private
  static final Comparator<ColumnFamilyDescriptor> COMPARATOR =
    (ColumnFamilyDescriptor lhs, ColumnFamilyDescriptor rhs) -> {
      int result = Bytes.compareTo(lhs.getName(), rhs.getName());
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptor.java`
#### Snippet
```java

  @InterfaceAudience.Private
  static final Comparator<ColumnFamilyDescriptor> COMPARATOR =
    (ColumnFamilyDescriptor lhs, ColumnFamilyDescriptor rhs) -> {
      int result = Bytes.compareTo(lhs.getName(), rhs.getName());
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptor.java`
#### Snippet
```java

  @InterfaceAudience.Private
  static final Comparator<ColumnFamilyDescriptor> COMPARATOR_IGNORE_REPLICATION =
    (ColumnFamilyDescriptor lcf, ColumnFamilyDescriptor rcf) -> {
      int result = Bytes.compareTo(lcf.getName(), rcf.getName());
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptor.java`
#### Snippet
```java

  @InterfaceAudience.Private
  static final Comparator<ColumnFamilyDescriptor> COMPARATOR_IGNORE_REPLICATION =
    (ColumnFamilyDescriptor lcf, ColumnFamilyDescriptor rcf) -> {
      int result = Bytes.compareTo(lcf.getName(), rcf.getName());
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ClientBackoffPolicy.java`
#### Snippet
```java
public interface ClientBackoffPolicy {

  public static final String BACKOFF_POLICY_CLASS = "hbase.client.statistics.backoff-policy";

  /** Returns the number of ms to wait on the client based on the */
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ClientBackoffPolicy.java`
#### Snippet
```java
public interface ClientBackoffPolicy {

  public static final String BACKOFF_POLICY_CLASS = "hbase.client.statistics.backoff-policy";

  /** Returns the number of ms to wait on the client based on the */
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ClientBackoffPolicy.java`
#### Snippet
```java
public interface ClientBackoffPolicy {

  public static final String BACKOFF_POLICY_CLASS = "hbase.client.statistics.backoff-policy";

  /** Returns the number of ms to wait on the client based on the */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ClientBackoffPolicy.java`
#### Snippet
```java

  /** Returns the number of ms to wait on the client based on the */
  public long getBackoffTime(ServerName serverName, byte[] region, ServerStatistics stats);
}

```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/security/SecurityCapability.java`
#### Snippet
```java
  }

  private SecurityCapability(int value) {
    this.value = value;
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java`
#### Snippet
```java
   * engines.
   */
  static interface Engine {
    /**
     * Returns the string representation of the configured regular expression for matching
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
  // Utility methods

  static enum SatisfiesCode {
    /** row satisfies fuzzy rule */
    YES,
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  }

  public static interface UserQuotasVisitor {
    void visitUserQuotas(final String userName, final Quotas quotas) throws IOException;

```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  }

  private static interface RegionServerQuotasVisitor {
    void visitRegionServerQuotas(final String regionServer, final Quotas quotas) throws IOException;
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  }

  public static interface NamespaceQuotasVisitor {
    void visitNamespaceQuotas(final String namespace, final Quotas quotas) throws IOException;
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  }

  public static interface TableQuotasVisitor {
    void visitTableQuotas(final TableName tableName, final Quotas quotas) throws IOException;
  }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  }

  public static interface QuotasVisitor extends UserQuotasVisitor, TableQuotasVisitor,
    NamespaceQuotasVisitor, RegionServerQuotasVisitor {
  }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java

  /** Cell level ACL */
  public static final String OP_ATTRIBUTE_ACL = "acl";
}

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java

  /** Cell level ACL */
  public static final String OP_ATTRIBUTE_ACL = "acl";
}

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java

  /** Cell level ACL */
  public static final String OP_ATTRIBUTE_ACL = "acl";
}

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
   * permissions grant access. Pre-0.98 compatible behavior
   */
  public static final String CF_ATTRIBUTE_EARLY_OUT = "hbase.security.access.early_out";
  /** Default setting for hbase.security.access.early_out */
  public static final boolean DEFAULT_ATTRIBUTE_EARLY_OUT = true;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
   * permissions grant access. Pre-0.98 compatible behavior
   */
  public static final String CF_ATTRIBUTE_EARLY_OUT = "hbase.security.access.early_out";
  /** Default setting for hbase.security.access.early_out */
  public static final boolean DEFAULT_ATTRIBUTE_EARLY_OUT = true;
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
   * permissions grant access. Pre-0.98 compatible behavior
   */
  public static final String CF_ATTRIBUTE_EARLY_OUT = "hbase.security.access.early_out";
  /** Default setting for hbase.security.access.early_out */
  public static final boolean DEFAULT_ATTRIBUTE_EARLY_OUT = true;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
  public static final String CF_ATTRIBUTE_EARLY_OUT = "hbase.security.access.early_out";
  /** Default setting for hbase.security.access.early_out */
  public static final boolean DEFAULT_ATTRIBUTE_EARLY_OUT = true;

  // Operation attributes for cell level security
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
  public static final String CF_ATTRIBUTE_EARLY_OUT = "hbase.security.access.early_out";
  /** Default setting for hbase.security.access.early_out */
  public static final boolean DEFAULT_ATTRIBUTE_EARLY_OUT = true;

  // Operation attributes for cell level security
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
  public static final String CF_ATTRIBUTE_EARLY_OUT = "hbase.security.access.early_out";
  /** Default setting for hbase.security.access.early_out */
  public static final boolean DEFAULT_ATTRIBUTE_EARLY_OUT = true;

  // Operation attributes for cell level security
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
   * coprocessor endpoint invocations.
   */
  public static final String EXEC_PERMISSION_CHECKS_KEY = "hbase.security.exec.permission.checks";
  /** Default setting for hbase.security.exec.permission.checks; false */
  public static final boolean DEFAULT_EXEC_PERMISSION_CHECKS = false;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
   * coprocessor endpoint invocations.
   */
  public static final String EXEC_PERMISSION_CHECKS_KEY = "hbase.security.exec.permission.checks";
  /** Default setting for hbase.security.exec.permission.checks; false */
  public static final boolean DEFAULT_EXEC_PERMISSION_CHECKS = false;
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
   * coprocessor endpoint invocations.
   */
  public static final String EXEC_PERMISSION_CHECKS_KEY = "hbase.security.exec.permission.checks";
  /** Default setting for hbase.security.exec.permission.checks; false */
  public static final boolean DEFAULT_EXEC_PERMISSION_CHECKS = false;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
  public static final String EXEC_PERMISSION_CHECKS_KEY = "hbase.security.exec.permission.checks";
  /** Default setting for hbase.security.exec.permission.checks; false */
  public static final boolean DEFAULT_EXEC_PERMISSION_CHECKS = false;

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
  public static final String EXEC_PERMISSION_CHECKS_KEY = "hbase.security.exec.permission.checks";
  /** Default setting for hbase.security.exec.permission.checks; false */
  public static final boolean DEFAULT_EXEC_PERMISSION_CHECKS = false;

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlConstants.java`
#### Snippet
```java
  public static final String EXEC_PERMISSION_CHECKS_KEY = "hbase.security.exec.permission.checks";
  /** Default setting for hbase.security.exec.permission.checks; false */
  public static final boolean DEFAULT_EXEC_PERMISSION_CHECKS = false;

  /**
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/SyncReplicationState.java`
#### Snippet
```java
  private final byte value;

  private SyncReplicationState(int value) {
    this.value = (byte) value;
  }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStore.java`
#### Snippet
```java
   * An Iterator over a collection of Procedure
   */
  public interface ProcedureIterator {
    /**
     * Reset the Iterator by seeking to the beginning of the list.
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStore.java`
#### Snippet
```java
   * Interface passed to the ProcedureStore.load() method to handle the store-load events.
   */
  public interface ProcedureLoader {
    /**
     * Called by ProcedureStore.load() to notify about the maximum proc-id in the store.
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/ProcedureStore.java`
#### Snippet
```java
   * The main process should register a listener and respond to the store events.
   */
  public interface ProcedureStoreListener {

    /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java`
#### Snippet
```java
      public String op;

      static enum ComparatorType {
        BinaryComparator,
        BinaryPrefixComparator,
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java`
#### Snippet
```java
    public List<Long> timestamps;

    static enum FilterType {
      ColumnCountGetFilter,
      ColumnPaginationFilter,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationListener.java`
#### Snippet
```java
   * @param regionServer the removed region server
   */
  public void regionServerRemoved(ServerName regionServer);
}

```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/RowCounter.java`
#### Snippet
```java
   */
  static class RowCounterMapper implements TableMap<ImmutableBytesWritable, Result> {
    private static enum Counters {
      ROWS
    }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
     * Counter enumeration to count the actual rows.
     */
    public static enum Counters {
      ROWS,
      CELLS,
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
   * associated properties.
   */
  protected static enum Counter {
    /** Number of aggregated writes */
    PUTS,
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java

    /** Counter enumeration to count the actual rows. */
    public static enum Counters {
      ROWS
    }
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    Throwable mapperException;

    public static enum Counter {
      BATCHES,
      HASHES_MATCHED,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;

  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";

  public static final String DEFAULT_HBASE_HTTP_STATIC_USER = "dr.stack";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;

  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";

  public static final String DEFAULT_HBASE_HTTP_STATIC_USER = "dr.stack";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;

  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";

  public static final String DEFAULT_HBASE_HTTP_STATIC_USER = "dr.stack";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_SSL_ENABLED_KEY = "hbase.ssl.enabled";

  public static final boolean HBASE_SSL_ENABLED_DEFAULT = false;

  /** Enable/Disable aliases serving from jetty */
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_SSL_ENABLED_KEY = "hbase.ssl.enabled";

  public static final boolean HBASE_SSL_ENABLED_DEFAULT = false;

  /** Enable/Disable aliases serving from jetty */
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_SSL_ENABLED_KEY = "hbase.ssl.enabled";

  public static final boolean HBASE_SSL_ENABLED_DEFAULT = false;

  /** Enable/Disable aliases serving from jetty */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";

  public static final String DEFAULT_HBASE_HTTP_STATIC_USER = "dr.stack";

}
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";

  public static final String DEFAULT_HBASE_HTTP_STATIC_USER = "dr.stack";

}
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";

  public static final String DEFAULT_HBASE_HTTP_STATIC_USER = "dr.stack";

}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_JETTY_LOGS_SERVE_ALIASES = "hbase.jetty.logs.serve.aliases";

  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;

  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_JETTY_LOGS_SERVE_ALIASES = "hbase.jetty.logs.serve.aliases";

  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;

  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java
  public static final String HBASE_JETTY_LOGS_SERVE_ALIASES = "hbase.jetty.logs.serve.aliases";

  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;

  public static final String HBASE_HTTP_STATIC_USER = "hbase.http.staticuser.user";
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java

  /** Enable/Disable aliases serving from jetty */
  public static final String HBASE_JETTY_LOGS_SERVE_ALIASES = "hbase.jetty.logs.serve.aliases";

  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java

  /** Enable/Disable aliases serving from jetty */
  public static final String HBASE_JETTY_LOGS_SERVE_ALIASES = "hbase.jetty.logs.serve.aliases";

  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java

  /** Enable/Disable aliases serving from jetty */
  public static final String HBASE_JETTY_LOGS_SERVE_ALIASES = "hbase.jetty.logs.serve.aliases";

  public static final boolean DEFAULT_HBASE_JETTY_LOGS_SERVE_ALIASES = true;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java

  /** Enable/Disable ssl for http server */
  public static final String HBASE_SSL_ENABLED_KEY = "hbase.ssl.enabled";

  public static final boolean HBASE_SSL_ENABLED_DEFAULT = false;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java

  /** Enable/Disable ssl for http server */
  public static final String HBASE_SSL_ENABLED_KEY = "hbase.ssl.enabled";

  public static final boolean HBASE_SSL_ENABLED_DEFAULT = false;
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ServerConfigurationKeys.java`
#### Snippet
```java

  /** Enable/Disable ssl for http server */
  public static final String HBASE_SSL_ENABLED_KEY = "hbase.ssl.enabled";

  public static final boolean HBASE_SSL_ENABLED_DEFAULT = false;
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java`
#### Snippet
```java
  private final Map<String, List<ServerName>> favoredNodesMap;

  public static enum Position {
    PRIMARY,
    SECONDARY,
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/Reference.java`
#### Snippet
```java
   * key range
   */
  static enum Range {
    /** HStoreFile contains upper half of key range */
    top,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java`
#### Snippet
```java
   *         compaction.
   */
  public DataBlockEncoding getEffectiveEncodingInCache(boolean isCompaction);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
  public interface Writer extends Closeable, CellSink, ShipperListener {
    /** Max memstore (mvcc) timestamp in FileInfo */
    public static final byte[] MAX_MEMSTORE_TS_KEY = Bytes.toBytes("MAX_MEMSTORE_TS_KEY");

    /** Add an element to the file info map. */
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
  public interface Writer extends Closeable, CellSink, ShipperListener {
    /** Max memstore (mvcc) timestamp in FileInfo */
    public static final byte[] MAX_MEMSTORE_TS_KEY = Bytes.toBytes("MAX_MEMSTORE_TS_KEY");

    /** Add an element to the file info map. */
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
  public interface Writer extends Closeable, CellSink, ShipperListener {
    /** Max memstore (mvcc) timestamp in FileInfo */
    public static final byte[] MAX_MEMSTORE_TS_KEY = Bytes.toBytes("MAX_MEMSTORE_TS_KEY");

    /** Add an element to the file info map. */
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
   * Enum of all built in external block caches. This is used for config.
   */
  private static enum ExternalBlockCaches {
    memcached("org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache");

```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java`
#### Snippet
```java
   * Maps between configuration names for strategies and implementation classes.
   */
  static enum Strategies {
    defaultStrategy(BoundedGroupingStrategy.class),
    identity(IdentityGroupingStrategy.class),
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RegionGroupingProvider.java`
#### Snippet
```java
   * Map identifiers to a group number.
   */
  public static interface RegionGroupingStrategy {
    String GROUP_NAME_DELIMITER = ".";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java`
#### Snippet
```java
   * @return the exit code of the Canary tool.
   */
  public int checkZooKeeper() throws Exception;

  public Map<String, String> getReadFailures();
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java`
#### Snippet
```java
   * @return the exit code of the Canary tool.
   */
  public int checkRegionServers(String[] targets) throws Exception;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java`
#### Snippet
```java
  public int checkZooKeeper() throws Exception;

  public Map<String, String> getReadFailures();

  public Map<String, String> getWriteFailures();
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java`
#### Snippet
```java
   * @return the exit code of the Canary tool.
   */
  public int checkRegions(String[] targets) throws Exception;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java`
#### Snippet
```java
  public Map<String, String> getReadFailures();

  public Map<String, String> getWriteFailures();
}

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
  static final String ALWAYS_COPY_FILES = "always.copy.files";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
  static final String ALWAYS_COPY_FILES = "always.copy.files";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
  static final String ALWAYS_COPY_FILES = "always.copy.files";

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
  static final String ALWAYS_COPY_FILES = "always.copy.files";

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
   */
  @InterfaceAudience.Public
  public static class LoadQueueItem {

    private final byte[] family;
```

### UnnecessaryModifier
Modifier `static` is redundant for inner classes of interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
   */
  @InterfaceAudience.Public
  public static class LoadQueueItem {

    private final byte[] family;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java

  static final String RETRY_ON_IO_EXCEPTION = "hbase.bulkload.retries.retryOnIOException";
  static final String MAX_FILES_PER_REGION_PER_FAMILY =
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java

  static final String RETRY_ON_IO_EXCEPTION = "hbase.bulkload.retries.retryOnIOException";
  static final String MAX_FILES_PER_REGION_PER_FAMILY =
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
public interface BulkLoadHFiles {

  static final String RETRY_ON_IO_EXCEPTION = "hbase.bulkload.retries.retryOnIOException";
  static final String MAX_FILES_PER_REGION_PER_FAMILY =
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
public interface BulkLoadHFiles {

  static final String RETRY_ON_IO_EXCEPTION = "hbase.bulkload.retries.retryOnIOException";
  static final String MAX_FILES_PER_REGION_PER_FAMILY =
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
  static final String MAX_FILES_PER_REGION_PER_FAMILY =
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
  static final String MAX_FILES_PER_REGION_PER_FAMILY =
    "hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily";
  static final String ASSIGN_SEQ_IDS = "hbase.mapreduce.bulkload.assign.sequenceNumbers";
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
  static final String ALWAYS_COPY_FILES = "always.copy.files";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
  static final String CREATE_TABLE_CONF_KEY = "create.table";
  static final String IGNORE_UNMATCHED_CF_CONF_KEY = "ignore.unmatched.families";
  static final String ALWAYS_COPY_FILES = "always.copy.files";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool.java`
#### Snippet
```java
  }

  public static enum ReferenceType {
    WEAK,
    SOFT
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
  LockManager getLockManager();

  public String getRegionServerVersion(final ServerName sn);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java

  /** Returns load balancer */
  public LoadBalancer getLoadBalancer();

  boolean isSplitOrMergeEnabled(MasterSwitchType switchType);
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * @param name namespace name
   */
  public List<TableDescriptor> listTableDescriptorsByNamespace(String name) throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * @return table names
   */
  public List<TableName> listTableNamesByNamespace(String name) throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * @return procedure list
   */
  public List<Procedure<?>> getProcedures() throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java

  /** Returns Tripped when Master has finished initialization. */
  public ProcedureEvent<?> getInitializedEvent();

  /** Returns Master's instance of {@link MetricsMaster} */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   *         HFile resulting from a major compaction exists
   */
  public long getLastMajorCompactionTimestamp(TableName table) throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * version than any existing server and will move system tables there if so.
   */
  public void checkIfShouldMoveSystemRegionAsync();

  String getClientIdAuditPrefix();
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java

  /** Returns Favored Nodes Manager */
  public FavoredNodesManager getFavoredNodesManager();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * HFile resulting from a major compaction exists
   */
  public long getLastMajorCompactionTimestampForRegion(byte[] regionName) throws IOException;

  /** Returns load balancer */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * @return true if aborted, false if procedure already completed or does not exist
   */
  public boolean abortProcedure(final long procId, final boolean mayInterruptIfRunning)
    throws IOException;

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * @param preserveSplits True if the splits should be preserved
   */
  public long truncateTable(final TableName tableName, final boolean preserveSplits,
    final long nonceGroup, final long nonce) throws IOException;

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
   * @return lock list
   */
  public List<LockedResource> getLocks() throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchema.java`
#### Snippet
```java
   * Default operation timeout in milliseconds.
   */
  public static final int DEFAULT_HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT = 5 * 60 * 1000;

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchema.java`
#### Snippet
```java
   * Default operation timeout in milliseconds.
   */
  public static final int DEFAULT_HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT = 5 * 60 * 1000;

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchema.java`
#### Snippet
```java
   * Default operation timeout in milliseconds.
   */
  public static final int DEFAULT_HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT = 5 * 60 * 1000;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchema.java`
#### Snippet
```java
   * Timeout for cluster operations in milliseconds.
   */
  public static final String HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT_KEY =
    "hbase.master.cluster.schema.operation.timeout";
  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchema.java`
#### Snippet
```java
   * Timeout for cluster operations in milliseconds.
   */
  public static final String HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT_KEY =
    "hbase.master.cluster.schema.operation.timeout";
  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterSchema.java`
#### Snippet
```java
   * Timeout for cluster operations in milliseconds.
   */
  public static final String HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT_KEY =
    "hbase.master.cluster.schema.operation.timeout";
  /**
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/DirScanPool.java`
#### Snippet
```java
    private final String cleanerPoolSizeConfigDefault;

    private Type(String cleanerPoolSizeConfigName, String cleanerPoolSizeConfigDefault) {
      this.cleanerPoolSizeConfigName = cleanerPoolSizeConfigName;
      this.cleanerPoolSizeConfigDefault = cleanerPoolSizeConfigDefault;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TableProcedureInterface.java`
#### Snippet
```java
   * not have namespace table any more.
   */
  public static final TableName DUMMY_NAMESPACE_TABLE_NAME = TableName.NAMESPACE_TABLE_NAME;

  public enum TableOperationType {
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TableProcedureInterface.java`
#### Snippet
```java
   * not have namespace table any more.
   */
  public static final TableName DUMMY_NAMESPACE_TABLE_NAME = TableName.NAMESPACE_TABLE_NAME;

  public enum TableOperationType {
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TableProcedureInterface.java`
#### Snippet
```java
   * not have namespace table any more.
   */
  public static final TableName DUMMY_NAMESPACE_TABLE_NAME = TableName.NAMESPACE_TABLE_NAME;

  public enum TableOperationType {
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TableProcedureInterface.java`
#### Snippet
```java
  public static final TableName DUMMY_NAMESPACE_TABLE_NAME = TableName.NAMESPACE_TABLE_NAME;

  public enum TableOperationType {
    CREATE,
    DELETE,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerProcedureInterface.java`
#### Snippet
```java
@InterfaceAudience.Private
public interface ServerProcedureInterface {
  public enum ServerOperationType {
    CRASH_HANDLER,
    SWITCH_RPC_THROTTLE,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/OperationQuota.java`
#### Snippet
```java
@InterfaceStability.Evolving
public interface OperationQuota {
  public enum OperationType {
    MUTATE,
    GET,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSnapshotStore.java`
#### Snippet
```java
   * Singleton to represent a table without a quota defined. It is never in violation.
   */
  public static final SpaceQuotaSnapshot NO_QUOTA =
    new SpaceQuotaSnapshot(SpaceQuotaStatus.notInViolation(), -1, -1);

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSnapshotStore.java`
#### Snippet
```java
   * Singleton to represent a table without a quota defined. It is never in violation.
   */
  public static final SpaceQuotaSnapshot NO_QUOTA =
    new SpaceQuotaSnapshot(SpaceQuotaStatus.notInViolation(), -1, -1);

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSnapshotStore.java`
#### Snippet
```java
   * Singleton to represent a table without a quota defined. It is never in violation.
   */
  public static final SpaceQuotaSnapshot NO_QUOTA =
    new SpaceQuotaSnapshot(SpaceQuotaStatus.notInViolation(), -1, -1);

```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java`
#### Snippet
```java
  }

  private static interface KeyFromRow<T> {
    T getKeyFromRow(final byte[] row);

```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
   * Encapsulates CRUD quota operations for some subject.
   */
  private static interface SetQuotaOperations {
    /**
     * Fetches the current quota settings for the subject.
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
  }

  static interface Fetcher<Key, Value> {
    Get makeGet(Map.Entry<Key, Value> entry);

```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlFilter.java`
#### Snippet
```java
class AccessControlFilter extends FilterBase {

  public static enum Strategy {
    /** Filter only by checking the table or CF permissions */
    CHECK_TABLE_AND_CF_ONLY,
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelOrdinalProvider.java`
#### Snippet
```java
   *         existing label.
   */
  public int getLabelOrdinal(String label);

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelOrdinalProvider.java`
#### Snippet
```java
   * @return label associated with the string, null if not found
   */
  public String getLabel(int ordinal);
}

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ScanLabelGenerator.java`
#### Snippet
```java
   * @return The labels
   */
  public List<String> getLabels(User user, Authorizations authorizations);
}

```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/Operator.java`
#### Snippet
```java
  private final char rep;

  private Operator(char rep) {
    this.rep = rep;
  }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTask.java`
#### Snippet
```java
  }

  public interface StatusJournalEntry {
    String getStatus();

```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java`
#### Snippet
```java
      private final String type;

      private TaskType(String type) {
        this.type = type.toLowerCase();
      }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/WALCellFilter.java`
#### Snippet
```java
   *         replication.
   */
  public Cell filterCell(Entry entry, Cell cell);

}
```

### UnnecessaryModifier
Modifier `static` is redundant for inner classes of interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationEndpoint.java`
#### Snippet
```java
   */
  @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.REPLICATION)
  static class ReplicateContext {
    List<Entry> entries;
    int size;
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/PeerActionListener.java`
#### Snippet
```java
public interface PeerActionListener {

  static final PeerActionListener DUMMY = new PeerActionListener() {
  };

```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/PeerActionListener.java`
#### Snippet
```java
public interface PeerActionListener {

  static final PeerActionListener DUMMY = new PeerActionListener() {
  };

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntrySinkFilter.java`
#### Snippet
```java
   * Name of configuration to set with name of implementing WALEntrySinkFilter class.
   */
  public static final String WAL_ENTRY_FILTER_KEY = "hbase.replication.sink.walentrysinkfilter";

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntrySinkFilter.java`
#### Snippet
```java
   * Name of configuration to set with name of implementing WALEntrySinkFilter class.
   */
  public static final String WAL_ENTRY_FILTER_KEY = "hbase.replication.sink.walentrysinkfilter";

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntrySinkFilter.java`
#### Snippet
```java
   * Name of configuration to set with name of implementing WALEntrySinkFilter class.
   */
  public static final String WAL_ENTRY_FILTER_KEY = "hbase.replication.sink.walentrysinkfilter";

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SourceFSConfigurationProvider.java`
#### Snippet
```java
   * @throws IOException for invalid directory or for a bad disk.
   */
  public Configuration getConf(Configuration sinkConf, String replicationClusterId)
    throws IOException;

```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushLifeCycleTracker.java`
#### Snippet
```java
public interface FlushLifeCycleTracker {

  static FlushLifeCycleTracker DUMMY = new FlushLifeCycleTracker() {
  };

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java`
#### Snippet
```java
   * Sets the global memstore limit to a new size.
   */
  public void setGlobalMemStoreLimit(long globalMemStoreSize);
}

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java`
#### Snippet
```java
   * @return true when passed listener is unregistered successfully.
   */
  public boolean unregisterFlushRequestListener(final FlushRequestListener listener);

  /**
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    private String type;

    private OpType(String type) {
      this.type = type;
    }
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   *         HFileWriterImpl#getMidpoint, or null if not known.
   */
  public Cell getNextIndexedKey();
}

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   * @return true if scanner has values left, false if the underlying data is empty
   */
  public boolean seekToLastRow() throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   * @return true if the scanner is at the valid KeyValue, false if such KeyValue does not exist
   */
  public boolean backwardSeek(Cell key) throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   * is always compared by reference.
   */
  public static final Cell NO_NEXT_INDEXED_KEY = new KeyValue();

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   * is always compared by reference.
   */
  public static final Cell NO_NEXT_INDEXED_KEY = new KeyValue();

  /**
```

### UnnecessaryModifier
Modifier `final` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   * is always compared by reference.
   */
  public static final Cell NO_NEXT_INDEXED_KEY = new KeyValue();

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java`
#### Snippet
```java
   *         Cell
   */
  public boolean seekToPreviousRow(Cell key) throws IOException;

  /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java`
#### Snippet
```java

  public interface WriterFactory {
    public StoreFileWriter createWriter() throws IOException;

    default StoreFileWriter createWriterWithStoragePolicy(String fileStoragePolicy)
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java`
#### Snippet
```java
   * Every class that wants to observe heap memory tune actions must implement this interface.
   */
  public static interface HeapMemoryTuneObserver {

    /**
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
    private final boolean limitReached;

    private NextState(boolean moreValues, boolean limitReached) {
      this.moreValues = moreValues;
      this.limitReached = limitReached;
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java`
#### Snippet
```java
   * simultaneously. The locks must be released by calling release() from the same thread.
   */
  public interface RowLock {
    /**
     * Release the given lock. If there are no remaining locks held by the current thread then
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java`
#### Snippet
```java
   *         region was opened
   */
  public Map<byte[], Long> getMaxStoreSeqId();

  /**
```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALActionsListener.java`
#### Snippet
```java

  /** The reason for the log roll request. */
  static enum RollRequestReason {
    /** The length of the log exceeds the roll size threshold. */
    SIZE,
```

### UnnecessaryModifier
Modifier `static` is redundant for interface fields
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionLifeCycleTracker.java`
#### Snippet
```java
public interface CompactionLifeCycleTracker {

  static CompactionLifeCycleTracker DUMMY = new CompactionLifeCycleTracker() {
  };

```

### UnnecessaryModifier
Modifier `static` is redundant for inner enums
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher.java`
#### Snippet
```java
   * File (memstore or Storefile), or to return immediately.
   */
  public static enum MatchCode {
    /**
     * Include KeyValue in the returned result
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java

    /** Returns All stripe boundaries; including the open ones on both ends. */
    public List<byte[]> getStripeBoundaries();

    /** Returns The stripes. */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
     * @return Start row. May be an open key.
     */
    public byte[] getStartRow(int stripeIndex);

    /**
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
     * @return End row. May be an open key.
     */
    public byte[] getEndRow(int stripeIndex);

    /** Returns Level 0 files. */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java

    /** Returns Level 0 files. */
    public List<HStoreFile> getLevel0Files();

    /** Returns All stripe boundaries; including the open ones on both ends. */
```

### UnnecessaryModifier
Modifier `static` is redundant for inner interfaces
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java

  /** The information about stripes that the policy needs to do its stuff */
  public static interface StripeInformationProvider {
    public Collection<HStoreFile> getStorefiles();

```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java

    /** Returns The stripes. */
    public ArrayList<ImmutableList<HStoreFile>> getStripes();

    /** Returns Stripe count. */
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java

    /** Returns Stripe count. */
    public int getStripeCount();
  }
}
```

### UnnecessaryModifier
Modifier `public` is redundant for interface members
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  /** The information about stripes that the policy needs to do its stuff */
  public static interface StripeInformationProvider {
    public Collection<HStoreFile> getStorefiles();

    /**
```

### UnnecessaryModifier
Modifier `private` is redundant for enum constructors
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ImplType.java`
#### Snippet
```java
  final boolean canSpecifyBindIP;

  private ImplType(String option, boolean isAlwaysFramed, Class<? extends TServer> serverClass,
    boolean canSpecifyBindIP) {
    this.option = option;
```

## RuleId[id=PointlessArithmeticExpression]
### PointlessArithmeticExpression
`1 * 1024` can be replaced with '1024'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    private final String confDefault;
    /** data input buffer size to absorb small reads from application. */
    private static final int DATA_IBUF_SIZE = 1 * 1024;
    /** data output buffer size to absorb small writes from application. */
    private static final int DATA_OBUF_SIZE = 4 * 1024;
```

### PointlessArithmeticExpression
`1 * 8` can be replaced with '8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
      // no more than 4 bytes
      if (value < (1L << (2 * 8))) {
        if (value < (1L << (1 * 8))) {
          return 1;
        }
```

### PointlessArithmeticExpression
`1 * 8` can be replaced with '8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java

    if (value < (1 << (2 * 8))) {
      if (value < (1 << (1 * 8))) {
        return 1;
      }
```

### PointlessArithmeticExpression
`Byte.SIZE / Byte.SIZE` can be replaced with '1'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
   * Size of boolean in bytes
   */
  public static final int SIZEOF_BOOLEAN = Byte.SIZE / Byte.SIZE;

  /**
```

### PointlessArithmeticExpression
`1 * Bytes.SIZEOF_LONG` can be replaced with 'Bytes.SIZEOF_LONG'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
      2 * ClassSize.REFERENCE +
      // Timestamp
      1 * Bytes.SIZEOF_LONG +
      // durability
      ClassSize.REFERENCE +
```

### PointlessArithmeticExpression
`1 * ClassSize.REFERENCE` can be replaced with 'ClassSize.REFERENCE'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
      + KeyValue.TIMESTAMP_SIZE // timestamp
      + Bytes.SIZEOF_LONG // sequence id
      + 1 * ClassSize.REFERENCE); // references to cell
    private final Cell cell;
    private long sequenceId;
```

### PointlessArithmeticExpression
`1L * (1 << 30)` can be replaced with '(1 \<\< 30)'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java`
#### Snippet
```java
    throws java.io.IOException, InterruptedException {
    // although reduce() is called per-row, handle pathological case
    long threshold = context.getConfiguration().getLong("reducer.row.threshold", 1L * (1 << 30));
    Iterator<Text> iter = lines.iterator();
    while (iter.hasNext()) {
```

### PointlessArithmeticExpression
`1 * 1024 * 1024` can be replaced with '1024 \* 1024'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    extends Mapper<BytesWritable, NullWritable, NullWritable, NullWritable> {
    private static final Logger LOG = LoggerFactory.getLogger(ExportMapper.class);
    final static int REPORT_SIZE = 1 * 1024 * 1024;
    final static int BUFFER_SIZE = 64 * 1024;

```

### PointlessArithmeticExpression
`1L * (1 << 30)` can be replaced with '(1 \<\< 30)'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java`
#### Snippet
```java
    // although reduce() is called per-row, handle pathological case
    long threshold =
      context.getConfiguration().getLong("putsortreducer.row.threshold", 1L * (1 << 30));
    Iterator<Put> iter = puts.iterator();
    while (iter.hasNext()) {
```

### PointlessArithmeticExpression
`1L * (1 << 30)` can be replaced with '(1 \<\< 30)'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutCombiner.java`
#### Snippet
```java
    // critical that all Puts are grouped perfectly.
    long threshold =
      context.getConfiguration().getLong("putcombiner.row.threshold", 1L * (1 << 30));
    int cnt = 0;
    long curSize = 0;
```

### PointlessArithmeticExpression
`1 * 1024 * 1024` can be replaced with '1024 \* 1024'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      }
      this.rsFatals = new MemoryBoundedLogMessageBuffer(
        conf.getLong("hbase.master.buffer.for.rs.fatals", 1 * 1024 * 1024));
      LOG.info("hbase.rootdir={}, hbase.cluster.distributed={}",
        CommonFSUtils.getRootDir(this.conf),
```

### PointlessArithmeticExpression
`1000 * 60 * 1` can be replaced with '1000 \* 60'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(QuotaObserverChore.class);
  static final String QUOTA_OBSERVER_CHORE_PERIOD_KEY = "hbase.master.quotas.observer.chore.period";
  static final int QUOTA_OBSERVER_CHORE_PERIOD_DEFAULT = 1000 * 60 * 1; // 1 minutes in millis

  static final String QUOTA_OBSERVER_CHORE_DELAY_KEY = "hbase.master.quotas.observer.chore.delay";
```

### PointlessArithmeticExpression
`1000 * 60 * 1` can be replaced with '1000 \* 60'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaRefresherChore.java`
#### Snippet
```java
  static final String POLICY_REFRESHER_CHORE_PERIOD_KEY =
    "hbase.regionserver.quotas.policy.refresher.chore.period";
  static final int POLICY_REFRESHER_CHORE_PERIOD_DEFAULT = 1000 * 60 * 1; // 1 minute in millis

  static final String POLICY_REFRESHER_CHORE_DELAY_KEY =
```

## RuleId[id=EmptyStatementBody]
### EmptyStatementBody
`while` statement has empty body
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    final int offset = src.getOffset(), start = src.getPosition();
    int i = src.getPosition();
    while (((comp ? DESCENDING : ASCENDING).apply(a[offset + i++]) & 1) != 0)
      ;
    src.setPosition(i);
```

### EmptyStatementBody
`for` statement has empty body
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    b.setPosition(buff.getPosition());
    int cnt = 0;
    for (; isEncodedValue(b); skip(b), cnt++)
      ;
    return cnt;
```

### EmptyStatementBody
`for` statement has empty body
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    final byte terminator = ord.apply(TERM);
    int rawStartPos = offset + start, rawTermPos = rawStartPos;
    for (; a[rawTermPos] != terminator; rawTermPos++)
      ;
    src.setPosition(rawTermPos - offset + 1); // advance position to TERM + 1
```

### EmptyStatementBody
`for` statement has empty body
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    int end;
    byte[] a = src.getBytes();
    for (end = start; (byte) (ord.apply(a[offset + end]) & 0x80) != TERM; end++)
      ;
    end++; // increment end to 1-past last byte
```

### EmptyStatementBody
`while` statement has empty body
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java`
#### Snippet
```java
      // evict remaining tasks and futures from taskPool.
      futures.clear();
      while (taskPool.poll() != null) {
      }
      stop();
```

### EmptyStatementBody
`while` statement has empty body
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java`
#### Snippet
```java
      // evict remaining tasks and futures from taskPool.
      futures.clear();
      while (taskPool.poll() != null) {
      }
      stop();
```

### EmptyStatementBody
`for` statement has empty body
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java

    int i;
    for (i = 0; i < minLen
      && CellComparator.getInstance().compare(expected.get(i), actual.get(i)) == 0; ++i) {
    }
```

## RuleId[id=WhileCanBeForeach]
### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
        ImmutableConfigMap map = configs.get(i);
        Iterator<Map.Entry<String, String>> iter = map.iterator();
        while (iter.hasNext()) {
          Map.Entry<String, String> entry = iter.next();
          ret.put(entry.getKey(), entry.getValue());
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
    if (mutableConf != null) {
      Iterator<Map.Entry<String, String>> miter = mutableConf.iterator();
      while (miter.hasNext()) {
        Map.Entry<String, String> entry = miter.next();
        ret.put(entry.getKey(), entry.getValue());
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
    partitionedOps.add(currentPartition);
    Iterator<ZKUtilOp> iter = ops.iterator();
    while (iter.hasNext()) {
      ZKUtilOp currentOp = iter.next();
      int currentOpSize = estimateSize(currentOp);
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    }
    Iterator<Map.Entry<TableName, SnapshotSentinel>> it = snapshotHandlers.entrySet().iterator();
    while (it.hasNext()) {
      Map.Entry<TableName, SnapshotSentinel> entry = it.next();
      SnapshotSentinel sentinel = entry.getValue();
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    }
    Iterator<Map.Entry<SnapshotDescription, Long>> spIt = snapshotToProcIdMap.entrySet().iterator();
    while (spIt.hasNext()) {
      Map.Entry<SnapshotDescription, Long> entry = spIt.next();
      if (
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java
      final Iterator<Map.Entry<String, Pair<String, String>>> it =
        parentsToChildrenPairMap.entrySet().iterator();
      while (it.hasNext()) {
        final Map.Entry<String, Pair<String, String>> entry = it.next();

```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java`
#### Snippet
```java
      final Iterator<Map.Entry<String, Pair<String, String>>> it =
        parentsToChildrenPairMap.entrySet().iterator();
      while (it.hasNext()) {
        final Map.Entry<String, Pair<String, String>> entry = it.next();

```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerQueueService.java`
#### Snippet
```java
    Queue<WALEventTrackerPayload> retQueue = new ArrayDeque<>();
    Iterator<WALEventTrackerPayload> iterator = queue.iterator();
    while (iterator.hasNext()) {
      retQueue.add(iterator.next());
    }
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
    java.util.Iterator<FlushQueueEntry> it = flushQueue.iterator();

    while (it.hasNext()) {
      queueList.append("    " + it.next().toString());
      queueList.append("\n");
```

### WhileCanBeForeach
`while` loop can be replaced with enhanced 'for'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    }
    Iterator<String> i = map.descendingKeySet().iterator();
    while (i.hasNext()) {
      list.add(map.get(i.next()));
    }
```

## RuleId[id=StaticInitializerReferencesSubClass]
### StaticInitializerReferencesSubClass
Referencing subclass NoopLatch from superclass ProcedurePrepareLatch initializer might lead to class loading deadlock
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedurePrepareLatch.java`
#### Snippet
```java
@InterfaceStability.Evolving
public abstract class ProcedurePrepareLatch {
  private static final NoopLatch noopLatch = new NoopLatch();

  /**
```

## RuleId[id=CommentedOutCode]
### CommentedOutCode
Commented out code (2 lines)
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  @Override
  public ReturnCode filterCell(final Cell c) {
    // System.out.println("REMOVE KEY=" + keyValue.toString() + ", value=" +
    // Bytes.toString(keyValue.getValue()));
    if (this.matchedColumn) {
```

### CommentedOutCode
Commented out code (2 lines)
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
        // procedure. But before executing, we need to set its state to RUNNABLE, otherwise, a
        // exception will throw:
        // Preconditions.checkArgument(procedure.getState() == ProcedureState.RUNNABLE,
        // "NOT RUNNABLE! " + procedure.toString());
        proc.setState(ProcedureState.RUNNABLE);
```

### CommentedOutCode
Commented out code (2 lines)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      // these cases in modTInfo, so we can evaluate those regions as part of
      // the region chain in META
      // if (hbi.foundRegionDir == null) continue;
      // if (hbi.deployedOn.size() != 1) continue;
      if (hbi.getDeployedOn().isEmpty()) {
```

### CommentedOutCode
Commented out code (2 lines)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
            + ", which is already online";
          LOG.warn(error);
          // server.abort(error);
          // throw new IOException(error);
          builder.addOpeningState(RegionOpeningState.OPENED);
```

## RuleId[id=AssignmentToStaticFieldFromInstanceMethod]
### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `groups` from instance context
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
    synchronized (UserProvider.class) {
      if (!(groups instanceof User.TestingGroups)) {
        groups = Groups.getUserToGroupsMappingService(conf);
      }
    }
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `RESTServer.conf` from instance context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java

  public RESTServer(Configuration conf) {
    RESTServer.conf = conf;
    this.userProvider = UserProvider.instantiate(conf);
  }
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `shuffle` from instance context
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java

        if (cmd.startsWith("--shuffle")) {
          shuffle = true;
          continue;
        }
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `DRY_RUN_TABLE_CREATED` from instance context
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java

    synchronized (ImportTsv.class) {
      DRY_RUN_TABLE_CREATED = false;
    }
    Job job = createSubmittableJob(getConf(), args);
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `CellWritableComparablePartitioner.START_KEYS` from instance context
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
            new CellWritableComparable(KeyValueUtil.createFirstOnRow(startKeys[i]));
        }
        CellWritableComparablePartitioner.START_KEYS = startKeyWraps;
      }
    }
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `JMX_CS` from instance context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
          throw new RuntimeException("Started by another thread?");
        }
        JMX_CS = JMXConnectorServerFactory.newJMXConnectorServer(serviceUrl, jmxEnv, mbs);
        JMX_CS.start();
      }
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `JMX_CS` from instance context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
        JMX_CS.stop();
        LOG.info("ConnectorServer stopped!");
        JMX_CS = null;
      }
      // deregister the RMI registry
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `unspecifiedStoragePolicyId` from instance context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
            // HdfsConstants#BLOCK_STORAGE_POLICY_ID_UNSPECIFIED
            Field idUnspecified = BlockStoragePolicySuite.class.getField("ID_UNSPECIFIED");
            unspecifiedStoragePolicyId = idUnspecified.getByte(BlockStoragePolicySuite.class);
          }
          byte storagePolicyId = status.getStoragePolicy();
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `rsSupportsOffline` from instance context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          "Using unassign region " + regionString + " instead of using offline method, you should"
            + " restart HMaster after these repairs");
        rsSupportsOffline = false; // in the future just use unassign
        admin.unassign(regionName, true);
        return;
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `groupService` from instance context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
      if (conf.getBoolean(User.TestingGroups.TEST_CONF, false)) {
        UserProvider.setGroups(new User.TestingGroups(UserProvider.getGroups()));
        groupService = UserProvider.getGroups();
      } else {
        groupService = Groups.getUserToGroupsMappingService(conf);
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `groupService` from instance context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
        groupService = UserProvider.getGroups();
      } else {
        groupService = Groups.getUserToGroupsMappingService(conf);
      }
    }
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `index` from instance context
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
  public JVMClusterUtil.MasterThread startMaster() throws IOException {
    Configuration c = HBaseConfiguration.create(conf);
    User user = HBaseTestingUtility.getDifferentUser(c, ".hfs." + index++);

    JVMClusterUtil.MasterThread t = null;
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `index` from instance context
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
  private JVMClusterUtil.RegionServerThread startRegionServer(Configuration configuration)
    throws IOException {
    User rsUser = HBaseTestingUtility.getDifferentUser(configuration, ".hfs." + index++);
    JVMClusterUtil.RegionServerThread t = null;
    try {
```

### AssignmentToStaticFieldFromInstanceMethod
Assignment to static field `index` from instance context
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
          rsConf.setInt(HConstants.REGIONSERVER_PORT, rsPorts.get(i));
        }
        User user = HBaseTestingUtility.getDifferentUser(rsConf, ".hfs." + index++);
        hbaseCluster.addRegionServer(rsConf, i, user);
      }
```

## RuleId[id=CStyleArrayDeclaration]
### CStyleArrayDeclaration
C-style array declaration of parameter `b`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffInputStream.java`
#### Snippet
```java
   */
  @Override
  public int read(byte b[], int off, int len) {
    int avail = available();
    if (avail <= 0) {
```

### CStyleArrayDeclaration
C-style array declaration of local variable `macBuffer`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/CryptoAES.java`
#### Snippet
```java

        /* First 10 bytes of HMAC_MD5 digest */
        byte macBuffer[] = new byte[10];
        System.arraycopy(hMAC_MD5, 0, macBuffer, 0, 10);

```

### CStyleArrayDeclaration
C-style array declaration of local variable `buf`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java`
#### Snippet
```java
    throws IOException {
    InputStream cin = d.createDecryptionStream(in);
    byte buf[] = new byte[8 * 1024];
    long remaining = outLen;
    try {
```

### CStyleArrayDeclaration
C-style array declaration of parameter `b`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java

    @Override
    public void write(byte b[], int off, int len) throws IOException {
      out.write(b, off, len);
    }
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java`
#### Snippet
```java

  /** Call this from the concrete tool class's main function. */
  protected void doStaticMain(String args[]) {
    int ret;
    try {
```

### CStyleArrayDeclaration
C-style array declaration of parameter `b`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java`
#### Snippet
```java

    @Override
    public int read(byte b[]) throws IOException {
      return read(b, 0, b.length);
    }
```

### CStyleArrayDeclaration
C-style array declaration of local variable `split`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableRegionModel.java`
#### Snippet
```java
   */
  public void setName(String name) {
    String split[] = name.split(",");
    this.table = split[0];
    this.startKey = Bytes.toBytes(split[1]);
```

### CStyleArrayDeclaration
C-style array declaration of local variable `parts`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
      }
      Put put = new Put(row);
      byte parts[][] = CellUtil.parseColumn(column);
      if (parts.length != 2) {
        return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)
```

### CStyleArrayDeclaration
C-style array declaration of local variable `timeRange`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    }
    // Set TimeRange if defined
    long timeRange[] = getTimeRange(args);
    if (timeRange != null) {
      LOG.info("Setting TimeRange for counter.");
```

### CStyleArrayDeclaration
C-style array declaration of parameter `pattern`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java

  private static void writeAttribute(JsonWriter writer, MBeanServer mBeanServer, ObjectName oname,
    boolean description, Pattern pattern[], MBeanAttributeInfo attr) throws IOException {
    if (!attr.isReadable()) {
      return;
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
  }

  public boolean parseOptions(String args[]) throws ParseException, IOException {
    if (args.length == 0) {
      HelpFormatter formatter = new HelpFormatter();
```

### CStyleArrayDeclaration
C-style array declaration of field `DEFAULT_BUCKET_SIZES`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
  // so we need add extra 1024 bytes for fit.
  // TODO Support the view of block size distribution statistics
  private static final int DEFAULT_BUCKET_SIZES[] =
    { 4 * 1024 + 1024, 8 * 1024 + 1024, 16 * 1024 + 1024, 32 * 1024 + 1024, 40 * 1024 + 1024,
      48 * 1024 + 1024, 56 * 1024 + 1024, 64 * 1024 + 1024, 96 * 1024 + 1024, 128 * 1024 + 1024,
```

### CStyleArrayDeclaration
C-style array declaration of field `freeList`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    private int itemAllocationSize, sizeIndex;
    private int itemCount;
    private int freeList[];
    private int freeCount, usedCount;

```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java`
#### Snippet
```java
   * to quit, otherwise exit the jvm
   */
  public void doMain(String args[]) {
    try {
      int ret = ToolRunner.run(HBaseConfiguration.create(), this, args);
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseConfTool.java`
#### Snippet
```java
@InterfaceAudience.Private
public class HBaseConfTool {
  public static void main(String args[]) {
    if (args.length < 1) {
      System.err.println("Usage: HBaseConfTool <CONFIGURATION_KEY>");
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/GetJavaProperty.java`
#### Snippet
```java
@InterfaceAudience.Private
public final class GetJavaProperty {
  public static void main(String args[]) {
    if (args.length == 0) {
      for (Object prop : System.getProperties().keySet()) {
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java

  @Override
  public int run(String args[]) throws Exception {
    boolean shutDownCluster = false;
    Options opt = new Options();
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
  }

  public static void main(String args[]) throws IOException {
    Options opt = new Options();
    opt.addOption("w", "write", false, "write the assignments to hbase:meta only");
```

### CStyleArrayDeclaration
C-style array declaration of parameter `args`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java`
#### Snippet
```java

  @Override
  public int run(String args[]) throws Exception {
    if (args.length != 1) {
      usage(null);
```

### CStyleArrayDeclaration
C-style array declaration of parameter `hosts`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
   * @return The mini dfs cluster created.
   */
  public MiniDFSCluster startMiniDFSCluster(int servers, final String hosts[]) throws Exception {
    return startMiniDFSCluster(servers, null, hosts);
  }
```

### CStyleArrayDeclaration
C-style array declaration of parameter `racks`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  }

  public MiniDFSCluster startMiniDFSCluster(int servers, final String racks[], String hosts[])
    throws Exception {
    createDirsAndSetProperties();
```

### CStyleArrayDeclaration
C-style array declaration of parameter `hosts`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  }

  public MiniDFSCluster startMiniDFSCluster(int servers, final String racks[], String hosts[])
    throws Exception {
    createDirsAndSetProperties();
```

### CStyleArrayDeclaration
C-style array declaration of parameter `hosts`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
   * @return The mini dfs cluster created.
   */
  public MiniDFSCluster startMiniDFSCluster(final String hosts[]) throws Exception {
    if (hosts != null && hosts.length != 0) {
      return startMiniDFSCluster(hosts.length, hosts);
```

## RuleId[id=RegExpRedundantEscape]
### RegExpRedundantEscape
Redundant character escape `\\|` in RegExp
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   */
  private static final Pattern CP_HTD_ATTR_VALUE_PATTERN =
    Pattern.compile("(^[^\\|]*)\\|([^\\|]+)\\|[\\s]*([\\d]*)[\\s]*(\\|.*)?$");

  private static final String CP_HTD_ATTR_VALUE_PARAM_KEY_PATTERN = "[^=,]+";
```

### RegExpRedundantEscape
Redundant character escape `\\|` in RegExp
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   */
  private static final Pattern CP_HTD_ATTR_VALUE_PATTERN =
    Pattern.compile("(^[^\\|]*)\\|([^\\|]+)\\|[\\s]*([\\d]*)[\\s]*(\\|.*)?$");

  private static final String CP_HTD_ATTR_VALUE_PARAM_KEY_PATTERN = "[^=,]+";
```

### RegExpRedundantEscape
Redundant character escape `\\:` in RegExp
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsValidator.java`
#### Snippet
```java
  private static final boolean[] validAuthChars = new boolean[256];

  public static final String regex = "[A-Za-z_\\-\\:\\/\\.0-9]+";
  public static final Pattern pattern = Pattern.compile(regex);

```

### RegExpRedundantEscape
Redundant character escape `\\/` in RegExp
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsValidator.java`
#### Snippet
```java
  private static final boolean[] validAuthChars = new boolean[256];

  public static final String regex = "[A-Za-z_\\-\\:\\/\\.0-9]+";
  public static final Pattern pattern = Pattern.compile(regex);

```

### RegExpRedundantEscape
Redundant character escape `\\.` in RegExp
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsValidator.java`
#### Snippet
```java
  private static final boolean[] validAuthChars = new boolean[256];

  public static final String regex = "[A-Za-z_\\-\\:\\/\\.0-9]+";
  public static final Pattern pattern = Pattern.compile(regex);

```

## RuleId[id=Java8ListReplaceAll]
### Java8ListReplaceAll
The loop can be replaced with 'List.replaceAll'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
      return;
    }
    for (int i = 0; i < cells.size(); i++) {
      Cell cell = cells.get(i);
      cells.set(i, cloneIfNecessary(cell));
```

## RuleId[id=UnnecessaryQualifierForThis]
### UnnecessaryQualifierForThis
Qualifier `QueueingFuture` on 'this' is unnecessary in this context
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/BoundedCompletionService.java`
#### Snippet
```java
    @Override
    protected void done() {
      completed.add(QueueingFuture.this);
    }
  }
```

### UnnecessaryQualifierForThis
Qualifier `ModifyableTableDescriptor` on 'this' is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
      // if we found a match, remove it
      if (match != null) {
        ModifyableTableDescriptor.this.removeValue(match);
      } else {
        throw new IllegalArgumentException(String.format(
```

### UnnecessaryQualifierForThis
Qualifier `AsyncBufferedMutatorImpl` on 'this' is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
      if (this.mutations.isEmpty() && periodicFlushTimeoutNs > 0) {
        periodicFlushTask = periodicalFlushTimer.newTimeout(timeout -> {
          synchronized (AsyncBufferedMutatorImpl.this) {
            // confirm that we are still valid, if there is already an internalFlush call before us,
            // then we should not execute any more. And in internalFlush we will set periodicFlush
```

### UnnecessaryQualifierForThis
Qualifier `RawAsyncTableImpl` on 'this' is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
        long nonceGroup = conn.getNonceGenerator().getNonceGroup();
        long nonce = conn.getNonceGenerator().newNonce();
        return RawAsyncTableImpl.this
          .<CheckAndMutateResult> newCaller(checkAndMutate.getRow(), mutation.getPriority(),
            rpcTimeoutNs)
```

### UnnecessaryQualifierForThis
Qualifier `RawAsyncTableImpl` on 'this' is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
        long nonceGroup = conn.getNonceGenerator().getNonceGroup();
        long nonce = conn.getNonceGenerator().newNonce();
        return RawAsyncTableImpl.this
          .<CheckAndMutateResult> newCaller(checkAndMutate.getRow(), rowMutations.getMaxPriority(),
            rpcTimeoutNs)
```

### UnnecessaryQualifierForThis
Qualifier `RawAsyncTableImpl` on 'this' is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
          .<CheckAndMutateResult> newCaller(checkAndMutate.getRow(), rowMutations.getMaxPriority(),
            rpcTimeoutNs)
          .action((controller, loc, stub) -> RawAsyncTableImpl.this.<CheckAndMutateResult,
            CheckAndMutateResult> mutateRow(controller, loc, stub, rowMutations,
              (rn, rm) -> RequestConverter.buildMultiRequest(rn, checkAndMutate.getRow(),
```

### UnnecessaryQualifierForThis
Qualifier `NettyRpcServer` on 'this' is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
  @InterfaceAudience.Private
  protected NettyRpcServerPreambleHandler createNettyRpcServerPreambleHandler() {
    return new NettyRpcServerPreambleHandler(NettyRpcServer.this);
  }

```

### UnnecessaryQualifierForThis
Qualifier `RSGroupInfoManagerImpl` on 'this' is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      }
      LOG.debug("Done migrating {}, failed tables {}", groupInfo.getName(), failedTables);
      synchronized (RSGroupInfoManagerImpl.this) {
        Map<String, RSGroupInfo> rsGroupMap = holder.groupName2Group;
        RSGroupInfo currentInfo = rsGroupMap.get(groupInfo.getName());
```

### UnnecessaryQualifierForThis
Qualifier `ChunkCreator` on 'this' is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
      // translate chunk ID to chunk, if chunk initially wasn't in pool
      // this translation will (most likely) return null
      Chunk chunk = ChunkCreator.this.getChunk(chunkID);
      if (chunk != null) {
        if (chunk.isFromPool() && chunk.isIndexChunk()) {
```

## RuleId[id=ObjectNotify]
### ObjectNotify
`notify` should probably be replaced with 'notifyAll()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
      entry.locked = false;
      if (entry.numWaiters > 0) {
        entry.notify();
      } else {
        map.remove(entry.id);
```

### ObjectNotify
`notify` should probably be replaced with 'notifyAll()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
      this.done = true;
      this.result = result;
      notify();
    }

```

### ObjectNotify
`notify` should probably be replaced with 'notifyAll()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
    synchronized (task) {
      task.status = DELETED;
      task.notify();
    }
    SplitLogCounters.tot_mgr_task_deleted.increment();
```

### ObjectNotify
`notify` should probably be replaced with 'notifyAll()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
                task.batch.error++;
              }
              task.batch.notify();
            }
          }
```

### ObjectNotify
`notify` should probably be replaced with 'notifyAll()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
      synchronized (taskReadySeq) {
        this.taskReadySeq.incrementAndGet();
        taskReadySeq.notify();
      }
    }
```

### ObjectNotify
`notify` should probably be replaced with 'notifyAll()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java`
#### Snippet
```java
      this.signaller.set(true);
      synchronized (this.signaller) {
        this.signaller.notify();
      }
    }
```

## RuleId[id=ForLoopReplaceableByWhile]
### ForLoopReplaceableByWhile
`for` loop statement may be replace by 'while' loop
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    }
    int tagsEndOffset = pos + tagsLen;
    for (; pos < tagsEndOffset;) {
      if (pos + Tag.TAG_LENGTH_SIZE > endOffset) {
        String msg =
```

### ForLoopReplaceableByWhile
`for` loop statement may be replace by 'while' loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java`
#### Snippet
```java
    ImmutableSegment suffixCurrent;
    ImmutableSegment pipelineCurrent;
    for (; suffixBackwardIterator.hasNext();) {
      if (!pipelineBackwardIterator.hasNext()) {
        // a suffix longer than pipeline is invalid
```

## RuleId[id=FinalPrivateMethod]
### FinalPrivateMethod
'private' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellComparatorImpl.java`
#### Snippet
```java
  }

  private final int compareQualifiers(Cell left, int leftQualLen, Cell right, int rightQualLen) {
    if (left instanceof ByteBufferExtendedCell && right instanceof ByteBufferExtendedCell) {
      return ByteBufferUtils.compareTo(((ByteBufferExtendedCell) left).getQualifierByteBuffer(),
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RootResource.java`
#### Snippet
```java
  }

  private final TableListModel getTableList() throws IOException {
    TableListModel tableList = new TableListModel();
    TableName[] tableNames = servlet.getAdmin().listTableNames();
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java`
#### Snippet
```java
  }

  final private static int validateCompositeKey(byte[] keyBytes) {

    int separatorIdx = Bytes.indexOf(keyBytes, tableSeparator);
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    }

    private final void checkTagsLen() {
      if (checkLen(this.currTagsLen)) {
        throw new IllegalStateException(
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    }

    private final boolean _next() throws IOException {
      // Small method so can be inlined. It is a hot one.
      if (blockBuffer.remaining() <= 0) {
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
  }

  private final void reconnectAfterExpiration() throws InterruptedException {
    LOG.warn("ZK session expired or lost. Retry a new connection...");
    try {
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java`
#### Snippet
```java
   * @return the final directory for the snapshot in the given filepath
   */
  private static final Path getSpecifiedSnapshotDir(final Path snapshotsDir, String snapshotName) {
    return new Path(snapshotsDir, snapshotName);
  }
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Checks whether the key indicates an open interval boundary (i.e. infinity).
   */
  private static final boolean isOpen(byte[] key) {
    return key != null && key.length == 0;
  }
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Compare two keys for equality.
   */
  private final boolean rowEquals(byte[] k1, byte[] k2) {
    return Bytes.equals(k1, 0, k1.length, k2, 0, k2.length);
  }
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Finds the stripe index by end row.
   */
  private final int findStripeIndexByEndRow(byte[] endRow) {
    assert !isInvalid(endRow);
    if (isOpen(endRow)) return state.stripeEndRows.length;
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private final int nonOpenRowCompare(Cell k1, byte[] k2) {
    assert !isOpen(k1) && !isOpen(k2);
    return cellComparator.compareRows(k1, k2, 0, k2.length);
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Checks whether the key is invalid (e.g. from an L0 file, or non-stripe-compacted files).
   */
  private static final boolean isInvalid(byte[] key) {
    // No need to use Arrays.equals because INVALID_KEY is null
    return key == INVALID_KEY;
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Compare two keys. Keys must not be open (isOpen(row) == false).
   */
  private final int nonOpenRowCompare(byte[] k1, byte[] k2) {
    assert !isOpen(k1) && !isOpen(k2);
    return Bytes.compareTo(k1, k2);
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java

    /** Returns A lazy L0 copy from current state. */
    private final ArrayList<HStoreFile> getLevel0Copy() {
      if (this.level0Files == null) {
        this.level0Files = new ArrayList<>(StripeStoreFileManager.this.state.level0Files);
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
     * @return A lazy stripe copy from current stripes.
     */
    private final ArrayList<HStoreFile> getStripeCopy(int index) {
      List<HStoreFile> stripeCopy = this.stripeFiles.get(index);
      ArrayList<HStoreFile> result = null;
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Finds the stripe index for the stripe containing a row provided externally for get/scan.
   */
  private final int findStripeForRow(byte[] row, boolean isStart) {
    if (isStart && Arrays.equals(row, HConstants.EMPTY_START_ROW)) return 0;
    if (!isStart && Arrays.equals(row, HConstants.EMPTY_END_ROW)) {
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private static final boolean isOpen(Cell key) {
    return key != null && key.getRowLength() == 0;
  }
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
   * </pre>
   */
  private final MatchCode mergeFilterResponse(Cell cell, MatchCode matchCode,
    ReturnCode filterResponse) {
    switch (filterResponse) {
```

### FinalPrivateMethod
'private' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.java`
#### Snippet
```java
  }

  private final String getRestoreSFT(Configuration conf) {
    Class<? extends StoreFileTracker> currentDstSFT = StoreFileTrackerFactory
      .getStoreFileTrackerClassForMigration(conf, MigrationStoreFileTracker.DST_IMPL);
```

## RuleId[id=SizeReplaceableByIsEmpty]
### SizeReplaceableByIsEmpty
`parts.size() == 0` can be replaced with 'parts.isEmpty()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java`
#### Snippet
```java
    }
    List<String> parts = Splitter.on('.').splitToList(hostname);
    if (parts.size() == 0) {
      return hostname;
    }
```

### SizeReplaceableByIsEmpty
`propertyValue.length() == 0` can be replaced with 'propertyValue.isEmpty()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/KeyStoreFileType.java`
#### Snippet
```java
   */
  public static KeyStoreFileType fromPropertyValue(String propertyValue) {
    if (propertyValue == null || propertyValue.length() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`prop.length() == 0` can be replaced with 'prop.isEmpty()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java
     */
    public static ClientAuth fromPropertyValue(String prop) {
      if (prop == null || prop.length() == 0) {
        return NEED;
      }
```

### SizeReplaceableByIsEmpty
`buffers.size() == 0` can be replaced with 'buffers.isEmpty()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java
  // Make this private because we don't want to expose the refCnt related wrap method to upstream.
  private static ByteBuff wrap(List<ByteBuffer> buffers, RefCnt refCnt) {
    if (buffers == null || buffers.size() == 0) {
      throw new IllegalArgumentException("buffers shouldn't be null or empty");
    }
```

### SizeReplaceableByIsEmpty
`src.size() > 0` can be replaced with '!src.isEmpty()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java`
#### Snippet
```java
          indexWithinComponent = 0;
          src = components.get(currentComponent);
          assert src.size() > 0;
          return src.get(indexWithinComponent);
        }
```

### SizeReplaceableByIsEmpty
`family.length() > 0` can be replaced with '!family.isEmpty()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    String timestampStr = humanReadableTimestamp(timestamp);
    byte type = b[o + l - 1];
    return row + "/" + family + (family != null && family.length() > 0 ? ":" : "") + qualifier + "/"
      + timestampStr + "/" + Type.codeToType(type);
  }
```

### SizeReplaceableByIsEmpty
`results.size() > 0` can be replaced with '!results.isEmpty()'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java`
#### Snippet
```java
          List<Cell> results = new ArrayList<>();
          hasMore = scanner.next(results);
          if (results.size() > 0) {
            deleteRows.add(results);
          }
```

### SizeReplaceableByIsEmpty
`deleteRows.size() > 0` can be replaced with '!deleteRows.isEmpty()'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java`
#### Snippet
```java
          }
        }
        if (deleteRows.size() > 0) {
          Mutation[] deleteArr = new Mutation[deleteRows.size()];
          int i = 0;
```

### SizeReplaceableByIsEmpty
`line.length() > 0` can be replaced with '!line.isEmpty()'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/TerminalImpl.java`
#### Snippet
```java
        new InputStreamReader(process.getErrorStream(), StandardCharsets.UTF_8))) {
        String line = stderr.readLine();
        if ((line != null) && (line.length() > 0)) {
          LOGGER.error("Error output from stty: " + line);
        }
```

### SizeReplaceableByIsEmpty
`group.length() > 0` can be replaced with '!group.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/CatalogFamilyFormat.java`
#### Snippet
```java
    if (matcher.matches() && matcher.groupCount() > 0) {
      String group = matcher.group(1);
      if (group != null && group.length() > 0) {
        return Integer.parseInt(group.substring(1), 16);
      } else {
```

### SizeReplaceableByIsEmpty
`className.length() == 0` can be replaced with 'className.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
    // "hbase.client.default.rpc.codec" also -- because default is to do cell block encoding.
    String className = conf.get(HConstants.RPC_CODEC_CONF_KEY, getDefaultCodec(this.conf));
    if (className == null || className.length() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`cols.size() > 0` can be replaced with '!cols.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
      byte[] fam = entry.getKey();
      NavigableSet<byte[]> cols = entry.getValue();
      if (cols != null && cols.size() > 0) {
        for (byte[] col : cols) {
          addColumn(fam, col);
```

### SizeReplaceableByIsEmpty
`value.length() == 0` can be replaced with 'value.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
     */
    public ModifyableColumnFamilyDescriptor setConfiguration(String key, String value) {
      if (value == null || value.length() == 0) {
        configuration.remove(key);
      } else {
```

### SizeReplaceableByIsEmpty
`cols.size() > 0` can be replaced with '!cols.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
      byte[] fam = entry.getKey();
      NavigableSet<byte[]> cols = entry.getValue();
      if (cols != null && cols.size() > 0) {
        for (byte[] col : cols) {
          addColumn(fam, col);
```

### SizeReplaceableByIsEmpty
`this.families.size() > 0` can be replaced with '!this.families.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    public int hashCode() {
      int result = this.name.hashCode();
      if (this.families.size() > 0) {
        for (ColumnFamilyDescriptor e : this.families.values()) {
          result ^= e.hashCode();
```

### SizeReplaceableByIsEmpty
`lesserOrEqualPrefixes.size() != 0` can be replaced with '!lesserOrEqualPrefixes.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java
      (TreeSet<byte[]>) sortedPrefixes.headSet(qualifier, true);

    if (lesserOrEqualPrefixes.size() != 0) {
      byte[] largestPrefixSmallerThanQualifier = lesserOrEqualPrefixes.last();

```

### SizeReplaceableByIsEmpty
`invalidRanges.size() != 0` can be replaced with '!invalidRanges.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    }
    // if invalid range exists, throw the exception
    if (invalidRanges.size() != 0) {
      throwExceptionForInvalidRanges(invalidRanges, true);
    }
```

### SizeReplaceableByIsEmpty
`charset.length() > 0` can be replaced with '!charset.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java`
#### Snippet
```java
    }
    String charset = proto.getCharset();
    if (charset.length() > 0) {
      try {
        comparator.getEngine().setCharset(charset);
```

### SizeReplaceableByIsEmpty
`basePeerConfigs.length() != 0` can be replaced with '!basePeerConfigs.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
    Map<String, String> receivedPeerConfigMap = receivedPeerConfig.getConfiguration();
    String basePeerConfigs = conf.get(HBASE_REPLICATION_PEER_BASE_CONFIG, "");
    if (basePeerConfigs.length() != 0) {
      Map<String, String> basePeerConfigMap = Splitter.on(';').trimResults().omitEmptyStrings()
        .withKeyValueSeparator("=").split(basePeerConfigs);
```

### SizeReplaceableByIsEmpty
`tableCFsConfig.trim().length() == 0` can be replaced with 'tableCFsConfig.trim().isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
   */
  public static ReplicationProtos.TableCF[] convert(String tableCFsConfig) {
    if (tableCFsConfig == null || tableCFsConfig.trim().length() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`tab.length() == 0` can be replaced with 'tab.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      // 1 ignore empty table config
      tab = tab.trim();
      if (tab.length() == 0) {
        continue;
      }
```

### SizeReplaceableByIsEmpty
`pair.size() > 0` can be replaced with '!pair.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
        continue;
      }
      assert pair.size() > 0;
      Iterator<String> i = pair.iterator();
      String tabName = i.next().trim();
```

### SizeReplaceableByIsEmpty
`tabName.length() == 0` can be replaced with 'tabName.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      Iterator<String> i = pair.iterator();
      String tabName = i.next().trim();
      if (tabName.length() == 0) {
        LOG.info("incorrect format:" + tableCFsConfig);
        continue;
```

### SizeReplaceableByIsEmpty
`cfName.length() > 0` can be replaced with '!cfName.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
        for (String cf : cfsList) {
          String cfName = cf.trim();
          if (cfName.length() > 0) {
            tableCFBuilder.addFamilies(ByteString.copyFromUtf8(cfName));
          }
```

### SizeReplaceableByIsEmpty
`families.size() > 0` can be replaced with '!families.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
        families.add(tableCF.getFamilies(j).toStringUtf8());
      }
      if (families.size() > 0) {
        tableCFsMap.put(ProtobufUtil.toTableName(tableCF.getTableName()), families);
      } else {
```

### SizeReplaceableByIsEmpty
`namespacesList.size() != 0` can be replaced with '!namespacesList.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java

    List<ByteString> namespacesList = peer.getNamespacesList();
    if (namespacesList != null && namespacesList.size() != 0) {
      builder.setNamespaces(
        namespacesList.stream().map(ByteString::toStringUtf8).collect(Collectors.toSet()));
```

### SizeReplaceableByIsEmpty
`excludeNamespacesList.size() != 0` can be replaced with '!excludeNamespacesList.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java

    List<ByteString> excludeNamespacesList = peer.getExcludeNamespacesList();
    if (excludeNamespacesList != null && excludeNamespacesList.size() != 0) {
      builder.setExcludeNamespaces(
        excludeNamespacesList.stream().map(ByteString::toStringUtf8).collect(Collectors.toSet()));
```

### SizeReplaceableByIsEmpty
`this.timestamps.size() > 0` can be replaced with '!this.timestamps.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java`
#### Snippet
```java

  private void init() {
    if (this.timestamps.size() > 0) {
      minTimestamp = this.timestamps.first();
    }
```

### SizeReplaceableByIsEmpty
`label.length() > 0` can be replaced with '!label.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java`
#### Snippet
```java
            VisibilityLabelsRequest.Builder builder = VisibilityLabelsRequest.newBuilder();
            for (String label : labels) {
              if (label.length() > 0) {
                VisibilityLabel.Builder newBuilder = VisibilityLabel.newBuilder();
                newBuilder.setLabel(UnsafeByteOperations.unsafeWrap(Bytes.toBytes(label)));
```

### SizeReplaceableByIsEmpty
`auth.length() > 0` can be replaced with '!auth.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java`
#### Snippet
```java
            setAuthReqBuilder.setUser(UnsafeByteOperations.unsafeWrap(Bytes.toBytes(user)));
            for (String auth : auths) {
              if (auth.length() > 0) {
                setAuthReqBuilder.addAuth(ByteString.copyFromUtf8(auth));
              }
```

### SizeReplaceableByIsEmpty
`runnables.size() > 0` can be replaced with '!runnables.isEmpty()'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SimpleProcedureScheduler.java`
#### Snippet
```java
  @Override
  public boolean queueHasRunnables() {
    return runnables.size() > 0;
  }

```

### SizeReplaceableByIsEmpty
`queue.size() > 0` can be replaced with '!queue.isEmpty()'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
          DelayedUtil.takeWithoutInterrupt(queue, 20, TimeUnit.SECONDS);
        if (task == null || task == DelayedUtil.DELAYED_POISON) {
          if (task == null && queue.size() > 0) {
            LOG.error("DelayQueue for RemoteProcedureDispatcher is not empty when timed waiting"
              + " elapsed. If this is repeated consistently, it means no element is getting expired"
```

### SizeReplaceableByIsEmpty
`input.length() == 0` can be replaced with 'input.isEmpty()'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/util/StringUtils.java`
#### Snippet
```java

  public static boolean isEmpty(final String input) {
    return input == null || input.length() == 0;
  }

```

### SizeReplaceableByIsEmpty
`logs.size() > 0` can be replaced with '!logs.isEmpty()'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
      logs.remove(log);
      LOG.debug("Removed log={}, activeLogs={}", log, logs);
      assert logs.size() > 0 : "expected at least one log";
    } catch (IOException e) {
      LOG.error("Unable to remove log: " + log, e);
```

### SizeReplaceableByIsEmpty
`filter.length() == 0` can be replaced with 'filter.isEmpty()'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java`
#### Snippet
```java
  public static Filter buildFilterFromModel(final ScannerModel model) throws Exception {
    String filter = model.getFilter();
    if (filter == null || filter.length() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`model.getProperties().size() > 0` can be replaced with '!model.getProperties().isEmpty()'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
    NamespaceDescriptor.Builder builder = NamespaceDescriptor.create(namespace);
    builder.addConfiguration(model.getProperties());
    if (model.getProperties().size() > 0) {
      builder.addConfiguration(model.getProperties());
    }
```

### SizeReplaceableByIsEmpty
`check.length() > 0` can be replaced with '!check.isEmpty()'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
    } else if (CHECK_INCREMENT.equalsIgnoreCase(check)) {
      return increment(model);
    } else if (check != null && check.length() > 0) {
      return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)
        .entity("Invalid check value '" + check + "'" + CRLF).build();
```

### SizeReplaceableByIsEmpty
`labels.size() > 0` can be replaced with '!labels.isEmpty()'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java`
#### Snippet
```java
      builder.setFilter(filter);
    }
    if (labels != null && labels.size() > 0) {
      for (String label : labels)
        builder.addLabels(label);
```

### SizeReplaceableByIsEmpty
`mutations.size() > 0` can be replaced with '!mutations.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static CheckAndMutate toCheckAndMutate(ClientProtos.Condition condition,
    List<Mutation> mutations) throws IOException {
    assert mutations.size() > 0;
    byte[] row = condition.getRow().toByteArray();
    CheckAndMutate.Builder builder = CheckAndMutate.newBuilder(row);
```

### SizeReplaceableByIsEmpty
`compactedStoreFiles.size() != 0` can be replaced with '!compactedStoreFiles.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      HFileProtos.CompactionEventTracker compactionEventTracker = builder.build();
      List<ByteString> compactedStoreFiles = compactionEventTracker.getCompactedStoreFileList();
      if (compactedStoreFiles != null && compactedStoreFiles.size() != 0) {
        return compactedStoreFiles.stream().map(ByteString::toStringUtf8)
          .collect(Collectors.toSet());
```

### SizeReplaceableByIsEmpty
`qualifiers.size() > 0` can be replaced with '!qualifiers.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
        NavigableSet<byte[]> qualifiers = family.getValue();
        columnBuilder.clearQualifier();
        if (qualifiers != null && qualifiers.size() > 0) {
          for (byte[] qualifier : qualifiers) {
            columnBuilder.addQualifier(UnsafeByteOperations.unsafeWrap(qualifier));
```

### SizeReplaceableByIsEmpty
`qualifiers.size() > 0` can be replaced with '!qualifiers.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
        columnBuilder.setFamily(UnsafeByteOperations.unsafeWrap(family.getKey()));
        columnBuilder.clearQualifier();
        if (qualifiers != null && qualifiers.size() > 0) {
          for (byte[] qualifier : qualifiers) {
            columnBuilder.addQualifier(UnsafeByteOperations.unsafeWrap(qualifier));
```

### SizeReplaceableByIsEmpty
`hostport.length() > 0` can be replaced with '!hostport.isEmpty()'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKMainServer.java`
#### Snippet
```java
      Configuration conf = HBaseConfiguration.create();
      String hostport = new ZKMainServer().parse(conf);
      if (hostport != null && hostport.length() > 0) {
        newArgs = new String[args.length + 2];
        System.arraycopy(args, 0, newArgs, 2, args.length);
```

### SizeReplaceableByIsEmpty
`stack.size() > 0` can be replaced with '!stack.isEmpty()'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
        stack.add(ZNodePaths.joinZNode(znodeToProcess, zNodeChild));
      }
    } while (stack.size() > 0);
  }

```

### SizeReplaceableByIsEmpty
`identifier.length() == 0` can be replaced with 'identifier.isEmpty()'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java
      new RetryCounterFactory(maxRetries + 1, retryIntervalMillis, maxSleepTime);

    if (identifier == null || identifier.length() == 0) {
      // the identifier = processID@hostName
      identifier = ManagementFactory.getRuntimeMXBean().getName();
```

### SizeReplaceableByIsEmpty
`children.size() > 0` can be replaced with '!children.isEmpty()'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterMaintenanceModeTracker.java`
#### Snippet
```java
      List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(watcher,
        watcher.getZNodePaths().masterMaintZNode);
      hasChildren = (children != null && children.size() > 0);
    } catch (KeeperException e) {
      // Ignore the ZK keeper exception
```

### SizeReplaceableByIsEmpty
`keytabFile.length() == 0` can be replaced with 'keytabFile.isEmpty()'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAuthentication.java`
#### Snippet
```java

    public JaasConfiguration(String loginContextName, String principal, String keytabFile) {
      this(loginContextName, principal, keytabFile, keytabFile == null || keytabFile.length() == 0);
    }

```

### SizeReplaceableByIsEmpty
`colArg.length() == 0` can be replaced with 'colArg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java`
#### Snippet
```java
    // expecting at least one column
    String colArg = job.get(COLUMN_LIST);
    if (colArg == null || colArg.length() == 0) {
      throw new IOException("expecting at least one column");
    }
```

### SizeReplaceableByIsEmpty
`result.size() > 0` can be replaced with '!result.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java`
#### Snippet
```java
      }

      if (result != null && result.size() > 0) {
        key.set(result.getRow());
        lastSuccessfulRow = key.get();
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
   */
  public static void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`splits.size() == 0` can be replaced with 'splits.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
  public List<InputSplit> calculateAutoBalancedSplits(List<InputSplit> splits,
    long maxAverageRegionSize) throws IOException {
    if (splits.size() == 0) {
      return splits;
    }
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
   */
  private void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
   */
  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`unmatchedFamilies.size() > 0` can be replaced with '!unmatchedFamilies.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
                }
              }
              if (unmatchedFamilies.size() > 0) {
                ArrayList<String> familyNames = new ArrayList<>();
                for (ColumnFamilyDescriptor family : table.getDescriptor().getColumnFamilies()) {
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
   */
  private static void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java

  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
      System.err.println();
```

### SizeReplaceableByIsEmpty
`snapshotFiles.size() > 0` can be replaced with '!snapshotFiles.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      List<Pair<SnapshotFileInfo, Long>> snapshotFiles = getSnapshotFiles(conf, fs, snapshotDir);
      int mappers = conf.getInt(CONF_NUM_SPLITS, 0);
      if (mappers == 0 && snapshotFiles.size() > 0) {
        mappers = 1 + (snapshotFiles.size() / conf.getInt(CONF_MAP_GROUP, 10));
        mappers = Math.min(mappers, snapshotFiles.size());
```

### SizeReplaceableByIsEmpty
`str.length() > 0` can be replaced with '!str.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java

    private boolean stringIsNotEmpty(final String str) {
      return str != null && str.length() > 0;
    }

```

### SizeReplaceableByIsEmpty
`value.size() > 0` can be replaced with '!value.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
      }

      if (value != null && value.size() > 0) {
        key.set(value.getRow());
        lastSuccessfulRow = key.get();
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java

  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
      System.err.println();
```

### SizeReplaceableByIsEmpty
`ret.length() > 0` can be replaced with '!ret.isEmpty()'
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java
              case "bool":
                String ret = execWithRetries(user, cmd.substring(4)).getSecond();
                status = Boolean.toString(ret.length() > 0);
                break;

```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
   */
  private static void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`tags.size() == 0` can be replaced with 'tags.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
          kv.getValueOffset(), // value offset
          kv.getValueLength(), // value length
          tags.size() == 0 ? null : tags);
      }
    }
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
   */
  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`message.length() > 0` can be replaced with '!message.isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java

  private void printUsage(final String message) {
    if (message != null && message.length() > 0) {
      System.err.println(message);
    }
```

### SizeReplaceableByIsEmpty
`descriptionStr.length() > 0` can be replaced with '!descriptionStr.isEmpty()'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
      writer.beginObject();
      writer.name("name").value(oname.toString());
      if (description && descriptionStr != null && descriptionStr.length() > 0) {
        writer.name("description").value(descriptionStr);
      }
```

### SizeReplaceableByIsEmpty
`descriptionStr.length() > 0` can be replaced with '!descriptionStr.isEmpty()'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
  private static void writeAttribute(JsonWriter writer, String attName, String descriptionStr,
    Object value) throws IOException {
    if (descriptionStr != null && descriptionStr.length() > 0 && !attName.equals(descriptionStr)) {
      writer.name(attName);
      writer.beginObject();
```

### SizeReplaceableByIsEmpty
`line.length() == 0` can be replaced with 'line.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/HeterogeneousRegionCountCostFunction.java`
#### Snippet
```java
    for (final String line : lines) {
      try {
        if (line.length() == 0) {
          continue;
        }
```

### SizeReplaceableByIsEmpty
`primaryRSToRegionCounterMap.keySet().size() != 0` can be replaced with '!primaryRSToRegionCounterMap.keySet().isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

    // Update the avg dispersion score
    if (primaryRSToRegionCounterMap.keySet().size() != 0) {
      this.avgDispersionScore =
        dispersionScoreSummary / (float) primaryRSToRegionCounterMap.keySet().size();
```

### SizeReplaceableByIsEmpty
`primaryRSToRegionCounterMap.keySet().size() != 0` can be replaced with '!primaryRSToRegionCounterMap.keySet().isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

    // Update the avg dispersion score
    if (primaryRSToRegionCounterMap.keySet().size() != 0) {
      this.avgDispersionScore =
        dispersionScoreSummary / (float) primaryRSToRegionCounterMap.keySet().size();
```

### SizeReplaceableByIsEmpty
`randomAssignRegions.size() > 0` can be replaced with '!randomAssignRegions.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java

    // If servers from prior assignment aren't present, then lets do randomAssignment on regions.
    if (randomAssignRegions.size() > 0) {
      BalancerClusterState cluster = createCluster(servers, regions.keySet());
      for (Map.Entry<ServerName, List<RegionInfo>> entry : assignments.entrySet()) {
```

### SizeReplaceableByIsEmpty
`pos.size() != 0` can be replaced with '!pos.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      List<Integer> pos =
        returnMap.get(regionsToReturn.get(regionsToReturn.size() - 1).getSource());
      if (pos != null && pos.size() != 0) {
        regionsToReturn.get(pos.get(pos.size() - 1))
          .setDestination(regionsToReturn.get(regionsToReturn.size() - 1).getDestination());
```

### SizeReplaceableByIsEmpty
`regionsToMove.size() > 0` can be replaced with '!regionsToMove.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      Arrays.asList(underloadedServers.keySet().toArray(new ServerName[serversUnderloaded]));
    Collections.shuffle(sns);
    while (regionsToMove.size() > 0) {
      int cnt = 0;
      int i = incr > 0 ? 0 : underloadedServers.size() - 1;
```

### SizeReplaceableByIsEmpty
`0 < regionsToMove.size()` can be replaced with '!regionsToMove.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      int numToTake = min - regionCount;
      int numTaken = 0;
      while (numTaken < numToTake && 0 < regionsToMove.size()) {
        addRegionPlan(regionsToMove, fetchFromTail, server.getKey().getServerName(),
          regionsToReturn);
```

### SizeReplaceableByIsEmpty
`regions.size() > 0` can be replaced with '!regions.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
    Map<RegionInfo, List<ServerName>> regionsNoFNMap;

    if (regions.size() > 0) {
      regionsNoFNMap = helper.generateFavoredNodesRoundRobin(assignmentMap, regions);
      fnm.updateFavoredNodes(regionsNoFNMap);
```

### SizeReplaceableByIsEmpty
`onlineServers.size() > 0` can be replaced with '!onlineServers.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java

    List<ServerName> onlineServers = getOnlineFavoredNodes(servers, favoredNodes);
    if (onlineServers.size() > 0) {
      destination = onlineServers.get(ThreadLocalRandom.current().nextInt(onlineServers.size()));
    }
```

### SizeReplaceableByIsEmpty
`excludeNodes.size() > 0` can be replaced with '!excludeNodes.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
    ServerName newServer = null;
    Set<ServerName> excludeFNSet = Sets.newHashSet(favoredNodes);
    if (excludeNodes != null && excludeNodes.size() > 0) {
      excludeFNSet.addAll(excludeNodes);
    }
```

### SizeReplaceableByIsEmpty
`skipServerSet.size() > 0` can be replaced with '!skipServerSet.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
    }

    if (skipServerSet != null && skipServerSet.size() > 0) {
      for (ServerName sn : skipServerSet) {
        serversToChooseFrom.remove(StartcodeAgnosticServerName.valueOf(sn));
```

### SizeReplaceableByIsEmpty
`excludeNodes.size() > 0` can be replaced with '!excludeNodes.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java

    Set<ServerName> favoredNodeSet = Sets.newHashSet(favoredNodes);
    if (excludeNodes != null && excludeNodes.size() > 0) {
      favoredNodeSet.addAll(excludeNodes);
    }
```

### SizeReplaceableByIsEmpty
`regions.size() > 0` can be replaced with '!regions.isEmpty()'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
    Map<ServerName, List<RegionInfo>> assignmentMap, List<RegionInfo> regions) throws IOException {

    if (regions.size() > 0) {
      if (canPlaceFavoredNodes()) {
        Map<RegionInfo, ServerName> primaryRSMap = new HashMap<>();
```

### SizeReplaceableByIsEmpty
`key.length() != 0` can be replaced with '!key.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    if (cmd.hasOption("w")) {
      String key = cmd.getOptionValue("w");
      if (key != null && key.length() != 0) {
        row = Bytes.toBytesBinary(key);
        isSeekToRow = true;
```

### SizeReplaceableByIsEmpty
`queues.size() > 0` can be replaced with '!queues.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcExecutor.java`
#### Snippet
```java
  public static QueueBalancer getBalancer(final String executorName, final Configuration conf,
    final List<BlockingQueue<CallRunner>> queues) {
    Preconditions.checkArgument(queues.size() > 0, "Queue size is <= 0, must be at least 1");
    if (queues.size() == 1) {
      return ONE_QUEUE;
```

### SizeReplaceableByIsEmpty
`freeBuckets.size() > 0` can be replaced with '!freeBuckets.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    public long allocateBlock(int blockSize) {
      Bucket b = null;
      if (freeBuckets.size() > 0) {
        // Use up an existing one first...
        b = (Bucket) freeBuckets.lastKey();
```

### SizeReplaceableByIsEmpty
`bucketList.size() > 0` can be replaced with '!bucketList.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    public Bucket findAndRemoveCompletelyFreeBucket() {
      Bucket b = null;
      assert bucketList.size() > 0;
      if (bucketList.size() == 1) {
        // So we never get complete starvation of a bucket for a size
```

### SizeReplaceableByIsEmpty
`completelyFreeBuckets.size() > 0` can be replaced with '!completelyFreeBuckets.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
      }

      if (completelyFreeBuckets.size() > 0) {
        b = (Bucket) completelyFreeBuckets.firstKey();
        removeBucket(b);
```

### SizeReplaceableByIsEmpty
`transformation.length() == 0` can be replaced with 'transformation.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
    }
    String transformation = connectionHeader.getRpcCryptoCipherTransformation();
    if (transformation == null || transformation.length() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`className.length() == 0` can be replaced with 'className.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
    }
    String className = connectionHeader.getCellBlockCodecClass();
    if (className == null || className.length() == 0) {
      return;
    }
```

### SizeReplaceableByIsEmpty
`notDeployedRegions.size() > 0` can be replaced with '!notDeployedRegions.isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
            List<HRegionLocation> notDeployedRegions = locations.stream()
              .filter(loc -> loc.getServerName() == null).collect(Collectors.toList());
            if (notDeployedRegions.size() > 0) {
              if (LOG.isDebugEnabled()) {
                LOG.debug("Table " + tableName + " has " + notDeployedRegions.size() + " regions");
```

### SizeReplaceableByIsEmpty
`regions.size() > 0` can be replaced with '!regions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java`
#### Snippet
```java
      for (RegionInfo ri : compacted) {
        toCompact.remove(ri);
        if (regions.size() > 0) {
          RegionInfo region = regions.remove(0);
          toCompact.add(region);
```

### SizeReplaceableByIsEmpty
`list.size() == 0` can be replaced with 'list.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
      TableDescriptor htd = admin.getDescriptor(table);
      List<ColumnFamilyDescriptor> list = MobUtils.getMobColumnFamilies(htd);
      if (list.size() == 0) {
        LOG.info("Skipping non-MOB table [{}]", table);
        return;
```

### SizeReplaceableByIsEmpty
`storeFiles.size() == 0` can be replaced with 'storeFiles.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
    List<Path> storeFiles) throws IOException {

    if (storeFiles.size() == 0) {
      // nothing to remove
      LOG.debug("Skipping archiving old MOB files - no files found for table={} cf={}", tableName,
```

### SizeReplaceableByIsEmpty
`storeFiles.size() == 0` can be replaced with 'storeFiles.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
    List<Path> storeFiles) throws IOException {

    if (storeFiles.size() == 0) {
      // nothing to remove
      LOG.debug("Skipping archiving old MOB files - no files found for table={} cf={}", tableName,
```

### SizeReplaceableByIsEmpty
`mobRefSet.size() > 0` can be replaced with '!mobRefSet.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java
   */
  public static byte[] serializeMobFileRefs(SetMultimap<TableName, String> mobRefSet) {
    if (mobRefSet != null && mobRefSet.size() > 0) {
      // Here we rely on the fact that '/' and ',' are not allowed in either table names nor hfile
      // names for serialization.
```

### SizeReplaceableByIsEmpty
`writerClsNames.size() > 0` can be replaced with '!writerClsNames.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java`
#### Snippet
```java
    if (log instanceof ProtobufLogReader) {
      List<String> writerClsNames = ((ProtobufLogReader) log).getWriterClsNames();
      if (writerClsNames != null && writerClsNames.size() > 0) {
        out.print("Writer Classes: ");
        for (int i = 0; i < writerClsNames.size(); i++) {
```

### SizeReplaceableByIsEmpty
`foundTableNames.size() > 0` can be replaced with '!foundTableNames.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      }

      if (foundTableNames.size() > 0) {
        System.err.println("Cannot pass a tablename when using the -regionserver "
          + "option, tablenames:" + foundTableNames.toString());
```

### SizeReplaceableByIsEmpty
`tmpTables.size() > 0` can be replaced with '!tmpTables.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        }

        if (tmpTables.size() > 0) {
          returnTables = tmpTables.toArray(new String[tmpTables.size()]);
        } else {
```

### SizeReplaceableByIsEmpty
`overlap.size() > 0` can be replaced with '!overlap.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
    public void handleOverlapGroup(Collection<HbckRegionInfo> overlap) throws IOException {
      Preconditions.checkNotNull(overlap);
      Preconditions.checkArgument(overlap.size() > 0);

      if (!this.fixOverlaps) {
```

### SizeReplaceableByIsEmpty
`tables.size() > 0` can be replaced with '!tables.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        Collection<Path> tableDirs = new ArrayList<>();
        Path rootdir = CommonFSUtils.getRootDir(getConf());
        if (tables.size() > 0) {
          for (TableName t : tables) {
            tableDirs.add(CommonFSUtils.getTableDir(rootdir, t));
```

### SizeReplaceableByIsEmpty
`peerDescriptions.size() > 0` can be replaced with '!peerDescriptions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      ReplicationStorageFactory.getReplicationQueueStorage(zkw, getConf());
    List<ReplicationPeerDescription> peerDescriptions = admin.listReplicationPeers();
    if (peerDescriptions != null && peerDescriptions.size() > 0) {
      List<String> peers = peerDescriptions.stream()
        .filter(
```

### SizeReplaceableByIsEmpty
`batch.size() > 0` can be replaced with '!batch.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
            }
          }
          if (batch.size() > 0) {
            queueStorage.removeLastSequenceIds(peer, batch);
            batch.clear();
```

### SizeReplaceableByIsEmpty
`orphanHdfsDirs.size() > 0` can be replaced with '!orphanHdfsDirs.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    }

    if (shouldFixHdfsOrphans() && orphanHdfsDirs.size() > 0) {
      adoptHdfsOrphans(orphanHdfsDirs);
      // TODO optimize by incrementally adding instead of reloading.
```

### SizeReplaceableByIsEmpty
`table.length() == 0` can be replaced with 'table.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java`
#### Snippet
```java

    // the top level node has come up, so read in all the tables
    if (table.length() == 0) {
      checkEnabledAndUpdate();
      return;
```

### SizeReplaceableByIsEmpty
`tables.size() > 0` can be replaced with '!tables.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java`
#### Snippet
```java
    LOG.debug("Starting archive for tables:" + tables);
    // if archiving is still enabled
    if (tables != null && tables.size() > 0) {
      getMonitor().setArchiveTables(tables);
    } else {
```

### SizeReplaceableByIsEmpty
`table.length() == 0` can be replaced with 'table.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java`
#### Snippet
```java
    String table = path.substring(archiveHFileZNode.length());
    // if we stop archiving all tables
    if (table.length() == 0) {
      // make sure we have the tracker before deleting the archive
      // but if we don't, we don't care about delete
```

### SizeReplaceableByIsEmpty
`errors.size() > 0` can be replaced with '!errors.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
      }
    }
    if (errors.size() > 0) {
      throw MultipleIOException.createIOException(errors);
    }
```

### SizeReplaceableByIsEmpty
`unmatchedFamilies.size() > 0` can be replaced with '!unmatchedFamilies.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
    List<String> unmatchedFamilies = queue.stream().map(item -> Bytes.toString(item.getFamily()))
      .filter(fn -> !familyNames.contains(fn)).distinct().collect(Collectors.toList());
    if (unmatchedFamilies.size() > 0) {
      String msg =
        "Unmatched family names found: unmatched family names in HFiles to be bulkloaded: "
```

### SizeReplaceableByIsEmpty
`hri.getDeployedOn().size() == 0` can be replaced with 'hri.getDeployedOn().isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckChore.java`
#### Snippet
```java
        continue;
      }
      if (hri.getDeployedOn().size() == 0) {
        // skip the offline region which belong to disabled table.
        if (report.getDisabledTableRegions().contains(hri.getRegionNameAsString())) {
```

### SizeReplaceableByIsEmpty
`failedDeletions.size() > 0` can be replaced with '!failedDeletions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
        getSplitLogManagerCoordination().getDetails().getFailedDeletions();
      // Retry previously failed deletes
      if (failedDeletions.size() > 0) {
        List<String> tmpPaths = new ArrayList<>(failedDeletions);
        for (String tmpPath : tmpPaths) {
```

### SizeReplaceableByIsEmpty
`tableNames.size() > 0` can be replaced with '!tableNames.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
      List<TableName> tableNames = server.listTableNames(null, null, false);
      ListTableNamesByStateResponse.Builder builder = ListTableNamesByStateResponse.newBuilder();
      if (tableNames != null && tableNames.size() > 0) {
        // Add the disabled table names to the response
        TableState.State state =
```

### SizeReplaceableByIsEmpty
`tableNames.size() > 0` can be replaced with '!tableNames.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java

      GetTableNamesResponse.Builder builder = GetTableNamesResponse.newBuilder();
      if (tableNames != null && tableNames.size() > 0) {
        // Add the table names to the response
        for (TableName table : tableNames) {
```

### SizeReplaceableByIsEmpty
`descriptors.size() > 0` can be replaced with '!descriptors.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
      ListTableDescriptorsByStateResponse.Builder builder =
        ListTableDescriptorsByStateResponse.newBuilder();
      if (descriptors != null && descriptors.size() > 0) {
        // Add the table descriptors to the response
        TableState.State state =
```

### SizeReplaceableByIsEmpty
`descriptors.size() > 0` can be replaced with '!descriptors.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java

      GetTableDescriptorsResponse.Builder builder = GetTableDescriptorsResponse.newBuilder();
      if (descriptors != null && descriptors.size() > 0) {
        // Add the table descriptors to the response
        for (TableDescriptor htd : descriptors) {
```

### SizeReplaceableByIsEmpty
`this.snapshots.size() > 0` can be replaced with '!this.snapshots.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java`
#### Snippet
```java
    if (ArrayUtils.isEmpty(snapshotDirs)) {
      // remove all the remembered snapshots because we don't have any left
      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {
        LOG.debug("No snapshots on-disk, clear cache");
      }
```

### SizeReplaceableByIsEmpty
`Sets.intersection(srcAddrs, desAddrs).size() > 0` can be replaced with '!Sets.intersection(srcAddrs, desAddrs).isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      Set<InetSocketAddress> srcAddrs = getNNAddresses((DistributedFileSystem) srcFs, conf);
      Set<InetSocketAddress> desAddrs = getNNAddresses((DistributedFileSystem) desFs, conf);
      if (Sets.intersection(srcAddrs, desAddrs).size() > 0) {
        return true;
      }
```

### SizeReplaceableByIsEmpty
`namespace.length() > 0` can be replaced with '!namespace.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      // request for all TableDescriptors
      Collection<TableDescriptor> allHtds;
      if (namespace != null && namespace.length() > 0) {
        // Do a check on the namespace existence. Will fail if does not exist.
        this.clusterSchemaService.getNamespace(namespace);
```

### SizeReplaceableByIsEmpty
`loads.size() > 0` can be replaced with '!loads.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    }
    for (List<Pair<ServerName, ReplicationLoadSource>> loads : replicationLoadSourceMap.values()) {
      if (loads.size() > 0) {
        loads.sort(Comparator.comparingLong(load -> (-1) * load.getSecond().getReplicationLag()));
      }
```

### SizeReplaceableByIsEmpty
`this.snapshotHandlers.size() > 0` can be replaced with '!this.snapshotHandlers.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
   */
  public synchronized boolean isTakingAnySnapshot() {
    return this.takingSnapshotLock.getReadHoldCount() > 0 || this.snapshotHandlers.size() > 0
      || this.snapshotToProcIdMap.size() > 0;
  }
```

### SizeReplaceableByIsEmpty
`this.snapshotToProcIdMap.size() > 0` can be replaced with '!this.snapshotToProcIdMap.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
  public synchronized boolean isTakingAnySnapshot() {
    return this.takingSnapshotLock.getReadHoldCount() > 0 || this.snapshotHandlers.size() > 0
      || this.snapshotToProcIdMap.size() > 0;
  }

```

### SizeReplaceableByIsEmpty
`enabled.trim().length() > 0` can be replaced with '!enabled.trim().isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    String enabled = conf.get(HBASE_SNAPSHOT_ENABLED);
    boolean snapshotEnabled = conf.getBoolean(HBASE_SNAPSHOT_ENABLED, false);
    boolean userDisabled = (enabled != null && enabled.trim().length() > 0 && !snapshotEnabled);

    // Extract cleaners from conf
```

### SizeReplaceableByIsEmpty
`storeFiles.size() > 0` can be replaced with '!storeFiles.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java`
#### Snippet
```java
        StoreFileTrackerFactory.create(env.getMasterConfiguration(), htd, hcd, regionFs);
      final Collection<StoreFileInfo> storeFiles = tracker.load();
      if (storeFiles != null && storeFiles.size() > 0) {
        final Configuration storeConfiguration =
          StoreUtils.createStoreConfiguration(env.getMasterConfiguration(), htd, hcd);
```

### SizeReplaceableByIsEmpty
`regions.size() > 0` can be replaced with '!regions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java`
#### Snippet
```java
  protected static List<RegionInfo> addTableToMeta(final MasterProcedureEnv env,
    final TableDescriptor tableDescriptor, final List<RegionInfo> regions) throws IOException {
    assert (regions != null && regions.size() > 0) : "expected at least 1 region, got " + regions;

    ProcedureSyncWait.waitMetaRegions(env);
```

### SizeReplaceableByIsEmpty
`storeFiles.size() > 0` can be replaced with '!storeFiles.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java`
#### Snippet
```java
      final ColumnFamilyDescriptor hcd = htd.getColumnFamily(familyName);
      final Collection<StoreFileInfo> storeFiles = e.getValue();
      if (storeFiles != null && storeFiles.size() > 0) {
        final Configuration storeConfiguration =
          StoreUtils.createStoreConfiguration(env.getMasterConfiguration(), htd, hcd);
```

### SizeReplaceableByIsEmpty
`familiesString.length() > 0` can be replaced with '!familiesString.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
      String[] params = new String[] { namespace != null ? "namespace=" + namespace : null,
        tableName != null ? "table=" + tableName.getNameWithNamespaceInclAsString() : null,
        familiesString.length() > 0 ? "family=" + familiesString : null,
        extraParams.isEmpty() ? null : concatenateExtraParams() };
      return Joiner.on(",").skipNulls().join(params);
```

### SizeReplaceableByIsEmpty
`familiesString.length() > 0` can be replaced with '!familiesString.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
        : table.getNameWithNamespaceInclAsString())
      .append(", ");
    if (namespace == null && familiesString.length() > 0) {
      sb.append("family=").append(familiesString).append(", ");
    }
```

### SizeReplaceableByIsEmpty
`paramsString.length() > 0` can be replaced with '!paramsString.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
    }
    String paramsString = params.toString();
    if (paramsString.length() > 0) {
      sb.append("params=[").append(paramsString).append("],");
    }
```

### SizeReplaceableByIsEmpty
`families.size() > 0` can be replaced with '!families.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java

    // 3. check permissions against the requested families
    if (families != null && families.size() > 0) {
      // all families must pass
      for (Map.Entry<byte[], ? extends Collection<?>> family : families.entrySet()) {
```

### SizeReplaceableByIsEmpty
`result.size() > 0` can be replaced with '!result.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    Result result, byte[] cf, byte[] cq, String user, boolean hasFilterUser) {
    ListMultimap<String, UserPermission> perms = ArrayListMultimap.create();
    if (result != null && result.size() > 0) {
      for (Cell kv : result.rawCells()) {
        Pair<String, Permission> permissionsOfUserOnTable =
```

### SizeReplaceableByIsEmpty
`qualifierSet.size() > 0` can be replaced with '!qualifierSet.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
      }

      if (qualifierSet.size() > 0) {
        Delete d = new Delete(tableName.getName());
        for (byte[] qualifier : qualifierSet) {
```

### SizeReplaceableByIsEmpty
`permissions.size() > 0` can be replaced with '!permissions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
        .isNotFamilyOrQualifierPermission((TablePermission) userPermission.getPermission()))
      .collect(Collectors.toList());
    return permissions.size() > 0 ? permissions.get(0) : null;
  }

```

### SizeReplaceableByIsEmpty
`permissions.size() > 0` can be replaced with '!permissions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
    List<UserPermission> permissions = PermissionStorage.getUserPermissions(conf,
      PermissionStorage.ACL_GLOBAL_NAME, null, null, userName, true);
    return permissions.size() > 0 ? permissions.get(0) : null;
  }

```

### SizeReplaceableByIsEmpty
`users.size() > 0` can be replaced with '!users.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
        ctx.getEnvironment().getConnection().getTable(PermissionStorage.ACL_TABLE_NAME)) {
        Set<String> users = SnapshotScannerHDFSAclStorage.getTableUsers(aclTable, tableName);
        if (users.size() > 0) {
          // 1. Remove table archive directory default ACLs
          hdfsAclHelper.removeTableDefaultAcl(tableName, users);
```

### SizeReplaceableByIsEmpty
`removeUsers.size() > 0` can be replaced with '!removeUsers.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
          // 3. Remove namespace access acls
          Set<String> removeUsers = filterUsersToRemoveNsAccessAcl(aclTable, tableName, users);
          if (removeUsers.size() > 0) {
            hdfsAclHelper.removeNamespaceAccessAcl(tableName, removeUsers, "delete");
          }
```

### SizeReplaceableByIsEmpty
`permissions.size() > 0` can be replaced with '!permissions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
    List<UserPermission> permissions =
      PermissionStorage.getUserNamespacePermissions(conf, namespace, userName, true);
    return permissions.size() > 0 ? permissions.get(0) : null;
  }

```

### SizeReplaceableByIsEmpty
`regions.size() == 0` can be replaced with 'regions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  private void processAssignQueue() {
    final HashMap<RegionInfo, RegionStateNode> regions = waitOnAssignQueue();
    if (regions == null || regions.size() == 0 || !isRunning()) {
      return;
    }
```

### SizeReplaceableByIsEmpty
`nlTop.getChildExps().size() != 0` can be replaced with '!nlTop.getChildExps().isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
        NonLeafExpressionNode nlTop = (NonLeafExpressionNode) top;
        if (
          (nlTop.getOperator() == Operator.NOT && nlTop.getChildExps().size() != 0)
            || (nlTop.getOperator() != Operator.NOT && nlTop.getChildExps().size() != 1)
        ) {
```

### SizeReplaceableByIsEmpty
`rsGroupInfos.size() > 0` can be replaced with '!rsGroupInfos.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    }

    if (rsGroupInfos.size() > 0) {
      Map<String, RSGroupInfo> newGroupMap = Maps.newHashMap(holder.groupName2Group);
      newGroupMap.putAll(rsGroupInfos);
```

### SizeReplaceableByIsEmpty
`groupRIT.size() > 0` can be replaced with '!groupRIT.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      // Only allow one balance run at at time.
      Map<String, RegionState> groupRIT = rsGroupGetRegionsInTransition(groupName);
      if (groupRIT.size() > 0 && !request.isIgnoreRegionsInTransition()) {
        LOG.debug("Not running balancer because {} region(s) in transition: {}", groupRIT.size(),
          StringUtils.abbreviate(masterServices.getAssignmentManager().getRegionStates()
```

### SizeReplaceableByIsEmpty
`mutations.size() > 0` can be replaced with '!mutations.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    }

    if (mutations.size() > 0) {
      multiMutate(mutations);
    }
```

### SizeReplaceableByIsEmpty
`v1Regions.size() > 0` can be replaced with '!v1Regions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
      dataManifestBuilder.setTableSchema(ProtobufUtil.toTableSchema(htd));

      if (v1Regions != null && v1Regions.size() > 0) {
        dataManifestBuilder.addAllRegionManifests(v1Regions);
      }
```

### SizeReplaceableByIsEmpty
`v2Regions.size() > 0` can be replaced with '!v2Regions.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
        dataManifestBuilder.addAllRegionManifests(v1Regions);
      }
      if (v2Regions != null && v2Regions.size() > 0) {
        dataManifestBuilder.addAllRegionManifests(v2Regions);
      }
```

### SizeReplaceableByIsEmpty
`tags.size() > 0` can be replaced with '!tags.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
    throws IOException {
    if (
      tags.size() > 0 && (serializationFormat == null
        || serializationFormat == SORTED_ORDINAL_SERIALIZATION_FORMAT)
    ) {
```

### SizeReplaceableByIsEmpty
`labels.size() > 0` can be replaced with '!labels.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
      }
      this.ordinalCounter.set(ordinal + 1);
      if (labels.size() > 0) {
        // If there is no data need not write to zk
        byte[] serialized = VisibilityUtils.getDataToWriteToZooKeeper(labels);
```

### SizeReplaceableByIsEmpty
`userAuths.size() > 0` can be replaced with '!userAuths.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
        this.labelsCache.refreshLabelsCache(serialized);
      }
      if (userAuths.size() > 0) {
        byte[] serialized = VisibilityUtils.getUserAuthsDataToWriteToZooKeeper(userAuths);
        this.labelsCache.writeToZookeeper(serialized, false);
```

### SizeReplaceableByIsEmpty
`procName.length() == 0` can be replaced with 'procName.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java`
#### Snippet
```java

    String procName = subproc.getName();
    if (procName == null || procName.length() == 0) {
      LOG.error("Subproc name cannot be null or the empty string");
      return false;
```

### SizeReplaceableByIsEmpty
`this.regionsToAdd.size() > 0` can be replaced with '!this.regionsToAdd.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    /** Returns true if there're new regions */
    public boolean hasRegionsToAdd() {
      return this.regionsToAdd != null && this.regionsToAdd.size() > 0;
    }

```

### SizeReplaceableByIsEmpty
`this.regionsToRemove.size() > 0` can be replaced with '!this.regionsToRemove.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    /** Returns true if there're regions to remove */
    public boolean hasRegionsToRemove() {
      return this.regionsToRemove != null && this.regionsToRemove.size() > 0;
    }

```

### SizeReplaceableByIsEmpty
`regionNames.size() > 0` can be replaced with '!regionNames.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    // Regions to Add: present in the snapshot but not in the current table
    List<RegionInfo> regionsToAdd = new ArrayList<>(regionNames.size());
    if (regionNames.size() > 0) {
      monitor.rethrowException();
      for (String regionName : regionNames) {
```

### SizeReplaceableByIsEmpty
`this.regionsToRestore.size() > 0` can be replaced with '!this.regionsToRestore.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    /** Returns true if there're regions to restore */
    public boolean hasRegionsToRestore() {
      return this.regionsToRestore != null && this.regionsToRestore.size() > 0;
    }

```

### SizeReplaceableByIsEmpty
`futures.size() != 0` can be replaced with '!futures.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java`
#### Snippet
```java

    boolean hasTasks() {
      return futures.size() != 0;
    }

```

### SizeReplaceableByIsEmpty
`removeUsers.size() > 0` can be replaced with '!removeUsers.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
    try {
      long start = EnvironmentEdgeManager.currentTime();
      if (removeUsers.size() > 0) {
        handleNamespaceAccessAcl(tableName.getNamespaceAsString(), removeUsers,
          HDFSAclOperation.OperationType.REMOVE);
```

### SizeReplaceableByIsEmpty
`users.size() > 0` can be replaced with '!users.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
    try {
      long start = EnvironmentEdgeManager.currentTime();
      if (users.size() > 0) {
        HDFSAclOperation.OperationType operationType = HDFSAclOperation.OperationType.MODIFY;
        handleNamespaceAccessAcl(tableName.getNamespaceAsString(), users, operationType);
```

### SizeReplaceableByIsEmpty
`users.size() > 0` can be replaced with '!users.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
    try {
      long start = EnvironmentEdgeManager.currentTime();
      if (users.size() > 0) {
        handleTableAcl(Sets.newHashSet(tableName), users, new HashSet<>(0), new HashSet<>(0),
          HDFSAclOperation.OperationType.REMOVE);
```

### SizeReplaceableByIsEmpty
`userSet.size() > 0` can be replaced with '!userSet.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
      // global user permission can be inherited from default acl automatically
      Set<String> userSet = getUsersWithTableReadAction(tableName, true, false);
      if (userSet.size() > 0) {
        Path path = pathHelper.getSnapshotDir(snapshot.getName());
        handleHDFSAcl(new HDFSAclOperation(fs, path, userSet, HDFSAclOperation.OperationType.MODIFY,
```

### SizeReplaceableByIsEmpty
`slowLogPayloads.size() > 0` can be replaced with '!slowLogPayloads.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/SlowLogPersistentService.java`
#### Snippet
```java
        }
      }
      if (slowLogPayloads.size() > 0) {
        SlowLogTableAccessor.addSlowLogRecords(slowLogPayloads, connection);
      }
```

### SizeReplaceableByIsEmpty
`key.length() != 0` can be replaced with '!key.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/HFileProcedurePrettyPrinter.java`
#### Snippet
```java
    if (cmd.hasOption("w")) {
      String key = cmd.getOptionValue("w");
      if (key != null && key.length() != 0) {
        procId = Long.parseLong(key);
      } else {
```

### SizeReplaceableByIsEmpty
`result.size() > 0` can be replaced with '!result.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java`
#### Snippet
```java
        if (result.isEmpty() && valueIsNull) {
          matches = true;
        } else if (result.size() > 0 && result.get(0).getValueLength() == 0 && valueIsNull) {
          matches = true;
        } else if (result.size() == 1 && !valueIsNull) {
```

### SizeReplaceableByIsEmpty
`filtered.getEdit().size() == 0` can be replaced with 'filtered.getEdit().isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
    }
    Entry filtered = filter.filter(entry);
    if (entry != null && (filtered == null || filtered.getEdit().size() == 0)) {
      LOG.trace("Filtered entry for replication: {}", entry);
      source.getSourceMetrics().incrLogEditsFiltered();
```

### SizeReplaceableByIsEmpty
`manager.getOldSources().size() > 0` can be replaced with '!manager.getOldSources().isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
        Thread.sleep(SLEEP_TIME);
      }
      while (manager.getOldSources().size() > 0) {
        Thread.sleep(SLEEP_TIME);
      }
```

### SizeReplaceableByIsEmpty
`message.length() > 0` can be replaced with '!message.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java

  protected static void printUsage(final String className, final String message) {
    if (message != null && message.length() > 0) {
      System.err.println(message);
    }
```

### SizeReplaceableByIsEmpty
`familyMap.size() > 0` can be replaced with '!familyMap.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    }

    if (familyMap != null && familyMap.size() > 0) {
      // at least one family must be allowed
      for (Map.Entry<byte[], ? extends Collection<byte[]>> family : familyMap.entrySet()) {
```

### SizeReplaceableByIsEmpty
`delimiterString.length() == 0` can be replaced with 'delimiterString.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitPolicy.java`
#### Snippet
```java
    // read the prefix length from the table descriptor
    String delimiterString = region.getTableDescriptor().getValue(DELIMITER_KEY);
    if (delimiterString == null || delimiterString.length() == 0) {
      LOG.error(DELIMITER_KEY + " not specified for table "
        + region.getTableDescriptor().getTableName() + ". Using default RegionSplitPolicy");
```

### SizeReplaceableByIsEmpty
`delimiterString.length() == 0` can be replaced with 'delimiterString.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitRestriction.java`
#### Snippet
```java
  public void initialize(TableDescriptor tableDescriptor, Configuration conf) throws IOException {
    String delimiterString = tableDescriptor.getValue(DELIMITER_KEY);
    if (delimiterString == null || delimiterString.length() == 0) {
      delimiterString = conf.get(DELIMITER_KEY);
      if (delimiterString == null || delimiterString.length() == 0) {
```

### SizeReplaceableByIsEmpty
`delimiterString.length() == 0` can be replaced with 'delimiterString.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitRestriction.java`
#### Snippet
```java
    if (delimiterString == null || delimiterString.length() == 0) {
      delimiterString = conf.get(DELIMITER_KEY);
      if (delimiterString == null || delimiterString.length() == 0) {
        LOG.error("{} not specified for table {}. " + "Using the default RegionSplitRestriction",
          DELIMITER_KEY, tableDescriptor.getTableName());
```

### SizeReplaceableByIsEmpty
`heap.size() > 0` can be replaced with '!heap.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java

        boolean seekResult;
        if (isLazy && heap.size() > 0) {
          // If there is only one scanner left, we don't do lazy seek.
          seekResult = scanner.requestSeek(seekKey, forward, useBloom);
```

### SizeReplaceableByIsEmpty
`mutations.size() == 0` can be replaced with 'mutations.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      }

      if (mutations.size() == 0) {
        return new CheckAndMutateResult(true, null);
      } else {
```

### SizeReplaceableByIsEmpty
`msg.length() == 0` can be replaced with 'msg.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  private IOException convertThrowableToIOE(final Throwable t, final String msg) {
    return (t instanceof IOException ? (IOException) t
      : msg == null || msg.length() == 0 ? new IOException(t)
      : new IOException(msg, t));
  }
```

### SizeReplaceableByIsEmpty
`region.getLockedRows().size() > 0` can be replaced with '!region.getLockedRows().isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    StringBuilder sb = new StringBuilder();
    for (HRegion region : getRegions()) {
      if (region.getLockedRows().size() > 0) {
        for (HRegion.RowLockContext rowLockContext : region.getLockedRows().values()) {
          sb.setLength(0);
```

### SizeReplaceableByIsEmpty
`futures.size() != 0` can be replaced with '!futures.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java`
#### Snippet
```java

    boolean hasTasks() {
      return futures.size() != 0;
    }

```

### SizeReplaceableByIsEmpty
`columns.size() != 0` can be replaced with '!columns.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
    RegionCoprocessorHost regionCoprocessorHost) throws IOException {
    boolean hasNullColumn =
      !(columns != null && columns.size() != 0 && columns.first().length != 0);
    Pair<DeleteTracker, ColumnTracker> trackers =
      getTrackers(regionCoprocessorHost, columns, scanInfo, oldestUnexpiredTS, scan);
```

### SizeReplaceableByIsEmpty
`columns.size() == 0` can be replaced with 'columns.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher.java`
#### Snippet
```java
    if (deleteTracker instanceof NewVersionBehaviorTracker) {
      columnTracker = (NewVersionBehaviorTracker) deleteTracker;
    } else if (columns == null || columns.size() == 0) {
      columnTracker = new ScanWildcardColumnTracker(scanInfo.getMinVersions(), maxVersionToCheck,
        oldestUnexpiredTS, scanInfo.getComparator());
```

### SizeReplaceableByIsEmpty
`columns.size() > 0` can be replaced with '!columns.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker.java`
#### Snippet
```java
    this.resultMaxVersions = resultMaxVersions;
    this.oldestStamp = oldestUnexpiredTS;
    if (columns != null && columns.size() > 0) {
      this.columns = new byte[columns.size()][];
      int i = 0;
```

### SizeReplaceableByIsEmpty
`existTableList.size() > 0` can be replaced with '!existTableList.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
    }

    if (existTableList.size() > 0) {
      if (!isOverwrite) {
        LOG.error("Existing table (" + existTableList + ") found in the restore target, please add "
```

### SizeReplaceableByIsEmpty
`disabledTableList.size() > 0` can be replaced with '!disabledTableList.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
          "Existing table found in target while no \"-o\" " + "as overwrite option found");
      } else {
        if (disabledTableList.size() > 0) {
          LOG.error("Found offline table in the restore target, "
            + "please enable them before restore with \"-overwrite\" option");
```

### SizeReplaceableByIsEmpty
`sessions.size() == 0` can be replaced with 'sessions.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
  private String getOngoingBackupId() throws IOException {
    ArrayList<BackupInfo> sessions = systemTable.getBackupInfos(BackupState.RUNNING);
    if (sessions.size() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`conf.size() > 0` can be replaced with '!conf.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/LogRollMasterProcedureManager.java`
#### Snippet
```java
    List<NameStringPair> conf = desc.getConfigurationList();
    byte[] data = new byte[0];
    if (conf.size() > 0) {
      // Get backup root path
      data = Bytes.toBytes(conf.get(0).getValue());
```

### SizeReplaceableByIsEmpty
`activeFiles.size() > 0` can be replaced with '!activeFiles.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java`
#### Snippet
```java
      String tgtDest = backupInfo.getBackupRootDir() + Path.SEPARATOR + backupInfo.getBackupId();
      int attempt = 1;
      while (activeFiles.size() > 0) {
        LOG.info("Copy " + activeFiles.size() + " active bulk loaded files. Attempt =" + attempt++);
        String[] toCopy = new String[activeFiles.size()];
```

### SizeReplaceableByIsEmpty
`archiveFiles.size() > 0` can be replaced with '!archiveFiles.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java`
#### Snippet
```java
      // we will have partially loaded files in backup destination (only files from active data
      // directory). It is OK, because the backup will marked as FAILED and data will be cleaned up
      if (archiveFiles.size() > 0) {
        String[] toCopy = new String[archiveFiles.size()];
        archiveFiles.toArray(toCopy);
```

### SizeReplaceableByIsEmpty
`newlyArchived.size() > 0` can be replaced with '!newlyArchived.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java`
#### Snippet
```java
    }

    if (newlyArchived.size() > 0) {
      activeFiles.removeAll(newlyArchived);
      archiveFiles.addAll(newlyArchived);
```

### SizeReplaceableByIsEmpty
`infos.size() > 0` can be replaced with '!infos.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        } else {
          List<BackupInfo> infos = sysTable.getBackupInfos(BackupState.RUNNING);
          if (infos != null && infos.size() > 0) {
            info = infos.get(0);
            backupId = info.getBackupId();
```

### SizeReplaceableByIsEmpty
`sessions.size() > 0` can be replaced with '!sessions.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          List<BackupInfo> sessions = table.getBackupInfos(BackupState.RUNNING);

          if (sessions.size() > 0) {
            System.err.println("Found backup session in a RUNNING state: ");
            System.err.println(sessions.get(0));
```

### SizeReplaceableByIsEmpty
`list.size() == 0` can be replaced with 'list.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        BackupInfo backupInfo;
        List<BackupInfo> list = sysTable.getBackupInfos(BackupState.RUNNING);
        if (list.size() == 0) {
          // No failed sessions found
          System.out.println("REPAIR status: no failed sessions found."
```

### SizeReplaceableByIsEmpty
`list.size() == 0` can be replaced with 'list.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java`
#### Snippet
```java
      Map<TableName, Map<String, Long>> incrTimeRanges = new HashMap<>();

      if (list == null || list.size() == 0) {
        return incrTimeRanges;
      }
```

### SizeReplaceableByIsEmpty
`errorMsg.length() > 0` can be replaced with '!errorMsg.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
   */
  private void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
```

### SizeReplaceableByIsEmpty
`list.size() != 0` can be replaced with '!list.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
      // Step 2: Make sure there is no failed session
      List<BackupInfo> list = sysTable.getBackupInfos(BackupState.RUNNING);
      if (list.size() != 0) {
        // ailed sessions found
        LOG.warn("Failed backup session found. Run backup repair tool first.");
```

### SizeReplaceableByIsEmpty
`regionDirList.size() == 0` can be replaced with 'regionDirList.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
        byte[][] keys = null;
        try {
          if (regionDirList == null || regionDirList.size() == 0) {
            admin.createTable(htd);
          } else {
```

### SizeReplaceableByIsEmpty
`srcFiles.size() == 0` can be replaced with 'srcFiles.isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
      try (SequenceFile.Writer writer = getWriter(fileListingPath)) {
        List<Path> srcFiles = getSourceFiles();
        if (srcFiles.size() == 0) {
          return fileListingPath;
        }
```

### SizeReplaceableByIsEmpty
`cache.size() == 0` can be replaced with 'cache.isEmpty()'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java`
#### Snippet
```java
    @Override
    public Result next() throws IOException {
      if (cache.size() == 0) {
        setupNextScanner();
        try {
```

### SizeReplaceableByIsEmpty
`cache.size() > 0` can be replaced with '!cache.isEmpty()'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java`
#### Snippet
```java
      }

      if (cache.size() > 0) {
        return cache.poll();
      } else {
```

### SizeReplaceableByIsEmpty
`in.getFamilyCellMap().size() == 0` can be replaced with 'in.getFamilyCellMap().isEmpty()'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    }
    // Delete the whole row
    if (in.getFamilyCellMap().size() == 0) {
      return out;
    }
```

### SizeReplaceableByIsEmpty
`in.size() == 0` can be replaced with 'in.isEmpty()'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java

  public static byte[][] splitKeyFromThrift(List<ByteBuffer> in) {
    if (in == null || in.size() == 0) {
      return null;
    }
```

### SizeReplaceableByIsEmpty
`files.size() > 0` can be replaced with '!files.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      }
      // Now delete the content of recovered edits. We're done w/ them.
      if (files.size() > 0 && this.conf.getBoolean("hbase.region.archive.recovered.edits", false)) {
        // For debugging data loss issues!
        // If this flag is set, make use of the hfile archiving by making recovered.edits a fake
```

### SizeReplaceableByIsEmpty
`failures.size() != 0` can be replaced with '!failures.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      }
      // validation failed, bail out before doing anything permanent.
      if (failures.size() != 0) {
        StringBuilder list = new StringBuilder();
        for (Pair<byte[], String> p : failures) {
```

### SizeReplaceableByIsEmpty
`result.size() > 0` can be replaced with '!result.isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            if (result.isEmpty() && valueIsNull) {
              matches = op != CompareOperator.NOT_EQUAL;
            } else if (result.size() > 0 && valueIsNull) {
              matches = (result.get(0).getValueLength() == 0) == (op != CompareOperator.NOT_EQUAL);
              cellTs = result.get(0).getTimestamp();
```

### SizeReplaceableByIsEmpty
`results.size() > 0` can be replaced with '!results.isEmpty()'
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      do {
        hasMoreRows = scanner.next(results);
        if (results.size() > 0) {
          counter++;
        }
```

### SizeReplaceableByIsEmpty
`result.getFirst().size() > 0` can be replaced with '!result.getFirst().isEmpty()'
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, Pair<List<S>, Long> result) {
        if (result.getFirst().size() > 0) {
          sumVal = ci.add(sumVal, result.getFirst().get(0));
          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));
```

## RuleId[id=TrivialStringConcatenation]
### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java`
#### Snippet
```java

  protected static String getTimestampString(final KeyValue kv) {
    return kv.getTimestamp() + "";
  }

```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java`
#### Snippet
```java
    String familyLength = kv.getFamilyLength() + " ";
    if (includeMeta) {
      leadingLengths += Strings.padFront(kv.getKeyLength() + "", '0', 4);
      leadingLengths += " ";
      leadingLengths += Strings.padFront(kv.getValueLength() + "", '0', 4);
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java`
#### Snippet
```java
      leadingLengths += Strings.padFront(kv.getKeyLength() + "", '0', 4);
      leadingLengths += " ";
      leadingLengths += Strings.padFront(kv.getValueLength() + "", '0', 4);
      leadingLengths += " ";
      leadingLengths += Strings.padFront(kv.getRowLength() + "", '0', 2);
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java`
#### Snippet
```java
      leadingLengths += Strings.padFront(kv.getValueLength() + "", '0', 4);
      leadingLengths += " ";
      leadingLengths += Strings.padFront(kv.getRowLength() + "", '0', 2);
      leadingLengths += " ";
    }
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
          if (debug) {
            if (LOG.isDebugEnabled()) {
              LOG.debug("" + index + " " + aField.getName() + " " + aField.getType());
            }
          }
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
            cmd.add(event.getInternalName());
            cmd.add("-d");
            cmd.add("" + duration);
            cmd.add("-o");
            cmd.add(output.name().toLowerCase());
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java`
#### Snippet
```java
          .forEach(line -> System.out.println(TAG.matcher(line).replaceAll("")));
      } catch (IOException ioe) {
        System.err.println("" + ioe);
      }
    }
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/Reference.java`
#### Snippet
```java
  @Override
  public String toString() {
    return "" + this.region;
  }

```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
            if (other.getCachedTime() < 0 || this.getCachedTime() < 0) {
              throw new IllegalStateException(
                "" + this.getCachedTime() + ", " + other.getCachedTime());
            }
            return Long.compare(other.getCachedTime(), this.getCachedTime());
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
      }
      throw new NotServingRegionException(
        "" + regionNameStr + " is not online on " + this.serverName);
    }
    return region;
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java`
#### Snippet
```java
    t = Thread.currentThread();
    if (!isDone()) {
      throw new IllegalStateException("" + txid + " " + Thread.currentThread());
    }
    this.doneTxid = NOT_DONE;
```

### TrivialStringConcatenation
Empty string used in concatenation
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
        String posAfterStr = "<unknown>";
        try {
          posAfterStr = this.inputStream.getPos() + "";
        } catch (Throwable t) {
          LOG.trace("Error getting pos for error message - ignoring", t);
```

## RuleId[id=FinalStaticMethod]
### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   *         cell's family is lesser than byte[] and 0 otherwise
   */
  public final static int compareFamilies(Cell left, byte[] right, int roffset, int rlength) {
    if (left instanceof ByteBufferExtendedCell) {
      return ByteBufferUtils.compareTo(((ByteBufferExtendedCell) left).getFamilyByteBuffer(),
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   *         cell's column is lesser than byte[] and 0 otherwise
   */
  public final static int compareColumns(Cell left, byte[] right, int rfoffset, int rflength,
    int rqoffset, int rqlength) {
    int diff = compareFamilies(left, right, rfoffset, rflength);
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   *         cell's qualifier is lesser than byte[] and 0 otherwise
   */
  public final static int compareQualifiers(Cell left, byte[] right, int rOffset, int rLength) {
    if (left instanceof ByteBufferExtendedCell) {
      return ByteBufferUtils.compareTo(((ByteBufferExtendedCell) left).getQualifierByteBuffer(),
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   */
  // compare a key against row/fam/qual/ts/type
  public static final int compareKeyBasedOnColHint(CellComparator comparator, Cell nextIndexedCell,
    Cell currentCell, int foff, int flen, byte[] colHint, int coff, int clen, long ts, byte type) {
    int compare = comparator.compareRows(nextIndexedCell, currentCell);
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   *         cells are equal
   */
  static final int compareWithoutRow(CellComparator comparator, Cell left, byte[] right,
    int roffset, int rlength, short rowlength) {
    /***
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   *         equal to 0 if left is equal to right
   */
  public static final int compareKeyIgnoresMvcc(CellComparator comparator, Cell left, Cell right) {
    return ((CellComparatorImpl) comparator).compare(left, right, true);
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   *         than right equal to 0 if left is equal to right
   */
  public static final int compare(CellComparator comparator, Cell left, byte[] key, int offset,
    int length) {
    // row
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
   * @return 0 if b is null, otherwise returns length
   */
  final public static int len(byte[] b) {
    return b == null ? 0 : b.length;
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsValidator.java`
#### Snippet
```java
  }

  public static final boolean isValidLabel(byte[] label) {
    for (int i = 0; i < label.length; i++) {
      if (!isValidAuthChar(label[i])) {
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsValidator.java`
#### Snippet
```java
  }

  static final boolean isValidAuthChar(byte b) {
    return validAuthChars[0xff & b];
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java`
#### Snippet
```java
  }

  final private static int validateCompositeKey(byte[] keyBytes) {

    int separatorIdx = Bytes.indexOf(keyBytes, tableSeparator);
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java`
#### Snippet
```java
   * @param hFileLinkPattern The path of the HFile Link.
   */
  public final static HFileLink buildFromHFileLinkPattern(final Path rootDir, final Path archiveDir,
    final Path hFileLinkPattern) {
    Path hfilePath = getHFileLinkPatternRelativePath(hFileLinkPattern);
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java`
#### Snippet
```java
   * @throws IOException on unexpected error.
   */
  public static final HFileLink buildFromHFileLinkPattern(Configuration conf, Path hFileLinkPattern)
    throws IOException {
    return buildFromHFileLinkPattern(CommonFSUtils.getRootDir(conf),
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * Returns the factory to be used to create {@link HFile} writers
   */
  public static final WriterFactory getWriterFactory(Configuration conf, CacheConfig cacheConf) {
    int version = getFormatVersion(conf);
    switch (version) {
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * Number of checksum verification failures. It also clears the counter.
   */
  public static final long getChecksumFailuresCount() {
    return CHECKSUM_FAILURES.sum();
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
  }

  public static final void updateWriteLatency(long latencyMillis) {
    metrics.updateFsWriteTime(latencyMillis);
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * Number of checksum verification failures. It also clears the counter.
   */
  public static final long getAndResetChecksumFailuresCount() {
    return CHECKSUM_FAILURES.sumThenReset();
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
  }

  public static final void updateReadLatency(long latencyMillis, boolean pread) {
    if (pread) {
      metrics.updateFsPreadTime(latencyMillis);
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * all writers created through the returned factory.
   */
  public static final WriterFactory getWriterFactoryNoCache(Configuration conf) {
    return HFile.getWriterFactory(conf, CacheConfig.DISABLED);
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java`
#### Snippet
```java
   * @return the final directory for the snapshot in the given filepath
   */
  private static final Path getSpecifiedSnapshotDir(final Path snapshotsDir, String snapshotName) {
    return new Path(snapshotsDir, snapshotName);
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java`
#### Snippet
```java
   * @return the directory for all completed snapshots;
   */
  public static final Path getSnapshotsDir(Path rootDir) {
    return new Path(rootDir, HConstants.SNAPSHOT_DIR_NAME);
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
  }

  protected static final boolean switched(WALEntryStream entryStream, Path path) {
    Path newPath = entryStream.getCurrentPath();
    return newPath == null || !path.getName().equals(newPath.getName());
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NoLimitScannerContext.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "MS_EXPOSE_REP",
      justification = "singleton pattern")
  public static final ScannerContext getInstance() {
    return NO_LIMIT;
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
  }

  static final void instrument() {
    seekCount = new LongAdder();
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java

  // Test methods
  static final long getSeekCount() {
    return seekCount.sum();
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Checks whether the key indicates an open interval boundary (i.e. infinity).
   */
  private static final boolean isOpen(byte[] key) {
    return key != null && key.length == 0;
  }
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * Checks whether the key is invalid (e.g. from an L0 file, or non-stripe-compacted files).
   */
  private static final boolean isInvalid(byte[] key) {
    // No need to use Arrays.equals because INVALID_KEY is null
    return key == INVALID_KEY;
```

### FinalStaticMethod
'static' method declared `final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private static final boolean isOpen(Cell key) {
    return key != null && key.getRowLength() == 0;
  }
```

## RuleId[id=NullableProblems]
### NullableProblems
Overridden methods are not annotated
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java`
#### Snippet
```java
   *         thrown if EOF is reached prematurely. Does not return null.
   */
  @NonNull
  protected abstract Cell parseCell() throws IOException;

```

### NullableProblems
Method annotated with @Nullable must not override @NotNull method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

    @Override
    public SortedSet<K> headSet(K toElement) {
      return null;
    }
```

### NullableProblems
Method annotated with @Nullable must not override @NotNull method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

    @Override
    public SortedSet<K> tailSet(K fromElement) {
      return null;
    }
```

### NullableProblems
Method annotated with @Nullable must not override @NotNull method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

    @Override
    public SortedSet<K> subSet(K fromElement, K toElement) {
      return null;
    }
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
    private final int infoServerPort;
    private final List<ReplicationLoadSource> sources;
    @Nullable
    private final ReplicationLoadSink sink;
    private final Map<byte[], RegionMetrics> regionStatus;
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
  private int infoServerPort;
  private List<ReplicationLoadSource> sources = Collections.emptyList();
  @Nullable
  private ReplicationLoadSink sink = null;
  private final Map<byte[], RegionMetrics> regionStatus = new TreeMap<>(Bytes.BYTES_COMPARATOR);
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
    private final List<ServerName> decommissionedServerNames;
    private final Map<ServerName, ServerMetrics> liveServerMetrics;
    @Nullable
    private final ServerName masterName;
    private final List<ServerName> backupMasterNames;
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
    private final String clusterId;
    private final List<String> masterCoprocessorNames;
    @Nullable
    private final Boolean balancerOn;
    private final int masterInfoPort;
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java

  private static class ClusterMetricsImpl implements ClusterMetrics {
    @Nullable
    private final String hbaseVersion;
    private final List<ServerName> deadServerNames;
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
    private final List<ServerName> backupMasterNames;
    private final List<RegionState> regionsInTransition;
    @Nullable
    private final String clusterId;
    private final List<String> masterCoprocessorNames;
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  private List<ServerName> backupMasterNames = Collections.emptyList();
  private List<RegionState> regionsInTransition = Collections.emptyList();
  @Nullable
  private String clusterId;
  private List<String> masterCoprocessorNames = Collections.emptyList();
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  private List<ServerName> unknownServerNames = Collections.emptyList();
  private Map<ServerName, ServerMetrics> liveServerMetrics = new TreeMap<>();
  @Nullable
  private ServerName masterName;
  private List<ServerName> backupMasterNames = Collections.emptyList();
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  private List<ServerName> serversName = Collections.emptyList();
  private Map<TableName, RegionStatesCount> tableRegionStatesCount = Collections.emptyMap();
  @Nullable
  private List<ServerTask> masterTasks;
  private List<ServerName> decommissionedServerNames = Collections.emptyList();
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  }

  @Nullable
  private String hbaseVersion;
  private List<ServerName> deadServerNames = Collections.emptyList();
```

### NullableProblems
The generated code will use '@org.jetbrains.annotations.Nullable' instead of '@edu.umd.cs.findbugs.annotations.Nullable'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  private String clusterId;
  private List<String> masterCoprocessorNames = Collections.emptyList();
  @Nullable
  private Boolean balancerOn;
  private int masterInfoPort;
```

### NullableProblems
Overridden methods are not annotated
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java`
#### Snippet
```java
   * @return List of plans
   */
  @NonNull
  Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,
    List<ServerName> servers) throws IOException;
```

### NullableProblems
Overridden methods are not annotated
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java`
#### Snippet
```java
   * @return Map of servername to regioninfos
   */
  @NonNull
  Map<ServerName, List<RegionInfo>> roundRobinAssignment(List<RegionInfo> regions,
    List<ServerName> servers) throws IOException;
```

### NullableProblems
Method annotated with @Nullable must not override @NotNull method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/NoOpRegionSizeStore.java`
#### Snippet
```java

  @Override
  public Iterator<Entry<RegionInfo, RegionSize>> iterator() {
    return null;
  }
```

## RuleId[id=EqualsBetweenInconvertibleTypes]
### EqualsBetweenInconvertibleTypes
`equals` between objects of inconvertible types 'ImmutableBytesWritable' and 'byte\[\]'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
        new ImmutableBytesWritable(MultiTableHFileOutputFormat.getSuffix(sorted.first().get()));
    }
    if (!first.equals(HConstants.EMPTY_BYTE_ARRAY)) {
      throw new IllegalArgumentException(
        "First region of table should have empty start key. Instead has: "
```

## RuleId[id=ExplicitArrayFilling]
### ExplicitArrayFilling
Can be replaced with single 'Arrays.fill()' method call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static byte[] createMaxByteArray(int maxByteCount) {
    byte[] maxByteArray = new byte[maxByteCount];
    for (int i = 0; i < maxByteArray.length; i++) {
      maxByteArray[i] = (byte) 0xff;
    }
```

### ExplicitArrayFilling
Can be replaced with single 'Arrays.fill()' method call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
  void updateCostsAndWeightsWithAction(BalancerClusterState cluster, BalanceAction action) {
    // Reset all the weights to 0
    for (int i = 0; i < weightsOfGenerators.length; i++) {
      weightsOfGenerators[i] = 0;
    }
```

### ExplicitArrayFilling
Can be replaced with single 'Arrays.fill()' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      baosInMemory = new ByteArrayOutputStream();
      prevOffsetByType = new long[BlockType.values().length];
      for (int i = 0; i < prevOffsetByType.length; ++i) {
        prevOffsetByType[i] = UNSET;
      }
```

### ExplicitArrayFilling
Can be replaced with single 'Arrays.fill()' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
    new Boolean[Compression.Algorithm.values().length];
  static {
    for (int i = 0; i < compressionTestResults.length; ++i) {
      compressionTestResults[i] = null;
    }
```

### ExplicitArrayFilling
Can be replaced with single 'Arrays.fill()' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

      OperationStatus[] statuses = new OperationStatus[mutations.length];
      for (int i = 0; i < statuses.length; i++) {
        statuses[i] = OperationStatus.SUCCESS;
      }
```

## RuleId[id=DuplicateCondition]
### DuplicateCondition
Duplicate condition `max <= min`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFunction.java`
#### Snippet
```java
  protected static double scale(double min, double max, double value) {
    if (
      max <= min || value <= min || Math.abs(max - min) <= COST_EPSILON
        || Math.abs(value - min) <= COST_EPSILON
    ) {
```

### DuplicateCondition
Duplicate condition `Math.abs(max - min) <= COST_EPSILON`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFunction.java`
#### Snippet
```java
  protected static double scale(double min, double max, double value) {
    if (
      max <= min || value <= min || Math.abs(max - min) <= COST_EPSILON
        || Math.abs(value - min) <= COST_EPSILON
    ) {
```

### DuplicateCondition
Duplicate condition `max <= min`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFunction.java`
#### Snippet
```java
      return 0;
    }
    if (max <= min || Math.abs(max - min) <= COST_EPSILON) {
      return 0;
    }
```

### DuplicateCondition
Duplicate condition `Math.abs(max - min) <= COST_EPSILON`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFunction.java`
#### Snippet
```java
      return 0;
    }
    if (max <= min || Math.abs(max - min) <= COST_EPSILON) {
      return 0;
    }
```

## RuleId[id=IgnoreResultOfCall]
### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
    try {
      if (!dir.exists()) {
        dir.mkdirs();
      }
    } catch (SecurityException e) {
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/JarFinder.java`
#### Snippet
```java
            testDir = testDir.getAbsoluteFile();
            if (!testDir.exists()) {
              testDir.mkdirs();
            }
            File tempJar = File.createTempFile("hadoop-", "", testDir);
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ZNodeClearer.java`
#### Snippet
```java

    if (fileName != null) {
      new File(fileName).delete();
    }
  }
```

### IgnoreResultOfCall
Result of `FilterInputStream.skip()` is ignored
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java`
#### Snippet
```java
      HFileProtos.FileTrailerProto.PARSER.parseDelimitedFrom(inputStream);
    int size = start - inputStream.available();
    inputStream.skip(getTrailerSize() - NOT_PB_SIZE - size);

    // process the PB
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java`
#### Snippet
```java
            LOG.debug("File " + filePath + " already exists. Deleting!!");
          }
          file.delete();
          // If deletion fails still we can manage with the writes
        }
```

### IgnoreResultOfCall
Result of `File.delete()` is ignored
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
    File f = new File(filename);
    if (f.exists()) {
      f.delete();
    }
  }
```

### IgnoreResultOfCall
Result of `File.mkdirs()` is ignored
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    System.setProperty(property, path);
    conf.set(property, path);
    new File(path).mkdirs();
    LOG.info("Setting " + property + " to " + path + " in system properties and HBase conf");
    return path;
```

## RuleId[id=UnnecessaryUnboxing]
### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleColumnInterpreter.java`
#### Snippet
```java
  @Override
  public double divideForAvg(Double d1, Long l2) {
    return (l2 == null || d1 == null) ? Double.NaN : (d1.doubleValue() / l2.doubleValue());
  }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    ) {
      if (traceEnabled) {
        LOG.trace("Waiting for pid=" + oldProcId.longValue() + " to be submitted");
      }
      Threads.sleep(100);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      Threads.sleep(100);
    }
    return oldProcId.longValue();
  }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    completed.computeIfAbsent(procId, (key) -> {
      Procedure<TEnvironment> proc =
        new FailedProcedure<>(procId.longValue(), procName, procOwner, nonceKey, exception);

      return new CompletedProcedureRetainer<>(proc);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    // Initialize the procedure
    proc.setNonceKey(nonceKey);
    proc.setProcId(currentProcId.longValue());

    // Commit the transaction
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java`
#### Snippet
```java
        ordinal = labels.get(label);
        if (ordinal != null) {
          return ordinal.intValue();
        }
        return VisibilityConstants.NON_EXIST_LABEL_ORDINAL;
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
          regionCounter = Integer.valueOf(0);
        }
        regionCounter = regionCounter.intValue() + 1;
        primaryRSToRegionCounterMap.put(primaryRS, regionCounter);

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      float dispersionScore = 0;
      int dispersionNum = 0;
      if (primaryToSecTerRSMap.get(primaryRS) != null && regionsOnPrimary.intValue() != 0) {
        dispersionNum = primaryToSecTerRSMap.get(primaryRS).size();
        dispersionScore = dispersionNum / ((float) regionsOnPrimary.intValue() * 2);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      if (primaryToSecTerRSMap.get(primaryRS) != null && regionsOnPrimary.intValue() != 0) {
        dispersionNum = primaryToSecTerRSMap.get(primaryRS).size();
        dispersionScore = dispersionNum / ((float) regionsOnPrimary.intValue() * 2);
      }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
          hostRegionCounter = Integer.valueOf(0);
        }
        hostRegionCounter = hostRegionCounter.intValue() + 1;
        serverToHostingRegionCounterMap.put(currentRS, hostRegionCounter);

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
          regionCounter = Integer.valueOf(0);
        }
        regionCounter = regionCounter.intValue() + 1;
        primaryRSToRegionCounterMap.put(primaryRS, regionCounter);

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      float dispersionScore = 0;
      int dispersionNum = 0;
      if (primaryToSecTerRSMap.get(primaryRS) != null && regionsOnPrimary.intValue() != 0) {
        dispersionNum = primaryToSecTerRSMap.get(primaryRS).size();
        dispersionScore = dispersionNum / ((float) regionsOnPrimary.intValue() * 2);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      if (primaryToSecTerRSMap.get(primaryRS) != null && regionsOnPrimary.intValue() != 0) {
        dispersionNum = primaryToSecTerRSMap.get(primaryRS).size();
        dispersionScore = dispersionNum / ((float) regionsOnPrimary.intValue() * 2);
      }
      // Update the max dispersion score
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    for (Map.Entry<ServerName, Integer> entry : serverToHostingRegionCounterMap.entrySet()) {
      ServerName currentRS = entry.getKey();
      int hostRegionCounter = entry.getValue().intValue();

      // Update the most loaded region server list and maxRegionsOnRS
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
          // Do not skip cell even if maxSeqId is null. Maybe we are in a rolling upgrade,
          // or the master was crashed before and we can not get the information.
          if (maxSeqId == null || maxSeqId.longValue() < logEntry.getKey().getSequenceId()) {
            keptCells.add(cell);
          }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
    RegionStoreSequenceIds.Builder builder = RegionStoreSequenceIds.newBuilder();
    Long seqId = flushedSequenceIdByRegion.get(encodedRegionName);
    builder.setLastFlushedSequenceId(seqId != null ? seqId.longValue() : HConstants.NO_SEQNUM);
    Map<byte[], Long> storeFlushedSequenceId =
      storeFlushedSequenceIdsByRegion.get(encodedRegionName);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
        builder.addStoreSequenceId(StoreSequenceId.newBuilder()
          .setFamilyName(UnsafeByteOperations.unsafeWrap(entry.getKey()))
          .setSequenceId(entry.getValue().longValue()).build());
      }
    }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
        }
        // Don't let smaller sequence ids override greater sequence ids.
        if (existingValue == null || (l != HConstants.NO_SEQNUM && l > existingValue.longValue())) {
          storeFlushedSequenceId.put(family, l);
        }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
            // because locality is only reported for servers which have some
            // blocks of a region local, then the locality for that pair is 0.
            localityPerServer[i][j * slotsPerServer + k] = locality.floatValue();
          }
        }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          }
          Float localityObj = rackLocality.get(region);
          float locality = localityObj == null ? 0 : localityObj.floatValue();
          locality += localityPerServer[i][j];
          rackLocality.put(region, locality);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          Float totalRackLocalityObj = rackRegionLocality.get(rack).get(regions.get(i));
          float totalRackLocality =
            totalRackLocalityObj == null ? 0 : totalRackLocalityObj.floatValue();

          // Primary cost aims to favor servers with high node locality and low
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java

    protected long getProcId() {
      return procId != null ? procId.longValue() : -1;
    }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/AbstractPeerProcedure.java`
#### Snippet
```java
      .getTableEncodedRegionNameAndLastBarrier(conn, tableName)) {
      LOG.trace("Update last pushed sequence id for {}, {}", tableName, name2Barrier);
      addToMap(lastSeqIds, name2Barrier.getFirst(), name2Barrier.getSecond().longValue() - 1,
        queueStorage);
    }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/GlobalQuotaSettingsImpl.java`
#### Snippet
```java
        getNamespace(), getRegionServer(), throttleProto));
    }
    if (bypassGlobals != null && bypassGlobals.booleanValue()) {
      settings.add(new QuotaGlobalsSettingsBypass(getUserName(), getTableName(), getNamespace(),
        getRegionServer(), true));
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
    }
    if (ordinal != null) {
      return ordinal.intValue();
    }
    // 0 denotes not available
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
            count = filesMap.get(p);
            if (count != null) {
              c = count.intValue();
            } else {
              al.addAndGet(size);
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/HFileProcedurePrettyPrinter.java`
#### Snippet
```java
      if (procId != null) {
        if (
          scanner.seekTo(PrivateCellUtil.createFirstOnRow(Bytes.toBytes(procId.longValue()))) != -1
        ) {
          do {
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/HFileProcedurePrettyPrinter.java`
#### Snippet
```java
            long currentProcId =
              Bytes.toLong(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());
            if (currentProcId != procId.longValue()) {
              break;
            }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ScopeWALEntryFilter.java`
#### Snippet
```java
  private boolean hasGlobalScope(NavigableMap<byte[], Integer> scopes, byte[] family) {
    Integer scope = scopes.get(family);
    return scope != null && scope.intValue() == HConstants.REPLICATION_SCOPE_GLOBAL;
  }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java`
#### Snippet
```java
    Long canReplicateUnderSeqId = canPushUnder.getIfPresent(encodedNameAsString);
    if (canReplicateUnderSeqId != null) {
      if (seqId < canReplicateUnderSeqId.longValue()) {
        LOG.trace("{} is before the end barrier {}, pass", entry, canReplicateUnderSeqId);
        return true;
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CustomizedScanInfoBuilder.java`
#### Snippet
```java
  @Override
  public int getMaxVersions() {
    return maxVersions != null ? maxVersions.intValue() : scanInfo.getMaxVersions();
  }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CustomizedScanInfoBuilder.java`
#### Snippet
```java
  @Override
  public long getTTL() {
    return ttl != null ? ttl.longValue() : scanInfo.getTtl();
  }

```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        server.removeFromMovedRegions(region.getEncodedName());

        if (previous == null || !previous.booleanValue()) {
          htd = htds.get(region.getTable());
          if (htd == null) {
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
            }
          }
          if (bypass != null && bypass.booleanValue()) {
            done = true;
          }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.java`
#### Snippet
```java
    action.accept(future);
    try {
      return future.get(waitTimeout, TimeUnit.MILLISECONDS).longValue();
    } catch (InterruptedException e) {
      InterruptedIOException ioe = new InterruptedIOException();
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
      }
      Long sid = entry.getValue();
      if (lowest == HConstants.NO_SEQNUM || sid.longValue() < lowest) {
        lowest = sid.longValue();
      }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
      Long sid = entry.getValue();
      if (lowest == HConstants.NO_SEQNUM || sid.longValue() < lowest) {
        lowest = sid.longValue();
      }
    }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
      for (Map.Entry<ImmutableByteArray, Long> e : flushing.entrySet()) {
        Long currentId = tmpMap.get(e.getKey());
        if (currentId != null && currentId.longValue() < e.getValue().longValue()) {
          String errorStr = Bytes.toString(encodedRegionName) + " family " + e.getKey().toString()
            + " acquired edits out of order current memstore seq=" + currentId
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
      for (Map.Entry<ImmutableByteArray, Long> e : flushing.entrySet()) {
        Long currentId = tmpMap.get(e.getKey());
        if (currentId != null && currentId.longValue() < e.getValue().longValue()) {
          String errorStr = Bytes.toString(encodedRegionName) + " family " + e.getKey().toString()
            + " acquired edits out of order current memstore seq=" + currentId
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
      }
      for (Map.Entry<ImmutableByteArray, Long> e : unflushed.entrySet()) {
        if (e.getValue().longValue() <= maxFlushedSeqId) {
          e.setValue(wrappedSeqId);
        }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        flushOpSeqId = getNextSequenceId(wal);
        // Back up 1, minus 1 from oldest sequence id in memstore to get last 'flushed' edit
        flushedSeqId = earliestUnflushedSequenceIdForTheRegion.longValue() == HConstants.NO_SEQNUM
          ? flushOpSeqId
          : earliestUnflushedSequenceIdForTheRegion.longValue() - 1;
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        flushedSeqId = earliestUnflushedSequenceIdForTheRegion.longValue() == HConstants.NO_SEQNUM
          ? flushOpSeqId
          : earliestUnflushedSequenceIdForTheRegion.longValue() - 1;
      } else {
        // use the provided sequence Id as WAL is not being used for this flush.
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      // An entry in this map will have a boolean value indicating if it is currently
      // eligible for interrupt; if so, we should interrupt it.
      if (entry.getValue().booleanValue()) {
        entry.getKey().interrupt();
      }
```

### UnnecessaryUnboxing
Unnecessary unboxing
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public void update(byte[] region, byte[] row, Long result) {
        rowCountL.addAndGet(result.longValue());
      }
    }
```

## RuleId[id=ClassNameSameAsAncestorName]
### ClassNameSameAsAncestorName
Class name `Comparator` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java`
#### Snippet
```java
  /** A Comparator optimized for ImmutableBytesWritable. */
  @InterfaceAudience.Public
  public static class Comparator extends WritableComparator {
    private BytesWritable.Comparator comparator = new BytesWritable.Comparator();

```

### ClassNameSameAsAncestorName
Class name `Builder` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/PEMFileLoader.java`
#### Snippet
```java
  }

  static class Builder extends FileKeyStoreLoader.Builder<PEMFileLoader> {
    @Override
    PEMFileLoader build() {
```

### ClassNameSameAsAncestorName
Class name `Builder` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/JKSFileLoader.java`
#### Snippet
```java
  }

  static class Builder extends FileKeyStoreLoader.Builder<JKSFileLoader> {
    @Override
    JKSFileLoader build() {
```

### ClassNameSameAsAncestorName
Class name `Context` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java`
#### Snippet
```java
   */
  @InterfaceAudience.Public
  public static class Context extends org.apache.hadoop.hbase.io.crypto.Context {

    /** The null crypto context */
```

### ClassNameSameAsAncestorName
Class name `Builder` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/PKCS12FileLoader.java`
#### Snippet
```java
  }

  static class Builder extends FileKeyStoreLoader.Builder<PKCS12FileLoader> {
    @Override
    PKCS12FileLoader build() {
```

### ClassNameSameAsAncestorName
Class name `Builder` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/BCFKSFileLoader.java`
#### Snippet
```java
  }

  static class Builder extends FileKeyStoreLoader.Builder<BCFKSFileLoader> {
    @Override
    BCFKSFileLoader build() {
```

### ClassNameSameAsAncestorName
Class name `Iterator` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "URF_UNREAD_PUBLIC_OR_PROTECTED_FIELD",
      justification = "nextWasCalled is using by StripeStoreFileManager")
  public class Iterator implements java.util.Iterator<T> {
    protected int currentComponent = 0;
    protected int indexWithinComponent = -1;
```

### ClassNameSameAsAncestorName
Class name `ByteBuffKeyValueDecoder` is the same as one of its superclass' names
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/KeyValueCodecWithTags.java`
#### Snippet
```java
  }

  public static class ByteBuffKeyValueDecoder extends KeyValueCodec.ByteBuffKeyValueDecoder {

    public ByteBuffKeyValueDecoder(ByteBuff buf) {
```

### ClassNameSameAsAncestorName
Class name `Entry` is the same as one of its superclass' names
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/Record.java`
#### Snippet
```java
  private final ImmutableMap<Field, FieldValue> values;

  public final static class Entry extends AbstractMap.SimpleImmutableEntry<Field, FieldValue> {
    private Entry(Field key, FieldValue value) {
      super(key, value);
```

### ClassNameSameAsAncestorName
Class name `Servlet` is the same as one of its superclass' names
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static class Servlet extends HttpServlet {
    private static final long serialVersionUID = 1L;

```

### ClassNameSameAsAncestorName
Class name `Writer` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/FSHLogProvider.java`
#### Snippet
```java

  // Only public so classes back in regionserver.wal can access
  public interface Writer extends WALProvider.Writer {
    /**
     * @throws IOException                    if something goes wrong initializing an output stream
```

### ClassNameSameAsAncestorName
Class name `Reader` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java

  // Only public so classes back in regionserver.wal can access
  public interface Reader extends WAL.Reader {
    /**
     * @param fs   File system.
```

### ClassNameSameAsAncestorName
Class name `AsyncWriter` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AsyncFSWALProvider.java`
#### Snippet
```java

  // Only public so classes back in regionserver.wal can access
  public interface AsyncWriter extends WALProvider.AsyncWriter {
    /**
     * @throws IOException                    if something goes wrong initializing an output stream
```

### ClassNameSameAsAncestorName
Class name `ServiceCallFunction` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServicesVersionWrapper.java`
#### Snippet
```java

  @FunctionalInterface
  public interface ServiceCallFunction<Req, Resp>
    extends VersionInfoUtil.ServiceCallFunction<RpcController, Req, Resp, ServiceException> {
  }
```

### ClassNameSameAsAncestorName
Class name `Binder` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/gson/GsonSerializationFeature.java`
#### Snippet
```java
   * Register this feature's provided functionality and defines their lifetime scopes.
   */
  private static class Binder extends AbstractBinder {

    @Override
```

### ClassNameSameAsAncestorName
Class name `Binder` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/jersey/MasterFeature.java`
#### Snippet
```java
   * Register this feature's provided functionality and defines their lifetime scopes.
   */
  private class Binder extends AbstractBinder {

    @Override
```

### ClassNameSameAsAncestorName
Class name `RowTooBigException` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RowTooBigException.java`
#### Snippet
```java
@edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "NM_SAME_SIMPLE_NAME_AS_SUPERCLASS",
    justification = "Temporary glue. To be removed")
public class RowTooBigException extends org.apache.hadoop.hbase.client.RowTooBigException {
  public RowTooBigException(String message) {
    super(message);
```

### ClassNameSameAsAncestorName
Class name `Iterator` is the same as one of its superclass' names
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    }

    public class Iterator extends ConcatenatedLists<HStoreFile>.Iterator {
      public ArrayList<List<HStoreFile>> getComponents() {
        return components;
```

### ClassNameSameAsAncestorName
Class name `Args` is the same as one of its superclass' names
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java`
#### Snippet
```java
  private final CallQueue callQueue;

  public static class Args extends TThreadPoolServer.Args {
    int maxQueuedRequests;
    int threadKeepAliveTimeSec;
```

### ClassNameSameAsAncestorName
Class name `ThriftServer` is the same as one of its superclass' names
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.TOOLS)
@SuppressWarnings({ "rawtypes", "unchecked" })
public class ThriftServer extends org.apache.hadoop.hbase.thrift.ThriftServer {
  private static final Logger log = LoggerFactory.getLogger(ThriftServer.class);

```

## RuleId[id=RedundantMethodOverride]
### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedByteBufferKeyValue.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `equals()` only delegates to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedByteBufferKeyValue.java`
#### Snippet
```java

  @Override
  public boolean equals(Object other) {
    return super.equals(other);
  }
```

### RedundantMethodOverride
Method `getSerializedSize()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedByteBufferKeyValue.java`
#### Snippet
```java
   */
  @Override
  public int getSerializedSize() {
    return this.length;
  }
```

### RedundantMethodOverride
Method `getSimpleComparator()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java

  @Override
  public Comparator getSimpleComparator() {
    return this;
  }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedNoTagsByteBufferKeyValue.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `equals()` only delegates to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedNoTagsByteBufferKeyValue.java`
#### Snippet
```java

  @Override
  public boolean equals(Object other) {
    return super.equals(other);
  }
```

### RedundantMethodOverride
Method `getSerializedSize()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedKeyValue.java`
#### Snippet
```java
   */
  @Override
  public int getSerializedSize() {
    return this.length;
  }
```

### RedundantMethodOverride
Method `getOrder()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union3.java`
#### Snippet
```java

  @Override
  public Order getOrder() {
    return null;
  }
```

### RedundantMethodOverride
Method `getOrder()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union4.java`
#### Snippet
```java

  @Override
  public Order getOrder() {
    return null;
  }
```

### RedundantMethodOverride
Method `init()` only delegates to its super method
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterProcSourceImpl.java`
#### Snippet
```java

  @Override
  public void init() {
    super.init();
  }
```

### RedundantMethodOverride
Method `getFamilyArray()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

    @Override
    public byte[] getFamilyArray() {
      return bytes;
    }
```

### RedundantMethodOverride
Method `getRowArray()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

    @Override
    public byte[] getRowArray() {
      return bytes;
    }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

    @Override
    public int hashCode() {
      return super.hashCode();
    }
```

### RedundantMethodOverride
Method `equals()` only delegates to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

    @Override
    public boolean equals(Object other) {
      return super.equals(other);
    }
```

### RedundantMethodOverride
Method `getQualifierArray()` is identical to its super method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

    @Override
    public byte[] getQualifierArray() {
      return bytes;
    }
```

### RedundantMethodOverride
Method `getCount()` is identical to its super method
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableRangeHistogram.java`
#### Snippet
```java

  @Override
  public long getCount() {
    return histogram.getCount();
  }
```

### RedundantMethodOverride
Method `canRetry()` is identical to its super method
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeSaslClientAuthenticationProvider.java`
#### Snippet
```java

  @Override
  public boolean canRetry() {
    // A static username/password either works or it doesn't. No kind of relogin/retry necessary.
    return false;
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ExampleRegionObserverWithMetrics.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment e) throws IOException {
    // we should NOT remove / deregister the metrics in stop(). The whole registry will be
    // removed when the last region of the table is closed.
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RefreshHFilesEndpoint.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
  }
}
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
    // nothing to do
  }
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
    // nothing to do
  }
```

### RedundantMethodOverride
Method `getFilter()` is identical to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  /** Returns RowFilter */
  @Override
  public Filter getFilter() {
    return filter;
  }
```

### RedundantMethodOverride
Method `acceptInboundMessage()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClusterStatusListener.java`
#### Snippet
```java

      @Override
      public boolean acceptInboundMessage(Object msg) throws Exception {
        return super.acceptInboundMessage(msg);
      }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `filterCell()` is identical to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java`
#### Snippet
```java

  @Override
  public ReturnCode filterCell(final Cell ignored) throws IOException {
    return ReturnCode.INCLUDE;
  }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `filterCell()` is identical to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java`
#### Snippet
```java

  @Override
  public ReturnCode filterCell(final Cell ignored) throws IOException {
    return ReturnCode.INCLUDE;
  }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `hasFilterRow()` is identical to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java
  // We cleaned result row in FilterRow to be consistent with scanning process.
  @Override
  public boolean hasFilterRow() {
    return true;
  }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/GlobalPermission.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `getRecordReader()` is identical to its super method
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java`
#### Snippet
```java

  @Override
  public RecordReader<ImmutableBytesWritable, Result> getRecordReader(InputSplit split, JobConf job,
    Reporter reporter) throws IOException {
    return new TableSnapshotRecordReader((TableSnapshotRegionSplit) split, job);
```

### RedundantMethodOverride
Method `cleanup()` only delegates to its super method
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    @Override
    protected void
      cleanup(Mapper<WALKey, WALEdit, ImmutableBytesWritable, Mutation>.Context context)
        throws IOException, InterruptedException {
      super.cleanup(context);
```

### RedundantMethodOverride
Method `execute()` only delegates to its super method
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java

    @Override
    public void execute() throws IOException {
      super.execute();
    }
```

### RedundantMethodOverride
Method `isSharedMem()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/SharedMemHFileBlock.java`
#### Snippet
```java

  @Override
  public boolean isSharedMem() {
    return true;
  }
```

### RedundantMethodOverride
Method `getServerName()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  @Override
  public ServerName getServerName() {
    return this.serverName;
  }
```

### RedundantMethodOverride
Method `getCurrentState()` only delegates to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SwitchRpcThrottleProcedure.java`
#### Snippet
```java

  @Override
  protected SwitchRpcThrottleState getCurrentState() {
    return super.getCurrentState();
  }
```

### RedundantMethodOverride
Method `waitInitialized()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure.java`
#### Snippet
```java

  @Override
  protected boolean waitInitialized(MasterProcedureEnv env) {
    return env.waitInitialized(this);
  }
```

### RedundantMethodOverride
Method `holdLock()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotRegionProcedure.java`
#### Snippet
```java

  @Override
  protected boolean holdLock(MasterProcedureEnv env) {
    return false;
  }
```

### RedundantMethodOverride
Method `disable()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/DisableTableViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void disable() throws IOException {
    // do nothing
  }
```

### RedundantMethodOverride
Method `enable()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/DisableTableViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void enable() throws IOException {
    // do nothing
  }
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenProvider.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
  }

```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplication.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
  }

```

### RedundantMethodOverride
Method `nodeDeleted()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ZKVisibilityLabelWatcher.java`
#### Snippet
```java

  @Override
  public void nodeDeleted(String path) {
    // There is no case of visibility labels path to get deleted.
  }
```

### RedundantMethodOverride
Method `nodeChildrenChanged()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ZKVisibilityLabelWatcher.java`
#### Snippet
```java

  @Override
  public void nodeChildrenChanged(String path) {
    // We are not dealing with child nodes under the label znode or userauths znode.
  }
```

### RedundantMethodOverride
Method `getWatcher()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java`
#### Snippet
```java

  @Override
  public ZKWatcher getWatcher() {
    return watcher;
  }
```

### RedundantMethodOverride
Method `postSetSplitOrMergeEnabled()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void postSetSplitOrMergeEnabled(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final boolean newValue, final MasterSwitchType switchType) throws IOException {
  }
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {

  }
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
    // nothing to do
  }
```

### RedundantMethodOverride
Method `isStopping()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java

    @Override
    public boolean isStopping() {
      return false;
    }
```

### RedundantMethodOverride
Method `run()` only delegates to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java`
#### Snippet
```java

    @Override
    public void run() {
      super.run();
    }
```

### RedundantMethodOverride
Method `hashCode()` only delegates to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobReferenceOnlyFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### RedundantMethodOverride
Method `getFlattenAction()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AdaptiveMemStoreCompactionStrategy.java`
#### Snippet
```java

  @Override
  protected Action getFlattenAction() {
    return Action.FLATTEN;
  }
```

### RedundantMethodOverride
Method `postEndpointInvocation()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postEndpointInvocation(ObserverContext<RegionCoprocessorEnvironment> ctx,
    Service service, String methodName, Message request, Message.Builder responseBuilder)
    throws IOException {
```

### RedundantMethodOverride
Method `postCreateReplicationEndPoint()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public ReplicationEndpoint postCreateReplicationEndPoint(
    ObserverContext<RegionServerCoprocessorEnvironment> ctx, ReplicationEndpoint endpoint) {
    return endpoint;
```

### RedundantMethodOverride
Method `postRollWALWriterRequest()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postRollWALWriterRequest(ObserverContext<RegionServerCoprocessorEnvironment> ctx)
    throws IOException {
  }
```

### RedundantMethodOverride
Method `postAbortProcedure()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postAbortProcedure(ObserverContext<MasterCoprocessorEnvironment> ctx)
    throws IOException {
    // There is nothing to do at this time after the procedure abort request was sent.
```

### RedundantMethodOverride
Method `postListNamespaces()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postListNamespaces(ObserverContext<MasterCoprocessorEnvironment> ctx,
    List<String> namespaces) throws IOException {
    /* always allow namespace listing */
```

### RedundantMethodOverride
Method `getScannerOrder()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SnapshotSegmentScanner.java`
#### Snippet
```java
   */
  @Override
  public long getScannerOrder() {
    return 0;
  }
```

### RedundantMethodOverride
Method `getTimeRangeTracker()` is identical to its super method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java`
#### Snippet
```java

  @Override
  public TimeRangeTracker getTimeRangeTracker() {
    return this.timeRangeTracker;
  }
```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
  }

```

### RedundantMethodOverride
Method `stop()` is identical to its super method
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java

  @Override
  public void stop(CoprocessorEnvironment env) throws IOException {
    // nothing to do
  }
```

### RedundantMethodOverride
Method `getConfiguration()` only delegates to its super method
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
   */
  @Override
  public Configuration getConfiguration() {
    return super.getConfiguration();
  }
```

## RuleId[id=OptionalAssignedToNull]
### OptionalAssignedToNull
Optional value is compared with null
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshot.java`
#### Snippet
```java
    @Override
    public int hashCode() {
      return new HashCodeBuilder().append(policy == null ? 0 : policy.hashCode())
        .append(inViolation).toHashCode();
    }
```

### OptionalAssignedToNull
Optional value is compared with null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
                HFile.createReader(fs, storeFile.getPath(), CacheConfig.DISABLED, true, getConf());
              if (
                (reader.getFirstKey() != null)
                  && ((storeFirstKey == null) || (comparator.compare(storeFirstKey,
                    ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey().get()).getKey()) > 0))
```

### OptionalAssignedToNull
Optional value is compared with null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
              }
              if (
                (reader.getLastKey() != null)
                  && ((storeLastKey == null) || (comparator.compare(storeLastKey,
                    ((KeyValue.KeyOnlyKeyValue) reader.getLastKey().get()).getKey())) < 0)
```

## RuleId[id=IntegerMultiplicationImplicitCastToLong]
### IntegerMultiplicationImplicitCastToLong
ClassSize.REFERENCE \* 2: integer multiplication implicitly cast to long
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      + Bytes.SIZEOF_BYTE // flength
      + Bytes.SIZEOF_INT * 3 // foffset, qoffset, qlength
      + ClassSize.REFERENCE * 2; // fArray, qArray
    // @formatter:on
    private final byte[] fArray;
```

### IntegerMultiplicationImplicitCastToLong
ClassSize.REFERENCE \* 2: integer multiplication implicitly cast to long
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    // @formatter:off
    private static final long FIXED_OVERHEAD = (long) LastOnRowCell.FIXED_OVERHEAD
      + ClassSize.REFERENCE * 2 // fArray and qArray
      + Bytes.SIZEOF_INT * 3 // foffset, qoffset, qlength
      + Bytes.SIZEOF_BYTE; // flength
```

### IntegerMultiplicationImplicitCastToLong
3 \* ClassSize.BYTE_BUFFER: integer multiplication implicitly cast to long
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
      (long) ClassSize.align(ClassSize.OBJECT + (3 * ClassSize.REFERENCE) + (2 * Bytes.SIZEOF_LONG)
        + (7 * Bytes.SIZEOF_INT) + Bytes.SIZEOF_SHORT) + (2 * Bytes.SIZEOF_BYTE)
        + (3 * ClassSize.BYTE_BUFFER);
    private ByteBuffer keyBuffer;
    private short rowLength;
```

### IntegerMultiplicationImplicitCastToLong
len \* ARRAY_BYTE_INDEX_SCALE: integer multiplication implicitly cast to long
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
    @Override
    long sizeOfByteArray(int len) {
      return align(ARRAY + len * ARRAY_BYTE_INDEX_SCALE);
    }
  }
```

### IntegerMultiplicationImplicitCastToLong
coeff\[2\] \* REFERENCE: integer multiplication implicitly cast to long
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
   */
  private static long estimateBaseFromCoefficients(int[] coeff, boolean debug) {
    long prealign_size = (long) OBJECT + coeff[0] + coeff[2] * REFERENCE;

    // Round up to a multiple of 8
```

### IntegerMultiplicationImplicitCastToLong
3 \* oopSize(): integer multiplication implicitly cast to long
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java

    int arrayHeaderSize() {
      return (int) align(3 * oopSize());
    }

```

### IntegerMultiplicationImplicitCastToLong
modified.length \<\< ADDRESS_BITS_PER_WORD: integer shift implicitly cast to long
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
   */
  public long getEnd() {
    return start + (modified.length << ADDRESS_BITS_PER_WORD) - 1;
  }

```

### IntegerMultiplicationImplicitCastToLong
wordIndex \<\< ADDRESS_BITS_PER_WORD: integer shift implicitly cast to long
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
        continue;
      }
      long baseProcId = getStart() + (wordIndex << ADDRESS_BITS_PER_WORD);
      for (int i = 0; i < (1 << ADDRESS_BITS_PER_WORD); i++) {
        long mask = 1L << i;
```

### IntegerMultiplicationImplicitCastToLong
waitBeforeRoll \* i: integer multiplication implicitly cast to long
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
    for (int i = 0; i < rollRetries && isRunning(); ++i) {
      if (i > 0) {
        Threads.sleepWithoutInterrupt(waitBeforeRoll * i);
      }

```

### IntegerMultiplicationImplicitCastToLong
(count - 1) \* (count - 1): integer multiplication implicitly cast to long
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionReplicaGroupingCostFunction.java`
#### Snippet
```java
    colocatedReplicaCounts.forEach((primary, count) -> {
      if (count > 1) { // means consecutive primaries, indicating co-location
        cost.getAndAdd((count - 1) * (count - 1));
      }
    });
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.REFERENCE: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      }
      // Add comparator and the midkey atomicreference
      heapSize += 2 * ClassSize.REFERENCE;
      return heapSize;
    }
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.REFERENCE: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
      }
      // Add comparator and the midkey atomicreference
      heapSize += 2 * ClassSize.REFERENCE;
      // Add rootCount and searchTreeLevel
      heapSize += 2 * Bytes.SIZEOF_INT;
```

### IntegerMultiplicationImplicitCastToLong
2 \* hfileName.length(): integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheKey.java`
#### Snippet
```java
  @Override
  public long heapSize() {
    return ClassSize.align(FIXED_OVERHEAD + ClassSize.STRING + 2 * hfileName.length());
  }

```

### IntegerMultiplicationImplicitCastToLong
itemCount \* itemAllocationSize: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
      offset -= baseOffset;
      assert offset >= 0;
      assert offset < itemCount * itemAllocationSize;
      assert offset % itemAllocationSize == 0;
      assert usedCount > 0;
```

### IntegerMultiplicationImplicitCastToLong
freeList\[--freeCount\] \* itemAllocationSize: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
      assert sizeIndex != -1;
      ++usedCount;
      long offset = baseOffset + (freeList[--freeCount] * itemAllocationSize);
      assert offset >= 0;
      return offset;
```

### IntegerMultiplicationImplicitCastToLong
2 \* codelTargetDelay: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/AdaptiveLifoCoDelCallQueue.java`
#### Snippet
```java
    }

    return isOverloaded.get() && callDelay > 2 * codelTargetDelay;
  }

```

### IntegerMultiplicationImplicitCastToLong
i \* hash2: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterChunk.java`
#### Snippet
```java
  private void setHashLoc(int hash1, int hash2) {
    for (int i = 0; i < this.hashCount; i++) {
      long hashLoc = Math.abs((hash1 + i * hash2) % (this.byteSize * 8));
      set(hashLoc);
    }
```

### IntegerMultiplicationImplicitCastToLong
maxWaitInSeconds \* 1000: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java`
#### Snippet
```java
      count = count + 1;
      admin.move(region.getEncodedNameAsBytes(), targetServer);
      long maxWait = startTime + (maxWaitInSeconds * 1000);
      while (EnvironmentEdgeManager.currentTime() < maxWait) {
        sameServer = isSameServer(region, sourceServer);
```

### IntegerMultiplicationImplicitCastToLong
maxWaitInSeconds \* 1000: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
    int maxWaitInSeconds =
      admin.getConfiguration().getInt(SERVERSTART_WAIT_MAX_KEY, DEFAULT_SERVERSTART_WAIT_MAX);
    long maxWait = EnvironmentEdgeManager.currentTime() + maxWaitInSeconds * 1000;
    while (EnvironmentEdgeManager.currentTime() < maxWait) {
      try {
```

### IntegerMultiplicationImplicitCastToLong
messagePeriod \* 2: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
  protected List<ServerName> generateDeadServersListToSend() {
    // We're getting the message sent since last time, and add them to the list
    long since = EnvironmentEdgeManager.currentTime() - messagePeriod * 2;
    for (Pair<ServerName, Long> dead : getDeadServers(since)) {
      lastSent.putIfAbsent(dead.getFirst(), 0);
```

### IntegerMultiplicationImplicitCastToLong
16 \* clusterIds.size(): integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKeyImpl.java`
#### Snippet
```java
    size += tablename != null ? tablename.toBytes().length : 0;
    if (clusterIds != null) {
      size += 16 * clusterIds.size();
    }
    if (nonceGroup != HConstants.NO_NONCE) {
```

### IntegerMultiplicationImplicitCastToLong
rpCount \* balanceInterval: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

        if (this.maxBalancingTime > 0) {
          balanceThrottling(balanceStartTime + rpCount * balanceInterval, maxRegionsInTransition,
            cutoffTime);
        }
```

### IntegerMultiplicationImplicitCastToLong
this.numberOfAttemptsSoFar \* this.numberOfAttemptsSoFar: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
      // up to max of 10 seconds (don't want to back off too much in case of situation change).
      submitTask(this,
        Math.min(rsRpcRetryInterval * (this.numberOfAttemptsSoFar * this.numberOfAttemptsSoFar),
          10 * 1000),
        TimeUnit.MILLISECONDS);
```

### IntegerMultiplicationImplicitCastToLong
ritThreshold \* 2: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
          }
          ritsOverThreshold.put(state.getRegion().getEncodedName(), state);
          totalRITsTwiceThreshold += (ritTime > (ritThreshold * 2)) ? 1 : 0;
        }
        if (oldestRITTime < ritTime) {
```

### IntegerMultiplicationImplicitCastToLong
ritThreshold \* 2: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
        return false;
      }
      return (statTimestamp - state.getStamp()) > (ritThreshold * 2);
    }

```

### IntegerMultiplicationImplicitCastToLong
maxRetriesMultiplier \* 1000: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
    if (numSinks == 0) {
      if (
        (EnvironmentEdgeManager.currentTime() - lastSinkFetchTime) >= (maxRetriesMultiplier * 1000)
      ) {
        LOG.warn("No replication sinks found, returning without replicating. "
```

### IntegerMultiplicationImplicitCastToLong
5 \* ClassSize.REFERENCE: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java
  // @formatter:off
  public final static long FIXED_OVERHEAD = (long) ClassSize.OBJECT
    + (5 * ClassSize.REFERENCE)
    + (2 * Bytes.SIZEOF_LONG); // snapshotId, timeOfOldestEdit
  // @formatter:on
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.ATOMIC_LONG: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java`
#### Snippet
```java
      + Bytes.SIZEOF_BOOLEAN); // tagsPresent
  public final static long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.ATOMIC_REFERENCE
    + ClassSize.CELL_SET + 2 * ClassSize.ATOMIC_LONG + ClassSize.REENTRANT_LOCK;

  private AtomicReference<CellSet> cellSet = new AtomicReference<>();
```

### IntegerMultiplicationImplicitCastToLong
6 \* ClassSize.REFERENCE: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java

  public static final long DEEP_OVERHEAD =
    ClassSize.align(AbstractMemStore.DEEP_OVERHEAD + 6 * ClassSize.REFERENCE // Store,
                                                                             // CompactionPipeline,
    // MemStoreCompactor, inMemoryCompactionInProgress,
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.ATOMIC_BOOLEAN: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java
      + Bytes.SIZEOF_LONG // inmemoryFlushSize
      + 2 * Bytes.SIZEOF_BOOLEAN // compositeSnapshot and inWalReplay
      + 2 * ClassSize.ATOMIC_BOOLEAN// inMemoryCompactionInProgress and allowCompaction
      + CompactionPipeline.DEEP_OVERHEAD + MemStoreCompactor.DEEP_OVERHEAD);

```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.LINKEDLIST: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java`
#### Snippet
```java
  public final static long FIXED_OVERHEAD =
    ClassSize.align(ClassSize.OBJECT + (3 * ClassSize.REFERENCE) + Bytes.SIZEOF_LONG);
  public final static long DEEP_OVERHEAD = FIXED_OVERHEAD + (2 * ClassSize.LINKEDLIST);

  private final RegionServicesForStores region;
```

### IntegerMultiplicationImplicitCastToLong
2 \* slowSyncCheckInterval: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    if (elapsedTime >= slowSyncCheckInterval) {
      if (slowSyncCount.get() >= slowSyncRollThreshold) {
        if (elapsedTime >= (2 * slowSyncCheckInterval)) {
          // If two or more slowSyncCheckInterval have elapsed this is a corner case
          // where a train of slow syncs almost triggered us but then there was a long
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.ATOMIC_BOOLEAN: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  // 1 x ReadPointCalculationLock - smallestReadPointCalcLock
  public static final long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.OBJECT + // closeLock
    (2 * ClassSize.ATOMIC_BOOLEAN) + // closed, closing
    (3 * ClassSize.ATOMIC_LONG) + // numPutsWithoutWAL, dataInMemoryWithoutWAL,
                                  // compactionsFailed
```

### IntegerMultiplicationImplicitCastToLong
3 \* ClassSize.ATOMIC_LONG: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  public static final long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.OBJECT + // closeLock
    (2 * ClassSize.ATOMIC_BOOLEAN) + // closed, closing
    (3 * ClassSize.ATOMIC_LONG) + // numPutsWithoutWAL, dataInMemoryWithoutWAL,
                                  // compactionsFailed
    (3 * ClassSize.CONCURRENT_HASHMAP) + // lockedRows, scannerReadPoints, regionLockHolders
```

### IntegerMultiplicationImplicitCastToLong
3 \* ClassSize.CONCURRENT_HASHMAP: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    (3 * ClassSize.ATOMIC_LONG) + // numPutsWithoutWAL, dataInMemoryWithoutWAL,
                                  // compactionsFailed
    (3 * ClassSize.CONCURRENT_HASHMAP) + // lockedRows, scannerReadPoints, regionLockHolders
    WriteState.HEAP_SIZE + // writestate
    ClassSize.CONCURRENT_SKIPLISTMAP + ClassSize.CONCURRENT_SKIPLISTMAP_ENTRY + // stores
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.REENTRANT_LOCK: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    WriteState.HEAP_SIZE + // writestate
    ClassSize.CONCURRENT_SKIPLISTMAP + ClassSize.CONCURRENT_SKIPLISTMAP_ENTRY + // stores
    (2 * ClassSize.REENTRANT_LOCK) + // lock, updatesLock
    MultiVersionConcurrencyControl.FIXED_SIZE // mvcc
    + 2 * ClassSize.TREEMAP // maxSeqIdInStores, replicationScopes
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.TREEMAP: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    (2 * ClassSize.REENTRANT_LOCK) + // lock, updatesLock
    MultiVersionConcurrencyControl.FIXED_SIZE // mvcc
    + 2 * ClassSize.TREEMAP // maxSeqIdInStores, replicationScopes
    + 2 * ClassSize.ATOMIC_INTEGER // majorInProgress, minorInProgress
    + ClassSize.STORE_SERVICES // store services
```

### IntegerMultiplicationImplicitCastToLong
2 \* ClassSize.ATOMIC_INTEGER: integer multiplication implicitly cast to long
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    MultiVersionConcurrencyControl.FIXED_SIZE // mvcc
    + 2 * ClassSize.TREEMAP // maxSeqIdInStores, replicationScopes
    + 2 * ClassSize.ATOMIC_INTEGER // majorInProgress, minorInProgress
    + ClassSize.STORE_SERVICES // store services
    + StoreHotnessProtector.FIXED_SIZE;
```

## RuleId[id=UNUSED_IMPORT]
### UNUSED_IMPORT
Unused import `import org.apache.hbase.thirdparty.org.apache.commons.cli.Options;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
import org.apache.hbase.thirdparty.org.apache.commons.cli.DefaultParser;
import org.apache.hbase.thirdparty.org.apache.commons.cli.Option;
import org.apache.hbase.thirdparty.org.apache.commons.cli.Options;
import org.apache.hbase.thirdparty.org.apache.commons.cli.ParseException;

```

### UNUSED_IMPORT
Unused import `import org.apache.thrift.protocol.TProtocolException;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
```

### UNUSED_IMPORT
Unused import `import org.apache.thrift.EncodingUtils;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import org.apache.thrift.async.AsyncMethodCallback;
```

### UNUSED_IMPORT
Unused import `import org.apache.thrift.TException;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import org.apache.thrift.async.AsyncMethodCallback;
import org.apache.thrift.server.AbstractNonblockingServer.*;
```

### UNUSED_IMPORT
Unused import `import org.apache.thrift.async.AsyncMethodCallback;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import org.apache.thrift.async.AsyncMethodCallback;
import org.apache.thrift.server.AbstractNonblockingServer.*;
import java.util.List;
```

### UNUSED_IMPORT
Unused import `import org.apache.thrift.server.AbstractNonblockingServer.*;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import org.apache.thrift.TException;
import org.apache.thrift.async.AsyncMethodCallback;
import org.apache.thrift.server.AbstractNonblockingServer.*;
import java.util.List;
import java.util.ArrayList;
```

### UNUSED_IMPORT
Unused import `import java.util.Set;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
```

### UNUSED_IMPORT
Unused import `import java.util.HashSet;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
```

### UNUSED_IMPORT
Unused import `import java.nio.ByteBuffer;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import javax.annotation.Generated;
```

### UNUSED_IMPORT
Unused import `import java.util.Arrays;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import javax.annotation.Generated;
import org.slf4j.Logger;
```

### UNUSED_IMPORT
Unused import `import org.slf4j.Logger;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import java.util.Arrays;
import javax.annotation.Generated;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

```

### UNUSED_IMPORT
Unused import `import org.slf4j.LoggerFactory;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/NamespaceDescriptor.java`
#### Snippet
```java
import javax.annotation.Generated;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
```

## RuleId[id=ListRemoveInLoop]
### ListRemoveInLoop
Can be replaced with 'List.subList().clear()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALUtil.java`
#### Snippet
```java
      }
    }
    for (int i = size - 1; i >= newSize; i--) {
      cells.remove(i);
    }
```

## RuleId[id=ReplaceAssignmentWithOperatorAssignment]
### ReplaceAssignmentWithOperatorAssignment
`ts = ts - 1` could be simplified to 'ts -= 1'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      type = KeyValue.Type.values()[KeyValue.Type.codeToType(type).ordinal() - 1].getCode();
    } else if (ts != PrivateConstants.OLDEST_TIMESTAMP) {
      ts = ts - 1;
      type = KeyValue.Type.Maximum.getCode();
    } else {
```

### ReplaceAssignmentWithOperatorAssignment
`current.timestamp = current.timestamp - timestampOrDiff` could be simplified to 'current.timestamp -= timestampOrDiff'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java`
#### Snippet
```java
        current.timestamp = timestampOrDiff;
      } else { // it is diff
        current.timestamp = current.timestamp - timestampOrDiff;
      }
      Bytes.putLong(current.keyBuffer, pos, current.timestamp);
```

### ReplaceAssignmentWithOperatorAssignment
`n = (short) (n << 8)` could be simplified to 'n \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    short n = 0;
    n = (short) (n ^ (ByteBufferUtils.toByte(item, offsetInItem) & 0xFF));
    n = (short) (n << 8);
    n = (short) (n ^ (ByteBufferUtils.toByte(nextItem, 0) & 0xFF));
    return n;
```

### ReplaceAssignmentWithOperatorAssignment
`offset = offset - this.itemBeginPos[itemIndex]` could be simplified to 'offset -= this.itemBeginPos\[itemIndex\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    int itemIndex = getItemIndex(offset);
    ByteBuffer item = this.items[itemIndex];
    offset = offset - this.itemBeginPos[itemIndex];
    if (item.limit() - offset >= length) {
      pair.setFirst(item);
```

### ReplaceAssignmentWithOperatorAssignment
`sourceOffset = sourceOffset - this.itemBeginPos[itemIndex]` could be simplified to 'sourceOffset -= this.itemBeginPos\[itemIndex\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    int itemIndex = getItemIndex(sourceOffset);
    ByteBuffer in = this.items[itemIndex];
    sourceOffset = sourceOffset - this.itemBeginPos[itemIndex];
    while (length > 0) {
      int toRead = Math.min(in.limit() - sourceOffset, length);
```

### ReplaceAssignmentWithOperatorAssignment
`sourceOffset = sourceOffset - this.itemBeginPos[itemIndex]` could be simplified to 'sourceOffset -= this.itemBeginPos\[itemIndex\]'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    int itemIndex = getItemIndex(sourceOffset);
    ByteBuffer item = this.items[itemIndex];
    sourceOffset = sourceOffset - this.itemBeginPos[itemIndex];
    while (length > 0) {
      int toRead = Math.min((item.limit() - sourceOffset), length);
```

### ReplaceAssignmentWithOperatorAssignment
`n = (short) (n << 8)` could be simplified to 'n \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    short n = 0;
    n = (short) (n ^ (get() & 0xFF));
    n = (short) (n << 8);
    n = (short) (n ^ (get() & 0xFF));
    return n;
```

### ReplaceAssignmentWithOperatorAssignment
`curItemIndex = (curItemIndex - 1)` could be simplified to 'curItemIndex -= 1'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        // reset
        curItem = buffer;
        curItemIndex = (curItemIndex - 1);
        break;
      }
```

### ReplaceAssignmentWithOperatorAssignment
`l = (short) (l << 8)` could be simplified to 'l \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    short l = 0;
    for (int i = offsetInItem; i < item.capacity(); i++) {
      l = (short) (l << 8);
      l = (short) (l ^ (ByteBufferUtils.toByte(item, i) & 0xFF));
    }
```

### ReplaceAssignmentWithOperatorAssignment
`l = (short) (l << 8)` could be simplified to 'l \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    }
    for (int i = 0; i < Bytes.SIZEOF_SHORT - remainingLen; i++) {
      l = (short) (l << 8);
      l = (short) (l ^ (ByteBufferUtils.toByte(nextItem, i) & 0xFF));
    }
```

### ReplaceAssignmentWithOperatorAssignment
`a[offset + dst.getPosition() - 1] = (byte) (a[offset + dst.getPosition() - 1] & 0xfe)` could be simplified to 'a\[offset + dst.getPosition() - 1\] \&= 0xfe'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    encodeToCentimal(dst, abs);
    // terminal digit should be 2x
    a[offset + dst.getPosition() - 1] = (byte) (a[offset + dst.getPosition() - 1] & 0xfe);
    if (isNeg) {
      // negative values encoded as ~M
```

### ReplaceAssignmentWithOperatorAssignment
`a[offset + dst.getPosition() - 1] = (byte) (a[offset + dst.getPosition() - 1] & 0xfe)` could be simplified to 'a\[offset + dst.getPosition() - 1\] \&= 0xfe'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    encodeToCentimal(dst, abs);
    // terminal digit should be 2x
    a[offset + dst.getPosition() - 1] = (byte) (a[offset + dst.getPosition() - 1] & 0xfe);
    if (isNeg) {
      // negative values encoded as ~M
```

### ReplaceAssignmentWithOperatorAssignment
`dst.getBytes()[offset + dst.getPosition() - 1] = (byte) (dst.getBytes()[offset + dst.getPo...` could be simplified to 'dst.getBytes()\[offset + dst.getPosition() - 1\] \&= 0x7f'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        dst.put((byte) (0x7f & t));
      } else {
        dst.getBytes()[offset + dst.getPosition() - 1] =
          (byte) (dst.getBytes()[offset + dst.getPosition() - 1] & 0x7f);
      }
    }
```

### ReplaceAssignmentWithOperatorAssignment
`k = k << 8` could be simplified to 'k \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
      int i_4 = (i << 2);
      int k = hashKey.get(i_4 + 3);
      k = k << 8;
      k = k | (hashKey.get(i_4 + 2) & 0xff);
      k = k << 8;
```

### ReplaceAssignmentWithOperatorAssignment
`k = k | (hashKey.get(i_4 + 2) & 0xff)` could be simplified to 'k \|= (hashKey.get(i_4 + 2) \& 0xff)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
      int k = hashKey.get(i_4 + 3);
      k = k << 8;
      k = k | (hashKey.get(i_4 + 2) & 0xff);
      k = k << 8;
      k = k | (hashKey.get(i_4 + 1) & 0xff);
```

### ReplaceAssignmentWithOperatorAssignment
`k = k << 8` could be simplified to 'k \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
      k = k << 8;
      k = k | (hashKey.get(i_4 + 2) & 0xff);
      k = k << 8;
      k = k | (hashKey.get(i_4 + 1) & 0xff);
      k = k << 8;
```

### ReplaceAssignmentWithOperatorAssignment
`k = k | (hashKey.get(i_4 + 1) & 0xff)` could be simplified to 'k \|= (hashKey.get(i_4 + 1) \& 0xff)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
      k = k | (hashKey.get(i_4 + 2) & 0xff);
      k = k << 8;
      k = k | (hashKey.get(i_4 + 1) & 0xff);
      k = k << 8;
      // noinspection PointlessArithmeticExpression
```

### ReplaceAssignmentWithOperatorAssignment
`k = k << 8` could be simplified to 'k \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
      k = k << 8;
      k = k | (hashKey.get(i_4 + 1) & 0xff);
      k = k << 8;
      // noinspection PointlessArithmeticExpression
      k = k | (hashKey.get(i_4 + 0) & 0xff);
```

### ReplaceAssignmentWithOperatorAssignment
`k = k | (hashKey.get(i_4 + 0) & 0xff)` could be simplified to 'k \|= (hashKey.get(i_4 + 0) \& 0xff)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
      k = k << 8;
      // noinspection PointlessArithmeticExpression
      k = k | (hashKey.get(i_4 + 0) & 0xff);
      k *= m;
      k ^= k >>> r;
```

### ReplaceAssignmentWithOperatorAssignment
`srcAddress = srcAddress + directBufferAddress(src)` could be simplified to 'srcAddress += directBufferAddress(src)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
    Object srcBase = null;
    if (src.isDirect()) {
      srcAddress = srcAddress + directBufferAddress(src);
    } else {
      srcAddress = srcAddress + BYTE_ARRAY_BASE_OFFSET + src.arrayOffset();
```

### ReplaceAssignmentWithOperatorAssignment
`srcAddress = srcAddress + BYTE_ARRAY_BASE_OFFSET + src.arrayOffset()` could be simplified to 'srcAddress += BYTE_ARRAY_BASE_OFFSET + src.arrayOffset()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
      srcAddress = srcAddress + directBufferAddress(src);
    } else {
      srcAddress = srcAddress + BYTE_ARRAY_BASE_OFFSET + src.arrayOffset();
      srcBase = src.array();
    }
```

### ReplaceAssignmentWithOperatorAssignment
`destAddress = destAddress + directBufferAddress(dest)` could be simplified to 'destAddress += directBufferAddress(dest)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
    Object destBase = null;
    if (dest.isDirect()) {
      destAddress = destAddress + directBufferAddress(dest);
    } else {
      destAddress = destAddress + BYTE_ARRAY_BASE_OFFSET + dest.arrayOffset();
```

### ReplaceAssignmentWithOperatorAssignment
`destAddress = destAddress + BYTE_ARRAY_BASE_OFFSET + dest.arrayOffset()` could be simplified to 'destAddress += BYTE_ARRAY_BASE_OFFSET + dest.arrayOffset()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
      destAddress = destAddress + directBufferAddress(dest);
    } else {
      destAddress = destAddress + BYTE_ARRAY_BASE_OFFSET + dest.arrayOffset();
      destBase = dest.array();
    }
```

### ReplaceAssignmentWithOperatorAssignment
`n = (short) (n << 8)` could be simplified to 'n \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractByteRange.java`
#### Snippet
```java
    short n = 0;
    n = (short) ((n ^ bytes[offset]) & 0xFF);
    n = (short) (n << 8);
    n = (short) ((n ^ bytes[offset + 1]) & 0xFF);
    return n;
```

### ReplaceAssignmentWithOperatorAssignment
`tmp = tmp >> 8` could be simplified to 'tmp \>\>= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
    long tmp = i;
    while (tmp != 0) {
      tmp = tmp >> 8;
      len--;
    }
```

### ReplaceAssignmentWithOperatorAssignment
`i = i << 8` could be simplified to 'i \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
    for (int idx = 0; idx < len - 1; idx++) {
      byte b = visitor.get();
      i = i << 8;
      i = i | (b & 0xFF);
    }
```

### ReplaceAssignmentWithOperatorAssignment
`i = i | (b & 0xFF)` could be simplified to 'i \|= (b \& 0xFF)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
      byte b = visitor.get();
      i = i << 8;
      i = i | (b & 0xFF);
    }
    return (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);
```

### ReplaceAssignmentWithOperatorAssignment
`i = i << 8` could be simplified to 'i \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    for (int idx = 0; idx < len - 1; idx++) {
      byte b = buffer[offset + 1 + idx];
      i = i << 8;
      i = i | (b & 0xFF);
    }
```

### ReplaceAssignmentWithOperatorAssignment
`i = i | (b & 0xFF)` could be simplified to 'i \|= (b \& 0xFF)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
      byte b = buffer[offset + 1 + idx];
      i = i << 8;
      i = i | (b & 0xFF);
    }
    return (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
```

### ReplaceAssignmentWithOperatorAssignment
`tmp = tmp >> 8` could be simplified to 'tmp \>\>= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    long tmp = i;
    while (tmp != 0) {
      tmp = tmp >> 8;
      len--;
    }
```

### ReplaceAssignmentWithOperatorAssignment
`n = (short) (n << 8)` could be simplified to 'n \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
        short n = 0;
        n = (short) ((n ^ bytes[offset]) & 0xFF);
        n = (short) (n << 8);
        n ^= (short) (bytes[offset + 1] & 0xFF);
        return n;
```

### ReplaceAssignmentWithOperatorAssignment
`i = i << 8` could be simplified to 'i \<\<= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    for (int idx = 0; idx < len - 1; idx++) {
      byte b = buffer[offset++];
      i = i << 8;
      i = i | (b & 0xFF);
    }
```

### ReplaceAssignmentWithOperatorAssignment
`i = i | (b & 0xFF)` could be simplified to 'i \|= (b \& 0xFF)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
      byte b = buffer[offset++];
      i = i << 8;
      i = i | (b & 0xFF);
    }
    return (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
```

### ReplaceAssignmentWithOperatorAssignment
`amo = (amo >> 8)` could be simplified to 'amo \>\>= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    for (int i = 0; i < value.length; i++) {
      int cur = ((int) amo % 256) * sign;
      amo = (amo >> 8);
      int val = value[value.length - i - 1] & 0x0ff;
      int total = val + cur;
```

### ReplaceAssignmentWithOperatorAssignment
`amo = (amo >> 8)` could be simplified to 'amo \>\>= 8'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    for (int i = 0; i < value.length; i++) {
      int cur = ((int) amo % 256) * sign;
      amo = (amo >> 8);
      int val = (~value[value.length - i - 1] & 0x0ff) + 1;
      int total = cur - val;
```

### ReplaceAssignmentWithOperatorAssignment
`h = h + 13 * Bytes.hashCode(this.family)` could be simplified to 'h += 13 \* Bytes.hashCode(this.family)'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java`
#### Snippet
```java
    public int hashCode() {
      int h = 31;
      h = h + 13 * Bytes.hashCode(this.family);
      h = h + 13 * Bytes.hashCode(this.qualifier);
      return h;
```

### ReplaceAssignmentWithOperatorAssignment
`h = h + 13 * Bytes.hashCode(this.qualifier)` could be simplified to 'h += 13 \* Bytes.hashCode(this.qualifier)'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java`
#### Snippet
```java
      int h = 31;
      h = h + 13 * Bytes.hashCode(this.family);
      h = h + 13 * Bytes.hashCode(this.qualifier);
      return h;
    }
```

### ReplaceAssignmentWithOperatorAssignment
`currentPosition = currentPosition + pageSize` could be simplified to 'currentPosition += pageSize'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/top/Paging.java`
#### Snippet
```java
    if (pageSize > 0 && currentPosition < recordsSize - 1) {

      currentPosition = currentPosition + pageSize;
      if (currentPosition >= recordsSize) {
        currentPosition = recordsSize - 1;
```

### ReplaceAssignmentWithOperatorAssignment
`maxReplicaId = maxReplicaId + (locations.length - (maxReplicaIdIndex + 1))` could be simplified to 'maxReplicaId += (locations.length - (maxReplicaIdIndex + 1))'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java
    }
    // account for the null elements in the array after maxReplicaIdIndex
    maxReplicaId = maxReplicaId + (locations.length - (maxReplicaIdIndex + 1));

    if (maxReplicaId + 1 == locations.length) {
```

### ReplaceAssignmentWithOperatorAssignment
`count = count - rowsToSend.length` could be simplified to 'count -= rowsToSend.length'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ProtobufStreamingOutput.java`
#### Snippet
```java
          break;
        }
        count = count - rowsToSend.length;
        writeToStream(createModelFromResults(rowsToSend), this.contentType, outStream);
      }
```

### ReplaceAssignmentWithOperatorAssignment
`this.capacity = this.capacity << 1` could be simplified to 'this.capacity \<\<= 1'
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/SendBufSizePredictor.java`
#### Snippet
```java
      if ((this.capacity << 1) <= LIMIT) {
        // increase the capacity in the range of power of 2
        this.capacity = this.capacity << 1;
      }
    } else {
```

### ReplaceAssignmentWithOperatorAssignment
`this.capacity = this.capacity >> 1` could be simplified to 'this.capacity \>\>= 1'
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/SendBufSizePredictor.java`
#### Snippet
```java
      // that reduction
      if ((this.capacity >> 1) >= bytesWritten) {
        this.capacity = this.capacity >> 1;
      }
    }
```

### ReplaceAssignmentWithOperatorAssignment
`totalSize = totalSize + nextRegionSize` could be simplified to 'totalSize += nextRegionSize'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
              && Bytes.equals(splitEndKey, nextRegion.getStartRow())
          ) {
            totalSize = totalSize + nextRegionSize;
            splitEndKey = nextRegion.getEndRow();
            j++;
```

### ReplaceAssignmentWithOperatorAssignment
`i = i << 16` could be simplified to 'i \<\<= 16'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
        if (remaining >= Bytes.SIZEOF_SHORT) {
          short s = blockBuffer.getShortAfterPosition(offsetFromPos);
          i = i << 16;
          i = i | (s & 0xFFFF);
          remaining -= Bytes.SIZEOF_SHORT;
```

### ReplaceAssignmentWithOperatorAssignment
`i = i | (s & 0xFFFF)` could be simplified to 'i \|= (s \& 0xFFFF)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
          short s = blockBuffer.getShortAfterPosition(offsetFromPos);
          i = i << 16;
          i = i | (s & 0xFFFF);
          remaining -= Bytes.SIZEOF_SHORT;
          offsetFromPos += Bytes.SIZEOF_SHORT;
```

### ReplaceAssignmentWithOperatorAssignment
`i = i << 8` could be simplified to 'i \<\<= 8'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
        for (int idx = 0; idx < remaining; idx++) {
          byte b = blockBuffer.getByteAfterPosition(offsetFromPos + idx);
          i = i << 8;
          i = i | (b & 0xFF);
        }
```

### ReplaceAssignmentWithOperatorAssignment
`i = i | (b & 0xFF)` could be simplified to 'i \|= (b \& 0xFF)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
          byte b = blockBuffer.getByteAfterPosition(offsetFromPos + idx);
          i = i << 8;
          i = i | (b & 0xFF);
        }
        currMemstoreTS = (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
```

### ReplaceAssignmentWithOperatorAssignment
`minimumMidpointArray[diffIdx] = (byte) (minimumMidpointArray[diffIdx] + 1)` could be simplified to 'minimumMidpointArray\[diffIdx\] += 1'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
    byte[] minimumMidpointArray = new byte[diffIdx + 1];
    ByteBufferUtils.copyFromBufferToArray(minimumMidpointArray, left, leftOffset, 0, diffIdx + 1);
    minimumMidpointArray[diffIdx] = (byte) (minimumMidpointArray[diffIdx] + 1);
    return minimumMidpointArray;
  }
```

### ReplaceAssignmentWithOperatorAssignment
`minimumMidpointArray[diffIdx] = (byte) (minimumMidpointArray[diffIdx] + 1)` could be simplified to 'minimumMidpointArray\[diffIdx\] += 1'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
    byte[] minimumMidpointArray = new byte[diffIdx + 1];
    System.arraycopy(leftArray, leftOffset, minimumMidpointArray, 0, diffIdx + 1);
    minimumMidpointArray[diffIdx] = (byte) (minimumMidpointArray[diffIdx] + 1);
    return minimumMidpointArray;
  }
```

### ReplaceAssignmentWithOperatorAssignment
`i = i + 1` could be simplified to 'i += 1'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
    if (!readonly) {
      // i + 1 to skip the one we load
      for (i = i + 1; i < descFiles.length; i++) {
        Path file = descFiles[i].getPath();
        LOG.info("Delete old table descriptor file {}", file);
```

### ReplaceAssignmentWithOperatorAssignment
`currentSum = currentSum - (double) data` could be simplified to 'currentSum -= (double) data'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RollingStatCalculator.java`
#### Snippet
```java
   */
  private void removeData(long data) {
    currentSum = currentSum - (double) data;
    currentSqrSum = currentSqrSum - ((double) data * data);
    numberOfDataValues--;
```

### ReplaceAssignmentWithOperatorAssignment
`currentSqrSum = currentSqrSum - ((double) data * data)` could be simplified to 'currentSqrSum -= ((double) data \* data)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RollingStatCalculator.java`
#### Snippet
```java
  private void removeData(long data) {
    currentSum = currentSum - (double) data;
    currentSqrSum = currentSqrSum - ((double) data * data);
    numberOfDataValues--;
  }
```

### ReplaceAssignmentWithOperatorAssignment
`currentSum = currentSum + (double) data` could be simplified to 'currentSum += (double) data'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RollingStatCalculator.java`
#### Snippet
```java
    }
    numberOfDataValues++;
    currentSum = currentSum + (double) data;
    currentSqrSum = currentSqrSum + ((double) data * data);
    if (rollingPeriod > 0) {
```

### ReplaceAssignmentWithOperatorAssignment
`currentSqrSum = currentSqrSum + ((double) data * data)` could be simplified to 'currentSqrSum += ((double) data \* data)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RollingStatCalculator.java`
#### Snippet
```java
    numberOfDataValues++;
    currentSum = currentSum + (double) data;
    currentSqrSum = currentSqrSum + ((double) data * data);
    if (rollingPeriod > 0) {
      dataValues[currentIndexPosition] = data;
```

### ReplaceAssignmentWithOperatorAssignment
`count = count + 1` could be simplified to 'count += 1'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java`
#### Snippet
```java
          region.getRegionNameAsString());
      }
      count = count + 1;
      admin.move(region.getEncodedNameAsBytes(), targetServer);
      long maxWait = startTime + (maxWaitInSeconds * 1000);
```

### ReplaceAssignmentWithOperatorAssignment
`regionCount = regionCount + entry.getValue().get()` could be simplified to 'regionCount += entry.getValue().get()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java`
#### Snippet
```java
    int regionCount = 0;
    for (Entry<TableName, AtomicInteger> entry : this.tableAndRegionInfo.entrySet()) {
      regionCount = regionCount + entry.getValue().get();
    }
    return regionCount;
```

### ReplaceAssignmentWithOperatorAssignment
`totalStoreFilesSize = (int) (totalStoreFilesSize + stores.get(j).getStoreFileSizeBytes...` could be simplified to 'totalStoreFilesSize += stores.get(j).getStoreFileSizeBytes()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
          int totalStores = stores.size();
          for (int j = 0; j < totalStores; j++) {
            totalStoreFilesSize =
              (int) (totalStoreFilesSize + stores.get(j).getStoreFileSizeBytes());
          }
        } catch (IOException e) {
```

### ReplaceAssignmentWithOperatorAssignment
`step = step / 2.00f` could be simplified to 'step /= 2.00f'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
      if (!offheapMemstore && step != minimumStepSize) {
        // we leave the step to be at minimumStepSize for offheap memstore
        step = step / 2.00f;
      }
    }
```

### ReplaceAssignmentWithOperatorAssignment
`decayingTunerStepSizeSum = (decayingTunerStepSizeSum) / 2.00f` could be simplified to 'decayingTunerStepSizeSum /= 2.00f'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
        prevTuneDirection = StepDirection.NEUTRAL;
        rollingStatsForTunerSteps.insertDataValue(0);
        decayingTunerStepSizeSum = (decayingTunerStepSizeSum) / 2.00f;
        return NO_OP_TUNER_RESULT;
    }
```

### ReplaceAssignmentWithOperatorAssignment
`this.data = this.data | WAITING_BIT` could be simplified to 'this.data \|= WAITING_BIT'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java

    public void setHasWait() {
      this.data = this.data | WAITING_BIT;
    }

```

### ReplaceAssignmentWithOperatorAssignment
`avgAgeNumerator = (long) (avgAgeNumerator + storeAvgStoreFileAge.getAsDouble() * sto...` could be simplified to 'avgAgeNumerator += storeAvgStoreFileAge.getAsDouble() \* storeHFiles'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
            OptionalDouble storeAvgStoreFileAge = store.getAvgStoreFileAge();
            if (storeAvgStoreFileAge.isPresent()) {
              avgAgeNumerator =
                (long) (avgAgeNumerator + storeAvgStoreFileAge.getAsDouble() * storeHFiles);
            }

```

### ReplaceAssignmentWithOperatorAssignment
`numRetries = numRetries / mult` could be simplified to 'numRetries /= mult'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
      int mult = conf.getInt(HConstants.HBASE_CLIENT_SERVERSIDE_RETRIES_MULTIPLIER,
        HConstants.DEFAULT_HBASE_CLIENT_SERVERSIDE_RETRIES_MULTIPLIER);
      numRetries = numRetries / mult; // reset if HRS has multiplied this already
    }
    return numRetries;
```

### ReplaceAssignmentWithOperatorAssignment
`stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1)` could be simplified to 'stopRow\[stopRow.length - 1\] += 1'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    byte[] startRow = Bytes.toBytes(SET_KEY_PREFIX);
    byte[] stopRow = Arrays.copyOf(startRow, startRow.length);
    stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1);
    scan.withStartRow(startRow);
    scan.withStopRow(stopRow);
```

### ReplaceAssignmentWithOperatorAssignment
`stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1)` could be simplified to 'stopRow\[stopRow.length - 1\] += 1'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    byte[] startRow = rowkey(RS_LOG_TS_PREFIX, backupRoot);
    byte[] stopRow = Arrays.copyOf(startRow, startRow.length);
    stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1);
    scan.withStartRow(startRow);
    scan.withStopRow(stopRow);
```

### ReplaceAssignmentWithOperatorAssignment
`stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1)` could be simplified to 'stopRow\[stopRow.length - 1\] += 1'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    byte[] startRow = rowkey(TABLE_RS_LOG_MAP_PREFIX, backupRoot);
    byte[] stopRow = Arrays.copyOf(startRow, startRow.length);
    stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1);
    scan.withStartRow(startRow);
    scan.withStopRow(stopRow);
```

### ReplaceAssignmentWithOperatorAssignment
`stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1)` could be simplified to 'stopRow\[stopRow.length - 1\] += 1'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    byte[] startRow = Bytes.toBytes(BACKUP_INFO_PREFIX);
    byte[] stopRow = Arrays.copyOf(startRow, startRow.length);
    stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1);
    scan.withStartRow(startRow);
    scan.withStopRow(stopRow);
```

### ReplaceAssignmentWithOperatorAssignment
`stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1)` could be simplified to 'stopRow\[stopRow.length - 1\] += 1'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      backupId == null ? BULK_LOAD_PREFIX_BYTES : rowkey(BULK_LOAD_PREFIX, backupId + BLK_LD_DELIM);
    byte[] stopRow = Arrays.copyOf(startRow, startRow.length);
    stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1);
    scan.withStartRow(startRow);
    scan.withStopRow(stopRow);
```

### ReplaceAssignmentWithOperatorAssignment
`stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1)` could be simplified to 'stopRow\[stopRow.length - 1\] += 1'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    byte[] startRow = rowkey(BULK_LOAD_PREFIX, table.toString(), BLK_LD_DELIM);
    byte[] stopRow = Arrays.copyOf(startRow, startRow.length);
    stopRow[stopRow.length - 1] = (byte) (stopRow[stopRow.length - 1] + 1);
    scan.withStartRow(startRow);
    scan.withStopRow(stopRow);
```

## RuleId[id=NestedAssignment]
### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java`
#### Snippet
```java
      final ByteBufferBackedNode comparisonNode = new ByteBufferBackedNode();
      comparisonNode.setContents(buf, offset, length);
      if ((s = nodeToIndex.get(comparisonNode)) != null) {
        moveToHead(indexToNode[s]);
        return s;
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java`
#### Snippet
```java
      final Node comparisonNode = new ByteArrayBackedNode();
      comparisonNode.setContents(array, offset, length);
      if ((s = nodeToIndex.get(comparisonNode)) != null) {
        moveToHead(indexToNode[s]);
        return s;
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
    }
    int result = tmp & 0x7f;
    if ((tmp = input.get()) >= 0) {
      result |= tmp << 7;
    } else {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
    } else {
      result |= (tmp & 0x7f) << 7;
      if ((tmp = input.get()) >= 0) {
        result |= tmp << 14;
      } else {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
      } else {
        result |= (tmp & 0x7f) << 14;
        if ((tmp = input.get()) >= 0) {
          result |= tmp << 21;
        } else {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
        } else {
          result |= (tmp & 0x7f) << 21;
          result |= (tmp = input.get()) << 28;
          if (tmp < 0) {
            // Discard upper 32 bits.
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
    }
    int result = tmp & 0x7f;
    if ((tmp = (byte) input.read()) >= 0) {
      result |= tmp << 7;
    } else {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
    } else {
      result |= (tmp & 0x7f) << 7;
      if ((tmp = (byte) input.read()) >= 0) {
        result |= tmp << 14;
      } else {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
      } else {
        result |= (tmp & 0x7f) << 14;
        if ((tmp = (byte) input.read()) >= 0) {
          result |= tmp << 21;
        } else {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
        } else {
          result |= (tmp & 0x7f) << 21;
          result |= (tmp = (byte) input.read()) << 28;
          if (tmp < 0) {
            // Discard upper 32 bits.
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JenkinsHash.java`
#### Snippet
```java
    int length = hashKey.length();
    int a, b, c;
    a = b = c = 0xdeadbeef + length + initval;
    int offset = 0;
    for (; length > 12; offset += 12, length -= 12) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JenkinsHash.java`
#### Snippet
```java
    int length = hashKey.length();
    int a, b, c;
    a = b = c = 0xdeadbeef + length + initval;
    int offset = 0;
    for (; length > 12; offset += 12, length -= 12) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcurrentMapUtils.java`
#### Snippet
```java
    IOExceptionSupplier<V> supplier) throws IOException {
    V v, newValue;
    return ((v = map.get(key)) == null && (newValue = supplier.get()) != null
      && (v = map.putIfAbsent(key, newValue)) == null) ? newValue : v;
  }
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcurrentMapUtils.java`
#### Snippet
```java
    IOExceptionSupplier<V> supplier) throws IOException {
    V v, newValue;
    return ((v = map.get(key)) == null && (newValue = supplier.get()) != null
      && (v = map.putIfAbsent(key, newValue)) == null) ? newValue : v;
  }
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcurrentMapUtils.java`
#### Snippet
```java
    V v, newValue;
    return ((v = map.get(key)) == null && (newValue = supplier.get()) != null
      && (v = map.putIfAbsent(key, newValue)) == null) ? newValue : v;
  }

```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
  private static int unsignedCmp(long x1, long x2) {
    int cmp;
    if ((cmp = (x1 < x2 ? -1 : (x1 == x2 ? 0 : 1))) == 0) return 0;
    // invert the result when either value is negative
    if ((x1 < 0) != (x2 < 0)) return -cmp;
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
    Entry entry = new Entry(id, currentThread);
    Entry existing;
    while ((existing = map.putIfAbsent(entry.id, entry)) != null) {
      synchronized (existing) {
        if (existing.locked) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
    long waitUtilTS = EnvironmentEdgeManager.currentTime() + time;
    long remaining = time;
    while ((existing = map.putIfAbsent(entry.id, entry)) != null) {
      synchronized (existing) {
        if (existing.locked) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java`
#### Snippet
```java
      bufferedReader = new BufferedReader(inputStreamReader);
      String openFileDesCount;
      if ((openFileDesCount = bufferedReader.readLine()) != null) {
        return Long.parseLong(openFileDesCount);
      }
```

### NestedAssignment
Result of assignment expression used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java`
#### Snippet
```java
      output = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
      String maxFileDesCount;
      if ((maxFileDesCount = output.readLine()) != null) {
        return Long.parseLong(maxFileDesCount);
      }
```

### NestedAssignment
Result of assignment expression used
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeSaslServerAuthenticationProvider.java`
#### Snippet
```java
      String line = null;
      int offset = 0;
      while ((line = reader.readLine()) != null) {
        line = line.trim();
        String[] parts = StringUtils.split(line, SEPARATOR);
```

### NestedAssignment
Result of assignment expression used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultScanner.java`
#### Snippet
```java
        }
        try {
          return (next = ResultScanner.this.next()) != null;
        } catch (IOException e) {
          throw new UncheckedIOException(e);
```

### NestedAssignment
Result of assignment expression used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
    long lastLog = EnvironmentEdgeManager.currentTime();
    long currentInProgress, oldInProgress = Long.MAX_VALUE;
    while ((currentInProgress = tasksInProgress.get()) > max) {
      if (oldInProgress != currentInProgress) { // Wait for in progress to change.
        long now = EnvironmentEdgeManager.currentTime();
```

### NestedAssignment
Result of assignment expression used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableImpl.java`
#### Snippet
```java
          consumer.onScanMetricsCreated(scanner.getScanMetrics());
        }
        for (Result result; (result = scanner.next()) != null;) {
          if (!consumer.onNext(result)) {
            break;
```

### NestedAssignment
Result of assignment expression used
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java
        RowModel rowModel = new RowModel(rowSpec.getRow());
        if (generator.hasNext()) {
          while ((value = generator.next()) != null) {
            rowModel.addCell(new CellModel(CellUtil.cloneFamily(value),
              CellUtil.cloneQualifier(value), value.getTimestamp(), CellUtil.cloneValue(value)));
```

### NestedAssignment
Result of assignment expression used
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      char c;
      StringBuilder column = new StringBuilder();
      while (i < path.length() && (c = path.charAt(i)) != '/') {
        if (c == ',') {
          if (column.length() < 1) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
        stamp = new StringBuilder();
        i++;
        while (i < path.length() && ((c = path.charAt(i)) != '/')) {
          stamp.append(c);
          i++;
```

### NestedAssignment
Result of assignment expression used
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      StringBuilder sb = new StringBuilder();
      char c;
      while (i < path.length() && (c = path.charAt(i)) != '/') {
        sb.append(c);
        i++;
```

### NestedAssignment
Result of assignment expression used
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      }
      i++;
      String row = startRow = sb.toString();
      int idx = startRow.indexOf(',');
      if (idx != -1) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/InstancePending.java`
#### Snippet
```java
    boolean interrupted = false;

    while ((instanceHolder = this.instanceHolder) == null) {
      try {
        pendingLatch.await();
```

### NestedAssignment
Result of assignment expression used
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
          // success
          ChannelPipeline p = ctx.pipeline();
          for (ChannelHandler handler; (handler = p.removeLast()) != null;) {
            // do not remove all handlers because we may have wrap or unwrap handlers at the header
            // of pipeline.
```

### NestedAssignment
Result of assignment expression used
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java

        long stime = EnvironmentEdgeManager.currentTime();
        while ((bytesRead = in.read(buffer)) > 0) {
          out.write(buffer, 0, bytesRead);
          totalBytesWritten += bytesRead;
```

### NestedAssignment
Result of assignment expression used
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java`
#### Snippet
```java
        scanner = labelsTable.getScanner(scan);
        Result next = null;
        while ((next = scanner.next()) != null) {
          byte[] row = next.getRow();
          byte[] value = next.getValue(LABELS_TABLE_FAMILY, LABEL_QUALIFIER);
```

### NestedAssignment
Result of assignment expression used
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
        int n;
        try {
          while ((n = reader.readLine(key)) > 0) {
            String[] hosts = getStoreDirHosts(fs, path);
            splits.add(new FileSplit(path, pos, n, hosts));
```

### NestedAssignment
Result of assignment expression used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/LogMonitoring.java`
#### Snippet
```java
      r.readLine(); // skip the first partial line
      String line;
      while ((line = r.readLine()) != null) {
        out.println(line);
      }
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java`
#### Snippet
```java
      try (ResultScanner scanner = metaTable.getScanner(scan)) {
        Result data;
        while ((data = scanner.next()) != null) {
          if (data.isEmpty()) {
            continue;
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
    link = null;
    hfs = null;
    useHBaseChecksumConfigured = useHBaseChecksum = false;
    dropBehind = false;
    readahead = 0;
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
      this.streamNoFsChecksum = (link != null) ? link.open(fsNc) : fsNc.open(path);
      setStreamOptions(streamNoFsChecksum);
      this.useHBaseChecksumConfigured = this.useHBaseChecksum = useHBaseChecksum;
      // Close the checksum stream; we will reopen it if we get an HBase checksum failure.
      this.stream.close();
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java

    // Initially we are going to read the tail block. Open the reader w/FS checksum.
    this.useHBaseChecksumConfigured = this.useHBaseChecksum = false;
    this.stream = (link != null) ? link.open(hfs) : hfs.open(path);
    setStreamOptions(stream);
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java
      // Load-On-Open info
      HFileBlock b;
      while ((b = blockIter.nextBlock()) != null) {
        loadOnOpenBlocks.add(b);
      }
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
      LruCachedBlock cb;
      long freedBytes = 0;
      while ((cb = queue.pollLast()) != null) {
        freedBytes += evictBlock(cb, true);
        if (freedBytes >= toFree) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java

        BlockBucket bucket;
        while ((bucket = bucketQueue.poll()) != null) {
          long overflow = bucket.overflow();
          if (overflow > 0) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java

        BlockBucket bucket;
        while ((bucket = bucketQueue.poll()) != null) {
          long overflow = bucket.overflow();
          if (overflow > 0) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
      LruCachedBlock cb;
      long freedBytes = 0;
      while ((cb = queue.pollLast()) != null) {
        freedBytes += evictBlock(cb, true);
        if (freedBytes >= toFree) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      // TODO avoid a cycling siutation. We find no block which is not in use and so no way to free
      // What to do then? Caching attempt fail? Need some changes in cacheBlock API?
      while ((entry = queue.pollLast()) != null) {
        BlockCacheKey blockCacheKey = entry.getKey();
        BucketEntry be = entry.getValue();
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

      BucketEntryGroup bucketGroup;
      while ((bucketGroup = bucketQueue.poll()) != null) {
        long overflow = bucketGroup.overflow();
        if (overflow > 0) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
        bucketQueue.add(bucketMemory);

        while ((bucketGroup = bucketQueue.poll()) != null) {
          long bucketBytesToFree = (bytesToFreeWithExtra - bytesFreed) / remainingBuckets;
          bytesFreed += bucketGroup.free(bucketBytesToFree);
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      ServerSocketChannel server = (ServerSocketChannel) key.channel();
      SocketChannel channel;
      while ((channel = server.accept()) != null) {
        channel.configureBlocking(false);
        channel.socket().setTcpNoDelay(tcpNoDelay);
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      Entry entry;
      startTS = EnvironmentEdgeManager.currentTime();
      while ((entry = getNextLogLine(walReader, wal, this.skipErrors)) != null) {
        if (WALEdit.isReplicationMarkerEdit(entry.getEdit())) {
          // Skip processing the replication marker edits.
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java`
#### Snippet
```java
    try {
      WAL.Entry entry;
      while ((entry = log.next()) != null) {
        WALKey key = entry.getKey();
        WALEdit edit = entry.getEdit();
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
            boolean refFound = false;
            for (ColumnFamilyDescriptor c : htd.getColumnFamilies()) {
              if ((refFound = regionFs.hasReferences(c.getNameAsString()))) {
                break;
              }
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    Result result;
    try (ResultScanner scanner = meta.getScanner(barrierScan)) {
      while ((result = scanner.next()) != null) {
        regionNames.add(result.getRow());
      }
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
    ZKWatcher zkw = master.getZooKeeper();
    int onlineServersCt;
    while ((onlineServersCt = onlineServers.size()) > 0) {
      if (EnvironmentEdgeManager.currentTime() > (previousLogTime + 1000)) {
        Set<ServerName> remainingServers = onlineServers.keySet();
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java

    String cmd = null;
    while ((cmd = args.poll()) != null) {
      if (cmd.equals("-h") || cmd.equals("--h") || cmd.equals("--help")) {
        // place item back onto queue so that caller knows parsing was incomplete
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
    try {
      try {
        if ((data = ZKUtil.getDataNoWatch(watcher, path, stat)) == null) {
          SplitLogCounters.tot_wkr_failed_to_grab_task_no_data.increment();
          return false;
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java

    KeyValueScanner scanner;
    while ((scanner = heap.poll()) != null) {
      Cell topKey = scanner.peek();
      if (
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java

    KeyValueScanner scanner;
    while ((scanner = heap.poll()) != null) {
      Cell topKey = scanner.peek();
      if (comparator.getComparator().compareRows(topKey, seekKey) < 0) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellArrayImmutableSegment.java`
#### Snippet
```java
    Cell prev = null;
    try {
      while ((curCell = segmentScanner.next()) != null) {
        cells[idx++] = curCell;
        if (action == MemStoreCompactionStrategy.Action.FLATTEN_COUNT_UNIQUE_KEYS) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java

    Cell next;
    while ((next = this.storeHeap.peek()) != null && CellUtil.matchingRows(next, curRowCell)) {
      // Check for thread interrupt status in case we have been signaled from
      // #interruptRegionOperation.
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.java`
#### Snippet
```java
    Cell prev = null;
    try {
      while ((curCell = segmentScanner.next()) != null) {
        assert (curCell instanceof ExtendedCell);
        if (((ExtendedCell) curCell).getChunkId() == ExtendedCell.CELL_NOT_BASED_ON_CHUNK) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java

      WAL.Entry e = null;
      while ((e = in.next()) != null)
        out.append(e);
    } finally {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractProtobufLogWriter.java`
#### Snippet
```java
        this.trailer = buildWALTrailer(WALTrailer.newBuilder());
        trailerSize = this.trailer.getSerializedSize();
      } else if ((trailerSize = this.trailer.getSerializedSize()) > this.trailerWarnSize) {
        // continue writing after warning the user.
        LOG.warn("Please investigate WALTrailer usage. Trailer size > maximum size : " + trailerSize
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
    private int releaseSyncFutures(final long currentSequence, final Throwable t) {
      int syncCount = 0;
      for (SyncFuture syncFuture; (syncFuture = this.syncFutures.peek()) != null;) {
        if (syncFuture.getTxid() > currentSequence) {
          break;
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
        return false;
      }
    } while ((nextCell = this.heap.peek()) != null && CellUtil.matchingRows(cell, nextCell));
    return true;
  }
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
        return false;
      }
    } while ((nextCell = this.heap.peek()) != null && CellUtil.matchingRowColumn(cell, nextCell));
    // We need this check because it may happen that the new scanner that we get
    // during heap.next() is requiring reseek due of fake KV previously generated for
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
          return scannerContext.setScannerState(NextState.MORE_VALUES).hasMoreValues();
        }
      } while ((cell = this.heap.peek()) != null);

      if (count > 0) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionProgress.java`
#### Snippet
```java
   */
  public void cancel() {
    this.currentCompactedKVs = this.totalCompactingKVs = 0;
  }

```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
          // There's a file with no information, must be an old one
          // assume we have very old puts
          fd.earliestPutTs = earliestPutTs = PrivateConstants.OLDEST_TIMESTAMP;
        } else {
          earliestPutTs = Bytes.toLong(tmp);
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
      assert request != null;
      this.request = request;
      this.majorRangeFromRow = this.majorRangeToRow = null;
    }
  }
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
        ResultScanner scanner = table.getScanner(scan)) {
        Result res = null;
        while ((res = scanner.next()) != null) {
          res.advance();
          String fam = null;
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      ResultScanner scanner = table.getScanner(scan)) {
      Result res = null;
      while ((res = scanner.next()) != null) {
        res.advance();
        TableName tbl = null;
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res = null;
      Map<byte[], String> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
      while ((res = scanner.next()) != null) {
        res.advance();
        byte[] row = CellUtil.cloneRow(res.listCells().get(0));
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      ResultScanner scanner = table.getScanner(scan)) {
      Result res;
      while ((res = scanner.next()) != null) {
        res.advance();
        Cell cell = res.current();
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      try (ResultScanner scanner = table.getScanner(scan)) {
        Result res;
        while ((res = scanner.next()) != null) {
          res.advance();
          list.add(cellKeyToBackupSetName(res.current()));
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      ResultScanner scanner = table.getScanner(scan)) {
      Result res;
      while ((res = scanner.next()) != null) {
        res.advance();
        BackupInfo context = cellToBackupInfo(res.current());
```

### NestedAssignment
Result of assignment expression used
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res;
      HashMap<String, Long> rsTimestampMap = new HashMap<>();
      while ((res = scanner.next()) != null) {
        res.advance();
        Cell cell = res.current();
```

### NestedAssignment
Result of assignment expression used
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
    long start = EnvironmentEdgeManager.currentTime();
    while (
      !(mts = getMasterThreads()).isEmpty()
        && (EnvironmentEdgeManager.currentTime() - start) < timeout
    ) {
```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      } else {
        // use the provided sequence Id as WAL is not being used for this flush.
        flushedSeqId = flushOpSeqId = myseqid;
      }

```

### NestedAssignment
Result of assignment expression used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        }

        while ((entry = reader.next()) != null) {
          WALKey key = entry.getKey();
          WALEdit val = entry.getEdit();
```

### NestedAssignment
Result of assignment expression used
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
            boolean tableFound = false;
            try (ResultScanner s = meta.getScanner(scan)) {
              for (Result r; (r = s.next()) != null;) {
                byte[] b = r.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
                RegionInfo info = RegionInfo.parseFromOrNull(b);
```

## RuleId[id=ExcessiveLambdaUsage]
### ExcessiveLambdaUsage
Excessive lambda usage
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      if (rsGroupInfo.containsServer(serverName.getAddress())) {
        for (Map<ServerName, List<RegionInfo>> map : result.values()) {
          map.computeIfAbsent(serverName, k -> Collections.emptyList());
        }
      }
```

## RuleId[id=ReturnFromFinallyBlock]
### ReturnFromFinallyBlock
'return' inside 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
      complete = true;
      LOG.debug("Subprocedure '" + barrierName + "' completed.");
      return null;
    }
  }
```

### ReturnFromFinallyBlock
'return' inside 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java`
#### Snippet
```java
      // tell the timer we are done, if we get here successfully
      timeoutInjector.complete();
      return null;
    }
  }
```

## RuleId[id=SimplifiableBooleanExpression]
### SimplifiableBooleanExpression
`(reExecute && subprocs == null) || !reExecute` can be simplified to '!reExecute\|\|subprocs == null'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      }

      assert (reExecute && subprocs == null) || !reExecute;
    } while (reExecute);

```

### SimplifiableBooleanExpression
`!(tail ^ inclusive)` can be simplified to 'tail==inclusive'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    // correct the insertion point in case the given anchor key DOES EXIST in the set
    if (index >= 0) {
      if (descending && !(tail ^ inclusive)) {
        // for the descending case
        // if anchor for head set (tail=false) AND anchor is not inclusive -> move the insertion pt
```

## RuleId[id=FieldAccessedSynchronizedAndUnsynchronized]
### FieldAccessedSynchronizedAndUnsynchronized
Field `initialChoreComplete` is accessed in both synchronized and unsynchronized contexts
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
  private long timeOfLastRun = -1; // system time millis
  private long timeOfThisRun = -1; // system time millis
  private boolean initialChoreComplete = false;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `hostAndWeights` is accessed in both synchronized and unsynchronized contexts
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java`
#### Snippet
```java
@InterfaceAudience.Private
public class HDFSBlocksDistribution {
  private Map<String, HostAndWeight> hostAndWeights = null;
  private long uniqueBlocksTotalWeight = 0;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `map` is accessed in both synchronized and unsynchronized contexts
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
  }

  private ConcurrentMap<Long, Entry> map = new ConcurrentHashMap<>();

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `groups` is accessed in both synchronized and unsynchronized contexts
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
  private LoadingCache<String, String[]> groupCache = null;

  static Groups groups = Groups.getUserToGroupsMappingService();

  public static Groups getGroups() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `costFunctionDescs` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancerSourceImpl.java`
#### Snippet
```java
      }
    };
  private Map<String, String> costFunctionDescs = new ConcurrentHashMap<>();

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `splitSuccess` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java
  // split related metrics
  private MutableFastCounter splitRequest;
  private MutableFastCounter splitSuccess;
  private MetricHistogram splitTimeHisto;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `splitTimeHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java
  private MutableFastCounter splitRequest;
  private MutableFastCounter splitSuccess;
  private MetricHistogram splitTimeHisto;

  // flush related metrics
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `tableWrapperAgg` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java
  // leak a whole table by way of keeping the
  // tableWrapper around too long.
  private MetricsTableWrapperAggregate tableWrapperAgg;
  private final MetricsTableAggregateSourceImpl agg;
  private final DynamicMetricsRegistry registry;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `splitRequest` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java

  // split related metrics
  private MutableFastCounter splitRequest;
  private MutableFastCounter splitSuccess;
  private MetricHistogram splitTimeHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `closed` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MetricsTableSourceImpl.class);

  private AtomicBoolean closed = new AtomicBoolean(false);

  // Non-final so that we can null out the wrapper
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `flushTimeHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java

  // flush related metrics
  private MetricHistogram flushTimeHisto;
  private MetricHistogram flushMemstoreSizeHisto;
  private MetricHistogram flushOutputSizeHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `closed` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java`
#### Snippet
```java
  private static final String _STORE = "_store_";

  private AtomicBoolean closed = new AtomicBoolean(false);

  // Non-final so that we can null out the wrapper
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `scanTimeHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java

  private MetricHistogram getHisto;
  private MetricHistogram scanTimeHisto;
  private MetricHistogram putHisto;
  private MetricHistogram deleteHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `deleteHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private MetricHistogram scanTimeHisto;
  private MetricHistogram putHisto;
  private MetricHistogram deleteHisto;
  private MetricHistogram incrementHisto;
  private MetricHistogram appendHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `replayHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private MetricHistogram incrementHisto;
  private MetricHistogram appendHisto;
  private MetricHistogram replayHisto;

  private final int hashCode;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `appendHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private MetricHistogram deleteHisto;
  private MetricHistogram incrementHisto;
  private MetricHistogram appendHisto;
  private MetricHistogram replayHisto;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `closed` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private final int hashCode;

  private AtomicBoolean closed = new AtomicBoolean(false);
  private final DynamicMetricsRegistry registry;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `putHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private MetricHistogram getHisto;
  private MetricHistogram scanTimeHisto;
  private MetricHistogram putHisto;
  private MetricHistogram deleteHisto;
  private MetricHistogram incrementHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `incrementHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private MetricHistogram putHisto;
  private MetricHistogram deleteHisto;
  private MetricHistogram incrementHisto;
  private MetricHistogram appendHisto;
  private MetricHistogram replayHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `getHisto` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  private final String userReplayKey;

  private MetricHistogram getHisto;
  private MetricHistogram scanTimeHisto;
  private MetricHistogram putHisto;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `histogram` is accessed in both synchronized and unsynchronized contexts
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableHistogram.java`
#### Snippet
```java
@InterfaceAudience.Private
public class MutableHistogram extends MutableMetric implements MetricHistogram {
  protected HistogramImpl histogram;
  protected final String name;
  protected final String desc;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `provider` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcConnection.java`
#### Snippet
```java
  protected long lastTouched;

  protected SaslClientAuthenticationProvider provider;

  protected RpcConnection(Configuration conf, HashedWheelTimer timeoutTimer, ConnectionId remoteId,
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `exception` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  private final List<RpcCallback<Object>> cancellationCbs = new ArrayList<>();

  private IOException exception;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `cellScanner` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
   * block that implements CellScanner.
   */
  private CellScanner cellScanner;

  public HBaseRpcControllerImpl() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `thread` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "IS2_INCONSISTENT_SYNC",
      justification = "We are always under lock actually")
  private Thread thread;

  // connected socket. protected for writing UT.
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `futures` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
  private List<Mutation> mutations = new ArrayList<>();

  private List<CompletableFuture<Void>> futures = new ArrayList<>();

  private long bufferedSize;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `periodicFlushTask` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
  private boolean closed;

  Timeout periodicFlushTask;

  AsyncBufferedMutatorImpl(HashedWheelTimer periodicalFlushTimer, AsyncTable<?> table,
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `mutations` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
  private final int maxKeyValueSize;

  private List<Mutation> mutations = new ArrayList<>();

  private List<CompletableFuture<Void>> futures = new ArrayList<>();
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `bufferedSize` is accessed in both synchronized and unsynchronized contexts
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
  private List<CompletableFuture<Void>> futures = new ArrayList<>();

  private long bufferedSize;

  private boolean closed;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `timeoutExecutor` is accessed in both synchronized and unsynchronized contexts
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
  private final int corePoolSize;

  private TimeoutExecutorThread timeoutExecutor;
  private ThreadPoolExecutor threadPool;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `operations` is accessed in both synchronized and unsynchronized contexts
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
  protected final class BufferNode extends DelayedContainerWithTimestamp<TRemote>
    implements RemoteNode<TEnv, TRemote> {
    private Set<RemoteProcedure> operations;
    private final Set<RemoteProcedure> dispatchedOperations = new HashSet<>();

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `suspendedProcedures` is accessed in both synchronized and unsynchronized contexts
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureEvent.java`
#### Snippet
```java
  private final T object;
  private boolean ready = false;
  private ProcedureDeque suspendedProcedures = new ProcedureDeque();

  public ProcedureEvent(final T object) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `source` is accessed in both synchronized and unsynchronized contexts
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MetricsREST.java`
#### Snippet
```java
  }

  private MetricsRESTSource source;

  public MetricsREST() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `conf` is accessed in both synchronized and unsynchronized contexts
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      value = { "ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD", "MS_CANNOT_BE_FINAL" },
      justification = "For testing")
  public static Configuration conf = null;
  private final UserProvider userProvider;
  private Server server;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `stopped` is accessed in both synchronized and unsynchronized contexts
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
  protected final Abortable abortable;

  private boolean stopped = false;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `outer` is accessed in both synchronized and unsynchronized contexts
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MultithreadedTableMapper.class);
  private Class<? extends Mapper<ImmutableBytesWritable, Result, K2, V2>> mapClass;
  private Context outer;
  private ExecutorService executor;
  public static final String NUMBER_OF_THREADS = "hbase.mapreduce.multithreadedmapper.threads";
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `value` is accessed in both synchronized and unsynchronized contexts
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java
  private class SubMapRecordReader extends RecordReader<ImmutableBytesWritable, Result> {
    private ImmutableBytesWritable key;
    private Result value;
    private Configuration conf;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `key` is accessed in both synchronized and unsynchronized contexts
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java

  private class SubMapRecordReader extends RecordReader<ImmutableBytesWritable, Result> {
    private ImmutableBytesWritable key;
    private Result value;
    private Configuration conf;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `conf` is accessed in both synchronized and unsynchronized contexts
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java
    private ImmutableBytesWritable key;
    private Result value;
    private Configuration conf;

    @Override
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `rmiRegistry` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
   */
  private static JMXConnectorServer JMX_CS = null;
  private Registry rmiRegistry = null;

  public static JMXServiceURL buildJMXServiceURL(int rmiRegistryPort, int rmiConnectorPort)
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `asyncClusterConnection` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
   * The asynchronous cluster connection to be shared by services.
   */
  protected AsyncClusterConnection asyncClusterConnection;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `metaBlockIndexReader` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java

  /** Meta block index reader -- always single level */
  protected HFileBlockIndex.ByteArrayKeyBlockIndexReader metaBlockIndexReader;

  protected FixedFileTrailer trailer;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `hfileContext` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
  private Configuration conf;

  protected HFileContext hfileContext;

  /** Filesystem-level block reader. */
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `fsBlockReader` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java

  /** Filesystem-level block reader. */
  protected HFileBlock.FSReader fsBlockReader;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `cellBlockStream` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  protected final long size; // size of current call
  protected boolean isError;
  protected ByteBufferListOutputStream cellBlockStream = null;
  protected CallCleanup reqCleanup = null;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `bucketList` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    // Free bucket means it has space to allocate a block;
    // Completely free bucket means it has no block.
    private LinkedMap bucketList, freeBuckets, completelyFreeBuckets;
    // only modified under synchronization, but also read outside it.
    private volatile long fragmentationBytes;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `sizeIndex` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    // only modified under synchronization, but also read outside it.
    private volatile long fragmentationBytes;
    private int sizeIndex;

    BucketSizeInfo(int sizeIndex) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `bucketSizeInfos` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
  private final long bucketCapacity;
  private Bucket[] buckets;
  private BucketSizeInfo[] bucketSizeInfos;
  private final long totalSize;
  private transient long usedSize = 0;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `usedSize` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
  private BucketSizeInfo[] bucketSizeInfos;
  private final long totalSize;
  private transient long usedSize = 0;

  BucketAllocator(long availableSpace, int[] bucketSizes) throws BucketAllocatorException {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `freeBuckets` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    // Free bucket means it has space to allocate a block;
    // Completely free bucket means it has no block.
    private LinkedMap bucketList, freeBuckets, completelyFreeBuckets;
    // only modified under synchronization, but also read outside it.
    private volatile long fragmentationBytes;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `buckets` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
  // The capacity size for each bucket
  private final long bucketCapacity;
  private Bucket[] buckets;
  private BucketSizeInfo[] bucketSizeInfos;
  private final long totalSize;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `completelyFreeBuckets` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    // Free bucket means it has space to allocate a block;
    // Completely free bucket means it has no block.
    private LinkedMap bucketList, freeBuckets, completelyFreeBuckets;
    // only modified under synchronization, but also read outside it.
    private volatile long fragmentationBytes;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `data` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.java`
#### Snippet
```java

  final SocketChannel channel;
  private ByteBuff data;
  private ByteBuffer dataLengthBuffer;
  private ByteBuffer preambleBuffer;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `connectionManager` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

  // maintains the set of client connections and handles idle timeouts
  private ConnectionManager connectionManager;
  private Listener listener = null;
  protected SimpleRpcServerResponder responder = null;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `selector` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
    private int currentReader = 0;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `controller` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
  public static class WriterThread extends Thread {
    private volatile boolean shouldStop = false;
    private WALSplitter.PipelineController controller;
    private EntryBuffers entryBuffers;
    private OutputSink outputSink = null;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `deployedOn` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private HdfsEntry hdfsEntry = null; // info in HDFS
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
  private List<ServerName> deployedOn = Lists.newArrayList(); // info on RS's
  private boolean skipChecks = false; // whether to skip further checks to this region info.
  private boolean isMerged = false;// whether this region has already been merged into another one
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `primaryHRIForDeployedReplica` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private boolean isMerged = false;// whether this region has already been merged into another one
  private int deployedReplicaId = RegionInfo.DEFAULT_REPLICA_ID;
  private RegionInfo primaryHRIForDeployedReplica = null;

  public HbckRegionInfo(MetaEntry metaEntry) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `metaEntry` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HbckRegionInfo.class.getName());

  private MetaEntry metaEntry = null; // info in META
  private HdfsEntry hdfsEntry = null; // info in HDFS
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `deployedEntries` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private MetaEntry metaEntry = null; // info in META
  private HdfsEntry hdfsEntry = null; // info in HDFS
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
  private List<ServerName> deployedOn = Lists.newArrayList(); // info on RS's
  private boolean skipChecks = false; // whether to skip further checks to this region info.
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `tableName` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
  private static final String TO_BE_LOADED = "to_be_loaded";

  TableName tableName;
  TreeSet<ServerName> deployedOn;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `admin` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private ClusterMetrics status;
  private Connection connection;
  private Admin admin;
  private Table meta;
  // threads to do ||izable tasks: retrieve data from regionservers, handle overlapping regions
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `errorCount` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

  static class PrintingErrorReporter implements HbckErrorReporter {
    public int errorCount = 0;
    private int showProgress;
    // How frequently calls to progress() will create output
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `meta` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private Connection connection;
  private Admin admin;
  private Table meta;
  // threads to do ||izable tasks: retrieve data from regionservers, handle overlapping regions
  protected ExecutorService executor;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `fs` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    private FileStatus tableDir;
    private HbckErrorReporter errors;
    private FileSystem fs;

    WorkItemHdfsDir(FileSystem fs, HbckErrorReporter errors, FileStatus status) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `executor` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private Table meta;
  // threads to do ||izable tasks: retrieve data from regionservers, handle overlapping regions
  protected ExecutorService executor;
  private long startMillis = EnvironmentEdgeManager.currentTime();
  private HFileCorruptionChecker hfcc;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `timelag` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   ***********/
  private static boolean details = false; // do we display the full report
  private long timelag = DEFAULT_TIME_LAG; // tables whose modtime is older
  private static boolean forceExclusive = false; // only this hbck can modify HBase
  private boolean fixAssignments = false; // fix assignment errors?
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `regionInfoMap` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * correct consistency (hdfs/meta/deployment) problems.
   */
  private TreeMap<String, HbckRegionInfo> regionInfoMap = new TreeMap<>();
  // Empty regioninfo qualifiers in hbase:meta
  private Set<Result> emptyRegionInfoQualifiers = new HashSet<>();
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `summary` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

  private boolean rerun = false; // if we tried to fix something, rerun hbck
  private static boolean summary = false; // if we want to print less output
  private boolean checkMetaOnly = false;
  private boolean checkRegionBoundaries = false;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `errorTables` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    private static final int progressThreshold = 100;

    Set<HbckTableInfo> errorTables = new HashSet<>();

    // for use by unit tests to verify which errors were discovered
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `errorList` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

    // for use by unit tests to verify which errors were discovered
    private ArrayList<ERROR_CODE> errorList = new ArrayList<>();

    @Override
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `skippedRegions` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private final RetryCounterFactory createZNodeRetryCounterFactory;

  private Map<TableName, Set<String>> skippedRegions = new HashMap<>();

  private ZKWatcher zkw = null;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `connection` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HBaseFsck.class.getName());
  private ClusterMetrics status;
  private Connection connection;
  private Admin admin;
  private Table meta;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `details` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * Options
   ***********/
  private static boolean details = false; // do we display the full report
  private long timelag = DEFAULT_TIME_LAG; // tables whose modtime is older
  private static boolean forceExclusive = false; // only this hbck can modify HBase
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `tablesInfo` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * table
   */
  private SortedMap<TableName, HbckTableInfo> tablesInfo = new ConcurrentSkipListMap<>();

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `archiveExecutor` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
  };

  private static ThreadPoolExecutor archiveExecutor;

  private HFileArchiver() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `publisher` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
  private final int messagePeriod; // time between two message
  private final ConcurrentMap<ServerName, Integer> lastSent = new ConcurrentHashMap<>();
  private Publisher publisher;
  private boolean connected = false;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `connected` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
  private final ConcurrentMap<ServerName, Integer> lastSent = new ConcurrentHashMap<>();
  private Publisher publisher;
  private boolean connected = false;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `listeners` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java

  /** Listeners that are called on server events. */
  private List<ServerListener> listeners = new CopyOnWriteArrayList<>();

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `flushedSeqIdFlusher` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
  /** File on hdfs to store last flushed sequence id of regions */
  private static final String LAST_FLUSHED_SEQ_ID_FILE = ".lastflushedseqids";
  private FlushedSequenceIdFlusher flushedSeqIdFlusher;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `prevRegionsRecoveryInterval` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryConfigManager.java`
#### Snippet
```java
  private RegionsRecoveryChore chore;
  private int prevMaxStoreFileRefCount;
  private int prevRegionsRecoveryInterval;

  RegionsRecoveryConfigManager(final HMaster hMaster) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `chore` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryConfigManager.java`
#### Snippet
```java

  private final HMaster hMaster;
  private RegionsRecoveryChore chore;
  private int prevMaxStoreFileRefCount;
  private int prevRegionsRecoveryInterval;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `prevMaxStoreFileRefCount` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryConfigManager.java`
#### Snippet
```java
  private final HMaster hMaster;
  private RegionsRecoveryChore chore;
  private int prevMaxStoreFileRefCount;
  private int prevRegionsRecoveryInterval;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `fetchInProgress` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
  // When true, indicates that a FileSystem fetch of ClusterID is in progress. This is used to
  // avoid multiple fetches from FS and let only one thread fetch the information.
  AtomicBoolean fetchInProgress = new AtomicBoolean(false);

  // When true, it means that the cluster ID has been fetched successfully from fs.
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `serverManager` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/DrainingServerTracker.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(DrainingServerTracker.class);

  private ServerManager serverManager;
  private final NavigableSet<ServerName> drainingServers = new TreeSet<>();
  private Abortable abortable;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `cleanersChain` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
  protected final Map<String, Object> params;
  private final AtomicBoolean enabled = new AtomicBoolean(true);
  protected List<T> cleanersChain;
  protected List<String> excludeDirs;
  private CompletableFuture<Boolean> future;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `tableStateManager` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  // handle table states
  private TableStateManager tableStateManager;

  /** jetty server for master to redirect requests to regionserver infoServer */
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `balancer` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private final LockManager lockManager = new LockManager(this);

  private RSGroupBasedLoadBalancer balancer;
  private BalancerChore balancerChore;
  private static boolean disableBalancerChoreForTest = false;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `assignmentManager` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  // manager of assignment nodes in zookeeper
  private AssignmentManager assignmentManager;

  private RSGroupInfoManager rsGroupInfoManager;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `snapshotCleanerChore` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private ClusterStatusChore clusterStatusChore;
  private ClusterStatusPublisher clusterStatusPublisherChore = null;
  private SnapshotCleanerChore snapshotCleanerChore = null;

  private HbckChore hbckChore;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `snapshot` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SnapshotVerifyProcedure.class);

  private SnapshotDescription snapshot;
  private RegionInfo region;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `region` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java

  private SnapshotDescription snapshot;
  private RegionInfo region;

  private RetryCounter retryCounter;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `rootDir` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
  private WorkerAssigner verifyWorkerAssigner;

  private Path rootDir;
  private ExecutorService executorService;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `master` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java

  private boolean stopped;
  private MasterServices master; // Needed by TableEventHandlers
  private ProcedureCoordinator coordinator;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `takingSnapshotLock` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
   * start to work. (See HBASE-21387)
   */
  private ReentrantReadWriteLock takingSnapshotLock = new ReentrantReadWriteLock(true);

  public SnapshotManager() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `executorService` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java

  private Path rootDir;
  private ExecutorService executorService;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `threads` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  private long cleanerThreadTimeoutMsec;
  private long cleanerThreadCheckIntervalMsec;
  private List<Thread> threads = new ArrayList<Thread>();
  private volatile boolean running;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `snapshot` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.java`
#### Snippet
```java

  private Configuration conf;
  private SnapshotDescription snapshot;
  private Path rootDir;
  private Path snapshotDir;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `workingDir` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.java`
#### Snippet
```java
  private Path rootDir;
  private Path snapshotDir;
  private Path workingDir;
  private FileSystem workingDirFS;
  private FileSystem rootFs;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `workingDirFS` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.java`
#### Snippet
```java
  private Path snapshotDir;
  private Path workingDir;
  private FileSystem workingDirFS;
  private FileSystem rootFs;
  private TableName snapshotTable;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `started` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerSpaceQuotaManager.java`
#### Snippet
```java
  private SpaceQuotaRefresherChore spaceQuotaRefresher;
  private AtomicReference<Map<TableName, SpaceQuotaSnapshot>> currentQuotaSnapshots;
  private boolean started = false;
  private final ConcurrentHashMap<TableName, SpaceViolationPolicyEnforcement> enforcedPolicies;
  private SpaceViolationPolicyEnforcementFactory factory;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `regionSizeStore` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerSpaceQuotaManager.java`
#### Snippet
```java
  private final ConcurrentHashMap<TableName, SpaceViolationPolicyEnforcement> enforcedPolicies;
  private SpaceViolationPolicyEnforcementFactory factory;
  private RegionSizeStore regionSizeStore;
  private RegionSizeReportingChore regionSizeReporter;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `namespaceLimiters` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
      + "but to me it looks like they are totally synchronized")
public class UserQuotaState extends QuotaState {
  private Map<String, QuotaLimiter> namespaceLimiters = null;
  private Map<TableName, QuotaLimiter> tableLimiters = null;
  private boolean bypassGlobals = false;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `disableCompactions` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/NoWritesCompactionsViolationPolicyEnforcement.java`
#### Snippet
```java
    LoggerFactory.getLogger(NoWritesCompactionsViolationPolicyEnforcement.class);

  private AtomicBoolean disableCompactions = new AtomicBoolean(false);

  @Override
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `keysParentZNode` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java`
#### Snippet
```java
  private AuthenticationTokenSecretManager secretManager;
  private String baseKeyZNode;
  private String keysParentZNode;

  public ZKSecretWatcher(Configuration conf, ZKWatcher watcher,
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `leaderElector` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
  private long tokenMaxLifetime;
  private ZKSecretWatcher zkWatcher;
  private LeaderElector leaderElector;
  private ZKClusterId clusterId;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `allKeys` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
  private ZKClusterId clusterId;

  private Map<Integer, AuthenticationKey> allKeys = new ConcurrentHashMap<>();
  private AuthenticationKey currentKey;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `zkWatcher` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
  private long keyUpdateInterval;
  private long tokenMaxLifetime;
  private ZKSecretWatcher zkWatcher;
  private LeaderElector leaderElector;
  private ZKClusterId clusterId;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `provider` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java

  private MasterServices masterServices;
  private ClusterInfoProvider provider;
  private FavoredNodesManager favoredNodesManager;
  private volatile RSGroupInfoManager rsGroupInfoManager;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `masterServices` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(RSGroupBasedLoadBalancer.class);

  private MasterServices masterServices;
  private ClusterInfoProvider provider;
  private FavoredNodesManager favoredNodesManager;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `vlsClazzName` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelServiceManager.java`
#### Snippet
```java

  private volatile VisibilityLabelService visibilityLabelService = null;
  private String vlsClazzName = null;

  private VisibilityLabelServiceManager() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `instance` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
  private static final List<String> EMPTY_LIST = Collections.emptyList();
  private static final Set<Integer> EMPTY_SET = Collections.emptySet();
  private static VisibilityLabelsCache instance;

  private ZKVisibilityLabelWatcher zkVisibilityWatcher;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `name` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java`
#### Snippet
```java
@InterfaceAudience.Private
class NamespaceTableAndRegionInfo {
  private String name;
  private Map<TableName, AtomicInteger> tableAndRegionInfo;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `tableAndRegionInfo` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java`
#### Snippet
```java
class NamespaceTableAndRegionInfo {
  private String name;
  private Map<TableName, AtomicInteger> tableAndRegionInfo;

  public NamespaceTableAndRegionInfo(String namespace) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `nsStateCache` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(NamespaceStateManager.class);
  private ConcurrentMap<String, NamespaceTableAndRegionInfo> nsStateCache;
  private MasterServices master;
  private volatile boolean initialized = false;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `master` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(NamespaceStateManager.class);
  private ConcurrentMap<String, NamespaceTableAndRegionInfo> nsStateCache;
  private MasterServices master;
  private volatile boolean initialized = false;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `visibilityLabelService` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  private Map<InternalScanner, String> scannerOwners = new MapMaker().weakKeys().makeMap();

  private VisibilityLabelService visibilityLabelService;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `authorizationEnabled` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
   * true in site configuration
   */
  boolean authorizationEnabled;

  // Add to this list if there are any reserved tag types
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `member` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
  private final ZKProcedureUtil zkController;

  protected ProcedureMember member;
  private String memberName;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `procMap` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.java`
#### Snippet
```java
  private MasterServices master;
  private ProcedureCoordinator coordinator;
  private Map<TableName, Procedure> procMap = new HashMap<>();
  private boolean stopped;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `remotePort` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
public class MonitoredRPCHandlerImpl extends MonitoredTaskImpl implements MonitoredRPCHandler {
  private String clientAddress;
  private int remotePort;
  private long rpcQueueTime;
  private long rpcStartTime;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `clientAddress` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
@InterfaceAudience.Private
public class MonitoredRPCHandlerImpl extends MonitoredTaskImpl implements MonitoredRPCHandler {
  private String clientAddress;
  private int remotePort;
  private long rpcQueueTime;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `rpcQueueTime` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
  private String clientAddress;
  private int remotePort;
  private long rpcQueueTime;
  private long rpcStartTime;
  private String methodName = "";
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `packet` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
  private String methodName = "";
  private Object[] params = {};
  private Message packet;
  private boolean snapshot = false;
  private Map<String, Object> callInfoMap = new HashMap<>();
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `rpcStartTime` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
  private int remotePort;
  private long rpcQueueTime;
  private long rpcStartTime;
  private String methodName = "";
  private Object[] params = {};
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `queue` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerQueueService.java`
#### Snippet
```java
public class WALEventTrackerQueueService implements NamedQueueService {

  private EvictingQueue<WALEventTrackerPayload> queue;
  private static final String WAL_EVENT_TRACKER_RING_BUFFER_SIZE =
    "hbase.regionserver.wal.event.tracker.ringbuffer.size";
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `ratio` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java

  // Ratio of total number of potential peer region servers to be used
  private float ratio;

  // Maximum number of times a sink can be reported as bad before the pool of
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `badSinkThreshold` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
  // Maximum number of times a sink can be reported as bad before the pool of
  // replication sinks is refreshed
  private int badSinkThreshold;
  // Count of "bad replication sink" reports per peer sink
  private Map<ServerName, Integer> badReportCounts;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `badReportCounts` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
  private int badSinkThreshold;
  // Count of "bad replication sink" reports per peer sink
  private Map<ServerName, Integer> badReportCounts;

  private List<ServerName> sinkServers = new ArrayList<>(0);
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `sinkServers` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
  private Map<ServerName, Integer> badReportCounts;

  private List<ServerName> sinkServers = new ArrayList<>(0);

  /*
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `conn` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
  protected Configuration conf;

  private AsyncClusterConnection conn;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `metrics` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java`
#### Snippet
```java
  volatile boolean sourceRunning = false;
  // Metrics for this source
  private MetricsSource metrics;
  // ReplicationEndpoint which will handle the actual replication
  private volatile ReplicationEndpoint replicationEndpoint;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `startupOngoing` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java`
#### Snippet
```java
  // so that it doesn't try submit another initialize thread.
  // NOTE: this should only be set to false at the end of initialize method, prior to return.
  private AtomicBoolean startupOngoing = new AtomicBoolean(false);
  // Flag that signalizes uncaught error happening while starting up the source
  // and a retry should be attempted
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `coprocessorNames` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
   * in case of server crash (HBASE-4014).
   */
  private static Set<String> coprocessorNames = Collections.synchronizedSet(new HashSet<String>());

  public static Set<String> getLoadedCoprocessors() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `worker` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
  private static final int FAILED_TO_OWN_TASK = -1;

  private SplitLogWorker worker;

  private TaskExecutor splitTaskExecutor;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `metadataMap` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
   * which it is not modified again.
   */
  private Map<byte[], byte[]> metadataMap;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `firstKey` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java

  // firstKey, lastkey and cellComparator will be set when openReader.
  private Optional<Cell> firstKey;

  private Optional<Cell> lastKey;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `lastKey` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  private Optional<Cell> firstKey;

  private Optional<Cell> lastKey;

  private CellComparator comparator;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `maxMemstoreTS` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // max of the MemstoreTS in the KV's in this store
  // Set when we obtain a Reader.
  private long maxMemstoreTS = -1;

  // firstKey, lastkey and cellComparator will be set when openReader.
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `excludeFromMinorCompaction` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // If true, this file should not be included in minor compactions.
  // It's set whenever you get a Reader.
  private boolean excludeFromMinorCompaction = false;

  // This file was product of these compacted store files
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `comparator` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  private Optional<Cell> lastKey;

  private CellComparator comparator;

  public CacheConfig getCacheConf() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `majorCompaction` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // If true, this file was product of a major compaction. Its then set
  // whenever you get a Reader.
  private AtomicBoolean majorCompaction = null;

  // If true, this file should not be included in minor compactions.
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `sequenceid` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // Keys for metadata stored in backing HFile.
  // Set when we obtain a Reader.
  private long sequenceid = -1;

  // max of the MemstoreTS in the KV's in this store
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `streamUnsupported` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/InputStreamBlockDistribution.java`
#### Snippet
```java
  private HDFSBlocksDistribution hdfsBlocksDistribution;
  private long lastCachedAt;
  private boolean streamUnsupported;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `region` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BusyRegionSplitPolicy.java`
#### Snippet
```java
  public static final long DEFAULT_AGGREGATION_WINDOW = 300000; // 5 minutes

  private HRegion region;
  private long prevTime;
  private long startTime;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `startTime` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BusyRegionSplitPolicy.java`
#### Snippet
```java
  private HRegion region;
  private long prevTime;
  private long startTime;
  private long writeRequestCount;
  private long blockedRequestCount;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `aggregationWindow` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BusyRegionSplitPolicy.java`
#### Snippet
```java

  // The window time in milliseconds over which the blocked requests rate is calculated
  private long aggregationWindow;
  public static final long DEFAULT_AGGREGATION_WINDOW = 300000; // 5 minutes

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `regionSplitLimit` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
   * regions is greater than this.
   */
  private int regionSplitLimit;

  CompactSplit(HRegionServer server) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `conflictWaitIterationMs` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java
   * that tests could override it and not wait.
   */
  private int conflictWaitIterationMs = 30000;

  private static final SimpleDateFormat tsFormat = new SimpleDateFormat("HH:mm:ss.SSS");
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `nonces` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java
   * it's much worse, we could use some sort of memory limit and cleanup.
   */
  private ConcurrentHashMap<NonceKey, OperationContext> nonces = new ConcurrentHashMap<>();

  private int deleteNonceGracePeriod;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `joinedHeap` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
   * if on-demand column family loading is enabled.
   */
  KeyValueHeap joinedHeap = null;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `storeHeap` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java

  // Package local for testability
  KeyValueHeap storeHeap = null;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `mobFileCache` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private BlockCache l1Cache = null;
  private BlockCache l2Cache = null;
  private MobFileCache mobFileCache;
  private CacheStats cacheStats;
  private CacheStats l1Stats = null;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `dataChunksPool` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
  static ChunkCreator instance;
  static boolean chunkPoolDisabled = false;
  private MemStoreChunkPool dataChunksPool;
  private final int chunkSize;
  private int indexChunkSize;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `chunkIdMap` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java

  // mapping from chunk IDs to chunks
  private Map<Integer, Chunk> chunkIdMap = new ConcurrentHashMap<Integer, Chunk>();

  private final boolean offheap;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `indexChunksPool` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
  private final int chunkSize;
  private int indexChunkSize;
  private MemStoreChunkPool indexChunksPool;

  ChunkCreator(int chunkSize, boolean offheap, long globalMemStoreSize, float poolSizePercentage,
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `rpcClient` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  private volatile LockService.BlockingInterface lockStub;
  // RPC client. Used to make the stub above that does region server status checking.
  private RpcClient rpcClient;

  private UncaughtExceptionHandler uncaughtExceptionHandler;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `start` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.java`
#### Snippet
```java
  private final Timer timer;
  private final TimerTask timerTask;
  private long start = -1;

  /**
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `lastFlushedSequenceId` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationSink.java`
#### Snippet
```java
  private volatile long pendingSize;

  private long lastFlushedSequenceId;

  private boolean sending;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `conf` is accessed in both synchronized and unsynchronized contexts
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
public class ThriftConnection implements Connection {
  private static final Logger LOG = LoggerFactory.getLogger(ThriftConnection.class);
  private Configuration conf;
  private User user;
  // For HTTP protocol
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `isClosed` is accessed in both synchronized and unsynchronized contexts
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  private HttpClient httpClient;
  private boolean httpClientCreated = false;
  private boolean isClosed = false;

  private String host;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `cause` is accessed in both synchronized and unsynchronized contexts
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  private static class TIOErrorWithCause extends TIOError {
    private Throwable cause;

    public TIOErrorWithCause(Throwable cause) {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `scannerMap` is accessed in both synchronized and unsynchronized contexts
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  // nextScannerId and scannerMap are used to manage scanner state
  private int nextScannerId = 0;
  private Cache<Integer, ResultScannerWrapper> scannerMap;
  IncrementCoalescer coalescer;

```

### FieldAccessedSynchronizedAndUnsynchronized
Field `prepareFlushResult` is accessed in both synchronized and unsynchronized contexts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  /** Saved state from replaying prepare flush cache */
  private PrepareFlushResult prepareFlushResult = null;

  private long lastReplayedSequenceId = HConstants.NO_SEQNUM;
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `max` is accessed in both synchronized and unsynchronized contexts
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
    class MaxCallBack implements Batch.Callback<R> {
      R max = null;

      R getMax() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `sumVal` is accessed in both synchronized and unsynchronized contexts
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java

    class SumCallBack implements Batch.Callback<S> {
      S sumVal = null;

      public S getSumResult() {
```

### FieldAccessedSynchronizedAndUnsynchronized
Field `min` is accessed in both synchronized and unsynchronized contexts
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
    class MinCallBack implements Batch.Callback<R> {
      private R min = null;

      public R getMinimum() {
```

## RuleId[id=EqualsAndHashcode]
### EqualsAndHashcode
Class has `equals()` defined but does not define `hashCode()`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMutableByteRange.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class SimpleMutableByteRange extends AbstractByteRange {

  /**
```

### EqualsAndHashcode
Class has `equals()` defined but does not define `hashCode()`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleByteRange.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class SimpleByteRange extends AbstractByteRange {
  public SimpleByteRange() {
  }
```

### EqualsAndHashcode
Class has `hashCode()` defined but does not define `equals()`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/StartcodeAgnosticServerName.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
class StartcodeAgnosticServerName extends ServerName {

  public StartcodeAgnosticServerName(final String hostname, final int port, long startcode) {
```

## RuleId[id=PointlessBitwiseExpression]
### PointlessBitwiseExpression
`1 << 0` can be replaced with '1'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java`
#### Snippet
```java
@InterfaceAudience.Private
public class FastDiffDeltaEncoder extends BufferedDataBlockEncoder {
  static final int MASK_TIMESTAMP_LENGTH = (1 << 0) | (1 << 1) | (1 << 2);
  static final int SHIFT_TIMESTAMP_LENGTH = 0;
  static final int FLAG_SAME_KEY_LENGTH = 1 << 3;
```

### PointlessBitwiseExpression
`i ^ -1L` can be replaced with '\~i'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
      i = i | (b & 0xFF);
    }
    return (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);
  }

```

### PointlessBitwiseExpression
`~(stride - 1)` can be replaced with '-stride'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
    final int stride = 8;
    final int minLength = Math.min(l1, l2);
    int strideLimit = minLength & ~(stride - 1);
    int i;

```

### PointlessBitwiseExpression
`~(stride - 1)` can be replaced with '-stride'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
        final int stride = 8;
        final int minLength = Math.min(length1, length2);
        int strideLimit = minLength & ~(stride - 1);
        final long offset1Adj = offset1 + UnsafeAccess.BYTE_ARRAY_BASE_OFFSET;
        final long offset2Adj = offset2 + UnsafeAccess.BYTE_ARRAY_BASE_OFFSET;
```

## RuleId[id=CallToStringConcatCanBeReplacedByOperator]
### CallToStringConcatCanBeReplacedByOperator
Call to `concat()` can be replaced with '+' expression
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/field/FieldValue.java`
#### Snippet
```java
    switch (type) {
      case STRING:
        return new FieldValue(((String) value).concat((String) o.value), type);

      case INTEGER:
```

## RuleId[id=DuplicateThrows]
### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public void snapshot(SnapshotDescription snapshot)
    throws IOException, SnapshotCreationException, IllegalArgumentException {
    get(admin.snapshot(snapshot));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public NamespaceDescriptor getNamespaceDescriptor(String name)
    throws NamespaceNotFoundException, IOException {
    return get(admin.getNamespaceDescriptor(name));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java

  @Override
  public void restoreSnapshot(String snapshotName) throws IOException, RestoreSnapshotException {
    get(admin.restoreSnapshot(snapshotName));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public boolean isSnapshotFinished(SnapshotDescription snapshot)
    throws IOException, HBaseSnapshotException, UnknownSnapshotException {
    return get(admin.isSnapshotFinished(snapshot));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public boolean isSnapshotFinished(SnapshotDescription snapshot)
    throws IOException, HBaseSnapshotException, UnknownSnapshotException {
    return get(admin.isSnapshotFinished(snapshot));
  }
```

### DuplicateThrows
There is a more general exception, 'org.apache.hadoop.hbase.snapshot.HBaseSnapshotException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public boolean isSnapshotFinished(SnapshotDescription snapshot)
    throws IOException, HBaseSnapshotException, UnknownSnapshotException {
    return get(admin.isSnapshotFinished(snapshot));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public Future<Void> snapshotAsync(SnapshotDescription snapshot)
    throws IOException, SnapshotCreationException {
    return admin.snapshot(snapshot);
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public TableDescriptor getDescriptor(TableName tableName)
    throws TableNotFoundException, IOException {
    return get(admin.getDescriptor(tableName));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  public Future<Void> cloneSnapshotAsync(String snapshotName, TableName tableName,
    boolean restoreAcl, String customSFT)
    throws IOException, TableExistsException, RestoreSnapshotException {
    return admin.cloneSnapshot(snapshotName, tableName, restoreAcl, customSFT);
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  public Future<Void> cloneSnapshotAsync(String snapshotName, TableName tableName,
    boolean restoreAcl, String customSFT)
    throws IOException, TableExistsException, RestoreSnapshotException {
    return admin.cloneSnapshot(snapshotName, tableName, restoreAcl, customSFT);
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public void restoreSnapshot(String snapshotName, boolean takeFailSafeSnapshot, boolean restoreAcl)
    throws IOException, RestoreSnapshotException {
    get(admin.restoreSnapshot(snapshotName, takeFailSafeSnapshot, restoreAcl));
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java

  @Override
  public void rollWALWriter(ServerName serverName) throws IOException, FailedLogCloseException {
    get(admin.rollWALWriter(serverName));
  }
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
  default <R extends Message> void batchCoprocessorService(
    Descriptors.MethodDescriptor methodDescriptor, Message request, byte[] startKey, byte[] endKey,
    R responsePrototype, Batch.Callback<R> callback) throws ServiceException, Throwable {
    throw new NotImplementedException("Add an implementation!");
  }
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
  default <T extends Service, R> void coprocessorService(final Class<T> service, byte[] startKey,
    byte[] endKey, final Batch.Call<T, R> callable, final Batch.Callback<R> callback)
    throws ServiceException, Throwable {
    throw new NotImplementedException("Add an implementation!");
  }
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
  default <T extends Service, R> Map<byte[], R> coprocessorService(final Class<T> service,
    byte[] startKey, byte[] endKey, final Batch.Call<T, R> callable)
    throws ServiceException, Throwable {
    Map<byte[], R> results =
      Collections.synchronizedMap(new TreeMap<byte[], R>(Bytes.BYTES_COMPARATOR));
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
  default <R extends Message> Map<byte[], R> batchCoprocessorService(
    Descriptors.MethodDescriptor methodDescriptor, Message request, byte[] startKey, byte[] endKey,
    R responsePrototype) throws ServiceException, Throwable {
    final Map<byte[], R> results =
      Collections.synchronizedMap(new TreeMap<byte[], R>(Bytes.BYTES_COMPARATOR));
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  public <R extends Message> void batchCoprocessorService(MethodDescriptor methodDescriptor,
    Message request, byte[] startKey, byte[] endKey, R responsePrototype,
    Batch.Callback<R> callback) throws ServiceException, Throwable {
    final Supplier<Span> supplier = new TableOperationSpanBuilder(conn)
      .setTableName(table.getName()).setOperation(HBaseSemanticAttributes.Operation.COPROC_EXEC);
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
    byte[] endKey, Batch.Call<T, R> callable, Batch.Callback<R> callback)
    throws ServiceException, Throwable {
    final Supplier<Span> supplier = new TableOperationSpanBuilder(conn)
      .setTableName(table.getName()).setOperation(HBaseSemanticAttributes.Operation.COPROC_EXEC);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java`
#### Snippet
```java

  public static boolean isAccessControllerRunning(Connection connection)
    throws MasterNotRunningException, ZooKeeperConnectionException, IOException {
    try (Admin admin = connection.getAdmin()) {
      return admin.isTableAvailable(ACL_TABLE_NAME);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java`
#### Snippet
```java

  public static boolean isAccessControllerRunning(Connection connection)
    throws MasterNotRunningException, ZooKeeperConnectionException, IOException {
    try (Admin admin = connection.getAdmin()) {
      return admin.isTableAvailable(ACL_TABLE_NAME);
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java`
#### Snippet
```java
  private static VisibilityLabelsResponse setOrClearAuths(Connection connection,
    final String[] auths, final String user, final boolean setOrClear)
    throws IOException, ServiceException, Throwable {

    try (Table table = connection.getTable(LABELS_TABLE_NAME)) {
```

### DuplicateThrows
There is a more general exception, 'java.lang.Throwable', in the throws list already.
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java`
#### Snippet
```java
  private static VisibilityLabelsResponse setOrClearAuths(Connection connection,
    final String[] auths, final String user, final boolean setOrClear)
    throws IOException, ServiceException, Throwable {

    try (Table table = connection.getTable(LABELS_TABLE_NAME)) {
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java`
#### Snippet
```java
  }

  private TableDescriptor getTableSchema() throws IOException, TableNotFoundException {
    try (Table table = servlet.getTable(tableResource.getName())) {
      return table.getDescriptor();
```

### DuplicateThrows
There is a more general exception, 'org.apache.zookeeper.KeeperException', in the throws list already.
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  public static void setData(ZKWatcher zkw, String znode, byte[] data)
    throws KeeperException, KeeperException.NoNodeException {
    setData(zkw, (SetData) ZKUtilOp.setData(znode, data));
  }
```

### DuplicateThrows
There is a more general exception, 'org.apache.zookeeper.KeeperException', in the throws list already.
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java

  private static void setData(ZKWatcher zkw, SetData setData)
    throws KeeperException, KeeperException.NoNodeException {
    SetDataRequest sd = (SetDataRequest) toZooKeeperOp(zkw, setData).toRequestRecord();
    setData(zkw, sd.getPath(), sd.getData(), sd.getVersion());
```

### DuplicateThrows
There is a more general exception, 'org.apache.zookeeper.KeeperException', in the throws list already.
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  public static boolean setData(ZKWatcher zkw, String znode, byte[] data, int expectedVersion)
    throws KeeperException, KeeperException.NoNodeException {
    try {
      return zkw.getRecoverableZooKeeper().setData(znode, data, expectedVersion) != null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.zookeeper.KeeperException', in the throws list already.
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java

  private static void processSequentially(ZKWatcher zkw, List<ZKUtilOp> ops)
    throws KeeperException, NoNodeException {
    for (ZKUtilOp op : ops) {
      if (op instanceof CreateAndFailSilent) {
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java`
#### Snippet
```java
  @Override
  public void checkOutputSpecs(FileSystem ignored, JobConf job)
    throws FileAlreadyExistsException, InvalidJobConfException, IOException {
    String tableName = job.get(OUTPUT_TABLE);
    if (tableName == null) {
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java`
#### Snippet
```java
  @Override
  public void checkOutputSpecs(FileSystem ignored, JobConf job)
    throws FileAlreadyExistsException, InvalidJobConfException, IOException {
    String tableName = job.get(OUTPUT_TABLE);
    if (tableName == null) {
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
      @Override
      public FanOutOneBlockAsyncDFSOutput doCall(Path p)
        throws IOException, UnresolvedLinkException {
        return createOutput(dfs, p.toUri().getPath(), overwrite, createParent, replication,
          blockSize, eventLoopGroup, channelClass, monitor);
```

### DuplicateThrows
Duplicate throws
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java

  public static MBeanAttributeInfo[] getMBeanAttributeInfo(ObjectName bean)
    throws IntrospectionException, InstanceNotFoundException, ReflectionException,
    IntrospectionException, javax.management.IntrospectionException {
    MBeanInfo mbinfo = mbServer.getMBeanInfo(bean);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java

  private void initializeWebServer(String name, String hostName, Configuration conf,
    String[] pathSpecs, HttpServer.Builder b) throws FileNotFoundException, IOException {

    Preconditions.checkNotNull(webAppContext);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WAL.java`
#### Snippet
```java
   *         {@link RegionInfo#getEncodedName()}
   */
  Map<byte[], List<byte[]>> rollWriter() throws FailedLogCloseException, IOException;

  /**
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
   */
  Map<LoadQueueItem, ByteBuffer> bulkLoad(TableName tableName, Path dir)
    throws TableNotFoundException, IOException;

  static BulkLoadHFiles create(Configuration conf) {
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFiles.java`
#### Snippet
```java
   */
  Map<LoadQueueItem, ByteBuffer> bulkLoad(TableName tableName, Map<byte[], List<Path>> family2Files)
    throws TableNotFoundException, IOException;

  /**
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java`
#### Snippet
```java
   */
  private static void forceOfflineInZK(Admin admin, final RegionInfo region)
    throws ZooKeeperConnectionException, KeeperException, IOException, InterruptedException {
    admin.assign(region.getRegionName());
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * Constructor Configuration object if the master is not running if unable to connect to ZooKeeper
   */
  public HBaseFsck(Configuration conf, ExecutorService exec) throws MasterNotRunningException,
    ZooKeeperConnectionException, IOException, ClassNotFoundException {
    super(conf);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   */
  public HBaseFsck(Configuration conf, ExecutorService exec) throws MasterNotRunningException,
    ZooKeeperConnectionException, IOException, ClassNotFoundException {
    super(conf);
    errors = getErrorReporter(getConf());
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java`
#### Snippet
```java

  public HFileArchiveManager(Connection connection, Configuration conf)
    throws ZooKeeperConnectionException, IOException {
    this.zooKeeper =
      new ZKWatcher(conf, "hfileArchiveManager-on-" + connection.toString(), connection);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
  // We actually throw the exceptions mentioned in the
  void checkTableModifiable(final TableName tableName)
    throws IOException, TableNotFoundException, TableNotDisabledException;

  /**
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
  // We actually throw the exceptions mentioned in the
  void checkTableModifiable(final TableName tableName)
    throws IOException, TableNotFoundException, TableNotDisabledException;

  /**
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  @Override
  public void checkTableModifiable(final TableName tableName)
    throws IOException, TableNotFoundException, TableNotDisabledException {
    if (isCatalogTable(tableName)) {
      throw new IOException("Can't modify catalog tables");
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  @Override
  public void checkTableModifiable(final TableName tableName)
    throws IOException, TableNotFoundException, TableNotDisabledException {
    if (isCatalogTable(tableName)) {
      throw new IOException("Can't modify catalog tables");
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  private void checkTableExists(final TableName tableName)
    throws IOException, TableNotFoundException {
    if (!tableDescriptors.exists(tableName)) {
      throw new TableNotFoundException(tableName);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java`
#### Snippet
```java
   */
  public OperationQuota checkQuota(final Region region, final List<ClientProtos.Action> actions)
    throws IOException, RpcThrottlingException {
    int numWrites = 0;
    int numReads = 0;
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java`
#### Snippet
```java
   */
  public OperationQuota checkQuota(final Region region, final OperationQuota.OperationType type)
    throws IOException, RpcThrottlingException {
    switch (type) {
      case SCAN:
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java`
#### Snippet
```java
   */
  private OperationQuota checkQuota(final Region region, final int numWrites, final int numReads,
    final int numScans) throws IOException, RpcThrottlingException {
    Optional<User> user = RpcServer.getRequestUser();
    UserGroupInformation ugi;
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
  private static void getLabelOrdinals(ExpressionNode node, List<Integer> labelOrdinals,
    Set<Integer> auths, boolean checkAuths, VisibilityLabelOrdinalProvider ordinalProvider)
    throws IOException, InvalidLabelException {
    if (node.isSingleNode()) {
      String identifier = null;
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DualAsyncFSWAL.java`
#### Snippet
```java
    String logDir, String archiveDir, Configuration conf, List<WALActionsListener> listeners,
    boolean failIfWALExists, String prefix, String suffix, EventLoopGroup eventLoopGroup,
    Class<? extends Channel> channelClass) throws FailedLogCloseException, IOException {
    super(fs, rootDir, logDir, archiveDir, conf, listeners, failIfWALExists, prefix, suffix,
      eventLoopGroup, channelClass);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
    boolean failIfWALExists, String prefix, String suffix, EventLoopGroup eventLoopGroup,
    Class<? extends Channel> channelClass, StreamSlowMonitor monitor)
    throws FailedLogCloseException, IOException {
    super(fs, abortable, rootDir, logDir, archiveDir, conf, listeners, failIfWALExists, prefix,
      suffix);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
    Configuration conf, List<WALActionsListener> listeners, boolean failIfWALExists, String prefix,
    String suffix, EventLoopGroup eventLoopGroup, Class<? extends Channel> channelClass)
    throws FailedLogCloseException, IOException {
    this(fs, null, rootDir, logDir, archiveDir, conf, listeners, failIfWALExists, prefix, suffix,
      eventLoopGroup, channelClass, StreamSlowMonitor.create(conf, "monitorForSuffix"));
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java

  @Override
  public Map<byte[], List<byte[]>> rollWriter() throws FailedLogCloseException, IOException {
    return rollWriter(false);
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    final String archiveDir, final Configuration conf, final List<WALActionsListener> listeners,
    final boolean failIfWALExists, final String prefix, final String suffix)
    throws FailedLogCloseException, IOException {
    this(fs, null, rootDir, logDir, archiveDir, conf, listeners, failIfWALExists, prefix, suffix);
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    final String logDir, final String archiveDir, final Configuration conf,
    final List<WALActionsListener> listeners, final boolean failIfWALExists, final String prefix,
    final String suffix) throws FailedLogCloseException, IOException {
    this.fs = fs;
    this.walDir = new Path(rootDir, logDir);
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java`
#### Snippet
```java
  @Override
  public TableDescriptor getDescriptor(TableName tableName)
    throws TableNotFoundException, IOException {
    TTableName tTableName = ThriftUtilities.tableNameFromHBase(tableName);
    try {
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java`
#### Snippet
```java
  @Override
  public Future<Void> cloneSnapshotAsync(String snapshotName, TableName tableName, boolean cloneAcl,
    String customSFT) throws IOException, TableExistsException, RestoreSnapshotException {
    throw new NotImplementedException("cloneSnapshotAsync not supported in ThriftAdmin");
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java`
#### Snippet
```java
  @Override
  public Future<Void> cloneSnapshotAsync(String snapshotName, TableName tableName, boolean cloneAcl,
    String customSFT) throws IOException, TableExistsException, RestoreSnapshotException {
    throw new NotImplementedException("cloneSnapshotAsync not supported in ThriftAdmin");
  }
```

### DuplicateThrows
There is a more general exception, 'java.io.IOException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java`
#### Snippet
```java
  @Override
  public NamespaceDescriptor getNamespaceDescriptor(String name)
    throws NamespaceNotFoundException, IOException {
    try {
      TNamespaceDescriptor tNamespaceDescriptor = client.getNamespaceDescriptor(name);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<Boolean> clearSlowLogResponses(Set<TServerName> tServerNames)
    throws TIOError, TException {
    Set<ServerName> serverNames = ThriftUtilities.getServerNamesFromThrift(tServerNames);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TTableDescriptor> getTableDescriptorsByPattern(String regex, boolean includeSysTables)
    throws TIOError, TException {
    try {
      Pattern pattern = (regex == null ? null : Pattern.compile(regex));
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TTableDescriptor> getTableDescriptors(List<TTableName> tables)
    throws TIOError, TException {
    try {
      List<TableName> tableNames = ThriftUtilities.tableNamesFromThrift(tables);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public boolean isTableAvailableWithSplit(TTableName tableName, List<ByteBuffer> splitKeys)
    throws TIOError, TException {
    throw new NotImplementedException("isTableAvailableWithSplit not supported");
  }
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean grant(TAccessControlEntity info) throws TIOError, TException {
    Permission.Action[] actions = ThriftUtilities.permissionActionsFromString(info.actions);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TOnlineLogRecord> getSlowLogResponses(Set<TServerName> tServerNames,
    TLogQueryFilter tLogQueryFilter) throws TIOError, TException {
    try {
      Set<ServerName> serverNames = ThriftUtilities.getServerNamesFromThrift(tServerNames);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public TNamespaceDescriptor getNamespaceDescriptor(String name) throws TIOError, TException {
    try {
      NamespaceDescriptor descriptor = connectionCache.getAdmin().getNamespaceDescriptor(name);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TTableDescriptor> getTableDescriptorsByNamespace(String name)
    throws TIOError, TException {
    try {
      List<TableDescriptor> descriptors =
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void createNamespace(TNamespaceDescriptor namespaceDesc) throws TIOError, TException {
    try {
      NamespaceDescriptor descriptor = namespaceDescriptorFromThrift(namespaceDesc);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void put(ByteBuffer table, TPut put) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TDelete> deleteMultiple(ByteBuffer table, List<TDelete> deletes)
    throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void deleteNamespace(String name) throws TIOError, TException {
    try {
      connectionCache.getAdmin().deleteNamespace(name);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void mutateRow(ByteBuffer table, TRowMutations rowMutations) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void enableTable(TTableName tableName) throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void deleteSingle(ByteBuffer table, TDelete deleteSingle) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public boolean checkAndDelete(ByteBuffer table, ByteBuffer row, ByteBuffer family,
    ByteBuffer qualifier, ByteBuffer value, TDelete deleteSingle) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<TNamespaceDescriptor> listNamespaceDescriptors() throws TIOError, TException {
    try {
      NamespaceDescriptor[] descriptors = connectionCache.getAdmin().listNamespaceDescriptors();
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean isTableDisabled(TTableName tableName) throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TResult> getScannerResults(ByteBuffer table, TScan scan, int numRows)
    throws TIOError, TException {
    Table htable = getTable(table);
    List<TResult> results = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public int openScanner(ByteBuffer table, TScan scan) throws TIOError, TException {
    Table htable = getTable(table);
    ResultScanner resultScanner = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void putMultiple(ByteBuffer table, List<TPut> puts) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public TResult get(ByteBuffer table, TGet get) throws TIOError, TException {
    Table htable = getTable(table);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void addColumnFamily(TTableName tableName, TColumnFamilyDescriptor column)
    throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<String> listNamespaces() throws TIOError, TException {
    try {
      String[] namespaces = connectionCache.getAdmin().listNamespaces();
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  public boolean checkAndMutate(ByteBuffer table, ByteBuffer row, ByteBuffer family,
    ByteBuffer qualifier, TCompareOperator compareOp, ByteBuffer value, TRowMutations rowMutations)
    throws TIOError, TException {
    checkReadOnlyMode();
    try (final Table htable = getTable(table)) {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<THRegionLocation> getAllRegionLocations(ByteBuffer table)
    throws TIOError, TException {
    RegionLocator locator = null;
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public TResult increment(ByteBuffer table, TIncrement increment) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean isTableAvailable(TTableName tableName) throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void modifyColumnFamily(TTableName tableName, TColumnFamilyDescriptor column)
    throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void deleteTable(TTableName tableName) throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void disableTable(TTableName tableName) throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public boolean checkAndPut(ByteBuffer table, ByteBuffer row, ByteBuffer family,
    ByteBuffer qualifier, ByteBuffer value, TPut put) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<TResult> getMultiple(ByteBuffer table, List<TGet> gets) throws TIOError, TException {
    Table htable = getTable(table);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void createTable(TTableDescriptor desc, List<ByteBuffer> splitKeys)
    throws TIOError, TException {
    try {
      TableDescriptor descriptor = tableDescriptorFromThrift(desc);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void closeScanner(int scannerId) throws TIOError, TIllegalArgument, TException {
    LOG.debug("scannerClose: id=" + scannerId);
    ResultScanner scanner = getScanner(scannerId);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void closeScanner(int scannerId) throws TIOError, TIllegalArgument, TException {
    LOG.debug("scannerClose: id=" + scannerId);
    ResultScanner scanner = getScanner(scannerId);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void deleteColumnFamily(TTableName tableName, ByteBuffer column)
    throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean revoke(TAccessControlEntity info) throws TIOError, TException {
    Permission.Action[] actions = ThriftUtilities.permissionActionsFromString(info.actions);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TTableName> getTableNamesByPattern(String regex, boolean includeSysTables)
    throws TIOError, TException {
    try {
      Pattern pattern = (regex == null ? null : Pattern.compile(regex));
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<TTableName> getTableNamesByNamespace(String name) throws TIOError, TException {
    try {
      TableName[] tableNames = connectionCache.getAdmin().listTableNamesByNamespace(name);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void modifyTable(TTableDescriptor desc) throws TIOError, TException {
    try {
      TableDescriptor descriptor = tableDescriptorFromThrift(desc);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean isTableEnabled(TTableName tableName) throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public TTableDescriptor getTableDescriptor(TTableName table) throws TIOError, TException {
    try {
      TableName tableName = ThriftUtilities.tableNameFromThrift(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public TResult append(ByteBuffer table, TAppend append) throws TIOError, TException {
    checkReadOnlyMode();
    Table htable = getTable(table);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean tableExists(TTableName tTableName) throws TIOError, TException {
    try {
      TableName tableName = tableNameFromThrift(tTableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<Boolean> existsAll(ByteBuffer table, List<TGet> gets) throws TIOError, TException {
    Table htable = getTable(table);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean exists(ByteBuffer table, TGet get) throws TIOError, TException {
    Table htable = getTable(table);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void truncateTable(TTableName tableName, boolean preserveSplits)
    throws TIOError, TException {
    try {
      TableName table = tableNameFromThrift(tableName);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public THRegionLocation getRegionLocation(ByteBuffer table, ByteBuffer row, boolean reload)
    throws TIOError, TException {

    RegionLocator locator = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TResult> getScannerRows(int scannerId, int numRows)
    throws TIOError, TIllegalArgument, TException {
    ResultScanner scanner = getScanner(scannerId);
    if (scanner == null) {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TResult> getScannerRows(int scannerId, int numRows)
    throws TIOError, TIllegalArgument, TException {
    ResultScanner scanner = getScanner(scannerId);
    if (scanner == null) {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void modifyNamespace(TNamespaceDescriptor namespaceDesc) throws TIOError, TException {
    try {
      NamespaceDescriptor descriptor = namespaceDescriptorFromThrift(namespaceDesc);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean grant(TAccessControlEntity info) throws IOError, TException {
    Permission.Action[] actions = ThriftUtilities.permissionActionsFromString(info.actions);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  protected long atomicIncrement(ByteBuffer tableName, ByteBuffer row, byte[] family,
    byte[] qualifier, long amount) throws IOError, IllegalArgument, TException {
    Table table = null;
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  protected long atomicIncrement(ByteBuffer tableName, ByteBuffer row, byte[] family,
    byte[] qualifier, long amount) throws IOError, IllegalArgument, TException {
    Table table = null;
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public Map<ByteBuffer, ColumnDescriptor> getColumnDescriptors(ByteBuffer tableName)
    throws IOError, TException {

    Table table = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public int scannerOpenWithPrefix(ByteBuffer tableName, ByteBuffer startAndPrefix,
    List<ByteBuffer> columns, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, TException {

    Table table = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void mutateRowsTs(ByteBuffer tableName, List<BatchMutation> rowBatches, long timestamp,
    Map<ByteBuffer, ByteBuffer> attributes) throws IOError, IllegalArgument, TException {
    List<Put> puts = new ArrayList<>();
    List<Delete> deletes = new ArrayList<>();
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void mutateRowsTs(ByteBuffer tableName, List<BatchMutation> rowBatches, long timestamp,
    Map<ByteBuffer, ByteBuffer> attributes) throws IOError, IllegalArgument, TException {
    List<Put> puts = new ArrayList<>();
    List<Delete> deletes = new ArrayList<>();
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void incrementRows(List<TIncrement> tincrements) throws IOError, TException {
    if (conf.getBoolean(COALESCE_INC_KEY, false)) {
      this.coalescer.queueIncrements(tincrements);
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  public int scannerOpenWithStopTs(ByteBuffer tableName, ByteBuffer startRow, ByteBuffer stopRow,
    List<ByteBuffer> columns, long timestamp, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError, TException {

    Table table = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void mutateRows(ByteBuffer tableName, List<BatchMutation> rowBatches,
    Map<ByteBuffer, ByteBuffer> attributes) throws IOError, IllegalArgument, TException {
    mutateRowsTs(tableName, rowBatches, HConstants.LATEST_TIMESTAMP, attributes);
  }
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public void mutateRows(ByteBuffer tableName, List<BatchMutation> rowBatches,
    Map<ByteBuffer, ByteBuffer> attributes) throws IOError, IllegalArgument, TException {
    mutateRowsTs(tableName, rowBatches, HConstants.LATEST_TIMESTAMP, attributes);
  }
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public int scannerOpenTs(ByteBuffer tableName, ByteBuffer startRow, List<ByteBuffer> columns,
    long timestamp, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, TException {

    Table table = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  public boolean checkAndPut(ByteBuffer tableName, ByteBuffer row, ByteBuffer column,
    ByteBuffer value, Mutation mput, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError, IllegalArgument, TException {
    Put put;
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  public boolean checkAndPut(ByteBuffer tableName, ByteBuffer row, ByteBuffer column,
    ByteBuffer value, Mutation mput, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError, IllegalArgument, TException {
    Put put;
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public long atomicIncrement(ByteBuffer tableName, ByteBuffer row, ByteBuffer column, long amount)
    throws IOError, IllegalArgument, TException {
    byte[][] famAndQf = CellUtil.parseColumn(getBytes(column));
    if (famAndQf.length == 1) {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public long atomicIncrement(ByteBuffer tableName, ByteBuffer row, ByteBuffer column, long amount)
    throws IOError, IllegalArgument, TException {
    byte[][] famAndQf = CellUtil.parseColumn(getBytes(column));
    if (famAndQf.length == 1) {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public boolean revoke(TAccessControlEntity info) throws IOError, TException {
    Permission.Action[] actions = ThriftUtilities.permissionActionsFromString(info.actions);
    try {
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public int scannerOpenWithStop(ByteBuffer tableName, ByteBuffer startRow, ByteBuffer stopRow,
    List<ByteBuffer> columns, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, TException {

    Table table = null;
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<TCell> append(TAppend tappend) throws IOError, TException {
    if (tappend.getRow().length == 0 || tappend.getTable().length == 0) {
      throw new TException("Must supply a table and a row key; can't append");
```

### DuplicateThrows
There is a more general exception, 'org.apache.thrift.TException', in the throws list already.
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void increment(TIncrement tincrement) throws IOError, TException {

    if (tincrement.getRow().length == 0 || tincrement.getTable().length == 0) {
```

## RuleId[id=SynchronizeOnThis]
### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java`
#### Snippet
```java
    // always lock chore first to prevent dead lock
    synchronized (chore) {
      synchronized (this) {
        try {
          // Chores should only ever be scheduled with a single ChoreService. If the choreService
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/FileChangeWatcher.java`
#### Snippet
```java
  private synchronized void setState(State newState) {
    state = newState;
    this.notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/FileChangeWatcher.java`
#### Snippet
```java
  synchronized void waitForState(State desiredState) throws InterruptedException {
    while (this.state != desiredState) {
      this.wait();
    }
  }
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/DictionaryCache.java`
#### Snippet
```java
    // Create the dictionary loading cache if we haven't already
    if (CACHE == null) {
      synchronized (DictionaryCache.class) {
        if (CACHE == null) {
          final int maxSize = conf.getInt(DICTIONARY_MAX_SIZE_KEY, DEFAULT_DICTIONARY_MAX_SIZE);
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ReflectionUtils.java`
#### Snippet
```java
    boolean dumpStack = false;
    if (log.isInfoEnabled()) {
      synchronized (ReflectionUtils.class) {
        long now = EnvironmentEdgeManager.currentTime();
        if (now - previousLogTime >= minInterval * 1000) {
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
    super.setConf(conf);

    synchronized (UserProvider.class) {
      if (!(groups instanceof User.TestingGroups)) {
        groups = Groups.getUserToGroupsMappingService(conf);
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
     */
    public static User createUserForTesting(Configuration conf, String name, String[] groups) {
      synchronized (UserProvider.class) {
        if (!(UserProvider.groups instanceof TestingGroups)) {
          UserProvider.groups = new TestingGroups(UserProvider.groups);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java
    // This ensures that removes of the metrics
    // can't happen while we are putting them back in.
    synchronized (this) {
      // It's possible that a close happened between checking
      // the closed variable and getting the lock.
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java`
#### Snippet
```java
    // While it's un-likely that snapshot and close happen at the same time it's still possible.
    // So grab the lock to ensure that all calls to snapshot are done before we remove the metrics
    synchronized (this) {
      if (LOG.isTraceEnabled()) {
        LOG.trace("Removing table Metrics for table ");
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSourceImpl.java`
#### Snippet
```java

  public void register(MetricsUserSource source) {
    synchronized (this) {
      source.register();
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateSourceImpl.java`
#### Snippet
```java
  public void deregister(MetricsUserSource toRemove) {
    try {
      synchronized (this) {
        MetricsUserSource source = userSources.remove(toRemove.getUser());
        if (source != null) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java`
#### Snippet
```java
    // While it's un-likely that snapshot and close happen at the same time it's still possible.
    // So grab the lock to ensure that all calls to snapshot are done before we remove the metrics
    synchronized (this) {
      if (LOG.isTraceEnabled()) {
        LOG.trace("Removing region Metrics: " + regionWrapper.getRegionName());
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java`
#### Snippet
```java
    // This ensures that removes of the metrics
    // can't happen while we are putting them back in.
    synchronized (this) {

      // It's possible that a close happened between checking
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
    }

    synchronized (this) {
      registry.removeMetric(userGetKey);
      registry.removeMetric(userScanTimeKey);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
  @Override
  public void register() {
    synchronized (this) {
      getHisto = registry.newTimeHistogram(userGetKey);
      scanTimeHisto = registry.newTimeHistogram(userScanTimeKey);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
    // This ensures that removes of the metrics
    // can't happen while we are putting them back in.
    synchronized (this) {

      // It's possible that a close happened between checking
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcConnection.java`
#### Snippet
```java
    System.arraycopy(HConstants.RPC_HEADER, 0, preamble, 0, rpcHeaderLen);
    preamble[rpcHeaderLen] = HConstants.RPC_CURRENT_VERSION;
    synchronized (this) {
      preamble[rpcHeaderLen + 1] = provider.getSaslAuthMethod().getCode();
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
      while (!resultSet) {
        try {
          this.wait();
        } catch (InterruptedException ie) {
          InterruptedIOException exception = new InterruptedIOException(ie.getMessage());
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
    @Override
    public void run(R parameter) {
      synchronized (this) {
        result = parameter;
        resultSet = true;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
        result = parameter;
        resultSet = true;
        this.notifyAll();
      }
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcCallback.java`
#### Snippet
```java
    while (!resultSet) {
      try {
        this.wait();
      } catch (InterruptedException ie) {
        InterruptedIOException exception = new InterruptedIOException(ie.getMessage());
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcCallback.java`
#### Snippet
```java
  @Override
  public void run(R parameter) {
    synchronized (this) {
      result = parameter;
      resultSet = true;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcCallback.java`
#### Snippet
```java
      result = parameter;
      resultSet = true;
      this.notifyAll();
    }
  }
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcClientConfigHelper.java`
#### Snippet
```java
      return eventLoop;
    }
    synchronized (NettyRpcClientConfigHelper.class) {
      eventLoop = DEFAULT_EVENT_LOOP;
      if (eventLoop != null) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
   */
  public void setResponse(Message response, final CellScanner cells) {
    synchronized (this) {
      if (done) {
        return;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
   */
  public void setTimeout(IOException error) {
    synchronized (this) {
      if (done) {
        return;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
   */
  public void setException(IOException error) {
    synchronized (this) {
      if (done) {
        return;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorOverAsyncBufferedMutator.java`
#### Snippet
```java
      });
    }
    synchronized (this) {
      if (bufferedSize.get() > mutator.getWriteBufferSize() * 2) {
        // We have too many mutations which are not completed yet, let's call a flush to release the
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
    // to copy it.
    List<RpcCallback<Object>> cbs;
    synchronized (this) {
      if (done) {
        return;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  @Override
  public void notifyOnCancel(RpcCallback<Object> callback) {
    synchronized (this) {
      if (done) {
        return;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
    // reset and startCancel. Although I think the race should be handled by the callable since the
    // reset may clear the cancel state...
    synchronized (this) {
      done = false;
      cancelled = false;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
      }
      callsToWrite.offer(call);
      BlockingRpcConnection.this.notifyAll();
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
    @Override
    public void run() {
      synchronized (BlockingRpcConnection.this) {
        while (!closed) {
          if (callsToWrite.isEmpty()) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
            // complicated, can do it later as an optimization.
            try {
              BlockingRpcConnection.this.wait();
            } catch (InterruptedException e) {
              // Restore interrupt status
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
      }
      try {
        wait(Math.min(this.rpcClient.minIdleTimeBeforeClose, 1000));
      } catch (InterruptedException e) {
        // Restore interrupt status
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
      }
    }
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
      public void run(Object parameter) {
        setCancelled(call);
        synchronized (BlockingRpcConnection.this) {
          if (callSender != null) {
            callSender.remove(call);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
          .setCallTimeMs(EnvironmentEdgeManager.currentTime() - call.callStats.getStartTime());
        if (isFatalConnectionException(exceptionResponse)) {
          synchronized (this) {
            closeConn(re);
          }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
        }
      } else {
        synchronized (this) {
          closeConn(e);
        }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncScanSingleRegionRpcRetryingCaller.java`
#### Snippet
```java
      ScanResponse localResp;
      int localNumberOfCompleteRows;
      synchronized (this) {
        if (state == ScanResumerState.INITIALIZED) {
          // user calls this method before we call prepare, so just set the state to
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
  public synchronized void onComplete() {
    closed = true;
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
      }
      try {
        wait();
      } catch (InterruptedException e) {
        throw new InterruptedIOException();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
  public synchronized void onError(Throwable error) {
    this.error = error;
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
      addToCache(result);
    }
    notifyAll();
    if (cacheSize >= maxCacheSize) {
      stopPrefetch(controller);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
      resumePrefetch();
    }
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java`
#### Snippet
```java
  private ExecutorService getBatchPool() throws IOException {
    if (batchPool == null) {
      synchronized (this) {
        if (isClosed()) {
          throw new DoNotRetryIOException("Connection is closed");
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CoprocessorBlockingRpcCallback.java`
#### Snippet
```java
  @Override
  public void run(R parameter) {
    synchronized (this) {
      result = parameter;
      resultSet = true;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CoprocessorBlockingRpcCallback.java`
#### Snippet
```java
      result = parameter;
      resultSet = true;
      this.notifyAll();
    }
  }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CoprocessorBlockingRpcCallback.java`
#### Snippet
```java
    while (!resultSet) {
      try {
        this.wait();
      } catch (InterruptedException ie) {
        InterruptedIOException exception = new InterruptedIOException(ie.getMessage());
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
    boolean firstRefresh = true;
    for (;;) {
      synchronized (this) {
        for (;;) {
          if (stopped) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
          }
          try {
            wait(waitTime);
          } catch (InterruptedException e) {
            LOG.warn("Interrupted during wait", e);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
  synchronized void stop() {
    stopped = true;
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
  synchronized void refreshNow() {
    refreshNow = true;
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
      }
    }
    synchronized (this) {
      if (closed) {
        IOException ioe = new IOException("Already closed");
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
      if (this.mutations.isEmpty() && periodicFlushTimeoutNs > 0) {
        periodicFlushTask = periodicalFlushTimer.newTimeout(timeout -> {
          synchronized (AsyncBufferedMutatorImpl.this) {
            // confirm that we are still valid, if there is already an internalFlush call before us,
            // then we should not execute any more. And in internalFlush we will set periodicFlush
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java
      IOUtils.closeQuietly(rpcClient, e -> LOG.warn("failed to close rpcClient", e));
      IOUtils.closeQuietly(registry, e -> LOG.warn("failed to close registry", e));
      synchronized (this) {
        if (choreService != null) {
          choreService.shutdown();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java
      return c;
    }
    synchronized (this) {
      c = this.conn;
      if (c != null) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
      // We expect a notification; but we wait with a
      // a timeout to lower the impact of a race condition if any
      wait(100);
      remaining = timeout - (EnvironmentEdgeManager.currentTime() - startTime);
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
      if (data != null) {
        this.data = data;
        notifyAll();
      } else {
        nodeDeleted(path);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
  public synchronized void stop() {
    this.stopped = true;
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
              if (isDryRun) {
                LOG.warn("Dry run: Table will be deleted at end of dry run.");
                synchronized (ImportTsv.class) {
                  DRY_RUN_TABLE_CREATED = true;
                }
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
    getConf().setLong(TIMESTAMP_CONF_KEY, timstamp);

    synchronized (ImportTsv.class) {
      DRY_RUN_TABLE_CREATED = false;
    }
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
    boolean success = job.waitForCompletion(true);
    boolean delete = false;
    synchronized (ImportTsv.class) {
      delete = DRY_RUN_TABLE_CREATED;
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java`
#### Snippet
```java
    // up data between construction and the scan of meta needed before this method
    // is called. See HBASE-23737 "[Flakey Tests] TestFavoredNodeTableImport fails 30% of the time"
    synchronized (this) {
      this.globalFavoredNodesAssignmentPlan
        .updateFavoredNodesMap(snapshot.getExistingAssignmentPlan());
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
    try {
      // Start the JMXListener with the connection string
      synchronized (JMXListener.class) {
        if (JMX_CS != null) {
          throw new RuntimeException("Started by another thread?");
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
    }

    synchronized (JMXListener.class) {
      if (JMX_CS != null) {
        LOG.info("JMXListener has been started at Registry port " + rmiRegistryPort);
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java

  public void stopConnectorServer() throws IOException {
    synchronized (JMXListener.class) {
      if (JMX_CS != null) {
        JMX_CS.stop();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
        justification = "This is what we want")
    public void evict() {
      synchronized (this) {
        this.notifyAll();
      }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    public void evict() {
      synchronized (this) {
        this.notifyAll();
      }
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
      enteringRun = true;
      while (this.go) {
        synchronized (this) {
          try {
            this.wait(1000 * 10/* Don't wait for ever */);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
        synchronized (this) {
          try {
            this.wait(1000 * 10/* Don't wait for ever */);
          } catch (InterruptedException e) {
            LOG.warn("Interrupted eviction thread ", e);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    synchronized void shutdown() {
      this.go = false;
      this.notifyAll();
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    synchronized void shutdown() {
      this.go = false;
      this.notifyAll();
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
        justification = "This is what we want")
    public void evict() {
      synchronized (this) {
        this.notifyAll();
      }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    public void evict() {
      synchronized (this) {
        this.notifyAll();
      }
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
      long startTime = EnvironmentEdgeManager.currentTime();
      while (this.go) {
        synchronized (this) {
          try {
            this.wait(1000 * 10/* Don't wait for ever */);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
        synchronized (this) {
          try {
            this.wait(1000 * 10/* Don't wait for ever */);
          } catch (InterruptedException e) {
            LOG.warn("Interrupted eviction thread ", e);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    responder.interrupt();
    scheduler.stop();
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  public synchronized void join() throws InterruptedException {
    while (running) {
      wait();
    }
  }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      }
      LOG.info(getName() + ": stopping");
      synchronized (this) {
        try {
          acceptChannel.close();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
    }
    // this is to avoid race between addWAL and requestRollAll.
    synchronized (this) {
      if (wals.putIfAbsent(wal, new RollController(wal)) == null) {
        wal.registerWALActionsListener(new WALActionsListener() {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
          public void logRollRequested(WALActionsListener.RollRequestReason reason) {
            // TODO logs will contend with each other here, replace with e.g. DelayedQueue
            synchronized (AbstractWALRoller.this) {
              RollController controller = wals.computeIfAbsent(wal, rc -> new RollController(wal));
              controller.requestRoll();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
              RollController controller = wals.computeIfAbsent(wal, rc -> new RollController(wal));
              controller.requestRoll();
              AbstractWALRoller.this.notifyAll();
            }
          }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
      long now = EnvironmentEdgeManager.currentTime();
      checkLowReplication(now);
      synchronized (this) {
        if (wals.values().stream().noneMatch(rc -> rc.needsRoll(now))) {
          try {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
        if (wals.values().stream().noneMatch(rc -> rc.needsRoll(now))) {
          try {
            wait(this.threadWakeFrequency);
          } catch (InterruptedException e) {
            // restore the interrupt state
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java

  public void requestRollAll() {
    synchronized (this) {
      for (RollController controller : wals.values()) {
        controller.requestRoll();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
        controller.requestRoll();
      }
      notifyAll();
    }
  }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java

  void doneWriting(RegionEntryBuffer buffer) {
    synchronized (this) {
      boolean removed = currentlyWriting.remove(buffer.encodedRegionName);
      assert removed;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
    RegionEntryBuffer buffer;
    long incrHeap;
    synchronized (this) {
      buffer = buffers.get(key.getEncodedRegionName());
      if (buffer == null) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionServerTracker.java`
#### Snippet
```java
      .forEach(sn -> server.getAssignmentManager().getRegionStates().getOrCreateServer(sn));
    ServerManager serverManager = server.getServerManager();
    synchronized (this) {
      Set<ServerName> liveServers = regionServers;
      for (ServerName serverName : liveServers) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryConfigManager.java`
#### Snippet
```java
    // Regions Reopen based on very high storeFileRefCount is considered enabled
    // only if hbase.regions.recovery.store.file.ref.count has value > 0
    synchronized (this) {
      if (chore != null) {
        chore.shutdown();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/DirScanPool.java`
#### Snippet
```java
    while (cleanerLatch != 0 && timeout > 0) {
      try {
        wait(timeout);
        timeout = stopTime - EnvironmentEdgeManager.currentTime();
      } catch (InterruptedException ie) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/DirScanPool.java`
#### Snippet
```java
  synchronized void latchCountDown() {
    cleanerLatch--;
    notifyAll();
  }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
  @Override
  protected boolean initialChore() {
    synchronized (this) {
      if (forceRun) {
        // wake up the threads waiting in triggerCleanerNow, as a triggerNow may triggers the first
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
        // loop where we will only call initialChore. We need to trigger another run immediately.
        forceRun = false;
        notifyAll();
      }
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
        return CompletableFuture.completedFuture(false);
      }
      wait();
    }
  }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
  protected void chore() {
    CompletableFuture<Boolean> f;
    synchronized (this) {
      if (!enabled.get()) {
        if (!forceRun) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
      f = new CompletableFuture<>();
      future = f;
      notifyAll();
    }
    pool.latchCountUp();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
    } finally {
      postRunCleaner();
      synchronized (this) {
        future = null;
        forceRun = false;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java`
#### Snippet
```java
            String fileName = file.getPath().getName();
            if (!refreshed && !currentCache.contains(fileName)) {
              synchronized (this) {
                refreshCache();
                currentCache = cache;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java`
#### Snippet
```java
    @Override
    public void run() {
      synchronized (SnapshotFileCache.this) {
        try {
          SnapshotFileCache.this.refreshCache();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
    synchronized void set(byte[] data) {
      this.data = data;
      notifyAll();
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
    synchronized byte[] get() throws InterruptedException {
      while (!delete && data == null) {
        wait();
      }
      byte[] d = data;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
    synchronized void delete() {
      this.delete = true;
      notifyAll();
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
      this.done = true;
      this.result = result;
      notify();
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
        while (!done) {
          long startTimeNanos = System.nanoTime();
          wait(waitIfNotFinished);
          waitTimeMsec +=
            TimeUnit.MILLISECONDS.convert(System.nanoTime() - startTimeNanos, TimeUnit.NANOSECONDS);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
          "Token keys could not be sync from zookeeper" + " because of ZKWatcher abort");
      }
      synchronized (this) {
        if (!leaderElector.isAlive() || leaderElector.isStopped()) {
          LOG.warn("Thread leaderElector[" + leaderElector.getName() + ":" + leaderElector.getId()
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelServiceManager.java`
#### Snippet
```java
      return this.visibilityLabelService;
    }
    synchronized (this) {
      if (this.visibilityLabelService != null) {
        checkForClusterLevelSingleConf(vlsClassName);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      } else {
        try {
          wait(1000);
        } catch (InterruptedException e) {
          LOG.warn("Sleep interrupted", e);
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      }
      LOG.debug("Done migrating {}, failed tables {}", groupInfo.getName(), failedTables);
      synchronized (RSGroupInfoManagerImpl.this) {
        Map<String, RSGroupInfo> rsGroupMap = holder.groupName2Group;
        RSGroupInfo currentInfo = rsGroupMap.get(groupInfo.getName());
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    // Hold a lock on the manager instance while moving servers to prevent
    // another writer changing our state while we are working.
    synchronized (this) {
      // Presume first server's source group. Later ensure all servers are from this group.
      Address firstServer = servers.iterator().next();
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    if (e.getEnvironment().getRegion().getRegionInfo().getTable().equals(LABELS_TABLE_NAME)) {
      this.labelsRegion = true;
      synchronized (this) {
        this.accessControllerAvailable =
          CoprocessorHost.getLoadedCoprocessors().contains(AccessController.class.getName());
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  public void initReader() throws IOException {
    if (initialReader == null) {
      synchronized (this) {
        if (initialReader == null) {
          try {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
          this.maxCount = newMaxCount;
          if (this.reclaimedChunks.size() > newMaxCount) {
            synchronized (this) {
              while (this.reclaimedChunks.size() > newMaxCount) {
                this.reclaimedChunks.poll();
```

### SynchronizeOnThis
Lock operations on a class may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationFlushRequester.java`
#### Snippet
```java
      return timer;
    }
    synchronized (RegionReplicationFlushRequester.class) {
      timer = TIMER;
      if (timer != null) {
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.java`
#### Snippet
```java
      public void run() {
        // ensure we don't run this task multiple times
        synchronized (this) {
          // quick exit if we already marked the task complete
          if (TimeoutExceptionInjector.this.complete) return;
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // C. Finally notify anyone waiting on memstore to clear:
        // e.g. checkResources().
        synchronized (this) {
          notifyAll(); // FindBugs NN_NAKED_NOTIFY
        }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // e.g. checkResources().
        synchronized (this) {
          notifyAll(); // FindBugs NN_NAKED_NOTIFY
        }
      }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // C. Finally notify anyone waiting on memstore to clear:
    // e.g. checkResources().
    synchronized (this) {
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // e.g. checkResources().
    synchronized (this) {
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }

```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // C. Finally notify anyone waiting on memstore to clear:
    // e.g. checkResources().
    synchronized (this) {
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // e.g. checkResources().
    synchronized (this) {
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }
  }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      // C. Finally notify anyone waiting on memstore to clear:
      // e.g. checkResources().
      synchronized (this) {
        notifyAll(); // FindBugs NN_NAKED_NOTIFY
      }
```

### SynchronizeOnThis
Lock operations on 'this' may have unforeseen side-effects
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      // e.g. checkResources().
      synchronized (this) {
        notifyAll(); // FindBugs NN_NAKED_NOTIFY
      }
      return totalFreedDataSize > 0;
```

## RuleId[id=CastConflictsWithInstanceof]
### CastConflictsWithInstanceof
Cast to 'MutableFastCounter' type conflicts with preceding 'instanceof MutableCounter' check
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/DynamicMetricsRegistry.java`
#### Snippet
```java
    }

    return (MutableFastCounter) counter;
  }

```

### CastConflictsWithInstanceof
Cast to 'ByteBufferKeyValue' type conflicts with preceding 'instanceof ExtendedCell' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.java`
#### Snippet
```java
          offsetInCurentChunk = ChunkCreator.SIZEOF_CHUNK_HEADER;
        }
        offsetInCurentChunk = createCellReference((ByteBufferKeyValue) curCell,
          chunks[currentChunkIdx].getData(), offsetInCurentChunk);
        if (action == MemStoreCompactionStrategy.Action.FLATTEN_COUNT_UNIQUE_KEYS) {
```

### CastConflictsWithInstanceof
Cast to 'ByteBufferKeyValue' type conflicts with preceding 'instanceof ExtendedCell' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.java`
#### Snippet
```java
      }
      offsetInCurentChunk = // add the Cell reference to the index chunk
        createCellReference((ByteBufferKeyValue) c, chunks[currentChunkIdx].getData(),
          offsetInCurentChunk);
      // the sizes still need to be updated in the new segment
```

## RuleId[id=UnusedAssignment]
### UnusedAssignment
Variable `length` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ArrayBackedTag.java`
#### Snippet
```java
  private final byte[] bytes;
  private int offset = 0;
  private int length = 0;

  /**
```

### UnusedAssignment
Variable `diff` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellComparatorImpl.java`
#### Snippet
```java
  @Override
  public int compare(final Cell l, final Cell r, boolean ignoreSequenceid) {
    int diff = 0;
    // "Peel off" the most common path.
    if (l instanceof KeyValue && r instanceof KeyValue) {
```

### UnusedAssignment
Variable `hostAndWeights` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java`
#### Snippet
```java
@InterfaceAudience.Private
public class HDFSBlocksDistribution {
  private Map<String, HostAndWeight> hostAndWeights = null;
  private long uniqueBlocksTotalWeight = 0;

```

### UnusedAssignment
Variable `kv` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java`
#### Snippet
```java
    bb.rewind();
    List<KeyValue> kvs = Lists.newArrayList();
    KeyValue kv = null;
    while (true) {
      kv = KeyValueUtil.nextShallowCopy(bb, includesMemstoreTS, useTags);
```

### UnusedAssignment
Variable `toWrite` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
  @Override
  public void write(byte[] b, int off, int len) throws IOException {
    int toWrite = 0;
    while (len > 0) {
      toWrite = Math.min(len, this.curBuf.remaining());
```

### UnusedAssignment
Variable `toWrite` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
  @Override
  public void write(ByteBuffer b, int off, int len) throws IOException {
    int toWrite = 0;
    while (len > 0) {
      toWrite = Math.min(len, this.curBuf.remaining());
```

### UnusedAssignment
Variable `initSize` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java`
#### Snippet
```java
    private HashMap<Node, Short> nodeToIndex = new HashMap<>();
    private Node[] indexToNode;
    private int initSize = 0;

    public BidirectionalLRUMap(int initialSize) {
```

### UnusedAssignment
Variable `sum` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java`
#### Snippet
```java
    // v should be > 0
    int length = iv.length;
    int sum = 0;
    for (int i = 0; i < length; i++) {
      if (v <= 0) {
```

### UnusedAssignment
Variable `rkCommonPrefix` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    }
    // Compare the RKs
    int rkCommonPrefix = 0;
    if (c1 instanceof ByteBufferExtendedCell && c2 instanceof ByteBufferExtendedCell) {
      rkCommonPrefix =
```

### UnusedAssignment
Variable `header` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java`
#### Snippet
```java
      // Capture the fixed ten-byte header hard-coded in GZIPOutputStream.
      ByteArrayOutputStream baos = new ByteArrayOutputStream();
      byte[] header = null;
      GZIPOutputStream gzipStream = null;
      try {
```

### UnusedAssignment
Variable `bos1` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    public OutputStream createCompressionStream(OutputStream downStream, Compressor compressor,
      int downStreamBufferSize) throws IOException {
      OutputStream bos1 = null;
      if (downStreamBufferSize > 0) {
        bos1 = new BufferedOutputStream(downStream, downStreamBufferSize);
```

### UnusedAssignment
Variable `baosBytes` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java`
#### Snippet
```java
  public byte[] encodeData() {
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    byte[] baosBytes = null;
    try {
      baos.write(HConstants.HFILEBLOCK_DUMMY_HEADER);
```

### UnusedAssignment
Variable `kv` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java`
#### Snippet
```java
      int tagsLength = 0;
      long memstoreTS = 0L;
      KeyValue kv = null;
      while (in.hasRemaining()) {
        int kvOffset = in.position();
```

### UnusedAssignment
Variable `limit` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
  private int curItemIndex = 0;

  private int limit = 0;
  private int limitedItemIndex;
  private int markedItemIndex = -1;
```

### UnusedAssignment
Variable `jump` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    checkRefCount();
    // Get available bytes from this item and remaining from next
    int jump = 0;
    while (true) {
      jump = this.curItem.remaining();
```

### UnusedAssignment
Variable `curItem` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
  private final ByteBuffer[] items;
  // Pointer to the current item in the MBB
  private ByteBuffer curItem = null;
  // Index of the current item in the MBB
  private int curItemIndex = 0;
```

### UnusedAssignment
Variable `len` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
   */
  static int putVaruint64(PositionedByteRange dst, long val, boolean comp) {
    int w, y, len = 0;
    final int offset = dst.getOffset(), start = dst.getPosition();
    byte[] a = dst.getBytes();
```

### UnusedAssignment
Variable `count` initializer `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMovingAverage.java`
#### Snippet
```java
public class SimpleMovingAverage<T> extends MovingAverage<T> {
  private double averageTime = 0.0;
  protected long count = 0;

  public SimpleMovingAverage(String label) {
```

### UnusedAssignment
Variable `averageTime` initializer `0.0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMovingAverage.java`
#### Snippet
```java
@InterfaceAudience.Private
public class SimpleMovingAverage<T> extends MovingAverage<T> {
  private double averageTime = 0.0;
  protected long count = 0;

```

### UnusedAssignment
Variable `value` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  private final static class COWEntry<K, V> implements Map.Entry<K, V> {
    K key = null;
    V value = null;

    COWEntry(K key, V value) {
```

### UnusedAssignment
Variable `key` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

  private final static class COWEntry<K, V> implements Map.Entry<K, V> {
    K key = null;
    V value = null;

```

### UnusedAssignment
Variable `instance` initializer `null` is redundant
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilityFactory.java`
#### Snippet
```java

  public static synchronized <T> T getInstance(Class<T> klass) {
    T instance = null;
    try {
      ServiceLoader<T> loader = ServiceLoader.load(klass);
```

### UnusedAssignment
The value `Bytes.putBytes(bytes, pos, tags, tagsOffset, tagsLength)` assigned to `pos` is never used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    if (tagsLength > 0) {
      pos = Bytes.putAsShort(bytes, pos, tagsLength);
      pos = Bytes.putBytes(bytes, pos, tags, tagsOffset, tagsLength);
    }
    return bytes;
```

### UnusedAssignment
Variable `newRowKey` initializer `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
        diffIdx++;
      }
      byte[] newRowKey = null;
      if (diffIdx >= minLength) {
        // leftKey's row is prefix of rightKey's.
```

### UnusedAssignment
The value `Bytes.putAsShort(bytes, pos, tagsLength)` assigned to `pos` is never used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    pos += vlength;
    if (tagsLength > 0) {
      pos = Bytes.putAsShort(bytes, pos, tagsLength);
    }
    return bytes;
```

### UnusedAssignment
Variable `prev` initializer `null` is redundant
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java

    ListIterator<SampleItem> it = samples.listIterator();
    SampleItem prev = null;
    SampleItem next = it.next();

```

### UnusedAssignment
Variable `line` initializer `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeSaslServerAuthenticationProvider.java`
#### Snippet
```java
    try (FSDataInputStream fdis = fs.open(passwordFile); BufferedReader reader =
      new BufferedReader(new InputStreamReader(fdis, StandardCharsets.UTF_8))) {
      String line = null;
      int offset = 0;
      while ((line = reader.readLine()) != null) {
```

### UnusedAssignment
Variable `hasMore` initializer `false` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java`
#### Snippet
```java
      scanner = env.getRegion().getScanner(new Scan());
      List<Cell> results = new ArrayList<>();
      boolean hasMore = false;
      long count = 0;
      do {
```

### UnusedAssignment
Variable `hasMore` initializer `false` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/RowCountEndpoint.java`
#### Snippet
```java
      scanner = env.getRegion().getScanner(scan);
      List<Cell> results = new ArrayList<>();
      boolean hasMore = false;
      byte[] lastRow = null;
      long count = 0;
```

### UnusedAssignment
Variable `cid` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterId.java`
#### Snippet
```java
      int pblen = ProtobufUtil.lengthOfPBMagic();
      ClusterIdProtos.ClusterId.Builder builder = ClusterIdProtos.ClusterId.newBuilder();
      ClusterIdProtos.ClusterId cid = null;
      try {
        ProtobufUtil.mergeFrom(builder, bytes, pblen, bytes.length - pblen);
```

### UnusedAssignment
Variable `row` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(Get.class);

  private byte[] row = null;
  private int maxVersions = 1;
  private boolean cacheBlocks = true;
```

### UnusedAssignment
Variable `name` initializer `""` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
    for (Throwable t : ths) {
      if (t == null) continue;
      String name = "";
      if (t instanceof DoNotRetryIOException || t instanceof RegionTooBusyException) {
        // If RegionTooBusyException, print message since it has Region name in it.
```

### UnusedAssignment
Variable `result` initializer `HConstants.NO_NONCE` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/PerClientRandomNonceGenerator.java`
#### Snippet
```java
  @Override
  public long newNonce() {
    long result = HConstants.NO_NONCE;
    do {
      result = ThreadLocalRandom.current().nextLong();
```

### UnusedAssignment
Variable `cfs` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
      int pblen = ProtobufUtil.lengthOfPBMagic();
      ColumnFamilySchema.Builder builder = ColumnFamilySchema.newBuilder();
      ColumnFamilySchema cfs = null;
      try {
        ProtobufUtil.mergeFrom(builder, bytes, pblen, bytes.length - pblen);
```

### UnusedAssignment
Variable `toSend` initializer `Optional.empty()` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
        + Bytes.toStringBinary(req.row) + "', locateType=" + req.locateType, error);
    }
    Optional<LocateRequest> toSend = Optional.empty();
    TableCache tableCache = getTableCache(tableName);
    if (locs != null) {
```

### UnusedAssignment
Variable `proto` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
    if (protoBytes == null) return null;
    ClientProtos.CellVisibility.Builder builder = ClientProtos.CellVisibility.newBuilder();
    ClientProtos.CellVisibility proto = null;
    try {
      ProtobufUtil.mergeFrom(builder, protoBytes);
```

### UnusedAssignment
Variable `currentRow` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    boolean stale = false;
    byte[] prevRow = null;
    byte[] currentRow = null;
    for (Iterator<Result> iter = partialResults.iterator(); iter.hasNext();) {
      Result r = iter.next();
```

### UnusedAssignment
Variable `prefix` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class ColumnPrefixFilter extends FilterBase {
  protected byte[] prefix = null;

  public ColumnPrefixFilter(final byte[] prefix) {
```

### UnusedAssignment
Variable `maxColumnInclusive` initializer `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
  protected boolean maxColumnInclusive = false;

  /**
```

### UnusedAssignment
Variable `minColumnInclusive` initializer `true` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
public class ColumnRangeFilter extends FilterBase {
  protected byte[] minColumn = null;
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
  protected boolean maxColumnInclusive = false;
```

### UnusedAssignment
Variable `minColumn` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class ColumnRangeFilter extends FilterBase {
  protected byte[] minColumn = null;
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
```

### UnusedAssignment
Variable `maxColumn` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
  protected byte[] minColumn = null;
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
  protected boolean maxColumnInclusive = false;

```

### UnusedAssignment
Variable `limit` initializer `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class ColumnCountGetFilter extends FilterBase {
  private int limit = 0;
  private int count = 0;

```

### UnusedAssignment
Variable `limit` initializer `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java`
#### Snippet
```java
public class ColumnPaginationFilter extends FilterBase {

  private int limit = 0;
  private int offset = -1;
  private byte[] columnOffset = null;
```

### UnusedAssignment
Variable `filter` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
    Stack<Filter> filterStack = new Stack<>();

    Filter filter = null;
    for (int i = 0; i < filterStringAsByteArray.length; i++) {
      if (filterStringAsByteArray[i] == ParseConstants.LPAREN) {
```

### UnusedAssignment
Variable `argumentStartIndex` initializer `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
    }

    int argumentStartIndex = 0;
    int argumentEndIndex = 0;
    ArrayList<byte[]> filterArguments = new ArrayList<>();
```

### UnusedAssignment
Variable `argumentEndIndex` initializer `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java

    int argumentStartIndex = 0;
    int argumentEndIndex = 0;
    ArrayList<byte[]> filterArguments = new ArrayList<>();

```

### UnusedAssignment
Variable `prefix` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class PrefixFilter extends FilterBase {
  protected byte[] prefix = null;
  protected boolean passedPrefix = false;
  protected boolean filterRow = true;
```

### UnusedAssignment
Variable `pageSize` initializer `Long.MAX_VALUE` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class PageFilter extends FilterBase {
  private long pageSize = Long.MAX_VALUE;
  private int rowsAccepted = 0;

```

### UnusedAssignment
Variable `prevFilterRCList` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
   * the filter. For MUST_PASS_ALL, the two list are meaningless.
   */
  private List<ReturnCode> prevFilterRCList = null;
  private List<Cell> prevCellList = null;

```

### UnusedAssignment
Variable `prevCellList` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
   */
  private List<ReturnCode> prevFilterRCList = null;
  private List<Cell> prevCellList = null;

  public FilterListWithOR(List<Filter> filters) {
```

### UnusedAssignment
Variable `namespace` initializer `NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/NamespacePermission.java`
#### Snippet
```java
public class NamespacePermission extends Permission {

  private String namespace = NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR;

  /**
```

### UnusedAssignment
Variable `tableCFsMap` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private final Map<byte[], byte[]> peerData;
  private final Map<String, String> configuration;
  private Map<TableName, ? extends Collection<String>> tableCFsMap = null;
  private Set<String> namespaces = null;
  // Default value is true, means replicate all user tables to peer cluster.
```

### UnusedAssignment
Variable `excludeTableCFsMap` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  // Default value is true, means replicate all user tables to peer cluster.
  private boolean replicateAllUserTables = true;
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
  private long bandwidth = 0;
```

### UnusedAssignment
Variable `namespaces` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private final Map<String, String> configuration;
  private Map<TableName, ? extends Collection<String>> tableCFsMap = null;
  private Set<String> namespaces = null;
  // Default value is true, means replicate all user tables to peer cluster.
  private boolean replicateAllUserTables = true;
```

### UnusedAssignment
Variable `excludeNamespaces` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private boolean replicateAllUserTables = true;
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
  private long bandwidth = 0;
  private final boolean serial;
```

### UnusedAssignment
Variable `bandwidth` initializer `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
  private long bandwidth = 0;
  private final boolean serial;
  // Used by synchronous replication
```

### UnusedAssignment
Variable `replicateAllUserTables` initializer `true` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private Set<String> namespaces = null;
  // Default value is true, means replicate all user tables to peer cluster.
  private boolean replicateAllUserTables = true;
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
```

### UnusedAssignment
The value `false` assigned to `hasMore` is never used
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java`
#### Snippet
```java
        if (entry == null) {
          out.println("No more entry, exiting with missing EOF");
          hasMore = false;
          break;
        }
```

### UnusedAssignment
Variable `logId` initializer `-1` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
    }

    long logId = -1;
    lock.lock();
    try {
```

### UnusedAssignment
Variable `newStream` initializer `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
      .setMinProcId(storeTracker.getActiveMinProcId()).setLogId(logId).build();

    FSDataOutputStream newStream = null;
    Path newLogFile = null;
    long startPos = -1;
```

### UnusedAssignment
Variable `newLogFile` initializer `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java

    FSDataOutputStream newStream = null;
    Path newLogFile = null;
    long startPos = -1;
    newLogFile = getLogFilePath(logId);
```

### UnusedAssignment
Variable `startPos` initializer `-1` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
    FSDataOutputStream newStream = null;
    Path newLogFile = null;
    long startPos = -1;
    newLogFile = getLogFilePath(logId);
    try {
```

### UnusedAssignment
Variable `value` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java
    int count = limit;
    do {
      Cell value = null;
      try {
        value = generator.next();
```

### UnusedAssignment
Variable `value` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java
        ResultGenerator generator = ResultGenerator.fromRowSpec(this.tableResource.getName(),
          rowSpec, null, !params.containsKey(NOCACHE_PARAM_NAME));
        Cell value = null;
        RowModel rowModel = new RowModel(rowSpec.getRow());
        if (generator.hasNext()) {
```

### UnusedAssignment
Variable `rowModel` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesResource.java`
#### Snippet
```java
    servlet.getMetrics().incrementRequests(1);
    try {
      NamespacesModel rowModel = null;
      rowModel = new NamespacesModel(servlet.getAdmin());
      servlet.getMetrics().incrementSucessfulGetRequests(1);
```

### UnusedAssignment
Variable `spec` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
    }
    byte[] endRow = model.hasEndRow() ? model.getEndRow() : null;
    RowSpec spec = null;
    if (model.getLabels() != null) {
      spec = new RowSpec(model.getStartRow(), endRow, model.getColumns(), model.getStartTime(),
```

### UnusedAssignment
Variable `admin` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
    }

    Admin admin = null;
    boolean namespaceExists = false;
    try {
```

### UnusedAssignment
Variable `namespaceExists` initializer `false` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java

    Admin admin = null;
    boolean namespaceExists = false;
    try {
      admin = servlet.getAdmin();
```

### UnusedAssignment
Variable `queryTables` initializer `false` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(NamespacesInstanceResource.class);
  String namespace;
  boolean queryTables = false;

  /**
```

### UnusedAssignment
Variable `reExecute` initializer `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java

    // Whether to 're-' -execute; run through the loop again.
    boolean reExecute = false;

    Procedure<TEnvironment>[] subprocs = null;
```

### UnusedAssignment
Variable `time0` initializer `0` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      return i;
    }
    long time0 = 0, time1 = 0;
    try {
      char c = 0;
```

### UnusedAssignment
Variable `startRow` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java

  private int parseRowKeys(final String path, int i) throws IllegalArgumentException {
    String startRow = null, endRow = null;
    try {
      StringBuilder sb = new StringBuilder();
```

### UnusedAssignment
The value `parseQueryParams(path, i)` assigned to `i` is never used
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
    i = parseColumns(path, i);
    i = parseTimestamp(path, i);
    i = parseQueryParams(path, i);
  }

```

### UnusedAssignment
Variable `check` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  private TableResource tableResource;
  private RowSpec rowspec;
  private String check = null;
  private boolean returnResult = false;

```

### UnusedAssignment
Variable `append` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  Response append(final CellSetModel model) {
    Table table = null;
    Append append = null;
    try {
      table = servlet.getTable(tableResource.getName());
```

### UnusedAssignment
Variable `delete` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
        .entity("Forbidden" + CRLF).build();
    }
    Delete delete = null;
    if (rowspec.hasTimestamp()) {
      delete = new Delete(rowspec.getRow(), rowspec.getTimestamp());
```

### UnusedAssignment
Variable `delete` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  Response checkAndDelete(final CellSetModel model) {
    Table table = null;
    Delete delete = null;
    try {
      table = servlet.getTable(tableResource.getName());
```

### UnusedAssignment
Variable `increment` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  Response increment(final CellSetModel model) {
    Table table = null;
    Increment increment = null;
    try {
      table = servlet.getTable(tableResource.getName());
```

### UnusedAssignment
Variable `conf` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/AuthFilter.java`
#### Snippet
```java
    props.setProperty(AuthenticationFilter.COOKIE_PATH, "/");

    Configuration conf = null;
    // Dirty hack to get at the RESTServer's configuration. These should be pulled out
    // of the FilterConfig.
```

### UnusedAssignment
Variable `obj` initializer `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/provider/consumer/ProtobufMessageBodyConsumer.java`
#### Snippet
```java
    Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, String> httpHeaders,
    InputStream inputStream) throws IOException, WebApplicationException {
    ProtobufMessageHandler obj = null;
    try {
      obj = type.getDeclaredConstructor().newInstance();
```

### UnusedAssignment
Variable `proto` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    if (protoBytes == null) return null;
    ClientProtos.Authorizations.Builder builder = ClientProtos.Authorizations.newBuilder();
    ClientProtos.Authorizations proto = null;
    try {
      ProtobufUtil.mergeFrom(builder, protoBytes);
```

### UnusedAssignment
Variable `cn` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      Class<? extends Throwable> c =
        (Class<? extends Throwable>) Class.forName(type, true, ClassLoaderHolder.CLASS_LOADER);
      Constructor<? extends Throwable> cn = null;
      try {
        cn = c.getDeclaredConstructor(String.class);
```

### UnusedAssignment
Variable `proto` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    if (protoBytes == null) return null;
    ClientProtos.CellVisibility.Builder builder = ClientProtos.CellVisibility.newBuilder();
    ClientProtos.CellVisibility proto = null;
    try {
      ProtobufUtil.mergeFrom(builder, protoBytes);
```

### UnusedAssignment
Variable `response` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    final RpcController controller, final AdminService.BlockingInterface admin) throws IOException {
    GetOnlineRegionRequest request = RequestConverter.buildGetOnlineRegionRequest();
    GetOnlineRegionResponse response = null;
    try {
      response = admin.getOnlineRegion(controller, request);
```

### UnusedAssignment
Variable `pScanMetrics` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java

  public static ScanMetrics toScanMetrics(final byte[] bytes) {
    MapReduceProtos.ScanMetrics pScanMetrics = null;
    try {
      pScanMetrics = MapReduceProtos.ScanMetrics.parseFrom(bytes);
```

### UnusedAssignment
Variable `position` initializer `0` is redundant
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
      }
      if (data != null && data.length > 0) { // log position
        long position = 0;
        try {
          position = ZKUtil.parseWALPositionFrom(ZKUtil.getData(zkw, znodeToProcess));
```

### UnusedAssignment
Variable `sn` initializer `null` is redundant
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java`
#### Snippet
```java

    long startTime = EnvironmentEdgeManager.currentTime();
    ServerName sn = null;
    while (true) {
      sn = getMetaRegionLocation(zkw, replicaId);
```

### UnusedAssignment
Variable `protobufMessageLiteBuilderClass` initializer `null` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java

  private static Class<?> protobufMessageLiteClass = null;
  private static Class<?> protobufMessageLiteBuilderClass = null;

  private static final boolean HAS_PARSER;
```

### UnusedAssignment
Variable `protobufMessageLiteClass` initializer `null` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufDecoder.class);

  private static Class<?> protobufMessageLiteClass = null;
  private static Class<?> protobufMessageLiteBuilderClass = null;

```

### UnusedAssignment
Variable `hasParser` initializer `false` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java

  static {
    boolean hasParser = false;

    // These are the protobuf classes coming from Hadoop. Not the one from hbase-shaded-protobuf
```

### UnusedAssignment
Variable `job` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java

      String id = getClass().getSimpleName() + UUID.randomUUID().toString().replace("-", "");
      Job job = null;
      Scan scan = new Scan();
      scan.addFamily(family);
```

### UnusedAssignment
Variable `locatedBlock` initializer `null` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
      beginFileLease(client, stat.getFileId());
      boolean succ = false;
      LocatedBlock locatedBlock = null;
      List<Future<Channel>> futureList = null;
      try {
```

### UnusedAssignment
Variable `cell` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java`
#### Snippet
```java
  protected void populatePut(byte[] lineBytes, ImportTsv.TsvParser.ParsedLine parsed, Put put,
    int i) throws BadTsvLineException, IOException {
    Cell cell = null;
    if (hfileOutPath == null) {
      cell = new KeyValue(lineBytes, parsed.getRowKeyOffset(), parsed.getRowKeyLength(),
```

### UnusedAssignment
Variable `rowFilter` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java

  private static Filter getRowFilter(String[] args) {
    Filter rowFilter = null;
    String filterCriteria = (args.length > 3) ? args[3] : null;
    if (filterCriteria == null) return null;
```

### UnusedAssignment
Variable `ipAddressString` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
    String hostName = this.reverseDNSCacheMap.get(ipAddress);
    if (hostName == null) {
      String ipAddressString = null;
      try {
        ipAddressString = DNS.reverseDns(ipAddress, null);
```

### UnusedAssignment
Variable `job` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  protected static Job createSubmittableJob(Configuration conf, String[] args)
    throws IOException, ClassNotFoundException {
    Job job = null;
    boolean isDryRun = conf.getBoolean(DRY_RUN_CONF_KEY, false);
    try (Connection connection = ConnectionFactory.createConnection(conf)) {
```

### UnusedAssignment
Variable `delete` initializer `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
    Job job = createSubmittableJob(getConf(), args);
    boolean success = job.waitForCompletion(true);
    boolean delete = false;
    synchronized (ImportTsv.class) {
      delete = DRY_RUN_TABLE_CREATED;
```

### UnusedAssignment
Variable `link` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      try {
        Configuration conf = context.getConfiguration();
        FileLink link = null;
        switch (fileInfo.getType()) {
          case HFILE:
```

### UnusedAssignment
Variable `snapshotFileAndSize` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
        public void storeFile(final RegionInfo regionInfo, final String family,
          final SnapshotRegionManifest.StoreFile storeFile) throws IOException {
          Pair<SnapshotFileInfo, Long> snapshotFileAndSize = null;
          if (!storeFile.hasReference()) {
            String region = regionInfo.getEncodedName();
```

### UnusedAssignment
Variable `link` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      try {
        Configuration conf = context.getConfiguration();
        FileLink link = null;
        switch (fileInfo.getType()) {
          case HFILE:
```

### UnusedAssignment
Variable `kv` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java`
#### Snippet
```java
            // Creating the KV which needs to be directly written to HFiles. Using the Facade
            // KVCreator for creation of kvs.
            KeyValue kv = null;
            TagUtil.carryForwardTags(tags, cell);
            if (!tags.isEmpty()) {
```

### UnusedAssignment
Variable `encodedRegionName` initializer `""` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
  private byte[] endRow;
  private String regionLocation;
  private String encodedRegionName = "";

  /**
```

### UnusedAssignment
Variable `printScan` initializer `""` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
    sb.append("tablename=").append(tableName);
    // null scan input is represented by ""
    String printScan = "";
    if (!scan.equals("")) {
      try {
```

### UnusedAssignment
Variable `ordinal` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java`
#### Snippet
```java
      @Override
      public int getLabelOrdinal(String label) {
        Integer ordinal = null;
        ordinal = labels.get(label);
        if (ordinal != null) {
```

### UnusedAssignment
Variable `next` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java`
#### Snippet
```java
      try {
        scanner = labelsTable.getScanner(scan);
        Result next = null;
        while ((next = scanner.next()) != null) {
          byte[] row = next.getRow();
```

### UnusedAssignment
Variable `scan` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java

  public static Scan extractScanFromConf(Configuration conf) throws IOException {
    Scan scan = null;
    if (conf.get(TableInputFormat.SCAN) != null) {
      scan = TableMapReduceUtil.convertStringToScan(conf.get(TableInputFormat.SCAN));
```

### UnusedAssignment
Variable `writerPath` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
        // If this is a new column family, verify that the directory exists
        if (wl == null) {
          Path writerPath = null;
          if (writeMultipleTables) {
            Path tableRelPath = getTableRelativePath(tableNameBytes);
```

### UnusedAssignment
Variable `endRow` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        scan.withStartRow(value.getRow());

        byte[] endRow = null;
        if (tableSplit instanceof TableSnapshotInputFormat.TableSnapshotRegionSplit) {
          endRow = ((TableSnapshotInputFormat.TableSnapshotRegionSplit) tableSplit).getRegion()
```

### UnusedAssignment
Variable `ret` initializer `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
   */
  private static String getJar(Class<?> my_class) {
    String ret = null;
    try {
      ret = JarFinder.getJar(my_class);
```

### UnusedAssignment
Variable `code` initializer `""` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
      }
      MBeanInfo minfo;
      String code = "";
      String descriptionStr = null;
      Object attributeinfo = null;
```

### UnusedAssignment
Variable `patternAttr` initializer `null` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
          }
          if (attribute != null) {
            String[] patternAttr = null;
            if (attribute.contains(ASTERICK)) {
              if (attribute.contains(COMMA)) {
```

### UnusedAssignment
Variable `value` initializer `null` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java

    String descriptionStr = description ? attr.getDescription() : null;
    Object value = null;
    try {
      value = mBeanServer.getAttribute(oname, attName);
```

### UnusedAssignment
Variable `numServers` initializer `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java`
#### Snippet
```java
  private boolean emptyRegionServerPresent = false;
  private int numRegions = 0;
  private int numServers = 0;

  public ClusterLoadState(Map<ServerName, List<RegionInfo>> clusterState) {
```

### UnusedAssignment
Variable `numRegions` initializer `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java`
#### Snippet
```java
  private final NavigableMap<ServerAndLoad, List<RegionInfo>> serversByLoad;
  private boolean emptyRegionServerPresent = false;
  private int numRegions = 0;
  private int numServers = 0;

```

### UnusedAssignment
Variable `listener` initializer `null` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java

      for (URI ep : endpoints) {
        ServerConnector listener = null;
        String scheme = ep.getScheme();
        HttpConfiguration httpConfig = new HttpConfiguration();
```

### UnusedAssignment
Variable `blockDistbn` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java

  HDFSBlocksDistribution getBlockDistribution(RegionInfo hri) {
    HDFSBlocksDistribution blockDistbn = null;
    try {
      if (cache.asMap().containsKey(hri)) {
```

### UnusedAssignment
Variable `cache` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java

  // The cache for where regions are located.
  private LoadingCache<RegionInfo, HDFSBlocksDistribution> cache = null;

  RegionHDFSBlockLocationFinder() {
```

### UnusedAssignment
Variable `plan` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    // Get the total region num for the current table
    this.totalRegions = regionInfoList.size();
    FavoredNodesPlan plan = null;
    if (newPlan == null) {
      plan = snapshot.getExistingAssignmentPlan();
```

### UnusedAssignment
Variable `rmiSSL` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java

  public void startConnectorServer(int rmiRegistryPort, int rmiConnectorPort) throws IOException {
    boolean rmiSSL = false;
    boolean authenticate = true;
    String passwordFile = null;
```

### UnusedAssignment
Variable `authenticate` initializer `true` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
  public void startConnectorServer(int rmiRegistryPort, int rmiConnectorPort) throws IOException {
    boolean rmiSSL = false;
    boolean authenticate = true;
    String passwordFile = null;
    String accessFile = null;
```

### UnusedAssignment
Variable `passwordFile` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
    boolean rmiSSL = false;
    boolean authenticate = true;
    String passwordFile = null;
    String accessFile = null;

```

### UnusedAssignment
Variable `accessFile` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
    boolean authenticate = true;
    String passwordFile = null;
    String accessFile = null;

    System.setProperty("java.rmi.server.randomIDs", "true");
```

### UnusedAssignment
Variable `sn` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
    List<ServerName> servers) {
    int numServers = servers.size(); // servers is not null, numServers > 1
    ServerName sn = null;
    final int maxIterations = numServers * 4;
    int iterations = 0;
```

### UnusedAssignment
Variable `clusterState` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
    // earlier. So we want to get the snapshot to see those assignments, but this will only contain
    // replicas of the regions that are passed (for performance).
    Map<ServerName, List<RegionInfo>> clusterState = null;
    if (!hasRegionReplica) {
      clusterState = getRegionAssignmentsByServer(regions);
```

### UnusedAssignment
Variable `metricsBalancer` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
  protected float slop;
  protected volatile RackManager rackManager;
  protected MetricsBalancer metricsBalancer = null;
  protected ClusterMetrics clusterStatus = null;
  protected ServerName masterServerName;
```

### UnusedAssignment
Variable `rp` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
  private void addRegionPlan(final MinMaxPriorityQueue<RegionPlan> regionsToMove,
    final boolean fetchFromTail, final ServerName sn, List<RegionPlan> regionsToReturn) {
    RegionPlan rp = null;
    if (!fetchFromTail) {
      rp = regionsToMove.remove();
```

### UnusedAssignment
Variable `fs` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
  private static FileSystem newInstanceFileSystem(Configuration conf) throws IOException {
    URI uri = FileSystem.getDefaultUri(conf);
    FileSystem fs = null;
    Class<?> clazz = conf.getClass("fs." + uri.getScheme() + ".impl", null);
    if (clazz != null) {
```

### UnusedAssignment
Variable `stream` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
   * have 2 handles; however we presume checksums fail so rarely that we don't care.
   */
  private volatile FSDataInputStream stream = null;
  private volatile FSDataInputStream streamNoFsChecksum = null;
  private final Object streamNoFsChecksumFirstCreateLock = new Object();
```

### UnusedAssignment
Variable `in` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
  private static class FileLinkInputStream extends InputStream
    implements Seekable, PositionedReadable, CanSetDropBehind, CanSetReadahead, CanUnbuffer {
    private FSDataInputStream in = null;
    private Path currentPath = null;
    private long pos = 0;
```

### UnusedAssignment
Variable `i` initializer `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
  int[] addRegionSorted(int[] regions, int regionIndex) {
    int[] newRegions = new int[regions.length + 1];
    int i = 0;
    for (i = 0; i < regions.length; i++) { // find the index to insert
      if (regions[i] > regionIndex) {
```

### UnusedAssignment
Variable `i` initializer `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java

  int[] replaceRegion(int[] regions, int regionIndex, int newRegionIndex) {
    int i = 0;
    for (i = 0; i < regions.length; i++) {
      if (regions[i] == regionIndex) {
```

### UnusedAssignment
Variable `regionPerServerIndex` initializer `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
    colocatedReplicaCountsPerRack = new Int2IntCounterMap[numRacks];

    int regionIndex = 0, regionPerServerIndex = 0;

    for (Map.Entry<ServerName, List<RegionInfo>> entry : clusterState.entrySet()) {
```

### UnusedAssignment
Variable `i` initializer `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
    // TODO: this maybe costly. Consider using linked lists
    int[] newRegions = new int[regions.length - 1];
    int i = 0;
    for (i = 0; i < regions.length; i++) {
      if (regions[i] == regionIndex) {
```

### UnusedAssignment
Variable `nextIndexedKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java

      // the next indexed key
      Cell nextIndexedKey = null;

      // Read the next-level (intermediate or leaf) index block.
```

### UnusedAssignment
Variable `index` initializer `-1` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java

      int lookupLevel = 1; // How many levels deep we are in our lookup.
      int index = -1;

      HFileBlock block = null;
```

### UnusedAssignment
Variable `mid` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      int low = 0;
      int high = numEntries - 1;
      int mid = 0;

      // Entries start after the number of entries and the secondary index.
```

### UnusedAssignment
The value `null` assigned to `curFirstKey` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      // clear current block index chunk
      curChunk.clear();
      curFirstKey = null;
    }

```

### UnusedAssignment
Variable `dataBlockEncoder` initializer `NoOpDataBlockEncoder.INSTANCE` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
   * What kind of data block encoding should be used while reading, writing, and handling cache.
   */
  protected HFileDataBlockEncoder dataBlockEncoder = NoOpDataBlockEncoder.INSTANCE;

  /** Block cache configuration. */
```

### UnusedAssignment
Variable `klass` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
      LOG.debug("Trying to use External l2 cache");
    }
    Class klass = null;

    // Get the class, from the config. s
```

### UnusedAssignment
Variable `bucketCache` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
      }
    }
    BucketCache bucketCache = null;
    try {
      int ioErrorsTolerationDuration =
```

### UnusedAssignment
Variable `nextIndexedKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java

      // the next indexed key
      Cell nextIndexedKey = null;

      // Read the next-level (intermediate or leaf) index block.
```

### UnusedAssignment
Variable `index` initializer `-1` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java

      int lookupLevel = 1; // How many levels deep we are in our lookup.
      int index = -1;

      HFileBlock block = null;
```

### UnusedAssignment
Variable `shouldFinishBlock` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
   */
  protected void checkBlockBoundary() throws IOException {
    boolean shouldFinishBlock = false;
    // This means hbase.writer.unified.encoded.blocksize.ratio was set to something different from 0
    // and we should use the encoding ratio
```

### UnusedAssignment
Variable `totalSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    private final String name;
    private LruCachedBlockQueue queue;
    private long totalSize = 0;
    private long bucketSize;

```

### UnusedAssignment
Variable `bytesToFree` initializer `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    }

    long bytesToFree = 0L;

    try {
```

### UnusedAssignment
Variable `freedDataOverheadPercent` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
      long freedSumMb = 0;
      int heavyEvictionCount = 0;
      int freedDataOverheadPercent = 0;
      long startTime = EnvironmentEdgeManager.currentTime();
      while (this.go) {
```

### UnusedAssignment
Variable `totalSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    private final String name;
    private final LruCachedBlockQueue queue;
    private long totalSize = 0;
    private final long bucketSize;

```

### UnusedAssignment
Variable `offset` initializer `UNSET` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
   * offset is not part of the block header.
   */
  private long offset = UNSET;

  /**
```

### UnusedAssignment
Variable `nextBlockOnDiskSize` initializer `UNSET` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
   * metadata.
   */
  private int nextBlockOnDiskSize = UNSET;

  private ByteBuffAllocator allocator;
```

### UnusedAssignment
Variable `raf` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.java`
#### Snippet
```java
  protected ByteBufferArray bufferArray;
  private final FileChannel fileChannel;
  private RandomAccessFile raf = null;

  public FileMmapIOEngine(String filePath, long capacity) throws IOException {
```

### UnusedAssignment
Variable `accessLen` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java`
#### Snippet
```java
    while (true) {
      FileChannel fileChannel = fileChannels[accessFileNum];
      int accessLen = 0;
      if (endFileNum > accessFileNum) {
        // short the limit;
```

### UnusedAssignment
Variable `favoredNodes` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
        // Get the rack for the primary region server
        String primaryRack = getRackOfServer(primaryRS);
        ServerName[] favoredNodes = null;
        if (getTotalNumberOfRacks() == 1) {
          // Single rack case: have to pick the secondary and tertiary
```

### UnusedAssignment
Variable `newServer` initializer `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
    int i = 0;
    Set<String> randomRacks = Sets.newHashSet();
    ServerName newServer = null;
    do {
      String randomRack = this.getOneRandomRack(skipRackSet);
```

### UnusedAssignment
Variable `responseBufs` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
      Message header = headerBuilder.build();
      ByteBuffer headerBuf = createHeaderAndMessageBytes(m, header, cellBlockSize, cellBlock);
      ByteBuffer[] responseBufs = null;
      int cellBlockBufferSize = 0;
      if (cellBlock != null) {
```

### UnusedAssignment
Variable `reqCleanup` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  protected boolean isError;
  protected ByteBufferListOutputStream cellBlockStream = null;
  protected CallCleanup reqCleanup = null;

  protected final User user;
```

### UnusedAssignment
Variable `bucketEntry` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      long offset = alloc.allocateBlock(len);
      boolean succ = false;
      BucketEntry bucketEntry = null;
      try {
        bucketEntry = new BucketEntry(offset, len, accessCounter, inMemory, createRecycler,
```

### UnusedAssignment
Variable `totalSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

    private CachedEntryQueue queue;
    private long totalSize = 0;
    private long bucketSize;

```

### UnusedAssignment
Variable `saslProviders` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected User user = null;
  protected UserGroupInformation ugi = null;
  protected SaslServerAuthenticationProviders saslProviders = null;

  public ServerRpcConnection(RpcServer rpcServer) {
```

### UnusedAssignment
Variable `offset` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected void processRequest(ByteBuff buf) throws IOException, InterruptedException {
    long totalRequestSize = buf.limit();
    int offset = 0;
    // Here we read in the header. We avoid having pb
    // do its default 4k allocation for CodedInputStream. We force it to use
```

### UnusedAssignment
Variable `md` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
        return;
      }
      MethodDescriptor md = null;
      Message param = null;
      CellScanner cellScanner = null;
```

### UnusedAssignment
Variable `peerConf` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  private CompletableFuture<Void> trySyncTableToPeerCluster(TableName tableName, byte[][] splits,
    ReplicationPeerDescription peer) {
    Configuration peerConf = null;
    try {
      peerConf =
```

### UnusedAssignment
Variable `createTableFuture` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          }
          if (!exist) {
            CompletableFuture<Void> createTableFuture = null;
            if (splits == null) {
              createTableFuture = peerAdmin.createTable(tableDesc);
```

### UnusedAssignment
Variable `newPeerConfig` initializer `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
    addListener(getReplicationPeerConfig(id), (peerConfig, error) -> {
      if (!completeExceptionally(future, error)) {
        ReplicationPeerConfig newPeerConfig = null;
        try {
          newPeerConfig = ReplicationPeerConfigUtil
```

### UnusedAssignment
Variable `abortable` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.java`
#### Snippet
```java
  private final int highPriorityLevel;

  private Abortable abortable = null;

  /**
```

### UnusedAssignment
Variable `listener` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  // maintains the set of client connections and handles idle timeouts
  private ConnectionManager connectionManager;
  private Listener listener = null;
  protected SimpleRpcServerResponder responder = null;

```

### UnusedAssignment
Variable `responder` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  private ConnectionManager connectionManager;
  private Listener listener = null;
  protected SimpleRpcServerResponder responder = null;

  /** Listens on the socket. Creates jobs for the handler threads */
```

### UnusedAssignment
The value `null` assigned to `key` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
                }
              }
              key = null;
            }
          } catch (InterruptedException e) {
```

### UnusedAssignment
Variable `readers` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
    private int currentReader = 0;
    private final int readerPendingConnectionQueueLength;
```

### UnusedAssignment
The value `null` assigned to `c` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      if (count < 0) {
        closeConnection(c);
        c = null;
      } else {
        c.setLastContact(EnvironmentEdgeManager.currentTime());
```

### UnusedAssignment
Variable `acceptChannel` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  private class Listener extends Thread {

    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
```

### UnusedAssignment
Variable `selector` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
    private int currentReader = 0;
```

### UnusedAssignment
Variable `regionBatchSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MobFileCompactionChore.class);
  private HMaster master;
  private int regionBatchSize = 0;// not set - compact all

  public MobFileCompactionChore(HMaster master) {
```

### UnusedAssignment
Variable `sf` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
                currentPath = pp;
                LOG.trace("Store file: {}", pp);
                HStoreFile sf = null;
                byte[] mobRefData = null;
                byte[] bulkloadMarkerData = null;
```

### UnusedAssignment
Variable `mobRefData` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
                LOG.trace("Store file: {}", pp);
                HStoreFile sf = null;
                byte[] mobRefData = null;
                byte[] bulkloadMarkerData = null;
                try {
```

### UnusedAssignment
Variable `bulkloadMarkerData` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
                HStoreFile sf = null;
                byte[] mobRefData = null;
                byte[] bulkloadMarkerData = null;
                try {
                  sf = new HStoreFile(fs, pp, conf, CacheConfig.DISABLED, BloomType.NONE, true);
```

### UnusedAssignment
Variable `map` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
    TableDescriptors htds = master.getTableDescriptors();

    Map<String, TableDescriptor> map = null;
    try {
      map = htds.getAll();
```

### UnusedAssignment
Variable `map` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java`
#### Snippet
```java

  // a ConcurrentHashMap, accesses to this map are synchronized.
  private Map<String, CachedMobFile> map = null;
  // caches access count
  private final AtomicLong count = new AtomicLong(0);
```

### UnusedAssignment
Variable `writer` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/FSHLogProvider.java`
#### Snippet
```java
    Class<? extends Writer> logWriterClass =
      conf.getClass("hbase.regionserver.hlog.writer.impl", ProtobufLogWriter.class, Writer.class);
    Writer writer = null;
    try {
      writer = logWriterClass.getDeclaredConstructor().newInstance();
```

### UnusedAssignment
Variable `map` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
      FileSystem fs = FileSystem.get(rs.getConfiguration());

      Map<String, TableDescriptor> map = null;
      try {
        map = htds.getAll();
```

### UnusedAssignment
Variable `mobCellValueSizeThreshold` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(DefaultMobStoreFlusher.class);
  private final Object flushLock = new Object();
  private long mobCellValueSizeThreshold = 0;
  private Path targetPath;
  private HMobStore mobStore;
```

### UnusedAssignment
Variable `mobFileWriter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java`
#### Snippet
```java
    ThroughputController throughputController, Consumer<Path> writerCreationTracker)
    throws IOException {
    StoreFileWriter mobFileWriter = null;
    int compactionKVMax =
      conf.getInt(HConstants.COMPACTION_KV_MAX, HConstants.COMPACTION_KV_MAX_DEFAULT);
```

### UnusedAssignment
Variable `serverName` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
      logDirName = logFile.getName();
    }
    ServerName serverName = null;
    if (logDirName.endsWith(SPLITTING_EXT)) {
      logDirName = logDirName.substring(0, logDirName.length() - SPLITTING_EXT.length());
```

### UnusedAssignment
Variable `outputSink` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
    private WALSplitter.PipelineController controller;
    private EntryBuffers entryBuffers;
    private OutputSink outputSink = null;

    WriterThread(WALSplitter.PipelineController controller, EntryBuffers entryBuffers,
```

### UnusedAssignment
Variable `key` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
    Cell previousCell = null;
    Mutation m = null;
    WALKeyImpl key = null;
    WALEdit val = null;
    if (logEntry != null) {
```

### UnusedAssignment
Variable `ioOptimizedMode` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
  protected long mobSizeThreshold;
  protected HMobStore mobStore;
  protected boolean ioOptimizedMode = false;

  /*
```

### UnusedAssignment
Variable `fileName` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
    long now = 0;
    boolean hasMore;
    byte[] fileName = null;
    StoreFileWriter mobFileWriter = null;
    /*
```

### UnusedAssignment
Variable `mobCell` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
      (long) request.getFiles().size() * this.store.getColumnFamilyDescriptor().getBlocksize();

    Cell mobCell = null;
    List<String> committedMobWriterFileNames = new ArrayList<>();
    try {
```

### UnusedAssignment
Variable `startTime` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    Monitor monitor = null;
    Thread monitorThread;
    long startTime = 0;
    long currentTimeLength = 0;
    boolean failOnError = conf.getBoolean(HBASE_CANARY_FAIL_ON_ERROR, true);
```

### UnusedAssignment
Variable `currentTimeLength` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    Thread monitorThread;
    long startTime = 0;
    long currentTimeLength = 0;
    boolean failOnError = conf.getBoolean(HBASE_CANARY_FAIL_ON_ERROR, true);
    long timeout = conf.getLong(HBASE_CANARY_TIMEOUT, DEFAULT_TIMEOUT);
```

### UnusedAssignment
Variable `sink` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
  private Configuration conf = null;
  private long interval = 0;
  private Sink sink = null;

  /**
```

### UnusedAssignment
Variable `regions` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        LOG.debug("Reading list of tables and locations");
        List<TableDescriptor> tableDescs = this.admin.listTableDescriptors();
        List<RegionInfo> regions = null;
        for (TableDescriptor tableDesc : tableDescs) {
          try (RegionLocator regionLocator =
```

### UnusedAssignment
Variable `filteredRsAndRMap` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      doFilterRegionServerByName(Map<String, List<RegionInfo>> fullRsAndRMap) {

      Map<String, List<RegionInfo>> filteredRsAndRMap = null;

      if (this.targets != null && this.targets.length > 0) {
```

### UnusedAssignment
Variable `pattern` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      if (this.targets != null && this.targets.length > 0) {
        filteredRsAndRMap = new HashMap<>();
        Pattern pattern = null;
        Matcher matcher = null;
        boolean regExpFound = false;
```

### UnusedAssignment
Variable `matcher` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        filteredRsAndRMap = new HashMap<>();
        Pattern pattern = null;
        Matcher matcher = null;
        boolean regExpFound = false;
        for (String rsName : this.targets) {
```

### UnusedAssignment
Variable `regExpFound` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        Pattern pattern = null;
        Matcher matcher = null;
        boolean regExpFound = false;
        for (String rsName : this.targets) {
          if (this.useRegExp) {
```

### UnusedAssignment
Variable `get` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      TableName tableName = null;
      Table table = null;
      Get get = null;
      byte[] startKey = null;
      Scan scan = null;
```

### UnusedAssignment
Variable `startKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      Table table = null;
      Get get = null;
      byte[] startKey = null;
      Scan scan = null;
      StopWatch stopWatch = new StopWatch();
```

### UnusedAssignment
Variable `scan` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      Get get = null;
      byte[] startKey = null;
      Scan scan = null;
      StopWatch stopWatch = new StopWatch();
      // monitor one region on every region server
```

### UnusedAssignment
The value `null` assigned to `scan` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          }
        }
        scan = null;
        get = null;
        startKey = null;
```

### UnusedAssignment
The value `null` assigned to `get` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        }
        scan = null;
        get = null;
        startKey = null;
      }
```

### UnusedAssignment
The value `null` assigned to `startKey` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        scan = null;
        get = null;
        startKey = null;
      }
      return null;
```

### UnusedAssignment
Variable `startKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

    private Void readColumnFamily(Table table, ColumnFamilyDescriptor column) {
      byte[] startKey = null;
      Get get = null;
      Scan scan = null;
```

### UnusedAssignment
Variable `tableNames` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    private boolean checkNoTableNames() {
      List<String> foundTableNames = new ArrayList<>();
      TableName[] tableNames = null;
      LOG.debug("Reading list of tables");
      try {
```

### UnusedAssignment
Variable `tableDesc` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    public Void read() {
      Table table = null;
      TableDescriptor tableDesc = null;
      try {
        LOG.debug("Reading table descriptor for table {}", region.getTable());
```

### UnusedAssignment
Variable `returnTables` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    /** Returns List of tables to use in test. */
    private String[] generateMonitorTables(String[] monitorTargets) throws IOException {
      String[] returnTables = null;

      if (this.useRegExp) {
```

### UnusedAssignment
Variable `tds` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      if (this.useRegExp) {
        Pattern pattern = null;
        List<TableDescriptor> tds = null;
        Set<String> tmpTables = new TreeSet<>();
        try {
```

### UnusedAssignment
Variable `allowedFailures` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    protected boolean done = false;
    protected int errorCode = 0;
    protected long allowedFailures = 0;
    protected Sink sink;
    protected ExecutorService executor;
```

### UnusedAssignment
Variable `table` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
     */
    private Void write() {
      Table table = null;
      TableDescriptor tableDesc = null;
      try {
```

### UnusedAssignment
Variable `tableDesc` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    private Void write() {
      Table table = null;
      TableDescriptor tableDesc = null;
      try {
        table = connection.getTable(region.getTable());
```

### UnusedAssignment
Variable `message` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/TableDescriptorChecker.java`
#### Snippet
```java
        }
        // FIFOCompaction
        String message = null;

        // 1. Check TTL
```

### UnusedAssignment
Variable `hRegionInfos` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
    byte[][] splitKeys) {
    long regionId = EnvironmentEdgeManager.currentTime();
    RegionInfo[] hRegionInfos = null;
    if (splitKeys == null || splitKeys.length == 0) {
      hRegionInfos = new RegionInfo[] { RegionInfoBuilder.newBuilder(tableDescriptor.getTableName())
```

### UnusedAssignment
Variable `endKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
      hRegionInfos = new RegionInfo[numRegions];
      byte[] startKey = null;
      byte[] endKey = null;
      for (int i = 0; i < numRegions; i++) {
        endKey = (i == splitKeys.length) ? null : splitKeys[i];
```

### UnusedAssignment
Variable `shutdownHookThread` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ShutdownHookManager.java`
#### Snippet
```java
    @Override
    public boolean removeShutdownHook(Runnable shutdownHook) {
      Thread shutdownHookThread = null;
      if (!(shutdownHook instanceof Thread)) {
        shutdownHookThread = new Thread(shutdownHook);
```

### UnusedAssignment
Variable `htd` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
    // Clear the cache to forcibly refresh region information
    connection.clearRegionLocationCache();
    TableDescriptor htd = null;
    try (Table table = connection.getTable(tableName)) {
      htd = table.getDescriptor();
```

### UnusedAssignment
The value `splitAlgo.firstRow()` assigned to `sk` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
          for (RegionInfo hri : check.toArray(new RegionInfo[check.size()])) {
            byte[] sk = hri.getStartKey();
            if (sk.length == 0) sk = splitAlgo.firstRow();

            HRegionFileSystem regionFs = HRegionFileSystem
```

### UnusedAssignment
Variable `tmpRegionSet` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java

      // Get a list of daughter regions to create
      LinkedList<Pair<byte[], byte[]>> tmpRegionSet = null;
      try (Table table = connection.getTable(tableName)) {
        tmpRegionSet = getSplits(connection, tableName, splitAlgo);
```

### UnusedAssignment
Variable `local_finished` initializer `Lists.newLinkedList()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java

                LinkedList<Pair<byte[], byte[]>> finished = Lists.newLinkedList();
                LinkedList<Pair<byte[], byte[]>> local_finished = Lists.newLinkedList();
                if (conf.getBoolean("split.verify", true)) {
                  // we need to verify and rate-limit our splits
```

### UnusedAssignment
Variable `tmp` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
      // Query meta for all regions in the table
      Set<Pair<byte[], byte[]>> rows = Sets.newHashSet();
      Pair<byte[][], byte[][]> tmp = null;
      try (RegionLocator regionLocator = connection.getRegionLocator(tableName)) {
        tmp = regionLocator.getStartEndKeys();
```

### UnusedAssignment
Variable `metaEntry` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HbckRegionInfo.class.getName());

  private MetaEntry metaEntry = null; // info in META
  private HdfsEntry hdfsEntry = null; // info in HDFS
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
```

### UnusedAssignment
Variable `result` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java`
#### Snippet
```java
  @Override
  public E take() throws InterruptedException {
    E result = null;
    lock.lockInterruptibly();
    try {
```

### UnusedAssignment
Variable `cc` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
    writer.appendFileInfo(Bytes.toBytes("compressioninfokey"), Bytes.toBytes("compressioninfoval"));
    writer.close();
    Cell cc = null;
    HFile.Reader reader = HFile.createReader(fs, path, CacheConfig.DISABLED, true, conf);
    try {
```

### UnusedAssignment
Variable `fixOverlaps` initializer `true` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
    Configuration conf;

    boolean fixOverlaps = true;

    HDFSIntegrityFixer(HbckTableInfo ti, HbckErrorReporter errors, Configuration conf,
```

### UnusedAssignment
Variable `statuses` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
   */
  protected void checkMobColFamDir(Path cfDir) throws IOException {
    FileStatus[] statuses = null;
    try {
      statuses = fs.listStatus(cfDir); // use same filter as scanner.
```

### UnusedAssignment
Variable `statuses` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
   */
  protected void checkRegionDir(Path regionDir) throws IOException {
    FileStatus[] statuses = null;
    try {
      statuses = fs.listStatus(regionDir);
```

### UnusedAssignment
Variable `hfs` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
      return;
    }
    FileStatus[] hfs = null;
    try {
      hfs = fs.listStatus(regionDir, new FamilyDirFilter(fs));
```

### UnusedAssignment
Variable `statuses` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
   */
  protected void checkColFamDir(Path cfDir) throws IOException {
    FileStatus[] statuses = null;
    try {
      statuses = fs.listStatus(cfDir); // use same filter as scanner.
```

### UnusedAssignment
Variable `sn` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
            return true;
          }
          ServerName sn = null;
          if (
            rl.getRegionLocation(RegionInfo.DEFAULT_REPLICA_ID) == null
```

### UnusedAssignment
Variable `m` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
            hri = h.getRegion();

            HbckRegionInfo.MetaEntry m = null;
            if (hri.getReplicaId() == RegionInfo.DEFAULT_REPLICA_ID) {
              m = new HbckRegionInfo.MetaEntry(hri, sn, ts, daughters.getFirst(),
```

### UnusedAssignment
Variable `dirs` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    // rename the contained into the container.
    FileSystem fs = targetRegionDir.getFileSystem(getConf());
    FileStatus[] dirs = null;
    try {
      dirs = fs.listStatus(contained.getHdfsRegionDir());
```

### UnusedAssignment
Variable `exception` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    private FSDataOutputStream createFileWithRetries(final FileSystem fs,
      final Path hbckLockFilePath, final FsPermission defaultPerms) throws IOException {
      IOException exception = null;
      do {
        try {
```

### UnusedAssignment
Variable `zkw` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private Map<TableName, Set<String>> skippedRegions = new HashMap<>();

  private ZKWatcher zkw = null;
  private String hbckEphemeralNodePath = null;
  private boolean hbckZodeCreated = false;
```

### UnusedAssignment
Variable `success` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    Path sidelineRegionDir = new Path(sidelineTableDir, regionDir.getName());
    fs.mkdirs(sidelineRegionDir);
    boolean success = false;
    FileStatus[] cfs = fs.listStatus(regionDir);
    if (cfs == null) {
```

### UnusedAssignment
Variable `commandLine` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java

    final CommandLineParser cmdLineParser = new DefaultParser();
    CommandLine commandLine = null;
    try {
      commandLine = cmdLineParser.parse(options, args);
```

### UnusedAssignment
Variable `filter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java`
#### Snippet
```java
@InterfaceAudience.Private
final public class FilterWrapper extends Filter {
  Filter filter = null;

  /**
```

### UnusedAssignment
Variable `pair` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
    fsDelegationToken.acquireDelegationToken(queue.peek().getFilePath().getFileSystem(getConf()));
    bulkToken = FutureUtils.get(conn.prepareBulkLoad(tableName));
    Pair<Multimap<ByteBuffer, LoadQueueItem>, Set<String>> pair = null;

    Map<LoadQueueItem, ByteBuffer> item2RegionMap = new HashMap<>();
```

### UnusedAssignment
Variable `existingServer` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
   */
  boolean checkAndRecordNewServer(final ServerName serverName, final ServerMetrics sl) {
    ServerName existingServer = null;
    synchronized (this.onlineServers) {
      existingServer = findServerWithSameHostnamePortWithLock(serverName);
```

### UnusedAssignment
Variable `persistFlushedSequenceId` initializer `true` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
    new ConcurrentSkipListMap<>(Bytes.BYTES_COMPARATOR);

  private boolean persistFlushedSequenceId = true;
  private volatile boolean isFlushSeqIdPersistInProgress = false;
  /** File on hdfs to store last flushed sequence id of regions */
```

### UnusedAssignment
Variable `maxRegions` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/TableNamespaceManager.java`
#### Snippet
```java
  public static long getMaxRegions(NamespaceDescriptor ns) throws IOException {
    String value = ns.getConfigurationValue(KEY_MAX_REGIONS);
    long maxRegions = 0;
    if (StringUtils.isNotEmpty(value)) {
      try {
```

### UnusedAssignment
Variable `maxTables` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/TableNamespaceManager.java`
#### Snippet
```java
  public static long getMaxTables(NamespaceDescriptor ns) throws IOException {
    String value = ns.getConfigurationValue(KEY_MAX_TABLES);
    long maxTables = 0;
    if (StringUtils.isNotEmpty(value)) {
      try {
```

### UnusedAssignment
Variable `currentLocality` initializer `0f` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {
            ServerName server = servers.get(p.ordinal());
            Float currentLocality = 0f;
            if (servers != null) {
              currentLocality = regionLocality.get(server.getHostname());
```

### UnusedAssignment
Variable `splitTime` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
   */
  public void splitLog(final Set<ServerName> serverNames, PathFilter filter) throws IOException {
    long splitTime = 0, splitLogSize = 0;
    List<Path> logDirs = getLogDirs(serverNames);

```

### UnusedAssignment
Variable `splitLogSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
   */
  public void splitLog(final Set<ServerName> serverNames, PathFilter filter) throws IOException {
    long splitTime = 0, splitLogSize = 0;
    List<Path> logDirs = getLogDirs(serverNames);

```

### UnusedAssignment
The value `splitDir` assigned to `logDir` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
            throw new IOException("Failed fs.rename for log split: " + logDir);
          }
          logDir = splitDir;
          LOG.debug("Renamed region directory: " + splitDir);
        } else if (!fs.exists(splitDir)) {
```

### UnusedAssignment
Variable `perms` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
        server.getMasterCoprocessorHost().preGetUserPermissions(userName, namespace, table, cf, cq);

        List<UserPermission> perms = null;
        if (permissionType == Type.Table) {
          boolean filter = (cf != null || userName != null) ? true : false;
```

### UnusedAssignment
Variable `cid` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      // If not pb'd, make it so.
      if (!ProtobufUtil.isPBMagicPrefix(content)) {
        String cid = null;
        in = fs.open(idPath);
        try {
```

### UnusedAssignment
Variable `status` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    throws IOException, DeserializationException {
    final Path versionFile = new Path(rootdir, HConstants.VERSION_FILE_NAME);
    FileStatus[] status = null;
    try {
      // hadoop 2.0 throws FNFE if directory does not exist.
```

### UnusedAssignment
Variable `status` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
  public static List<FileStatus> listStatusWithStatusFilter(final FileSystem fs, final Path dir,
    final FileStatusFilter filter) throws IOException {
    FileStatus[] status = null;
    try {
      status = fs.listStatus(dir);
```

### UnusedAssignment
Variable `tableCount` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java`
#### Snippet
```java
    }

    int tableCount = 0;
    try {
      tableCount = env.getMasterServices().listTableDescriptorsByNamespace(namespaceName).size();
```

### UnusedAssignment
Variable `desc` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java

    // check to see if the table exists
    TableDescriptor desc = null;
    try {
      desc = master.getTableDescriptors().get(TableName.valueOf(snapshot.getTable()));
```

### UnusedAssignment
Variable `task` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
    try {
      while (running) {
        HFileDeleteTask task = null;
        try {
          task = queue.take();
```

### UnusedAssignment
The value `metaLock` assigned to `queue` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
        break;
      case META:
        queue = metaLock;
      default:
        queue = null;
```

### UnusedAssignment
Variable `request` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
    AsyncRegionServerAdmin admin =
      env.getMasterServices().getAsyncClusterConnection().getRegionServerAdmin(regionLocation);
    GetRegionInfoRequest request = null;
    if (includeBestSplitRow) {
      request = RequestConverter.buildGetRegionInfoRequest(hri.getRegionName(), false, true);
```

### UnusedAssignment
Variable `finished` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java`
#### Snippet
```java
        return Flow.HAS_MORE_STATE;
      case RELEASE_WORKER:
        boolean finished = false;
        try {
          finished = syncReplicationReplayWALManager.isReplayWALFinished(wals.get(0));
```

### UnusedAssignment
Variable `lastUpdate` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      + "are mostly synchronized...but to me it looks like they are totally synchronized")
public class QuotaState {
  protected long lastUpdate = 0;
  protected long lastQuery = 0;

```

### UnusedAssignment
Variable `peerClusterId` initializer `""` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java`
#### Snippet
```java

  private void checkSameClusterKey(String clusterKey) throws DoNotRetryIOException {
    String peerClusterId = "";
    try {
      // Create the peer cluster config for get peer cluster id
```

### UnusedAssignment
Variable `reqCapacityUnitLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
  private RateLimiter readCapacityUnitLimiter = null;
```

### UnusedAssignment
Variable `readSizeLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
```

### UnusedAssignment
Variable `writeSizeLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
```

### UnusedAssignment
Variable `readCapacityUnitLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
  private RateLimiter readCapacityUnitLimiter = null;

  private TimeBasedLimiter() {
```

### UnusedAssignment
Variable `writeCapacityUnitLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
  private RateLimiter readCapacityUnitLimiter = null;

```

### UnusedAssignment
Variable `readReqsLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
```

### UnusedAssignment
Variable `writeReqsLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter reqsLimiter = null;
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
```

### UnusedAssignment
Variable `reqsLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
public class TimeBasedLimiter implements QuotaLimiter {
  private static final Configuration conf = HBaseConfiguration.create();
  private RateLimiter reqsLimiter = null;
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
```

### UnusedAssignment
Variable `reqSizeLimiter` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private static final Configuration conf = HBaseConfiguration.create();
  private RateLimiter reqsLimiter = null;
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
```

### UnusedAssignment
Variable `filterUser` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
  public User validateCallerWithFilterUser(User caller, TablePermission tPerm, String inputUserName)
    throws IOException {
    User filterUser = null;
    if (!caller.getShortName().equals(inputUserName)) {
      // User should have admin privilege if checking permission for other users
```

### UnusedAssignment
Variable `row` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    Get get = new Get(entryName);
    get.addFamily(ACL_LIST_FAMILY);
    Result row = null;
    if (t == null) {
      try (Connection connection = ConnectionFactory.createConnection(conf)) {
```

### UnusedAssignment
Variable `scanner` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    scan.addFamily(ACL_LIST_FAMILY);

    ResultScanner scanner = null;
    // TODO: Pass in a Connection rather than create one each time.
    try (Connection connection = ConnectionFactory.createConnection(conf)) {
```

### UnusedAssignment
Variable `labelOffset` initializer `++index` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
          break;
        case DOUBLE_QUOTES:
          int labelOffset = ++index;
          // We have to rewrite the expression within double quotes as incase of expressions
          // with escape characters we may have to avoid them as the original expression did
```

### UnusedAssignment
Variable `deleteCellVisTagsFormat` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java`
#### Snippet
```java
    // If tag is present in the delete
    boolean hasVisTag = false;
    Byte deleteCellVisTagsFormat = null;
    switch (type) {
      case DeleteFamily:
```

### UnusedAssignment
Variable `visibilityLabels` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java

  public void refreshLabelsCache(byte[] data) throws IOException {
    List<VisibilityLabel> visibilityLabels = null;
    try {
      visibilityLabels = VisibilityUtils.readLabelsFromZKData(data);
```

### UnusedAssignment
Variable `groupAuthOrdinals` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
      Set<Integer> authOrdinals = new HashSet<>();
      if (groups != null && groups.length > 0) {
        Set<Integer> groupAuthOrdinals = null;
        for (String group : groups) {
          groupAuthOrdinals = groupAuths.get(group);
```

### UnusedAssignment
Variable `multiUserAuths` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java

  public void refreshUserAuthsCache(byte[] data) throws IOException {
    MultiUserAuthorizations multiUserAuths = null;
    try {
      multiUserAuths = VisibilityUtils.readUserAuthsFromZKData(data);
```

### UnusedAssignment
Variable `ordinal` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
  @Override
  public int getLabelOrdinal(String label) {
    Integer ordinal = null;
    this.lock.readLock().lock();
    try {
```

### UnusedAssignment
Variable `node` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
    boolean withSerializationFormat, boolean checkAuths, Set<Integer> auths,
    VisibilityLabelOrdinalProvider ordinalProvider) throws IOException {
    ExpressionNode node = null;
    try {
      node = EXP_PARSER.parse(visExpression);
```

### UnusedAssignment
Variable `identifier` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
    throws IOException, InvalidLabelException {
    if (node.isSingleNode()) {
      String identifier = null;
      int labelOrdinal = 0;
      if (node instanceof LeafExpressionNode) {
```

### UnusedAssignment
Variable `labelOrdinal` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
    if (node.isSingleNode()) {
      String identifier = null;
      int labelOrdinal = 0;
      if (node instanceof LeafExpressionNode) {
        identifier = ((LeafExpressionNode) node).getIdentifier();
```

### UnusedAssignment
Variable `link` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java`
#### Snippet
```java

    // check if the linked file exists (in the archive, or in the table dir)
    HFileLink link = null;
    if (MobUtils.isMobRegionInfo(regionInfo)) {
      // for mob region
```

### UnusedAssignment
Variable `tracker` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
        Object familyData = visitor.familyOpen(regionData, cfd.getName());
        monitor.rethrowException();
        StoreFileTracker tracker = null;
        if (isMobRegion) {
          // MOB regions are always using the default SFT implementation
```

### UnusedAssignment
Variable `implClass` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureManagerHost.java`
#### Snippet
```java
   */
  protected void loadUserProcedures(Configuration conf, String confKey) {
    Class<?> implClass = null;

    // load default procedures from configure file
```

### UnusedAssignment
Variable `o` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureManagerHost.java`
#### Snippet
```java
    // create the instance
    E impl;
    Object o = null;
    try {
      o = implClass.getDeclaredConstructor().newInstance();
```

### UnusedAssignment
Variable `cellVisibility` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    }

    CellVisibility cellVisibility = null;
    try {
      cellVisibility = delete.getCellVisibility();
```

### UnusedAssignment
Variable `authorizations` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    }
    Region region = e.getEnvironment().getRegion();
    Authorizations authorizations = null;
    try {
      authorizations = get.getAuthorizations();
```

### UnusedAssignment
Variable `cellVisibility` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    for (int i = 0; i < miniBatchOp.size(); i++) {
      Mutation m = miniBatchOp.getOperation(i);
      CellVisibility cellVisibility = null;
      try {
        cellVisibility = m.getCellVisibility();
```

### UnusedAssignment
Variable `cellVisibility` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  private Cell createNewCellWithTags(Mutation mutation, Cell newCell) throws IOException {
    List<Tag> tags = Lists.newArrayList();
    CellVisibility cellVisibility = null;
    try {
      cellVisibility = mutation.getCellVisibility();
```

### UnusedAssignment
Variable `authorizations` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    }
    Region region = e.getEnvironment().getRegion();
    Authorizations authorizations = null;
    try {
      authorizations = scan.getAuthorizations();
```

### UnusedAssignment
Variable `msg` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
      rethrowException();
    } catch (Exception e) {
      String msg = null;
      if (e instanceof InterruptedException) {
        msg = "Procedure '" + barrierName + "' aborting due to interrupt!"
```

### UnusedAssignment
Variable `removed` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java`
#### Snippet
```java
   */
  public void barrierReleasedByMember(String member, byte[] dataFromMember) {
    boolean removed = false;
    synchronized (joinBarrierLock) {
      removed = this.inBarrierMembers.remove(member);
```

### UnusedAssignment
Variable `stillGettingNotifications` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
  final public void resetMembers(Procedure proc) throws IOException {
    String procName = proc.getName();
    boolean stillGettingNotifications = false;
    do {
      try {
```

### UnusedAssignment
Variable `state` initializer `Coprocessor.State.UNINSTALLED` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseEnvironment.java`
#### Snippet
```java
  protected int priority = Coprocessor.PRIORITY_USER;
  /** Current coprocessor state */
  Coprocessor.State state = Coprocessor.State.UNINSTALLED;
  private int seq;
  private Configuration conf;
```

### UnusedAssignment
Variable `priority` initializer `Coprocessor.PRIORITY_USER` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseEnvironment.java`
#### Snippet
```java
  public C impl;
  /** Chaining priority */
  protected int priority = Coprocessor.PRIORITY_USER;
  /** Current coprocessor state */
  Coprocessor.State state = Coprocessor.State.UNINSTALLED;
```

### UnusedAssignment
Variable `bld` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java`
#### Snippet
```java
  public Cell filterCell(Cell cell, Predicate<byte[]> famPredicate) {
    byte[] fam;
    BulkLoadDescriptor bld = null;
    try {
      bld = WALEdit.getBulkLoadDescriptor(cell);
```

### UnusedAssignment
Variable `currentPositionOfEntry` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java`
#### Snippet
```java
  // choose to return before reading the current entry, so it is not safe to return the value below
  // in getPosition.
  private long currentPositionOfEntry = 0;
  // position after reading current entry
  private long currentPositionOfReader = 0;
```

### UnusedAssignment
Variable `exist` initializer `true` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
        for (Entry e : oldEntries) {
          TableName tableName = e.getKey().getTableName();
          boolean exist = true;
          if (existMap.containsKey(tableName)) {
            exist = existMap.get(tableName);
```

### UnusedAssignment
Variable `clusterId` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java`
#### Snippet
```java
      throw new IOException("Failed replication handler create", e);
    }
    UUID clusterId = null;
    try {
      clusterId = ZKClusterId.getUUIDForCluster(this.server.getZooKeeper());
```

### UnusedAssignment
Variable `cmd` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    DumpOptions opts = new DumpOptions();

    String cmd = null;
    while ((cmd = args.poll()) != null) {
      if (cmd.equals("-h") || cmd.equals("--h") || cmd.equals("--help")) {
```

### UnusedAssignment
Variable `errCode` initializer `-1` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
  public int run(String[] args) throws Exception {

    int errCode = -1;
    LinkedList<String> argv = new LinkedList<>();
    argv.addAll(Arrays.asList(args));
```

### UnusedAssignment
Variable `allQueues` initializer `Collections.emptyList()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java

  List<String> getAllQueues() throws IOException {
    List<String> allQueues = Collections.emptyList();
    try {
      allQueues = queueStorage.getAllQueues(server.getServerName());
```

### UnusedAssignment
Variable `added` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java
   */
  public void addPeer(String peerId) throws IOException {
    boolean added = false;
    try {
      added = this.replicationPeers.addPeer(peerId);
```

### UnusedAssignment
Variable `hdfsClientFinalizer` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java`
#### Snippet
```java
      // registered as a shutdown hook. If the latter is present, then we are
      // on 0.21 or cloudera patched 0.20.
      Runnable hdfsClientFinalizer = null;
      // Look into the FileSystem#Cache class for clientFinalizer
      Class<?>[] classes = FileSystem.class.getDeclaredClasses();
```

### UnusedAssignment
Variable `latestZKVersion` initializer `FAILED_TO_OWN_TASK` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
  protected static int attemptToOwnTask(boolean isFirstTime, ZKWatcher zkw, ServerName server,
    String task, int taskZKVersion) {
    int latestZKVersion = FAILED_TO_OWN_TASK;
    try {
      SplitLogTask slt = new SplitLogTask.Owned(server);
```

### UnusedAssignment
Variable `reader` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
  public StoreFileReader createReader(ReaderContext context, CacheConfig cacheConf)
    throws IOException {
    StoreFileReader reader = null;
    if (this.reference != null) {
      reader = new HalfStoreFileReader(context, hfileInfo, cacheConf, reference, this, conf);
```

### UnusedAssignment
Variable `start` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
    }

    long start = 0;
    long length = 0;

```

### UnusedAssignment
Variable `length` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java

    long start = 0;
    long length = 0;

    if (Reference.isTopFileRegion(reference.getFileRegion())) {
```

### UnusedAssignment
Variable `memstoreReadCount` initializer `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.java`
#### Snippet
```java
          localMetricsTableMap.put(tbl, mt);
        }
        long memstoreReadCount = 0L;
        long mixedReadCount = 0L;
        String tempKey = null;
```

### UnusedAssignment
Variable `mixedReadCount` initializer `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.java`
#### Snippet
```java
        }
        long memstoreReadCount = 0L;
        long mixedReadCount = 0L;
        String tempKey = null;
        if (r.getStores() != null) {
```

### UnusedAssignment
Variable `tempKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.java`
#### Snippet
```java
        long memstoreReadCount = 0L;
        long mixedReadCount = 0L;
        String tempKey = null;
        if (r.getStores() != null) {
          String familyName = null;
```

### UnusedAssignment
Variable `familyName` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.java`
#### Snippet
```java
        String tempKey = null;
        if (r.getStores() != null) {
          String familyName = null;
          for (Store store : r.getStores()) {
            familyName = store.getColumnFamilyName();
```

### UnusedAssignment
Variable `heap` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreMergerSegmentsIterator.java`
#### Snippet
```java

  // heap of scanners, lazily initialized
  private KeyValueHeap heap = null;
  // remember the initial version of the scanners list
  List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>();
```

### UnusedAssignment
Variable `cacheMobBlocks` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(ReversedMobStoreScanner.class);
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
```

### UnusedAssignment
Variable `rawMobScan` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ReversedMobStoreScanner.class);
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
```

### UnusedAssignment
Variable `readEmptyValueOnMobCellMiss` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.java`
#### Snippet
```java
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
  private final List<MobCell> referencedMobCells;
```

### UnusedAssignment
Variable `nextLeaseDelay` initializer `Long.MAX_VALUE` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LeaseManager.java`
#### Snippet
```java
    long toWait = leaseCheckFrequency;
    Lease nextLease = null;
    long nextLeaseDelay = Long.MAX_VALUE;

    while (!stopRequested || (stopRequested && !leases.isEmpty())) {
```

### UnusedAssignment
Variable `segmentCells` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/VersionedSegmentsList.java`
#### Snippet
```java
  // Estimates fraction of unique keys
  double getEstimatedUniquesFrac() {
    int segmentCells = 0;
    int maxCells = 0;
    double est = 0;
```

### UnusedAssignment
Variable `more` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    boolean foundColumn = false;
    try {
      boolean more = false;
      ScannerContext scannerContext = ScannerContext.newBuilder().setBatchLimit(1).build();

```

### UnusedAssignment
Variable `authResult` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
      TableName table = c.getEnvironment().getRegion().getRegionInfo().getTable();
      Map<byte[], ? extends Collection<byte[]>> families = makeFamilyMap(family, qualifier);
      AuthResult authResult = null;
      User user = getActiveUser(c);
      if (
```

### UnusedAssignment
Variable `getUserPermissionsRequest` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
          request.hasColumnQualifier() ? request.getColumnQualifier().toByteArray() : null;
        preGetUserPermissions(caller, userName, namespace, table, cf, cq);
        GetUserPermissionsRequest getUserPermissionsRequest = null;
        if (request.getType() == AccessControlProtos.Permission.Type.Table) {
          getUserPermissionsRequest = GetUserPermissionsRequest.newBuilder(table).withFamily(cf)
```

### UnusedAssignment
Variable `authResult` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
      TableName table = c.getEnvironment().getRegion().getRegionInfo().getTable();
      Map<byte[], ? extends Collection<byte[]>> families = makeFamilyMap(family, qualifier);
      AuthResult authResult = null;
      User user = getActiveUser(c);
      if (
```

### UnusedAssignment
Variable `families` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    User user = getActiveUser(c);
    RegionCoprocessorEnvironment env = c.getEnvironment();
    Map<byte[], ? extends Collection<byte[]>> families = null;
    switch (opType) {
      case GET:
```

### UnusedAssignment
Variable `authResult` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
            continue;
          }
          AuthResult authResult = null;
          if (
            checkCoveringPermission(user, opType, c.getEnvironment(), m.getRow(),
```

### UnusedAssignment
Variable `onlyMetaRefresh` initializer `true` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java`
#### Snippet
```java
  private long hfileTtl;
  private int period;
  private boolean onlyMetaRefresh = true;

  // ts of last time regions store files are refreshed
```

### UnusedAssignment
Variable `regionSpecifier` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSAnnotationReadingPriorityFunction.java`
#### Snippet
```java
    String cls = param.getClass().getName();
    Class<? extends Message> rpcArgClass = argumentToClassMap.get(cls);
    RegionSpecifier regionSpecifier = null;
    // check whether the request has reference to meta region or now.
    try {
```

### UnusedAssignment
Variable `result` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactor.java`
#### Snippet
```java
    throws IOException {

    ImmutableSegment result = null;
    MemStoreSegmentsIterator iterator = null;
    List<ImmutableSegment> segments = versionedList.getStoreSegments();
```

### UnusedAssignment
Variable `iterator` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactor.java`
#### Snippet
```java

    ImmutableSegment result = null;
    MemStoreSegmentsIterator iterator = null;
    List<ImmutableSegment> segments = versionedList.getStoreSegments();
    for (ImmutableSegment s : segments) {
```

### UnusedAssignment
Variable `tempMaxCompactionQueueSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionWrapperImpl.java`
#### Snippet
```java
      long tempMinStoreFileAge = Long.MAX_VALUE;
      long tempNumReferenceFiles = 0;
      long tempMaxCompactionQueueSize = 0;
      long tempMaxFlushQueueSize = 0;
      long avgAgeNumerator = 0;
```

### UnusedAssignment
Variable `tempMaxFlushQueueSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionWrapperImpl.java`
#### Snippet
```java
      long tempNumReferenceFiles = 0;
      long tempMaxCompactionQueueSize = 0;
      long tempMaxFlushQueueSize = 0;
      long avgAgeNumerator = 0;
      long numHFiles = 0;
```

### UnusedAssignment
Variable `readyToWriteCount` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  private final int lastIndexExclusive;

  private int readyToWriteCount = 0;
  private int cellCount = 0;
  private int numOfPuts = 0;
```

### UnusedAssignment
Variable `kvKey` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
    }
    // Used in ROW_COL bloom
    Cell kvKey = null;
    // Already if the incoming key is a fake rowcol key then use it as it is
    if (cell.getTypeByte() == KeyValue.Type.Maximum.getCode() && cell.getFamilyLength() == 0) {
```

### UnusedAssignment
Variable `key` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java

    // Used in ROW bloom
    byte[] key = null;
    if (rowOffset != 0 || rowLen != row.length) {
      throw new AssertionError("For row-only Bloom filters the row must occupy the whole array");
```

### UnusedAssignment
Variable `lastIOE` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
   */
  boolean deleteDir(Path dir) throws IOException {
    IOException lastIOE = null;
    int i = 0;
    do {
```

### UnusedAssignment
Variable `lastIOE` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
  boolean createDir(Path dir) throws IOException {
    int i = 0;
    IOException lastIOE = null;
    do {
      try {
```

### UnusedAssignment
Variable `lastIOE` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
    throws IOException {
    int i = 0;
    IOException lastIOE = null;
    int hdfsClientRetriesNumber =
      conf.getInt("hdfs.client.retries.number", DEFAULT_HDFS_CLIENT_RETRIES_NUMBER);
```

### UnusedAssignment
Variable `lastIOE` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
   */
  boolean rename(Path srcpath, Path dstPath) throws IOException {
    IOException lastIOE = null;
    int i = 0;
    do {
```

### UnusedAssignment
Variable `factor` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java

  private void initInmemoryFlushSize(Configuration conf) {
    double factor = 0;
    long memstoreFlushSize = getRegionServices().getMemStoreFlushSize();
    int numStores = getRegionServices().getNumStores();
```

### UnusedAssignment
Variable `indexType` initializer `IndexType.ARRAY_MAP` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java
  }

  private IndexType indexType = IndexType.ARRAY_MAP; // default implementation

  public static final long DEEP_OVERHEAD =
```

### UnusedAssignment
Variable `scannersForDelayedClose` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
  // scans are collected and when the final scanner.close() happens will perform the
  // actual close.
  protected List<KeyValueScanner> scannersForDelayedClose = null;

  /**
```

### UnusedAssignment
Variable `filesCompactingClone` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  @Override
  public boolean needsCompaction() {
    List<HStoreFile> filesCompactingClone = null;
    synchronized (filesCompacting) {
      filesCompactingClone = Lists.newArrayList(filesCompacting);
```

### UnusedAssignment
Variable `request` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java

    final CompactionContext compaction = storeEngine.createCompaction();
    CompactionRequestImpl request = null;
    this.storeEngine.readLock();
    try {
```

### UnusedAssignment
Variable `ms` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  /** Returns MemStore Instance to use in this store. */
  private MemStore getMemstore() {
    MemStore ms = null;
    // Check if in-memory-compaction configured. Note MemoryCompactionPolicy is an enum!
    MemoryCompactionPolicy inMemoryCompaction = null;
```

### UnusedAssignment
Variable `inMemoryCompaction` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    MemStore ms = null;
    // Check if in-memory-compaction configured. Note MemoryCompactionPolicy is an enum!
    MemoryCompactionPolicy inMemoryCompaction = null;
    if (this.getTableName().isSystemTable()) {
      inMemoryCompaction = MemoryCompactionPolicy
```

### UnusedAssignment
Variable `regionsBySize` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
   */
  private boolean flushOneForGlobalPressure(FlushType flushType) {
    SortedMap<Long, Collection<HRegion>> regionsBySize = null;
    switch (flushType) {
      case ABOVE_OFFHEAP_HIGHER_MARK:
```

### UnusedAssignment
Variable `keepSeeking` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
    try {
      try {
        boolean keepSeeking = false;
        Cell key = originalKey;
        do {
```

### UnusedAssignment
Variable `moreCellsInRow` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
    ScannerContext scannerContext, Cell currentRowCell) throws IOException {
    Cell nextKv;
    boolean moreCellsInRow = false;
    boolean tmpKeepProgress = scannerContext.getKeepProgress();
    // Scanning between column families and thus the scope is between cells
```

### UnusedAssignment
Variable `moreValues` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
      throw new UnknownScannerException("Scanner was closed");
    }
    boolean moreValues = false;
    if (outResults.isEmpty()) {
      // Usually outResults is empty. This is true when next is called
```

### UnusedAssignment
Variable `result` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
        throw new IllegalArgumentException("Row cannot be null.");
      }
      boolean result = false;
      region.startRegionOperation();
      Cell kv = PrivateCellUtil.createFirstOnRow(row, 0, (short) row.length);
```

### UnusedAssignment
Variable `origPermissions` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    // Source filesystem
    private FileSystem srcFs = null;
    private Map<String, FsPermission> origPermissions = null;
    private Map<String, String> origSources = null;

```

### UnusedAssignment
Variable `fs` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
        @Override
        public Map<byte[], List<Path>> run() {
          FileSystem fs = null;
          try {
            /*
```

### UnusedAssignment
Variable `origSources` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    private FileSystem srcFs = null;
    private Map<String, FsPermission> origPermissions = null;
    private Map<String, String> origSources = null;

    public SecureBulkLoadListener(FileSystem fs, String stagingDir, Configuration conf) {
```

### UnusedAssignment
Variable `c` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
      return null;
    }
    Chunk c = null;
    int allocOffset = 0;
    while (true) {
```

### UnusedAssignment
Variable `allocOffset` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
    }
    Chunk c = null;
    int allocOffset = 0;
    while (true) {
      // Try to get the chunk
```

### UnusedAssignment
Variable `c` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
      return null;
    }
    Chunk c = null;
    int allocOffset = 0;
    while (true) {
```

### UnusedAssignment
Variable `allocOffset` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
    }
    Chunk c = null;
    int allocOffset = 0;
    while (true) {
      // Try to get the chunk
```

### UnusedAssignment
Variable `limitReached` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        contextBuilder.setTrackMetrics(trackMetrics);
        ScannerContext scannerContext = contextBuilder.build();
        boolean limitReached = false;
        while (numOfResults < maxResults) {
          // Reset the batch progress to 0 before every call to RegionScanner#nextRaw. The
```

### UnusedAssignment
Variable `hasResultOrException` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
    ClientProtos.ResultOrException.Builder resultOrExceptionBuilder =
      ResultOrException.newBuilder();
    boolean hasResultOrException = false;
    for (ClientProtos.Action action : actions.getActionList()) {
      hasResultOrException = false;
```

### UnusedAssignment
Variable `pbResult` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        }
        if (r != null) {
          ClientProtos.Result pbResult = null;
          if (isClientCellBlockSupport(context)) {
            pbResult = ProtobufUtil.toResultNoData(r);
```

### UnusedAssignment
Variable `flushResult` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
          request.hasWriteFlushWalMarker() ? request.getWriteFlushWalMarker() : false;
        // Go behind the curtain so we can manage writing of the flush WAL marker
        HRegion.FlushResultImpl flushResult = null;
        if (request.hasFamily()) {
          List families = new ArrayList();
```

### UnusedAssignment
Variable `tempPercentFileLocal` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long tempNumMutationsWithoutWAL = 0;
        long tempDataInMemoryWithoutWAL = 0;
        double tempPercentFileLocal = 0;
        double tempPercentFileLocalSecondaryRegions = 0;
        long tempFlushedCellsCount = 0;
```

### UnusedAssignment
Variable `tempPercentFileLocalSecondaryRegions` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long tempDataInMemoryWithoutWAL = 0;
        double tempPercentFileLocal = 0;
        double tempPercentFileLocalSecondaryRegions = 0;
        long tempFlushedCellsCount = 0;
        long tempCompactedCellsCount = 0;
```

### UnusedAssignment
Variable `currentReadRequestsCount` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long tempReadRequestsCount = 0;
        long tempWriteRequestsCount = 0;
        long currentReadRequestsCount = 0;
        long currentWriteRequestsCount = 0;
        long lastReadRequestsCount = 0;
```

### UnusedAssignment
Variable `currentWriteRequestsCount` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long tempWriteRequestsCount = 0;
        long currentReadRequestsCount = 0;
        long currentWriteRequestsCount = 0;
        long lastReadRequestsCount = 0;
        long lastWriteRequestsCount = 0;
```

### UnusedAssignment
Variable `lastReadRequestsCount` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long currentReadRequestsCount = 0;
        long currentWriteRequestsCount = 0;
        long lastReadRequestsCount = 0;
        long lastWriteRequestsCount = 0;
        long readRequestsDelta = 0;
```

### UnusedAssignment
Variable `lastWriteRequestsCount` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long currentWriteRequestsCount = 0;
        long lastReadRequestsCount = 0;
        long lastWriteRequestsCount = 0;
        long readRequestsDelta = 0;
        long writeRequestsDelta = 0;
```

### UnusedAssignment
Variable `readRequestsDelta` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long lastReadRequestsCount = 0;
        long lastWriteRequestsCount = 0;
        long readRequestsDelta = 0;
        long writeRequestsDelta = 0;
        long totalReadRequestsDelta = 0;
```

### UnusedAssignment
Variable `writeRequestsDelta` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        long lastWriteRequestsCount = 0;
        long readRequestsDelta = 0;
        long writeRequestsDelta = 0;
        long totalReadRequestsDelta = 0;
        long totalWriteRequestsDelta = 0;
```

### UnusedAssignment
Variable `cacheMobBlocks` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobStoreScanner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MobStoreScanner.class);

  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
```

### UnusedAssignment
Variable `readEmptyValueOnMobCellMiss` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobStoreScanner.java`
#### Snippet
```java
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
  private final List<MobCell> referencedMobCells;
```

### UnusedAssignment
Variable `rawMobScan` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobStoreScanner.java`
#### Snippet
```java

  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
```

### UnusedAssignment
Variable `source` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
      for (HStoreFile oldFile : this.compactedFiles) {
        byte[] oldEndRow = endOf(oldFile);
        List<HStoreFile> source = null;
        if (isInvalid(oldEndRow)) {
          source = getLevel0Copy();
```

### UnusedAssignment
Variable `result` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    private final ArrayList<HStoreFile> getStripeCopy(int index) {
      List<HStoreFile> stripeCopy = this.stripeFiles.get(index);
      ArrayList<HStoreFile> result = null;
      if (stripeCopy instanceof ImmutableList<?>) {
        result = new ArrayList<>(stripeCopy);
```

### UnusedAssignment
Variable `stripeFiles` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   */
  private class CompactionOrFlushMergeCopy {
    private ArrayList<List<HStoreFile>> stripeFiles = null;
    private ArrayList<HStoreFile> level0Files = null;
    private ArrayList<byte[]> stripeEndRows = null;
```

### UnusedAssignment
Variable `filesForL0` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java

      boolean canAddNewStripes = true;
      Collection<HStoreFile> filesForL0 = null;
      if (hasStripes) {
        // Determine which stripes will need to be removed because they conflict with new stripes.
```

### UnusedAssignment
Variable `maxCellIdx` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  private final Comparator<? super Cell> comparator;
  protected int minCellIdx = 0; // the index of the minimal cell (for sub-sets)
  protected int maxCellIdx = 0; // the index of the cell after the maximal cell (for sub-sets)
  private boolean descending = false;

```

### UnusedAssignment
Variable `minCellIdx` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(CellFlatMap.class);
  private final Comparator<? super Cell> comparator;
  protected int minCellIdx = 0; // the index of the minimal cell (for sub-sets)
  protected int maxCellIdx = 0; // the index of the cell after the maximal cell (for sub-sets)
  private boolean descending = false;
```

### UnusedAssignment
Variable `descending` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  protected int minCellIdx = 0; // the index of the minimal cell (for sub-sets)
  protected int maxCellIdx = 0; // the index of the cell after the maximal cell (for sub-sets)
  private boolean descending = false;

  /* C-tor */
```

### UnusedAssignment
Variable `chunk` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
   */
  private Chunk createChunk(boolean pool, ChunkType chunkType, int size) {
    Chunk chunk = null;
    int id = chunkID.getAndIncrement();
    assert id > 0;
```

### UnusedAssignment
Variable `hasEntry` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ReaderBase.java`
#### Snippet
```java
    }

    boolean hasEntry = false;
    try {
      hasEntry = readNext(e);
```

### UnusedAssignment
Variable `pauseTime` initializer `this.retryPauseTime` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java

    int tries = 0;
    long pauseTime = this.retryPauseTime;
    // Keep looping till we get an error. We want to send reports even though server is going down.
    // Only go down if clusterConnection is null. It is set to null almost as last thing as the
```

### UnusedAssignment
The value `highestProcessedAppendTxid` assigned to `newHighestProcessedAppendTxid` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
      highestProcessedAppendTxid = newHighestProcessedAppendTxid;
    } else {
      newHighestProcessedAppendTxid = highestProcessedAppendTxid;
    }

```

### UnusedAssignment
Variable `flushing` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
   */
  boolean areAllLower(Map<byte[], Long> sequenceids, Collection<byte[]> keysBlocking) {
    Map<byte[], Long> flushing = null;
    Map<byte[], Long> unflushed = null;
    synchronized (this.tieLock) {
```

### UnusedAssignment
Variable `unflushed` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
  boolean areAllLower(Map<byte[], Long> sequenceids, Collection<byte[]> keysBlocking) {
    Map<byte[], Long> flushing = null;
    Map<byte[], Long> unflushed = null;
    synchronized (this.tieLock) {
      // Get a flattened -- only the oldest sequenceid -- copy of current flushing and unflushed
```

### UnusedAssignment
Variable `flushing` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
    // Method is called when we are crashing down because failed write flush AND it is called
    // if we fail prepare. The below is for the fail prepare case; we restore the old sequence ids.
    Map<ImmutableByteArray, Long> flushing = null;
    Map<ImmutableByteArray, Long> tmpMap = new HashMap<>();
    // Here we are moving sequenceids from flushing back to unflushed; doing opposite of what
```

### UnusedAssignment
Variable `seqId` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
        for (Map.Entry<byte[], Long> entry : familyToSeq.entrySet()) {
          ImmutableByteArray familyNameWrapper = ImmutableByteArray.wrap((byte[]) entry.getKey());
          Long seqId = null;
          if (entry.getValue() == HConstants.NO_SEQNUM) {
            seqId = m.remove(familyNameWrapper);
```

### UnusedAssignment
Variable `ivLength` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java`
#### Snippet
```java
        return super.parseCell();
      }
      int ivLength = 0;

      ivLength = StreamUtils.readRawVarint32(in);
```

### UnusedAssignment
Variable `length` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java`
#### Snippet
```java
      int vlength = StreamUtils.readRawVarint32(cin);
      int tagsLength = StreamUtils.readRawVarint32(cin);
      int length = 0;
      if (tagsLength == 0) {
        length = KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE + keylength + vlength;
```

### UnusedAssignment
Variable `e` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
      out = WALFactory.createWALWriter(outFS, output, conf);

      WAL.Entry e = null;
      while ((e = in.next()) != null)
        out.append(e);
```

### UnusedAssignment
The value `null` assigned to `out` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
      if (out != null) {
        out.close();
        out = null;
      }
    }
```

### UnusedAssignment
Variable `length` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java`
#### Snippet
```java
      int vlength = StreamUtils.readRawVarint32(in);
      int tagsLength = StreamUtils.readRawVarint32(in);
      int length = 0;
      if (tagsLength == 0) {
        length = KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE + keylength + vlength;
```

### UnusedAssignment
Variable `trailerSize` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractProtobufLogWriter.java`
#### Snippet
```java
  protected void writeWALTrailer() {
    try {
      int trailerSize = 0;
      if (this.trailer == null) {
        // use default trailer.
```

### UnusedAssignment
The value `this.trailer.getSerializedSize()` assigned to `trailerSize` is never used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractProtobufLogWriter.java`
#### Snippet
```java
        LOG.warn("WALTrailer is null. Continuing with default.");
        this.trailer = buildWALTrailer(WALTrailer.newBuilder());
        trailerSize = this.trailer.getSerializedSize();
      } else if ((trailerSize = this.trailer.getSerializedSize()) > this.trailerWarnSize) {
        // continue writing after warning the user.
```

### UnusedAssignment
Variable `nextCell` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
   */
  protected boolean trySkipToNextRow(Cell cell) throws IOException {
    Cell nextCell = null;
    // used to guard against a changed next indexed key by doing a identity comparison
    // when the identity changes we need to compare the bytes again
```

### UnusedAssignment
Variable `nextCell` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
   */
  protected boolean trySkipToNextColumn(Cell cell) throws IOException {
    Cell nextCell = null;
    // used to guard against a changed next indexed key by doing a identity comparison
    // when the identity changes we need to compare the bytes again
```

### UnusedAssignment
Variable `trx` initializer `MultiVersionConcurrencyControl.NONE` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALUtil.java`
#### Snippet
```java
    // TODO: Pass in current time to use?
    WALKeyImpl walKey = createWALKey(hri, mvcc, replicationScope, extendedAttributes);
    long trx = MultiVersionConcurrencyControl.NONE;
    try {
      trx = wal.appendMarker(hri, walKey, edit);
```

### UnusedAssignment
Variable `fs` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
      return 1;
    }
    FileSystem fs = null;
    if (path != null) {
      try {
```

### UnusedAssignment
Variable `ret` initializer `EXIT_FAILURE` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
    processOptions(cmd);

    int ret = EXIT_FAILURE;
    try {
      ret = doWork();
```

### UnusedAssignment
Variable `tmp` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
      fd.totalCompactedFilesSize += r.length();

      byte[] tmp = null;
      // Get and set the real MVCCReadpoint for bulk loaded files, which is the
      // SeqId number.
```

### UnusedAssignment
Variable `ret` initializer `EXIT_FAILURE` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
    processOptions(cmd);

    int ret = EXIT_FAILURE;
    try {
      ret = doWork();
```

### UnusedAssignment
Variable `cmd` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
    }

    String cmd = null;
    String[] remainArgs = null;
    if (args == null || args.length == 0) {
```

### UnusedAssignment
Variable `remainArgs` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java

    String cmd = null;
    String[] remainArgs = null;
    if (args == null || args.length == 0) {
      printToolUsage();
```

### UnusedAssignment
Variable `type` initializer `BackupCommand.HELP` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
    }

    BackupCommand type = BackupCommand.HELP;
    if (BackupCommand.CREATE.name().equalsIgnoreCase(cmd)) {
      type = BackupCommand.CREATE;
```

### UnusedAssignment
Variable `stripePolicy` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  private final static Logger LOG = LoggerFactory.getLogger(StripeCompactionPolicy.class);
  // Policy used to compact individual stripes.
  private ExploringCompactionPolicy stripePolicy = null;

  private StripeStoreConfig config;
```

### UnusedAssignment
Variable `conf` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(BackupManager.class);

  protected Configuration conf = null;
  protected BackupInfo backupInfo = null;
  protected BackupSystemTable systemTable;
```

### UnusedAssignment
Variable `htds` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
      // If table list is null for full backup, which means backup all tables. Then fill the table
      // list with all user tables from meta. It no table available, throw the request exception.
      List<TableDescriptor> htds = null;
      try (Admin admin = conn.getAdmin()) {
        htds = admin.listTableDescriptors();
```

### UnusedAssignment
Variable `proto` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java`
#### Snippet
```java
            throw new BackupException(e.getMessage());
          }
          BackupProtos.BackupImage proto = null;
          try {
            proto = BackupProtos.BackupImage.parseFrom(pbBytes);
```

### UnusedAssignment
Variable `days` initializer `0` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    private void executeDeleteOlderThan(CommandLine cmdline) throws IOException {
      String value = cmdline.getOptionValue(OPTION_KEEP);
      int days = 0;
      try {
        days = Integer.parseInt(value);
```

### UnusedAssignment
Variable `history` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        }
      };
      List<BackupInfo> history = null;
      try (final BackupSystemTable sysTable = new BackupSystemTable(conn);
        BackupAdminImpl admin = new BackupAdminImpl(conn)) {
```

### UnusedAssignment
Variable `newMsg` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    } catch (IOException e) {
      String expMsg = e.getMessage();
      String newMsg = null;
      if (expMsg.contains("No FileSystem for scheme")) {
        newMsg =
```

### UnusedAssignment
Variable `job` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
      assert fieldInputOptions.get(this) != null;

      Job job = null;
      try {

```

### UnusedAssignment
Variable `totalRecords` initializer `0` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
      }
      long totalBytesExpected = 0;
      int totalRecords = 0;
      Path fileListingPath = getFileListingPath();
      try (SequenceFile.Writer writer = getWriter(fileListingPath)) {
```

### UnusedAssignment
Variable `options` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
      IllegalArgumentException, IllegalAccessException, NoSuchMethodException,
      ClassNotFoundException, InvocationTargetException, IOException {
      Field options = null;
      try {
        options = DistCp.class.getDeclaredField("inputOptions");
```

### UnusedAssignment
Variable `f` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java

    private Field getInputOptionsField(Class<?> classDistCp) throws IOException {
      Field f = null;
      try {
        f = classDistCp.getDeclaredField("inputOptions");
```

### UnusedAssignment
Variable `name` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java
    String backupId = backupDirPath.getName();
    Deque<String> stack = new ArrayDeque<String>();
    String name = null;
    while (true) {
      name = p.getName();
```

### UnusedAssignment
Variable `res` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      try (Table table = connection.getTable(bulkLoadTableName);
        ResultScanner scanner = table.getScanner(scan)) {
        Result res = null;
        while ((res = scanner.next()) != null) {
          res.advance();
```

### UnusedAssignment
Variable `res` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    try (Table table = connection.getTable(bulkLoadTableName);
      ResultScanner scanner = table.getScanner(scan)) {
      Result res = null;
      while ((res = scanner.next()) != null) {
        res.advance();
```

### UnusedAssignment
Variable `res` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    try (Table table = connection.getTable(bulkLoadTableName);
      ResultScanner scanner = table.getScanner(scan)) {
      Result res = null;
      Map<byte[], String> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
      while ((res = scanner.next()) != null) {
```

### UnusedAssignment
Variable `union` initializer `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      LOG.trace("Backup set add: " + name + " tables [" + StringUtils.join(newTables, " ") + "]");
    }
    String[] union = null;
    try (Table table = connection.getTable(tableName)) {
      Get get = createGetForBackupSet(name);
```

### UnusedAssignment
Variable `processor` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java`
#### Snippet
```java
    @Override
    public void run() {
      TProcessor processor = null;
      TTransport inputTransport = null;
      TTransport outputTransport = null;
```

### UnusedAssignment
Variable `inputProtocol` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java`
#### Snippet
```java
      TTransport inputTransport = null;
      TTransport outputTransport = null;
      TProtocol inputProtocol = null;
      TProtocol outputProtocol = null;
      try {
```

### UnusedAssignment
Variable `outputProtocol` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java`
#### Snippet
```java
      TTransport outputTransport = null;
      TProtocol inputProtocol = null;
      TProtocol outputProtocol = null;
      try {
        processor = processorFactory_.getProcessor(client);
```

### UnusedAssignment
Variable `client` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java`
#### Snippet
```java
    stopped = false;
    while (!stopped && !Thread.interrupted()) {
      TTransport client = null;
      try {
        client = serverTransport_.accept();
```

### UnusedAssignment
Variable `list` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java`
#### Snippet
```java
   */
  public static List<TCell> cellFromHBase(Cell[] in) {
    List<TCell> list = null;
    if (in != null) {
      list = new ArrayList<>(in.length);
```

### UnusedAssignment
Variable `isCompact` initializer `false` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  private int port;
  private boolean isFramed = false;
  private boolean isCompact = false;

  // TODO: We can rip out the ThriftClient piece of it rather than creating a new client every time.
```

### UnusedAssignment
Variable `tTransport` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
    @Override
    public Pair<THBaseService.Client, TTransport> getClient() throws IOException {
      TTransport tTransport = null;
      try {
        TSocket sock = new TSocket(connection.getHost(), connection.getPort());
```

### UnusedAssignment
Variable `isFramed` initializer `false` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  private String host;
  private int port;
  private boolean isFramed = false;
  private boolean isCompact = false;

```

### UnusedAssignment
Variable `results` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
    throws TIOError, TException {
    Table htable = getTable(table);
    List<TResult> results = null;
    ResultScanner scanner = null;
    try {
```

### UnusedAssignment
Variable `resultScanner` initializer `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  public int openScanner(ByteBuffer table, TScan scan) throws TIOError, TException {
    Table htable = getTable(table);
    ResultScanner resultScanner = null;
    try {
      resultScanner = htable.getScanner(scanFromThrift(scan));
```

### UnusedAssignment
Variable `port` initializer `0` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCommonTestingUtility.java`
#### Snippet
```java
     */
    public int randomFreePort() {
      int port = 0;
      do {
        port = randomPort();
```

### UnusedAssignment
Variable `user` initializer `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
  public static class MiniHBaseClusterRegionServer extends HRegionServer {
    private Thread shutdownThread = null;
    private User user = null;
    /**
     * List of RegionServers killed so far. ServerName also comprises startCode of a server, so any
```

### UnusedAssignment
Variable `t` initializer `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
    User user = HBaseTestingUtility.getDifferentUser(c, ".hfs." + index++);

    JVMClusterUtil.MasterThread t = null;
    try {
      t = hbaseCluster.addMaster(c, hbaseCluster.getMasters().size(), user);
```

### UnusedAssignment
Variable `t` initializer `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
    throws IOException {
    User rsUser = HBaseTestingUtility.getDifferentUser(configuration, ".hfs." + index++);
    JVMClusterUtil.RegionServerThread t = null;
    try {
      t =
```

### UnusedAssignment
Variable `isSuccessful` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // we need writeLock for multi-family bulk load
    startBulkRegionOperation(hasMultipleColumnFamilies(familyPaths));
    boolean isSuccessful = false;
    try {
      this.writeRequestsCount.increment();
```

### UnusedAssignment
Variable `pair` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
              reqTmp ? null : fs.getRegionDir().toString());
          }
          Pair<Path, Path> pair = null;
          if (reqTmp || !StoreFileInfo.isHFile(finalPath)) {
            pair = store.preBulkLoadHFile(finalPath, seqId);
```

### UnusedAssignment
Variable `specificStoresToFlush` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // caused by logRoller, we should select stores which must be flushed
        // rather than could be flushed.
        Collection<HStore> specificStoresToFlush = null;
        if (families != null) {
          specificStoresToFlush = getSpecificStores(families);
```

### UnusedAssignment
Variable `ctx` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      }
      List<String> flushFiles = storeFlush.getFlushOutputList();
      StoreFlushContext ctx = null;
      long startTime = EnvironmentEdgeManager.currentTime();
      if (prepareFlushResult == null || prepareFlushResult.storeFlushCtxs == null) {
```

### UnusedAssignment
Variable `matches` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  private boolean matches(final CompareOperator op, final int compareResult) {
    boolean matches = false;
    switch (op) {
      case LESS:
```

### UnusedAssignment
Variable `flushedSeqId` initializer `HConstants.NO_SEQNUM` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // The max flushed sequence id after this flush operation completes. All edits in memstore
    // will be in advance of this sequence id.
    long flushedSeqId = HConstants.NO_SEQNUM;
    byte[] encodedRegionName = getRegionInfo().getEncodedNameAsBytes();
    try {
```

### UnusedAssignment
Variable `canFlush` initializer `true` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    }
    status.setStatus("Disabling compacts and flushes for region");
    boolean canFlush = true;
    synchronized (writestate) {
      // Disable compacting and flushing by background threads for this
```

### UnusedAssignment
Variable `acquired` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        remainingWaitTime = closeWaitInterval;
      }
      boolean acquired = false;
      do {
        long start = EnvironmentEdgeManager.currentTime();
```

### UnusedAssignment
Variable `tmp` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        int failedfFlushCount = 0;
        int flushCount = 0;
        long tmp = 0;
        long remainingSize = this.memStoreSizing.getDataSize();
        while (remainingSize > 0) {
```

### UnusedAssignment
Variable `isLoadingCfsOnDemandDefault` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * region. Requests can override it.
   */
  private boolean isLoadingCfsOnDemandDefault = false;

  private final AtomicInteger majorInProgress = new AtomicInteger(0);
```

### UnusedAssignment
Variable `origLogSeqNum` initializer `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private static final class ReplayBatchOperation extends BatchOperation<MutationReplay> {

    private long origLogSeqNum = 0;

    public ReplayBatchOperation(final HRegion region, MutationReplay[] operations,
```

### UnusedAssignment
Variable `htableDescriptor` initializer `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private volatile RegionCoprocessorHost coprocessorHost;

  private TableDescriptor htableDescriptor = null;
  private RegionSplitPolicy splitPolicy;
  private RegionSplitRestriction splitRestriction;
```

### UnusedAssignment
Variable `currentReplaySeqId` initializer `-1` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      reader = WALFactory.createReader(fs, edits, conf);
      long currentEditSeqId = -1;
      long currentReplaySeqId = -1;
      long firstSeqIdInLog = -1;
      long skippedEdits = 0;
```

### UnusedAssignment
Variable `flush` initializer `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          }

          boolean flush = false;
          MemStoreSizing memStoreSizing = new NonThreadSafeMemStoreSizing();
          for (Cell cell : val.getCells()) {
```

### UnusedAssignment
Variable `tempVal` initializer `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
    try {
      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);
      S sumVal = null, sumWeights = null, tempVal = null, tempWeight = null;
      Scan scan = ProtobufUtil.toScan(request.getScan());
      scanner = env.getRegion().getScanner(scan);
```

### UnusedAssignment
Variable `tempWeight` initializer `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
    try {
      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);
      S sumVal = null, sumWeights = null, tempVal = null, tempWeight = null;
      Scan scan = ProtobufUtil.toScan(request.getScan());
      scanner = env.getRegion().getScanner(scan);
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      List<Cell> results = new ArrayList<>();

      boolean hasMoreRows = false;

      do {
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      }
      scanner = env.getRegion().getScanner(scan);
      boolean hasMoreRows = false;
      do {
        hasMoreRows = scanner.next(results);
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      }
      List<Cell> results = new ArrayList<>();
      boolean hasMoreRows = false;
      do {
        hasMoreRows = scanner.next(results);
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
        qualifier = qualifiers.pollFirst();
      }
      boolean hasMoreRows = false;
      do {
        hasMoreRows = scanner.next(results);
```

### UnusedAssignment
Variable `tempVal` initializer `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
    try {
      ColumnInterpreter<T, S, P, Q, R> ci = constructColumnInterpreterFromRequest(request);
      S sumVal = null, sumSqVal = null, tempVal = null;
      long rowCountVal = 0L;
      Scan scan = ProtobufUtil.toScan(request.getScan());
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      List<Cell> results = new ArrayList<>();

      boolean hasMoreRows = false;

      do {
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      }
      List<Cell> results = new ArrayList<>();
      boolean hasMoreRows = false;

      do {
```

### UnusedAssignment
Variable `hasMoreRows` initializer `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java`
#### Snippet
```java
      }
      // qualifier can be null.
      boolean hasMoreRows = false;
      do {
        hasMoreRows = scanner.next(results);
```

### UnusedAssignment
Variable `results` initializer `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      }
      scanner = table.getScanner(scan2);
      Result[] results = null;
      byte[] qualifier = quals.pollFirst();
      // qualifier for the weight column
```

## RuleId[id=IndexOfReplaceableByContains]
### IndexOfReplaceableByContains
`attName.indexOf("=") >= 0` can be replaced with 'attName.contains("=")'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
      return;
    }
    if (attName.indexOf("=") >= 0 || attName.indexOf(" ") >= 0) {
      return;
    }
```

### IndexOfReplaceableByContains
`attName.indexOf(" ") >= 0` can be replaced with 'attName.contains(" ")'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
      return;
    }
    if (attName.indexOf("=") >= 0 || attName.indexOf(" ") >= 0) {
      return;
    }
```

### IndexOfReplaceableByContains
`name.indexOf(REGION_SEP) < 0` can be replaced with '!name.contains(REGION_SEP)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileName.java`
#### Snippet
```java

  public static boolean isOldMobFileName(String name) {
    return name.indexOf(REGION_SEP) < 0;
  }

```

### IndexOfReplaceableByContains
`msg.indexOf(definedPolicy.name()) != -1` can be replaced with 'msg.contains(definedPolicy.name())'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SpaceLimitingException.java`
#### Snippet
```java
    if (msg != null) {
      for (SpaceViolationPolicy definedPolicy : SpaceViolationPolicy.values()) {
        if (msg.indexOf(definedPolicy.name()) != -1) {
          policyName = definedPolicy.name();
          return;
```

## RuleId[id=SuspiciousNameCombination]
### SuspiciousNameCombination
'y' should probably not be passed as parameter 'x1'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    w = (int) (val >>> 32);
    if (w == 0) {
      if (-1 == unsignedCmp(y, 16777216L)) {
        dst.put((byte) 250).put((byte) (y >>> 16)).put((byte) (y >>> 8)).put((byte) y);
        len = dst.getPosition() - start;
```

### SuspiciousNameCombination
'topKey' should probably not be passed as parameter 'right'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
      Cell topKey = scanner.peek();
      if (
        (CellUtil.matchingRows(seekKey, topKey)
          && comparator.getComparator().compare(seekKey, topKey) <= 0)
          || comparator.getComparator().compareRows(seekKey, topKey) > 0
```

### SuspiciousNameCombination
'topKey' should probably not be passed as parameter 'rightCell'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
      if (
        (CellUtil.matchingRows(seekKey, topKey)
          && comparator.getComparator().compare(seekKey, topKey) <= 0)
          || comparator.getComparator().compareRows(seekKey, topKey) > 0
      ) {
```

### SuspiciousNameCombination
'topKey' should probably not be passed as parameter 'rightCell'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
        (CellUtil.matchingRows(seekKey, topKey)
          && comparator.getComparator().compare(seekKey, topKey) <= 0)
          || comparator.getComparator().compareRows(seekKey, topKey) > 0
      ) {
        heap.add(scanner);
```

### SuspiciousNameCombination
'topKey' should probably not be passed as parameter 'leftCell'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
    while ((scanner = heap.poll()) != null) {
      Cell topKey = scanner.peek();
      if (comparator.getComparator().compareRows(topKey, seekKey) < 0) {
        // Row of Top KeyValue is before Seek row.
        heap.add(scanner);
```

### SuspiciousNameCombination
'topScanner' should probably not be passed as parameter 'right'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
    } else {
      KeyValueScanner topScanner = this.heap.peek();
      if (topScanner != null && this.comparator.compare(this.current, topScanner) > 0) {
        this.heap.add(this.current);
        this.current = null;
```

### SuspiciousNameCombination
'topKey' should probably not be passed as parameter 'rightCell'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
      while (scanner != null) {
        Cell topKey = scanner.peek();
        if (comparator.getComparator().compare(seekKey, topKey) <= 0) {
          // Top KeyValue is at-or-after Seek KeyValue. We only know that all
          // scanners are at or after seekKey (because fake keys of
```

### SuspiciousNameCombination
'lastTop' should probably not be passed as parameter 'leftCell'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
    resetKVHeap(this.currentScanners, store.getComparator());
    resetQueryMatcher(lastTop);
    if (heap.peek() == null || store.getComparator().compareRows(lastTop, this.heap.peek()) != 0) {
      LOG.info("Storescanner.peek() is changed where before = " + lastTop.toString()
        + ",and after = " + heap.peek());
```

## RuleId[id=AssignmentUsedAsCondition]
### AssignmentUsedAsCondition
Assignment `refFound = regionFs.hasReferences(c.getNameAsString())` used as condition
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
            boolean refFound = false;
            for (ColumnFamilyDescriptor c : htd.getColumnFamilies()) {
              if ((refFound = regionFs.hasReferences(c.getNameAsString()))) {
                break;
              }
```

## RuleId[id=ManualArrayToCollectionCopy]
### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java`
#### Snippet
```java
    CommandLine cmd;
    List<String> argsList = new ArrayList<>(args.length);
    for (String arg : args) {
      argsList.add(arg);
    }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      prevRow = currentRow;
      stale = stale || r.isStale();
      for (Cell c : r.rawCells()) {
        cells.add(c);
      }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java`
#### Snippet
```java
      if (status.isDirectory()) {
        FileSystem fs = status.getPath().getFileSystem(job.getConfiguration());
        for (FileStatus match : fs.listStatus(status.getPath(), HIDDEN_FILE_FILTER)) {
          result.add(match);
        }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      String[] tables = conf.getStrings(TABLES_KEY);
      this.multiTableSupport = conf.getBoolean(MULTI_TABLES_SUPPORT, false);
      for (String table : tables) {
        tableSet.add(table);
      }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
        m.put(key, strings);
      }
      for (String v : entry.getValue()) {
        strings.add(v);
      }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.java`
#### Snippet
```java

  public CoprocessorMethod withParameters(String... parameters) {
    for (String parameter : parameters) {
      this.parameters.add(parameter);
    }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
      // Set sequence first because the add to the queue will wake the thread if sleeping.
      this.sequence = sequence;
      for (int i = 0; i < syncFutureCount; ++i) {
        this.syncFutures.add(syncFutures[i]);
      }
```

### ManualArrayToCollectionCopy
Manual array to collection copy
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
      String[] namespaces = connectionCache.getAdmin().listNamespaces();
      List<String> result = new ArrayList<>(namespaces.length);
      for (String ns : namespaces) {
        result.add(ns);
      }
```

## RuleId[id=UnnecessarySemicolon]
### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerTask.java`
#### Snippet
```java
    WAITING,
    COMPLETE,
    ABORTED;
  }

```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractResponse.java`
#### Snippet
```java

    SINGLE,
    MULTI;

  }
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      FileOutputFormat.setOutputPath(job, outputDir);
      job.setMapOutputValueClass(MapReduceExtendedCell.class);
      try (Connection conn = ConnectionFactory.createConnection(conf);) {
        List<TableInfo> tableInfoList = new ArrayList<TableInfo>();
        for (TableName tableName : tableNames) {
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
    long maxCreationTimeToArchive = EnvironmentEdgeManager.currentTime() - minAgeToArchive;
    try (final Connection conn = ConnectionFactory.createConnection(conf);
      final Admin admin = conn.getAdmin();) {
      TableDescriptor htd = admin.getDescriptor(table);
      List<ColumnFamilyDescriptor> list = MobUtils.getMobColumnFamilies(htd);
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
      }
    }
  };

  /**
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
  default void postListNamespaces(ObserverContext<MasterCoprocessorEnvironment> ctx,
    List<String> namespaces) throws IOException {
  };

  /**
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.java`
#### Snippet
```java
      throws IOException {
      return createWriter();
    };
  }

```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
  // This flag is for closing this instance, its set when clearing snapshot of
  // memstore
  private final AtomicBoolean closed = new AtomicBoolean(false);;
  // This flag is for reclaiming chunks. Its set when putting chunks back to
  // pool
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALActionsListener.java`
#### Snippet
```java
    /** I/O or other error. */
    ERROR
  };

  /**
```

### UnnecessarySemicolon
Unnecessary semicolon `;`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java`
#### Snippet
```java
    protected TScan scan;
    protected Result lastResult = null;
    protected final Queue<Result> cache = new ArrayDeque<>();;

    public Scanner(Scan scan) throws IOException {
```

## RuleId[id=SimplifyStreamApiCallChains]
### SimplifyStreamApiCallChains
Can be replaced with 'Collectors.joining'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/top/FilterDisplayModeScreenView.java`
#### Snippet
```java
    if (!filters.isEmpty()) {
      filtersString = String.join(" + ",
        filters.stream().map(f -> String.format("'%s'", f)).collect(Collectors.toList()));
    }

```

### SimplifyStreamApiCallChains
Can be replaced with '.values().stream()'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/top/TopScreenModel.java`
#### Snippet
```java
    double averageLoad = clusterMetrics.getAverageLoad();
    long aggregateRequestPerSecond = clusterMetrics.getLiveServerMetrics().entrySet().stream()
      .mapToLong(e -> e.getValue().getRequestCountPerSecond()).sum();

    summary = new Summary(currentTime, version, clusterId, liveServers + deadServers, liveServers,
```

### SimplifyStreamApiCallChains
Can be replaced with '.values().stream()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetrics.java`
#### Snippet
```java
  default int getRegionCount() {
    return getLiveServerMetrics().entrySet().stream()
      .mapToInt(v -> v.getValue().getRegionMetrics().size()).sum();
  }

```

### SimplifyStreamApiCallChains
Can be replaced with 'String.join'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java`
#### Snippet
```java

  private static String toCommaDelimitedString(List<String> list) {
    return list.stream().collect(Collectors.joining(", "));
  }

```

### SimplifyStreamApiCallChains
'filter()' and 'map()' can be swapped
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableRegionLocator.java`
#### Snippet
```java
    return getAllRegionLocations().thenApply(
      locs -> locs.stream().filter(loc -> RegionReplicaUtil.isDefaultReplica(loc.getRegion()))
        .map(HRegionLocation::getRegion).map(r -> Pair.newPair(r.getStartKey(), r.getEndKey()))
        .collect(Collectors.toList()));
  }
```

### SimplifyStreamApiCallChains
Can be replaced with 'java.util.ArrayList' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
    if (!errors.isEmpty()) {
      throw new RetriesExhaustedException(errors.size(),
        errors.stream().collect(Collectors.toList()));
    }
  }
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      Map<ServerName, Boolean> serverStates = new ConcurrentHashMap<>(serverNames.size());
      List<CompletableFuture<Boolean>> futures = new ArrayList<>(serverNames.size());
      serverNames.stream().forEach(serverName -> {
        futures.add(switchCompact(serverName, switchState).whenComplete((serverState, err2) -> {
          if (err2 != null) {
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
      }
      LOG.debug("Found: {} active mob refs for table={}", allActiveMobFileName.size(), table);
      allActiveMobFileName.stream().forEach(LOG::trace);

      // Now scan MOB directories and find MOB files with no references to them
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
        }
        if (LOG.isTraceEnabled()) {
          referencedMOBs.values().stream().forEach(innerMap -> innerMap.values().stream()
            .forEach(mobFileList -> mobFileList.stream().forEach(LOG::trace)));
        }
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
        if (LOG.isTraceEnabled()) {
          referencedMOBs.values().stream().forEach(innerMap -> innerMap.values().stream()
            .forEach(mobFileList -> mobFileList.stream().forEach(LOG::trace)));
        }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
        if (LOG.isTraceEnabled()) {
          referencedMOBs.values().stream().forEach(innerMap -> innerMap.values().stream()
            .forEach(mobFileList -> mobFileList.stream().forEach(LOG::trace)));
        }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
        Set<String> regionsCovered = new HashSet<>();
        referencedMOBs.values().stream()
          .forEach(regionMap -> regionsCovered.addAll(regionMap.keySet()));

        for (ColumnFamilyDescriptor hcd : list) {
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
    // Tell our listeners that a server was removed
    if (!this.listeners.isEmpty()) {
      this.listeners.stream().forEach(l -> l.serverRemoved(serverName));
    }
    // trigger a persist of flushedSeqId
```

### SimplifyStreamApiCallChains
Can be replaced with 'java.util.ArrayList' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/TableNamespaceManager.java`
#### Snippet
```java

  public List<NamespaceDescriptor> list() throws IOException {
    return cache.values().stream().collect(Collectors.toList());
  }

```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      new HashMap<>(peerList.size());
    peerList.stream()
      .forEach(peer -> replicationLoadSourceMap.put(peer.getPeerId(), new ArrayList<>()));
    for (ServerName serverName : serverNames) {
      List<ReplicationLoadSource> replicationLoadSources =
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java`
#### Snippet
```java
            LOG.info("{} had {} regions", serverName, regionsOnCrashedServer.size());
            if (LOG.isTraceEnabled()) {
              this.regionsOnCrashedServer.stream().forEach(ri -> LOG.trace(ri.getShortNameToLog()));
            }
          }
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java`
#### Snippet
```java
            if (!CollectionUtils.isEmpty(mergeRegions)) {
              mergeRegions.stream()
                .forEach(r -> regionDirList.add(FSUtils.getRegionDirFromTableDir(tableDir, r)));
            }
          }
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java`
#### Snippet
```java
      ReplaySyncReplicationWALParameter.newBuilder();
    builder.setPeerId(peerId);
    wals.stream().forEach(builder::addWal);
    return Optional.of(new ServerOperation(this, getProcId(),
      ReplaySyncReplicationWALCallable.class, builder.build().toByteArray()));
```

### SimplifyStreamApiCallChains
''stream().forEach()'' can be replaced with 'forEach()'' (may change semantics)
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java`
#### Snippet
```java
      SyncReplicationReplayWALRemoteStateData.newBuilder().setPeerId(peerId)
        .setTargetServer(ProtobufUtil.toServerName(targetServer));
    wals.stream().forEach(builder::addWal);
    serializer.serialize(builder.build());
  }
```

### SimplifyStreamApiCallChains
'collect(summingLong())' can be replaced with 'mapToLong().sum()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
  long getSizeOfStoreFile(TableName tn, StoreFileReference storeFileName) {
    String regionName = storeFileName.getRegionName();
    return storeFileName.getFamilyToFilesMapping().entries().stream().collect(
      Collectors.summingLong((e) -> getSizeOfStoreFile(tn, regionName, e.getKey(), e.getValue())));
  }

```

### SimplifyStreamApiCallChains
'collect(summingLong())' can be replaced with 'mapToLong().sum()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
  long getSizeOfStoreFiles(TableName tn, Set<StoreFileReference> storeFileNames) {
    return storeFileNames.stream()
      .collect(Collectors.summingLong((sfr) -> getSizeOfStoreFile(tn, sfr)));
  }

```

### SimplifyStreamApiCallChains
'Arrays.asList().stream()' can be replaced with 'Arrays.stream()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      res.advance();
      String[] tables = cellValueToBackupSet(res.current());
      return Arrays.asList(tables).stream().map(item -> TableName.valueOf(item))
        .collect(Collectors.toList());
    }
```

## RuleId[id=TypeParameterHidesVisibleType]
### TypeParameterHidesVisibleType
Type parameter `K` hides type parameter 'K'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayValueCollection<K, V> implements Collection<V> {

    private final ArrayHolder<K, V> holder;
```

### TypeParameterHidesVisibleType
Type parameter `V` hides type parameter 'V'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayValueCollection<K, V> implements Collection<V> {

    private final ArrayHolder<K, V> holder;
```

### TypeParameterHidesVisibleType
Type parameter `K` hides type parameter 'K'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayEntrySet<K, V> implements Set<Map.Entry<K, V>> {
    private final ArrayHolder<K, V> holder;

```

### TypeParameterHidesVisibleType
Type parameter `V` hides type parameter 'V'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayEntrySet<K, V> implements Set<Map.Entry<K, V>> {
    private final ArrayHolder<K, V> holder;

```

### TypeParameterHidesVisibleType
Type parameter `K` hides type parameter 'K'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayKeySet<K, V> implements NavigableSet<K> {

    private final ArrayHolder<K, V> holder;
```

### TypeParameterHidesVisibleType
Type parameter `V` hides type parameter 'V'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayKeySet<K, V> implements NavigableSet<K> {

    private final ArrayHolder<K, V> holder;
```

## RuleId[id=StringOperationCanBeSimplified]
### StringOperationCanBeSimplified
Unnecessary string length argument
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyStoreKeyProvider.java`
#### Snippet
```java
      String value = URLDecoder.decode(params.substring(valueStart, valueEnd), "UTF-8");
      processParameter(name, value);
      params = params.substring(valueEnd, params.length());
    } while (!params.isEmpty());
  }
```

### StringOperationCanBeSimplified
Unnecessary string length argument
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/DictionaryCache.java`
#### Snippet
```java
      throw new IOException("Path does not start with " + RESOURCE_SCHEME);
    }
    final String path = s.substring(RESOURCE_SCHEME.length(), s.length());
    LOG.info("Loading resource {}", path);
    final InputStream in = DictionaryCache.class.getClassLoader().getResourceAsStream(path);
```

### StringOperationCanBeSimplified
Unnecessary empty string argument
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      }

      System.out.println("");
    }

```

### StringOperationCanBeSimplified
Unnecessary empty string argument
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
  private void printUsage(Options opts) {
    new HelpFormatter().printHelp("hbase hbtop [opts] [-D<property=value>]*", opts);
    System.out.println("");
    System.out.println(" Note: -D properties will be applied to the conf used.");
    System.out.println("  For example:");
```

### StringOperationCanBeSimplified
Unnecessary empty string argument
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
    System.out.println("   -Dhbase.client.zookeeper.quorum=<zookeeper quorum>");
    System.out.println("   -Dzookeeper.znode.parent=<znode parent>");
    System.out.println("");
  }

```

### StringOperationCanBeSimplified
`toUpperCase()` call can be replaced with 'equalsIgnoreCase()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
        final String filterByOperator = (String) filterParams.get("filterByOperator");
        if (StringUtils.isNotEmpty(filterByOperator)) {
          if (filterByOperator.toUpperCase().equals("AND")) {
            filterByAnd = true;
          }
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      this.endRow[this.row.length] = (byte) 255;
    } else {
      this.row = Bytes.toBytes(startRow.toString());
      if (endRow != null) {
        this.endRow = Bytes.toBytes(endRow.toString());
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      this.row = Bytes.toBytes(startRow.toString());
      if (endRow != null) {
        this.endRow = Bytes.toBytes(endRow.toString());
      }
    }
```

### StringOperationCanBeSimplified
Unnecessary string length argument
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    if (filterCriteria == null) return null;
    if (filterCriteria.startsWith("^")) {
      String regexPattern = filterCriteria.substring(1, filterCriteria.length());
      exportFilter = new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(regexPattern));
    } else {
```

### StringOperationCanBeSimplified
Unnecessary string length argument
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    if (filterCriteria == null) return null;
    if (filterCriteria.startsWith("^")) {
      String regexPattern = filterCriteria.substring(1, filterCriteria.length());
      rowFilter = new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(regexPattern));
    } else {
```

### StringOperationCanBeSimplified
Call to `substring()` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
    StringBuilder buffer = new StringBuilder();
    while (next != -1) {
      buffer.append(item.substring(posn, next));
      if (item.startsWith("&amp;", next)) {
        buffer.append('&');
```

### StringOperationCanBeSimplified
Call to `substring()` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
      next = item.indexOf('&', posn);
    }
    buffer.append(item.substring(posn, len));
    return buffer.toString();
  }
```

### StringOperationCanBeSimplified
`toLowerCase()` call can be replaced with 'equalsIgnoreCase()'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
    while (headers.hasMoreElements()) {
      String header = headers.nextElement();
      if (header.toLowerCase().equals("doas")) {
        doas = request.getHeader(header);
        break;
```

### StringOperationCanBeSimplified
Unnecessary empty string argument
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -permittedZookeeperFailures <N>  Ignore first N failures attempting to ");
    System.err.println("                connect to individual zookeeper nodes in ensemble");
    System.err.println("");
    System.err.println(" -D<configProperty>=<value> to assign or override configuration params");
    System.err.println(" -Dhbase.canary.read.raw.enabled=<true/false> Set to enable/disable "
```

### StringOperationCanBeSimplified
Unnecessary empty string argument
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      " -Dhbase.canary.info.port=PORT_NUMBER  Set for a Canary UI; " + "default=-1 (None)");
    System.err.println("");
    System.err.println(
      "Canary runs in one of three modes: region (default), regionserver, or " + "zookeeper.");
```

### StringOperationCanBeSimplified
Unnecessary empty string argument
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java`
#### Snippet
```java
    if (message != null) {
      System.err.println(message);
      System.err.println("");
    }

```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      Path backRefPath = FileLink.getBackReferencesDir(
        HFileArchiveUtil.getStoreArchivePath(conf,
          HFileLink.getReferencedTableName(path.getName().toString()),
          HFileLink.getReferencedRegionName(path.getName().toString()), path.getParent().getName()),
        HFileLink.getReferencedHFileName(path.getName().toString()));
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        HFileArchiveUtil.getStoreArchivePath(conf,
          HFileLink.getReferencedTableName(path.getName().toString()),
          HFileLink.getReferencedRegionName(path.getName().toString()), path.getParent().getName()),
        HFileLink.getReferencedHFileName(path.getName().toString()));
      success = sidelineFile(fs, hbaseRoot, backRefPath);
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          HFileLink.getReferencedTableName(path.getName().toString()),
          HFileLink.getReferencedRegionName(path.getName().toString()), path.getParent().getName()),
        HFileLink.getReferencedHFileName(path.getName().toString()));
      success = sidelineFile(fs, hbaseRoot, backRefPath);

```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/CoprocessorWhitelistMasterObserver.java`
#### Snippet
```java
      String coprocPathHost = coprocPath.toUri().getHost();
      if (wlPathScheme != null) {
        wlPathScheme = wlPathScheme.toString().toLowerCase();
      } else {
        wlPathScheme = "";
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/CoprocessorWhitelistMasterObserver.java`
#### Snippet
```java
      }
      if (wlPathHost != null) {
        wlPathHost = wlPathHost.toString().toLowerCase();
      } else {
        wlPathHost = "";
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/CoprocessorWhitelistMasterObserver.java`
#### Snippet
```java
      }
      if (coprocPathScheme != null) {
        coprocPathScheme = coprocPathScheme.toString().toLowerCase();
      } else {
        coprocPathScheme = "";
```

### StringOperationCanBeSimplified
Call to `toString()` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/CoprocessorWhitelistMasterObserver.java`
#### Snippet
```java
      }
      if (coprocPathHost != null) {
        coprocPathHost = coprocPathHost.toString().toLowerCase();
      } else {
        coprocPathHost = "";
```

## RuleId[id=OptionalContainsCollection]
### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
    Optional<byte[]> getFirstRowKey();

    Optional<byte[]> getLastRowKey();

    FixedFileTrailer getTrailer();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
    long indexSize();

    Optional<byte[]> getFirstRowKey();

    Optional<byte[]> getLastRowKey();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
   */
  @Override
  public Optional<byte[]> getLastRowKey() {
    // We have to copy the row part to form the row key alone
    return getLastKey().map(CellUtil::cloneRow);
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
   */
  @Override
  public Optional<byte[]> getFirstRowKey() {
    // We have to copy the row part to form the row key alone
    return getFirstKey().map(CellUtil::cloneRow);
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
    List<Pair<byte[], byte[]>> startEndKeys) throws IOException {
    Path hfilePath = item.getFilePath();
    Optional<byte[]> first, last;
    try (HFile.Reader hfr = HFile.createReader(hfilePath.getFileSystem(getConf()), hfilePath,
      CacheConfig.DISABLED, true, getConf())) {
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java`
#### Snippet
```java
   * @return The mid-point if possible.
   */
  Optional<byte[]> getSplitPoint() throws IOException;

  /** Returns The store compaction priority. */
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
  }

  public Optional<byte[]> getLastRowKey() {
    return reader.getLastRowKey();
  }
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java`
#### Snippet
```java
    long largestStoreSize = 0;
    for (HStore s : stores) {
      Optional<byte[]> splitPoint = s.getSplitPoint();
      // Store also returns null if it has references as way of indicating it is not splittable
      long storeSize = s.getSize();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * @return The split point row, or null if splitting is not possible, or reader is null.
   */
  static Optional<byte[]> getFileSplitPoint(HStoreFile file, CellComparator comparator)
    throws IOException {
    StoreFileReader reader = file.getReader();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * Gets the mid point of the largest file passed in as split point.
   */
  static Optional<byte[]> getSplitPoint(Collection<HStoreFile> storefiles,
    CellComparator comparator) throws IOException {
    Optional<HStoreFile> largestFile = StoreUtils.getLargestFile(storefiles);
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    try (HFile.Reader reader =
      HFile.createReader(srcFs, path, getCacheConfig(), isPrimaryReplicaStore(), conf)) {
      Optional<byte[]> firstKey = reader.getFirstRowKey();
      Preconditions.checkState(firstKey.isPresent(), "First key can not be null");
      Optional<Cell> lk = reader.getLastKey();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
      reader = HFile.createReader(srcFs, srcPath, getCacheConfig(), isPrimaryReplicaStore(), conf);

      Optional<byte[]> firstKey = reader.getFirstRowKey();
      Preconditions.checkState(firstKey.isPresent(), "First key can not be null");
      Optional<Cell> lk = reader.getLastKey();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
   * Determines if Store should be split.
   */
  public Optional<byte[]> getSplitPoint() {
    this.storeEngine.readLock();
    try {
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java`
#### Snippet
```java

  @Override
  public final Optional<byte[]> getSplitPoint() throws IOException {
    return StoreUtils.getSplitPoint(storefiles, cellComparator);
  }
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
  @Override
  public boolean seekToLastRow() throws IOException {
    Optional<byte[]> lastRow = reader.getLastRowKey();
    if (!lastRow.isPresent()) {
      return false;
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private Optional<byte[]> getSplitPointFromAllFiles() throws IOException {
    ConcatenatedLists<HStoreFile> sfs = new ConcatenatedLists<>();
    sfs.addSublist(state.level0Files);
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   */
  @Override
  public Optional<byte[]> getSplitPoint() throws IOException {
    if (this.getStorefileCount() == 0) {
      return Optional.empty();
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * Return the split point. An empty result indicates the region isn't splittable.
   */
  public Optional<byte[]> checkSplit(boolean force) {
    // Can't split META
    if (this.getRegionInfo().isMetaRegion()) {
```

### OptionalContainsCollection
'Optional' contains array `byte[]`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  }

  public Optional<byte[]> checkSplit() {
    return checkSplit(false);
  }
```

## RuleId[id=MethodOverloadsParentMethod]
### MethodOverloadsParentMethod
Method `areSerializedFieldsEqual()` overloads a compatible method of a superclass, when overriding might have been intended
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BigDecimalComparator.java`
#### Snippet
```java
   */
  @SuppressWarnings("ReferenceEquality")
  boolean areSerializedFieldsEqual(BigDecimalComparator other) {
    if (other == this) {
      return true;
```

### MethodOverloadsParentMethod
Method `areSerializedFieldsEqual()` overloads a compatible method of a superclass, when overriding might have been intended
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/LongComparator.java`
#### Snippet
```java
   * corresponding fields in other. Used for testing.
   */
  boolean areSerializedFieldsEqual(LongComparator other) {
    if (other == this) {
      return true;
```

### MethodOverloadsParentMethod
Method `setFailure()` overloads a compatible method of a superclass, when overriding might have been intended
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.java`
#### Snippet
```java
  }

  protected void setFailure(Throwable cause) {
    super.setFailure(getClass().getSimpleName(), cause);
  }
```

## RuleId[id=UnnecessaryCallToStringValueOf]
### UnnecessaryCallToStringValueOf
Unnecessary `String.valueOf()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PrettyPrinter.java`
#### Snippet
```java
      case LONG:
        byte[] longBytes = Bytes.toBytesBinary(value);
        human.append(String.valueOf(Bytes.toLong(longBytes)));
        break;
      case BOOLEAN:
```

### UnnecessaryCallToStringValueOf
Unnecessary `String.valueOf()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PrettyPrinter.java`
#### Snippet
```java
      case BOOLEAN:
        byte[] booleanBytes = Bytes.toBytesBinary(value);
        human.append(String.valueOf(Bytes.toBoolean(booleanBytes)));
        break;
      case BYTE:
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
      }
      maxCoprocessorNumber++;
      String key = "coprocessor$" + Integer.toString(maxCoprocessorNumber);
      return setValue(new Bytes(Bytes.toBytes(key)), new Bytes(Bytes.toBytes(specStr)));
    }
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java

      String value = cp.getJarPath().orElse("") + "|" + cp.getClassName() + "|"
        + Integer.toString(cp.getPriority()) + "|" + kvString.toString();
      return setCoprocessorToMap(value);
    }
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java`
#### Snippet
```java
        ServerMetrics load = entry.getValue();
        StorageClusterStatusModel.Node node =
          model.addLiveNode(sn.getHostname() + ":" + Integer.toString(sn.getPort()),
            sn.getStartcode(), (int) load.getUsedHeapSize().get(Size.Unit.MEGABYTE),
            (int) load.getMaxHeapSize().get(Size.Unit.MEGABYTE));
```

### UnnecessaryCallToStringValueOf
Unnecessary `Long.toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java`
#### Snippet
```java
      scanner = table.getScanner(scan);
      cached = null;
      id = Long.toString(EnvironmentEdgeManager.currentTime())
        + Integer.toHexString(scanner.hashCode());
    } finally {
```

### UnnecessaryCallToStringValueOf
Unnecessary `Long.toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
    }
    result.append(" ], startTime => ");
    result.append(Long.toString(startTime));
    result.append(", endTime => ");
    result.append(Long.toString(endTime));
```

### UnnecessaryCallToStringValueOf
Unnecessary `Long.toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
    result.append(Long.toString(startTime));
    result.append(", endTime => ");
    result.append(Long.toString(endTime));
    result.append(", maxVersions => ");
    result.append(Integer.toString(maxVersions));
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
    result.append(Long.toString(endTime));
    result.append(", maxVersions => ");
    result.append(Integer.toString(maxVersions));
    result.append(", maxValues => ");
    result.append(Integer.toString(maxValues));
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
    result.append(Integer.toString(maxVersions));
    result.append(", maxValues => ");
    result.append(Integer.toString(maxValues));
    result.append("}");
    return result.toString();
```

### UnnecessaryCallToStringValueOf
Unnecessary `String.valueOf()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    @Override
    public String toString() {
      return "HFileScanner for reader " + String.valueOf(getReader());
    }

```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
      }
      this.filename = defaultDir + File.separator + System.getProperty("user.name") + this.hostname
        + ":" + Integer.toString(this.port);
      this.conf = conf;
    }
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    @Override
    public synchronized int summarize() {
      System.out.println(Integer.toString(errorCount) + " inconsistencies detected.");
      if (errorCount == 0) {
        System.out.println("Status: OK");
```

### UnnecessaryCallToStringValueOf
Unnecessary `Long.toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    RetryCounter retryCounter = createZNodeRetryCounterFactory.create();
    hbckEphemeralNodePath = ZNodePaths.joinZNode(zkw.getZNodePaths().masterMaintZNode,
      "hbck-" + Long.toString(EnvironmentEdgeManager.currentTime()));
    do {
      try {
```

### UnnecessaryCallToStringValueOf
Unnecessary `Long.toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/FullTableBackupClient.java`
#### Snippet
```java
      backupInfo.setPhase(BackupPhase.SNAPSHOT);
      for (TableName tableName : tableList) {
        String snapshotName = "snapshot_" + Long.toString(EnvironmentEdgeManager.currentTime())
          + "_" + tableName.getNamespaceAsString() + "_" + tableName.getQualifierAsString();

```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java`
#### Snippet
```java
      }
      LOG.info("starting HBase {} server on {}", implType.simpleClassName(),
        Integer.toString(listenPort));
    } else if (implType == ImplType.THREAD_POOL) {
      this.tserver =
```

### UnnecessaryCallToStringValueOf
Unnecessary `Integer.toString()` call
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java`
#### Snippet
```java
      ProxyUsers.refreshSuperUserGroupsConfiguration(conf);
    }
    LOG.info("Starting Thrift HTTP Server on {}", Integer.toString(listenPort));
  }

```

## RuleId[id=JavaReflectionMemberAccess]
### JavaReflectionMemberAccess
Cannot resolve method 'read' with specified argument types
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/BlockIOUtils.java`
#### Snippet
```java
      // long position, ByteBuffer buf
      byteBufferPositionedReadMethod =
        FSDataInputStream.class.getMethod("read", long.class, ByteBuffer.class);
    } catch (NoSuchMethodException e) {
      LOG.debug("Unable to find positioned bytebuffer read API of FSDataInputStream. "
```

### JavaReflectionMemberAccess
Cannot resolve method 'create' with specified argument types
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java

  private static FileCreator createFileCreator3_3() throws NoSuchMethodException {
    Method createMethod = ClientProtocol.class.getMethod("create", String.class, FsPermission.class,
      String.class, EnumSetWritable.class, boolean.class, short.class, long.class,
      CryptoProtocolVersion[].class, String.class, String.class);
```

### JavaReflectionMemberAccess
Cannot resolve method 'decryptEncryptedDataEncryptionKey'
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
    throws NoSuchMethodException {
    Method decryptEncryptedDataEncryptionKeyMethod = DFSClient.class
      .getDeclaredMethod("decryptEncryptedDataEncryptionKey", FileEncryptionInfo.class);
    decryptEncryptedDataEncryptionKeyMethod.setAccessible(true);
    return new TransparentCryptoHelper() {
```

### JavaReflectionMemberAccess
Cannot resolve field 'ID_UNSPECIFIED'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
            // In later version BlockStoragePolicySuite#ID_UNSPECIFIED is moved to
            // HdfsConstants#BLOCK_STORAGE_POLICY_ID_UNSPECIFIED
            Field idUnspecified = BlockStoragePolicySuite.class.getField("ID_UNSPECIFIED");
            unspecifiedStoragePolicyId = idUnspecified.getByte(BlockStoragePolicySuite.class);
          }
```

### JavaReflectionMemberAccess
Cannot resolve field 'clientFinalizer'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java`
#### Snippet
```java
      } else {
        // Then we didnt' find clientFinalizer in Cache. Presume clean 0.20 hadoop.
        field = FileSystem.class.getDeclaredField(CLIENT_FINALIZER_DATA_METHOD);
        field.setAccessible(true);
        hdfsClientFinalizer = (Runnable) field.get(null);
```

### JavaReflectionMemberAccess
Cannot resolve field 'inputOptions'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
      Field options = null;
      try {
        options = DistCp.class.getDeclaredField("inputOptions");
      } catch (NoSuchFieldException | SecurityException e) {
        options = DistCp.class.getDeclaredField("context");
```

## RuleId[id=ProtectedMemberInFinalClass]
### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
    private final Set<RemoteProcedure> dispatchedOperations = new HashSet<>();

    protected BufferNode(final TRemote key) {
      super(key, 0);
    }
```

### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java
  }

  protected static void waitRegionInTransition(final MasterProcedureEnv env,
    final List<RegionInfo> regions) throws IOException {
    final RegionStates states = env.getAssignmentManager().getRegionStates();
```

### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java
  }

  protected static MasterQuotaManager getMasterQuotaManager(final MasterProcedureEnv env)
    throws IOException {
    return ProcedureSyncWait.waitFor(env, "quota manager to be available",
```

### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java
  }

  protected static void waitMetaRegions(final MasterProcedureEnv env) throws IOException {
    int timeout = env.getMasterConfiguration().getInt("hbase.client.catalog.timeout", 10000);
    try {
```

### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
  }

  protected void addRegion(Path tableDir, RegionInfo regionInfo, RegionVisitor visitor)
    throws IOException {
    boolean isMobRegion = MobUtils.isMobRegionInfo(regionInfo);
```

### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
  }

  protected void addMobRegion(RegionInfo regionInfo, RegionVisitor visitor) throws IOException {
    // 1. dump region meta info into the snapshot directory
    final String snapshotName = desc.getName();
```

### ProtectedMemberInFinalClass
Class member declared `protected` in 'final' class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
  }

  protected void addRegion(final HRegion region, RegionVisitor visitor) throws IOException {
    // 1. dump region meta info into the snapshot directory
    final String snapshotName = desc.getName();
```

## RuleId[id=SwitchStatementWithConfusingDeclaration]
### SwitchStatementWithConfusingDeclaration
Local variable `labelOffset` declared in one 'switch' branch and used in another
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
          break;
        case DOUBLE_QUOTES:
          int labelOffset = ++index;
          // We have to rewrite the expression within double quotes as incase of expressions
          // with escape characters we may have to avoid them as the original expression did
```

### SwitchStatementWithConfusingDeclaration
Local variable `leafExp` declared in one 'switch' branch and used in another
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
          byte[] array =
            org.apache.hbase.thirdparty.com.google.common.primitives.Bytes.toArray(list);
          String leafExp = Bytes.toString(array).trim();
          if (leafExp.isEmpty()) {
            throw new ParseException("Error parsing expression " + expS + " at column : " + index);
```

### SwitchStatementWithConfusingDeclaration
Local variable `delTags` declared in one 'switch' branch and used in another
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java`
#### Snippet
```java
    switch (type) {
      case DeleteFamily:
        List<Tag> delTags = new ArrayList<>();
        if (visibilityTagsDeleteFamily == null) {
          visibilityTagsDeleteFamily = new ArrayList<>();
```

## RuleId[id=PublicFieldAccessedInSynchronizedContext]
### PublicFieldAccessedInSynchronizedContext
Non-private field `groups` accessed in synchronized context
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java

    synchronized (UserProvider.class) {
      if (!(groups instanceof User.TestingGroups)) {
        groups = Groups.getUserToGroupsMappingService(conf);
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `groups` accessed in synchronized context
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
    synchronized (UserProvider.class) {
      if (!(groups instanceof User.TestingGroups)) {
        groups = Groups.getUserToGroupsMappingService(conf);
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `histogram` accessed in synchronized context
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableRangeHistogram.java`
#### Snippet
```java
  public synchronized void snapshot(MetricsRecordBuilder metricsRecordBuilder, boolean all) {
    // Get a reference to the old histogram.
    Snapshot snapshot = histogram.snapshot();
    if (snapshot != null) {
      updateSnapshotMetrics(name, desc, histogram, snapshot, metricsRecordBuilder);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `histogram` accessed in synchronized context
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableRangeHistogram.java`
#### Snippet
```java
    Snapshot snapshot = histogram.snapshot();
    if (snapshot != null) {
      updateSnapshotMetrics(name, desc, histogram, snapshot, metricsRecordBuilder);
      updateSnapshotRangeMetrics(metricsRecordBuilder, snapshot);
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `histogram` accessed in synchronized context
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableHistogram.java`
#### Snippet
```java
  @Override
  public synchronized void snapshot(MetricsRecordBuilder metricsRecordBuilder, boolean all) {
    snapshot(name, desc, histogram, metricsRecordBuilder, all);
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `provider` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcConnection.java`
#### Snippet
```java
    preamble[rpcHeaderLen] = HConstants.RPC_CURRENT_VERSION;
    synchronized (this) {
      preamble[rpcHeaderLen + 1] = provider.getSaslAuthMethod().getCode();
    }
    return preamble;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.response` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
      }
      this.done = true;
      this.response = response;
      this.cells = cells;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cells` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
      this.done = true;
      this.response = response;
      this.cells = cells;
    }
    callComplete();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.error` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
      }
      this.done = true;
      this.error = error;
    }
    callback.run(this);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.error` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/Call.java`
#### Snippet
```java
      }
      this.done = true;
      this.error = error;
    }
    callComplete();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `periodicFlushTask` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
      }
      if (this.mutations.isEmpty() && periodicFlushTimeoutNs > 0) {
        periodicFlushTask = periodicalFlushTimer.newTimeout(timeout -> {
          synchronized (AsyncBufferedMutatorImpl.this) {
            // confirm that we are still valid, if there is already an internalFlush call before us,
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `periodicFlushTask` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
            // to null, and since we may schedule a new one, so here we check whether the references
            // are equal.
            if (timeout == periodicFlushTask) {
              periodicFlushTask = null;
              internalFlush();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `periodicFlushTask` accessed in synchronized context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutatorImpl.java`
#### Snippet
```java
            // are equal.
            if (timeout == periodicFlushTask) {
              periodicFlushTask = null;
              internalFlush();
            }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
  public synchronized void run() throws Exception {
    Pair<FilterHolder, Class<? extends ServletContainer>> pair =
      loginServerPrincipal(userProvider, conf);
    FilterHolder authFilter = pair.getFirst();
    Class<? extends ServletContainer> containerClass = pair.getSecond();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
    FilterHolder authFilter = pair.getFirst();
    Class<? extends ServletContainer> containerClass = pair.getSecond();
    RESTServlet servlet = RESTServlet.getInstance(conf, userProvider);

    // set up the Jersey servlet container for Jetty
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
    ServerConnector serverConnector;
    boolean isSecure = false;
    if (conf.getBoolean(REST_SSL_ENABLED, false)) {
      isSecure = true;
      HttpConfiguration httpsConfig = new HttpConfiguration(httpConfig);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java

      SslContextFactory.Server sslCtxFactory = new SslContextFactory.Server();
      String keystore = conf.get(REST_SSL_KEYSTORE_STORE);
      String keystoreType = conf.get(REST_SSL_KEYSTORE_TYPE);
      String password = HBaseConfiguration.getPassword(conf, REST_SSL_KEYSTORE_PASSWORD, null);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      SslContextFactory.Server sslCtxFactory = new SslContextFactory.Server();
      String keystore = conf.get(REST_SSL_KEYSTORE_STORE);
      String keystoreType = conf.get(REST_SSL_KEYSTORE_TYPE);
      String password = HBaseConfiguration.getPassword(conf, REST_SSL_KEYSTORE_PASSWORD, null);
      String keyPassword =
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      String keystore = conf.get(REST_SSL_KEYSTORE_STORE);
      String keystoreType = conf.get(REST_SSL_KEYSTORE_TYPE);
      String password = HBaseConfiguration.getPassword(conf, REST_SSL_KEYSTORE_PASSWORD, null);
      String keyPassword =
        HBaseConfiguration.getPassword(conf, REST_SSL_KEYSTORE_KEYPASSWORD, password);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      String password = HBaseConfiguration.getPassword(conf, REST_SSL_KEYSTORE_PASSWORD, null);
      String keyPassword =
        HBaseConfiguration.getPassword(conf, REST_SSL_KEYSTORE_KEYPASSWORD, password);
      sslCtxFactory.setKeyStorePath(keystore);
      if (StringUtils.isNotBlank(keystoreType)) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      sslCtxFactory.setKeyManagerPassword(keyPassword);

      String trustStore = conf.get(REST_SSL_TRUSTSTORE_STORE);
      if (StringUtils.isNotBlank(trustStore)) {
        sslCtxFactory.setTrustStorePath(trustStore);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      }
      String trustStorePassword =
        HBaseConfiguration.getPassword(conf, REST_SSL_TRUSTSTORE_PASSWORD, null);
      if (StringUtils.isNotBlank(trustStorePassword)) {
        sslCtxFactory.setTrustStorePassword(trustStorePassword);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
        sslCtxFactory.setTrustStorePassword(trustStorePassword);
      }
      String trustStoreType = conf.get(REST_SSL_TRUSTSTORE_TYPE);
      if (StringUtils.isNotBlank(trustStoreType)) {
        sslCtxFactory.setTrustStoreType(trustStoreType);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      ctxHandler.addFilter(filter, PATH_SPEC_ANY, EnumSet.of(DispatcherType.REQUEST));
    }
    addCSRFFilter(ctxHandler, conf);
    addClickjackingPreventionFilter(ctxHandler, conf);
    addSecurityHeadersFilter(ctxHandler, conf, isSecure);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
    }
    addCSRFFilter(ctxHandler, conf);
    addClickjackingPreventionFilter(ctxHandler, conf);
    addSecurityHeadersFilter(ctxHandler, conf, isSecure);
    HttpServerUtil.constrainHttpMethods(ctxHandler, servlet.getConfiguration()
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
    addCSRFFilter(ctxHandler, conf);
    addClickjackingPreventionFilter(ctxHandler, conf);
    addSecurityHeadersFilter(ctxHandler, conf, isSecure);
    HttpServerUtil.constrainHttpMethods(ctxHandler, servlet.getConfiguration()
      .getBoolean(REST_HTTP_ALLOW_OPTIONS_METHOD, REST_HTTP_ALLOW_OPTIONS_METHOD_DEFAULT));
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java

    // Put up info server.
    int port = conf.getInt("hbase.rest.info.port", 8085);
    if (port >= 0) {
      conf.setLong("startcode", EnvironmentEdgeManager.currentTime());
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
    int port = conf.getInt("hbase.rest.info.port", 8085);
    if (port >= 0) {
      conf.setLong("startcode", EnvironmentEdgeManager.currentTime());
      String a = conf.get("hbase.rest.info.bindAddress", "0.0.0.0");
      this.infoServer = new InfoServer("rest", a, port, false, conf);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
    if (port >= 0) {
      conf.setLong("startcode", EnvironmentEdgeManager.currentTime());
      String a = conf.get("hbase.rest.info.bindAddress", "0.0.0.0");
      this.infoServer = new InfoServer("rest", a, port, false, conf);
      this.infoServer.setAttribute("hbase.conf", conf);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      conf.setLong("startcode", EnvironmentEdgeManager.currentTime());
      String a = conf.get("hbase.rest.info.bindAddress", "0.0.0.0");
      this.infoServer = new InfoServer("rest", a, port, false, conf);
      this.infoServer.setAttribute("hbase.conf", conf);
      this.infoServer.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `conf` accessed in synchronized context
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      String a = conf.get("hbase.rest.info.bindAddress", "0.0.0.0");
      this.infoServer = new InfoServer("rest", a, port, false, conf);
      this.infoServer.setAttribute("hbase.conf", conf);
      this.infoServer.start();
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
    if (path.equals(node)) {
      try {
        if (ZKUtil.watchAndCheckExists(watcher, node)) {
          nodeCreated(path);
        } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
      try {
        // This does not create a watch if the node does not exists
        this.data = ZKUtil.getDataAndWatch(watcher, node);
      } catch (KeeperException e) {
        // We use to abort here, but in some cases the abort is ignored (
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
      if (!nodeExistsChecked) {
        try {
          nodeExistsChecked = (ZKUtil.checkExists(watcher, node) != -1);
        } catch (KeeperException e) {
          LOG.warn("Got exception while trying to check existence in  ZooKeeper" + " of the node: "
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
          try {
            // This does not create a watch if the node does not exists
            this.data = ZKUtil.getDataAndWatch(watcher, node);
          } catch (KeeperException e) {
            LOG.warn("Unexpected exception handling blockUntilAvailable", e);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java

    try {
      byte[] data = ZKUtil.getDataAndWatch(watcher, node);
      if (data != null) {
        this.data = data;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
    if (refresh) {
      try {
        this.data = ZKUtil.getDataAndWatch(watcher, node);
      } catch (KeeperException e) {
        abortable.abort("Unexpected exception handling getData", e);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
   */
  public synchronized void start() {
    this.watcher.registerListener(this);
    try {
      if (ZKUtil.watchAndCheckExists(watcher, node)) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
    this.watcher.registerListener(this);
    try {
      if (ZKUtil.watchAndCheckExists(watcher, node)) {
        byte[] data = ZKUtil.getDataAndWatch(watcher, node);
        if (data != null) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
    try {
      if (ZKUtil.watchAndCheckExists(watcher, node)) {
        byte[] data = ZKUtil.getDataAndWatch(watcher, node);
        if (data != null) {
          this.data = data;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java`
#### Snippet
```java
    try {
      synchronized (lock) {
        if (ZKUtil.watchAndCheckExists(watcher, leaderZNode)) {
          LOG.info("Found new leader for znode: {}", leaderZNode);
          leaderExists.set(true);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java`
#### Snippet
```java
          return;
        }
        byte[] leaderId = ZKUtil.getData(watcher, leaderZNode);
        if (leaderId != null && Bytes.equals(nodeId, leaderId)) {
          LOG.info("Stepping down as leader");
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java`
#### Snippet
```java
        if (leaderId != null && Bytes.equals(nodeId, leaderId)) {
          LOG.info("Stepping down as leader");
          ZKUtil.deleteNodeFailSilent(watcher, leaderZNode);
          leaderExists.set(false);
        } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `asyncClusterConnection` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
   */
  protected final synchronized void setupClusterConnection() throws IOException {
    if (asyncClusterConnection == null) {
      InetSocketAddress localAddress =
        new InetSocketAddress(rpcServices.getSocketAddress().getAddress(), 0);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `asyncClusterConnection` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
        new InetSocketAddress(rpcServices.getSocketAddress().getAddress(), 0);
      User user = userProvider.getCurrent();
      asyncClusterConnection =
        ClusterConnectionFactory.createAsyncClusterConnection(this, conf, localAddress, user);
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `metaBlockIndexReader` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    // is OK to do for meta blocks because the meta block index is always
    // single-level.
    synchronized (metaBlockIndexReader.getRootBlockKey(block)) {
      // Check cache for block. If found return.
      long metaBlockOffset = metaBlockIndexReader.getRootBlockOffset(block);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `metaBlockIndexReader` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    synchronized (metaBlockIndexReader.getRootBlockKey(block)) {
      // Check cache for block. If found return.
      long metaBlockOffset = metaBlockIndexReader.getRootBlockOffset(block);
      BlockCacheKey cacheKey =
        new BlockCacheKey(name, metaBlockOffset, this.isPrimaryReplicaReader(), BlockType.META);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fsBlockReader` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java

      HFileBlock compressedBlock =
        fsBlockReader.readBlockData(metaBlockOffset, blockSize, true, false, true);
      HFileBlock uncompressedBlock = compressedBlock.unpack(hfileContext, fsBlockReader);
      if (compressedBlock != uncompressedBlock) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `hfileContext` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
      HFileBlock compressedBlock =
        fsBlockReader.readBlockData(metaBlockOffset, blockSize, true, false, true);
      HFileBlock uncompressedBlock = compressedBlock.unpack(hfileContext, fsBlockReader);
      if (compressedBlock != uncompressedBlock) {
        compressedBlock.release();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fsBlockReader` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
      HFileBlock compressedBlock =
        fsBlockReader.readBlockData(metaBlockOffset, blockSize, true, false, true);
      HFileBlock uncompressedBlock = compressedBlock.unpack(hfileContext, fsBlockReader);
      if (compressedBlock != uncompressedBlock) {
        compressedBlock.release();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.isError` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  public synchronized void setResponse(Message m, final CellScanner cells, Throwable t,
    String errorMsg) {
    if (this.isError) {
      return;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.isError` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
    }
    if (t != null) {
      this.isError = true;
      TraceUtil.setError(span, t);
    } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cellBlockStream` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
      int cellBlockSize = 0;
      if (bbAllocator.isReservoirEnabled()) {
        this.cellBlockStream = this.cellBlockBuilder.buildCellBlockStream(this.connection.codec,
          this.connection.compressionCodec, cells, bbAllocator);
        if (this.cellBlockStream != null) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cellBlockStream` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
        this.cellBlockStream = this.cellBlockBuilder.buildCellBlockStream(this.connection.codec,
          this.connection.compressionCodec, cells, bbAllocator);
        if (this.cellBlockStream != null) {
          cellBlock = this.cellBlockStream.getByteBuffers();
          cellBlockSize = this.cellBlockStream.size();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cellBlockStream` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
          this.connection.compressionCodec, cells, bbAllocator);
        if (this.cellBlockStream != null) {
          cellBlock = this.cellBlockStream.getByteBuffers();
          cellBlockSize = this.cellBlockStream.size();
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cellBlockStream` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
        if (this.cellBlockStream != null) {
          cellBlock = this.cellBlockStream.getByteBuffers();
          cellBlockSize = this.cellBlockStream.size();
        }
      } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.response` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
      RpcServer.LOG.warn("Exception while creating response " + e);
    }
    this.response = bc;
    // Once a response message is created and set to this.response, this Call can be treated as
    // done. The Responder thread will do the n/w write of this message back to client.
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.rpcCallback` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
    // Once a response message is created and set to this.response, this Call can be treated as
    // done. The Responder thread will do the n/w write of this message back to client.
    if (this.rpcCallback != null) {
      try (Scope ignored = span.makeCurrent()) {
        this.rpcCallback.run();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.rpcCallback` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
    if (this.rpcCallback != null) {
      try (Scope ignored = span.makeCurrent()) {
        this.rpcCallback.run();
      } catch (Exception e) {
        // Don't allow any exception here to kill this handler thread.
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `response` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  @Override
  public synchronized BufferChain getResponse() {
    return response;
  }
}
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.rpcCallback` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  @Override
  public synchronized void setCallBack(RpcCallback callback) {
    this.rpcCallback = callback;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.param` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyServerCall.java`
#### Snippet
```java
  public synchronized void sendResponseIfReady() throws IOException {
    // set param null to reduce memory pressure
    this.param = null;
    connection.channel.writeAndFlush(this);
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.param` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerCall.java`
#### Snippet
```java
  public synchronized void sendResponseIfReady() throws IOException {
    // set param null to reduce memory pressure
    this.param = null;
    this.responder.doRespond(getConnection(), this);
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `port` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  @Override
  public synchronized void stop() {
    LOG.info("Stopping server on " + port);
    running = false;
    if (authTokenSecretMgr != null) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `running` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  public synchronized void stop() {
    LOG.info("Stopping server on " + port);
    running = false;
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    LOG.info("Stopping server on " + port);
    running = false;
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
      authTokenSecretMgr = null;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    running = false;
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
      authTokenSecretMgr = null;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
      authTokenSecretMgr = null;
    }
    listener.interrupt();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `responder` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    listener.interrupt();
    listener.doStop();
    responder.interrupt();
    scheduler.stop();
    notifyAll();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `running` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

      private synchronized void doRunLoop() {
        while (running) {
          try {
            // Consume as many connections as currently queued to avoid
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `running` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
            }
          } catch (InterruptedException e) {
            if (running) { // unexpected -- log it
              LOG.info(Thread.currentThread().getName() + " unexpectedly interrupted", e);
            }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `started` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  @Override
  public synchronized void start() {
    if (started) {
      return;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      return;
    }
    authTokenSecretMgr = createSecretManager();
    if (authTokenSecretMgr != null) {
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    }
    authTokenSecretMgr = createSecretManager();
    if (authTokenSecretMgr != null) {
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
      // LeaderElector start. See HBASE-25875
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
      // LeaderElector start. See HBASE-25875
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      // LeaderElector start. See HBASE-25875
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.authManager` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      }
    }
    this.authManager = new ServiceAuthorizationManager();
    HBasePolicyProvider.init(conf, authManager);
    responder.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authManager` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    }
    this.authManager = new ServiceAuthorizationManager();
    HBasePolicyProvider.init(conf, authManager);
    responder.start();
    listener.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `responder` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    this.authManager = new ServiceAuthorizationManager();
    HBasePolicyProvider.init(conf, authManager);
    responder.start();
    listener.start();
    scheduler.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `started` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    listener.start();
    scheduler.start();
    started = true;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `running` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  @Override
  public synchronized void join() throws InterruptedException {
    while (running) {
      wait();
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `running` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
  @Override
  public synchronized void stop() {
    if (!running) {
      return;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      ts.stop();
    }
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
      authTokenSecretMgr = null;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
    }
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
      authTokenSecretMgr = null;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
    if (authTokenSecretMgr != null) {
      authTokenSecretMgr.stop();
      authTokenSecretMgr = null;
    }
    allChannels.close().awaitUninterruptibly();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `running` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
    scheduler.stop();
    closed.countDown();
    running = false;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `started` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
  @Override
  public synchronized void start() {
    if (started) {
      return;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      return;
    }
    authTokenSecretMgr = createSecretManager();
    if (authTokenSecretMgr != null) {
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
    }
    authTokenSecretMgr = createSecretManager();
    if (authTokenSecretMgr != null) {
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
      // LeaderElector start. See HBASE-25875
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
      // LeaderElector start. See HBASE-25875
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      // LeaderElector start. See HBASE-25875
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authTokenSecretMgr` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.authManager` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      }
    }
    this.authManager = new ServiceAuthorizationManager();
    HBasePolicyProvider.init(conf, authManager);
    scheduler.start();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authManager` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
    }
    this.authManager = new ServiceAuthorizationManager();
    HBasePolicyProvider.init(conf, authManager);
    scheduler.start();
    started = true;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `started` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
    HBasePolicyProvider.init(conf, authManager);
    scheduler.start();
    started = true;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.authManager` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
    // it'll break if you go via static route.
    System.setProperty("hadoop.policy.file", "hbase-policy.xml");
    this.authManager.refresh(conf, pp);
    LOG.info("Refreshed hbase-policy.xml successfully");
    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authManager` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
    if (authorize) {
      Class<?> c = getServiceInterface(services, connection.getServiceName());
      authManager.authorize(user, c, getConf(), addr);
    }
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `store` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java`
#### Snippet
```java
      // list of store files. Add cleanup of anything put on filesystem if we fail.
      synchronized (flushLock) {
        status.setStatus("Flushing " + store + ": creating writer");
        // Write the map out to the disk
        writer = createWriter(snapshot, true, writerCreationTracker);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `totalBuffered` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java

    synchronized (controller.dataAvailable) {
      totalBuffered -= size;
      // We may unblock writers
      controller.dataAvailable.notifyAll();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `totalBuffered` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
    // If we crossed the chunk threshold, wait for more space to be available
    synchronized (controller.dataAvailable) {
      totalBuffered += incrHeap;
      while (totalBuffered > maxHeapUsage && controller.thrown.get() == null) {
        LOG.debug("Used {} bytes of buffered edits, waiting for IO threads", totalBuffered);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `totalBuffered` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
    synchronized (controller.dataAvailable) {
      totalBuffered += incrHeap;
      while (totalBuffered > maxHeapUsage && controller.thrown.get() == null) {
        LOG.debug("Used {} bytes of buffered edits, waiting for IO threads", totalBuffered);
        controller.dataAvailable.wait(2000);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `totalBuffered` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
      totalBuffered += incrHeap;
      while (totalBuffered > maxHeapUsage && controller.thrown.get() == null) {
        LOG.debug("Used {} bytes of buffered edits, waiting for IO threads", totalBuffered);
        controller.dataAvailable.wait(2000);
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `totalBuffered` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedEntryBuffers.java`
#### Snippet
```java
  @Override
  synchronized RegionEntryBuffer getChunkToWrite() {
    if (totalBuffered < maxHeapUsage) {
      return null;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `tableName` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      List<RegionInfo> regions = new ArrayList<>();
      for (HbckRegionInfo h : regionInfoMap.values()) {
        if (tableName.equals(h.getTableName())) {
          if (h.getMetaEntry() != null) {
            regions.add(h.getMetaEntry().getRegionInfo());
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `errorCount` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    @Override
    public synchronized int summarize() {
      System.out.println(Integer.toString(errorCount) + " inconsistencies detected.");
      if (errorCount == 0) {
        System.out.println("Status: OK");
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `errorCount` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized int summarize() {
      System.out.println(Integer.toString(errorCount) + " inconsistencies detected.");
      if (errorCount == 0) {
        System.out.println("Status: OK");
        return 0;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `executor` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          }

          futures.add(executor.submit(new Runnable() {
            @Override
            public void run() {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `errorCount` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        System.out.println("ERROR: " + message);
      }
      errorCount++;
      showProgress = 0;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `errorTables` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void reportError(ERROR_CODE errorCode, String message, HbckTableInfo table,
      HbckRegionInfo info1, HbckRegionInfo info2) {
      errorTables.add(table);
      String reference =
        "(regions " + info1.getRegionNameAsString() + " and " + info2.getRegionNameAsString() + ")";
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `errorTables` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void reportError(ERROR_CODE errorCode, String message, HbckTableInfo table,
      HbckRegionInfo info) {
      errorTables.add(table);
      String reference = "(region " + info.getRegionNameAsString() + ")";
      reportError(errorCode, reference + " " + message);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `errorTables` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void reportError(ERROR_CODE errorCode, String message,
      HbckTableInfo table) {
      errorTables.add(table);
      reportError(errorCode, message);
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java`
#### Snippet
```java
    try {
      synchronized (clusterHasActiveMaster) {
        if (ZKUtil.watchAndCheckExists(watcher, watcher.getZNodePaths().masterAddressZNode)) {
          // A master node exists, there is an active master
          LOG.trace("A master is now available");
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java`
#### Snippet
```java
    try {
      synchronized (clusterHasActiveMaster) {
        if (ZKUtil.watchAndCheckExists(watcher, watcher.getZNodePaths().masterAddressZNode)) {
          // A master node exists, there is an active master
          LOG.trace("A master is now available");
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `activeMasterServerName` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java`
#### Snippet
```java
        // Reset the active master sn. Will be re-fetched later if needed.
        // We don't want to make a synchronous RPC under a monitor.
        activeMasterServerName = null;
      }
    } catch (KeeperException ke) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionServerTracker.java`
#### Snippet
```java
    try (final Scope ignored = span.makeCurrent()) {
      try {
        names = ZKUtil.listChildrenAndWatchForNewChildren(watcher, watcher.getZNodePaths().rsZNode);
      } catch (KeeperException e) {
        // here we need to abort as we failed to set watcher on the rs node which means that we can
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionServerTracker.java`
#### Snippet
```java
    try (final Scope ignored = span.makeCurrent()) {
      try {
        names = ZKUtil.listChildrenAndWatchForNewChildren(watcher, watcher.getZNodePaths().rsZNode);
      } catch (KeeperException e) {
        // here we need to abort as we failed to set watcher on the rs node which means that we can
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fetchInProgress` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java

  private void waitForFetchToFinish() throws InterruptedException {
    synchronized (fetchInProgress) {
      while (fetchInProgress.get()) {
        // We don't want the fetches to block forever, for example if there are bugs
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fetchInProgress` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
  private void waitForFetchToFinish() throws InterruptedException {
    synchronized (fetchInProgress) {
      while (fetchInProgress.get()) {
        // We don't want the fetches to block forever, for example if there are bugs
        // of missing notifications.
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fetchInProgress` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
        // We don't want the fetches to block forever, for example if there are bugs
        // of missing notifications.
        fetchInProgress.wait(MAX_FETCH_TIMEOUT_MS);
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fetchInProgress` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
      } finally {
        Preconditions.checkState(fetchInProgress.compareAndSet(true, false));
        synchronized (fetchInProgress) {
          fetchInProgress.notifyAll();
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `fetchInProgress` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
        Preconditions.checkState(fetchInProgress.compareAndSet(true, false));
        synchronized (fetchInProgress) {
          fetchInProgress.notifyAll();
        }
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cleanersChain` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
  @Override
  public synchronized void cleanup() {
    for (T lc : this.cleanersChain) {
      try {
        lc.stop("Exiting");
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
    if (validate(path)) {
      try {
        if (ZKUtil.watchAndCheckExists(watcher, path)) {
          getDataAndWatch(path);
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
  @Override
  protected synchronized void completionCleanup(MasterProcedureEnv env) {
    env.getRemoteDispatcher().removeCompletedOperation(targetServer, this);
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `event` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
      return;
    }
    if (event == null) {
      LOG.warn("procedure event for {} is null, maybe the procedure is created when recovery",
        getProcId());
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `event` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    }
    complete(env, error);
    event.wake(env.getProcedureScheduler());
    event = null;
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `event` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    complete(env, error);
    event.wake(env.getProcedureScheduler());
    event = null;
  }
}
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `dispatched` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
  protected synchronized Procedure<MasterProcedureEnv>[] execute(MasterProcedureEnv env)
    throws ProcedureYieldException, ProcedureSuspendedException, InterruptedException {
    if (dispatched) {
      if (succ) {
        return null;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `succ` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    throws ProcedureYieldException, ProcedureSuspendedException, InterruptedException {
    if (dispatched) {
      if (succ) {
        return null;
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `dispatched` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
        return null;
      }
      dispatched = false;
    }
    try {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    }
    try {
      env.getRemoteDispatcher().addOperationToNode(targetServer, this);
    } catch (FailedRemoteDispatchException frde) {
      LOG.warn("Can not send remote operation {} to {}, this operation will "
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    } catch (FailedRemoteDispatchException frde) {
      LOG.warn("Can not send remote operation {} to {}, this operation will "
        + "be retried to send to another server", this.getProcId(), targetServer);
      return null;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `dispatched` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
      return null;
    }
    dispatched = true;
    event = new ProcedureEvent<>(this);
    event.suspendIfNotReady(this);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `event` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    }
    dispatched = true;
    event = new ProcedureEvent<>(this);
    event.suspendIfNotReady(this);
    throw new ProcedureSuspendedException();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `event` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    dispatched = true;
    event = new ProcedureEvent<>(this);
    event.suspendIfNotReady(this);
    throw new ProcedureSuspendedException();
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cpHost` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      }

      if (this.cpHost != null) {
        try {
          if (this.cpHost.preBalance(request)) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cpHost` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      if (this.cpHost != null) {
        try {
          if (this.cpHost.preBalance(request)) {
            LOG.debug("Coprocessor bypassing balancer request");
            return responseBuilder.build();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cpHost` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
        request.isDryRun() ? Collections.emptyList() : executeRegionPlansWithThrottling(plans);

      if (this.cpHost != null) {
        try {
          this.cpHost.postBalance(request, sucRPs);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.cpHost` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      if (this.cpHost != null) {
        try {
          this.cpHost.postBalance(request, sucRPs);
        } catch (IOException ioe) {
          // balancing already succeeded so don't change the result
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `dispatched` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
      // if we've already known the snapshot is corrupted, then stop scheduling
      // the new procedures and the undispatched procedures
      if (!dispatched) {
        SnapshotProcedure parent = env.getMasterServices().getMasterProcedureExecutor()
          .getProcedure(SnapshotProcedure.class, getParentProcId());
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `dispatched` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
      }
      // acquire a worker
      if (!dispatched && targetServer == null) {
        targetServer =
          env.getMasterServices().getSnapshotManager().acquireSnapshotVerifyWorker(this);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
      }
      // acquire a worker
      if (!dispatched && targetServer == null) {
        targetServer =
          env.getMasterServices().getSnapshotManager().acquireSnapshotVerifyWorker(this);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
      // acquire a worker
      if (!dispatched && targetServer == null) {
        targetServer =
          env.getMasterServices().getSnapshotManager().acquireSnapshotVerifyWorker(this);
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `dispatched` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
      Procedure<MasterProcedureEnv>[] res = super.execute(env);
      // retry if necessary
      if (!dispatched) {
        // the mostly like thing is that a FailedRemoteDispatchException is thrown.
        // we need to retry on another remote server
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
        // the mostly like thing is that a FailedRemoteDispatchException is thrown.
        // we need to retry on another remote server
        targetServer = null;
        throw new FailedRemoteDispatchException("Failed sent request");
      } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `succ` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
          if (remoteEx instanceof CorruptedSnapshotException) {
            // snapshot is corrupted, will touch a flag file and finish the procedure
            succ = true;
            SnapshotProcedure parent = env.getMasterServices().getMasterProcedureExecutor()
              .getProcedure(SnapshotProcedure.class, getParentProcId());
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `succ` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
          } else {
            // unexpected exception in remote server, will retry on other servers
            succ = false;
          }
        } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `succ` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
        } else {
          // the mostly like thing is that remote call failed, will retry on other servers
          succ = false;
        }
      } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `succ` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
      } else {
        // remote operation finished without error
        succ = true;
      }
    } catch (IOException e) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `targetServer` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
    } finally {
      // release the worker
      env.getMasterServices().getSnapshotManager().releaseSnapshotVerifyWorker(this, targetServer,
        env.getProcedureScheduler());
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      builder.append(" bypass");
    } else {
      if (globalLimiter != NoopQuotaLimiter.get()) {
        // builder.append(" global-limiter");
        builder.append(" " + globalLimiter);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      if (globalLimiter != NoopQuotaLimiter.get()) {
        // builder.append(" global-limiter");
        builder.append(" " + globalLimiter);
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastQuery` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java

  public synchronized long getLastQuery() {
    return lastQuery;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
   */
  public synchronized void update(final QuotaState other) {
    if (globalLimiter == NoopQuotaLimiter.get()) {
      globalLimiter = other.globalLimiter;
    } else if (other.globalLimiter == NoopQuotaLimiter.get()) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
  public synchronized void update(final QuotaState other) {
    if (globalLimiter == NoopQuotaLimiter.get()) {
      globalLimiter = other.globalLimiter;
    } else if (other.globalLimiter == NoopQuotaLimiter.get()) {
      globalLimiter = NoopQuotaLimiter.get();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      globalLimiter = other.globalLimiter;
    } else if (other.globalLimiter == NoopQuotaLimiter.get()) {
      globalLimiter = NoopQuotaLimiter.get();
    } else {
      globalLimiter = QuotaLimiterFactory.update(globalLimiter, other.globalLimiter);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      globalLimiter = NoopQuotaLimiter.get();
    } else {
      globalLimiter = QuotaLimiterFactory.update(globalLimiter, other.globalLimiter);
    }
    lastUpdate = other.lastUpdate;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      globalLimiter = NoopQuotaLimiter.get();
    } else {
      globalLimiter = QuotaLimiterFactory.update(globalLimiter, other.globalLimiter);
    }
    lastUpdate = other.lastUpdate;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastUpdate` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      globalLimiter = QuotaLimiterFactory.update(globalLimiter, other.globalLimiter);
    }
    lastUpdate = other.lastUpdate;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
  /** Returns true if there is no quota information associated to this object */
  public synchronized boolean isBypass() {
    return globalLimiter == NoopQuotaLimiter.get();
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastUpdate` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java

  public synchronized long getLastUpdate() {
    return lastUpdate;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
   */
  synchronized QuotaLimiter getGlobalLimiterWithoutUpdatingLastQuery() {
    return globalLimiter;
  }
}
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
  public synchronized void setQuotas(final Quotas quotas) {
    if (quotas.hasThrottle()) {
      globalLimiter = QuotaLimiterFactory.fromThrottle(quotas.getThrottle());
    } else {
      globalLimiter = NoopQuotaLimiter.get();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      globalLimiter = QuotaLimiterFactory.fromThrottle(quotas.getThrottle());
    } else {
      globalLimiter = NoopQuotaLimiter.get();
    }
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastQuery` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
   */
  public synchronized QuotaLimiter getGlobalLimiter() {
    lastQuery = EnvironmentEdgeManager.currentTime();
    return globalLimiter;
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `globalLimiter` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
  public synchronized QuotaLimiter getGlobalLimiter() {
    lastQuery = EnvironmentEdgeManager.currentTime();
    return globalLimiter;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastQuery` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
   */
  public synchronized QuotaLimiter getTableLimiter(final TableName table) {
    lastQuery = EnvironmentEdgeManager.currentTime();
    if (tableLimiters != null) {
      QuotaLimiter limiter = tableLimiters.get(table);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java`
#### Snippet
```java
    try {
      List<ZKUtil.NodeAndData> nodes =
        ZKUtil.getChildDataAndWatchForNewChildren(watcher, keysParentZNode);
      refreshNodes(nodes);
    } catch (KeeperException ke) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `watcher` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java`
#### Snippet
```java
    } catch (KeeperException ke) {
      LOG.error(HBaseMarkers.FATAL, "Error reading data from zookeeper", ke);
      watcher.abort("Error reading changed keys from zookeeper", ke);
    }
  }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authorizationEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
      List<byte[]> labelAuths = new ArrayList<>(auths.size());
      try {
        if (authorizationEnabled) {
          checkCallingUserAuth();
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authorizationEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
      List<byte[]> labels = new ArrayList<>(visLabels.size());
      try {
        if (authorizationEnabled) {
          checkCallingUserAuth();
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authorizationEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
      try {
        // When AC is ON, do AC based user auth check
        if (authorizationEnabled && accessControllerAvailable && !isSystemOrSuperUser()) {
          User user = VisibilityUtils.getActiveUser();
          throw new AccessDeniedException("User '" + (user != null ? user.getShortName() : "null")
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authorizationEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
            + " is not authorized to perform this action.");
        }
        if (authorizationEnabled) {
          checkCallingUserAuth(); // When AC is not in place the calling user should have
                                  // SYSTEM_LABEL auth to do this action.
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authorizationEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
        // We do ACL check here as we create scanner directly on region. It will not make calls to
        // AccessController CP methods.
        if (authorizationEnabled && accessControllerAvailable && !isSystemOrSuperUser()) {
          User requestingUser = VisibilityUtils.getActiveUser();
          throw new AccessDeniedException(
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `authorizationEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
        // We do ACL check here as we create scanner directly on region. It will not make calls to
        // AccessController CP methods.
        if (authorizationEnabled && accessControllerAvailable && !isSystemOrSuperUser()) {
          User requestingUser = VisibilityUtils.getActiveUser();
          throw new AccessDeniedException(
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `member` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
      }
    } catch (KeeperException e) {
      member.controllerConnectionFailure(
        "Failed to get the abort znode (" + abortZNode + ") for procedure :" + opName, e, opName);
      return;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `member` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
      data = Arrays.copyOfRange(data, ProtobufUtil.lengthOfPBMagic(), data.length);
      LOG.debug("Found data for znode:" + path);
      subproc = member.createSubprocedure(opName, data);
      member.submitSubprocedure(subproc);
    } catch (IllegalArgumentException iae) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `member` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
      LOG.debug("Found data for znode:" + path);
      subproc = member.createSubprocedure(opName, data);
      member.submitSubprocedure(subproc);
    } catch (IllegalArgumentException iae) {
      LOG.error("Illegal argument exception", iae);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `member` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
      sendMemberAborted(subproc, new ForeignException(getMemberName(), ise));
    } catch (KeeperException e) {
      member.controllerConnectionFailure("Failed to get data for new procedure:" + opName, e,
        opName);
    } catch (InterruptedException e) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `member` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
        opName);
    } catch (InterruptedException e) {
      member.controllerConnectionFailure("Failed to get data for new procedure:" + opName, e,
        opName);
      Thread.currentThread().interrupt();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.state` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
    setWarnTime(now);
    this.rpcQueueTime = queueTime;
    this.state = State.RUNNING;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `ctx` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
      }
      zkw =
        new ZKWatcher(ctx.getConfiguration(), "connection to cluster: " + ctx.getPeerId(), this);
      zkw.registerListener(new PeerRegionServerListener(this));
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `ctx` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
      }
      zkw =
        new ZKWatcher(ctx.getConfiguration(), "connection to cluster: " + ctx.getPeerId(), this);
      zkw.registerListener(new PeerRegionServerListener(this));
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `store` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java`
#### Snippet
```java
      // list of store files. Add cleanup of anything put on filesystem if we fail.
      synchronized (flushLock) {
        status.setStatus("Flushing " + store + ": creating writer");
        // Write the map out to the disk
        writer = createWriter(snapshot, false, writerCreationTracker);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `forceMajor` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
          try {
            compaction.select(this.filesCompacting, isUserCompaction, mayUseOffPeak,
              forceMajor && filesCompacting.isEmpty());
          } catch (IOException e) {
            if (mayUseOffPeak) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.forceMajor` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java

        // If we're enqueuing a major, clear the force flag.
        this.forceMajor = this.forceMajor && !request.isMajor();

        // Set common request properties.
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.forceMajor` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java

        // If we're enqueuing a major, clear the force flag.
        this.forceMajor = this.forceMajor && !request.isMajor();

        // Set common request properties.
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `zooKeeperWatcher` accessed in synchronized context
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseZKTestingUtility.java`
#### Snippet
```java
   */
  public synchronized ZKWatcher getZooKeeperWatcher() throws IOException {
    if (zooKeeperWatcher == null) {
      zooKeeperWatcher = new ZKWatcher(conf, "testing utility", new Abortable() {
        @Override
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `zooKeeperWatcher` accessed in synchronized context
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseZKTestingUtility.java`
#### Snippet
```java
  public synchronized ZKWatcher getZooKeeperWatcher() throws IOException {
    if (zooKeeperWatcher == null) {
      zooKeeperWatcher = new ZKWatcher(conf, "testing utility", new Abortable() {
        @Override
        public void abort(String why, Throwable e) {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `zooKeeperWatcher` accessed in synchronized context
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseZKTestingUtility.java`
#### Snippet
```java
      });
    }
    return zooKeeperWatcher;
  }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // possibly flushing recovered.edits), after seeing this event, we can ignore every edit
        // smaller than this seqId
        if (this.lastReplayedOpenRegionSeqId <= regionEvent.getLogSequenceNumber()) {
          this.lastReplayedOpenRegionSeqId = regionEvent.getLogSequenceNumber();
        } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // smaller than this seqId
        if (this.lastReplayedOpenRegionSeqId <= regionEvent.getLogSequenceNumber()) {
          this.lastReplayedOpenRegionSeqId = regionEvent.getLogSequenceNumber();
        } else {
          LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying region event :"
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            + TextFormat.shortDebugString(regionEvent)
            + " because its sequence id is smaller than this regions lastReplayedOpenRegionSeqId "
            + " of " + lastReplayedOpenRegionSeqId);
          return;
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    synchronized (writestate) {
      try {
        if (flush.getFlushSequenceNumber() < lastReplayedOpenRegionSeqId) {
          LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying flush event :"
            + TextFormat.shortDebugString(flush)
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            + TextFormat.shortDebugString(flush)
            + " because its sequence id is smaller than this regions lastReplayedOpenRegionSeqId "
            + " of " + lastReplayedOpenRegionSeqId);
          return null;
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.writesEnabled` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
     */
    synchronized void setReadOnly(final boolean onOff) {
      this.writesEnabled = !onOff;
      this.readOnly = onOff;
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.readOnly` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    synchronized void setReadOnly(final boolean onOff) {
      this.writesEnabled = !onOff;
      this.readOnly = onOff;
    }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private void replayWALFlushCannotFlushMarker(FlushDescriptor flush, long replaySeqId) {
    synchronized (writestate) {
      if (this.lastReplayedOpenRegionSeqId > replaySeqId) {
        LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying flush event :"
          + TextFormat.shortDebugString(flush) + " because its sequence id " + replaySeqId
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          + TextFormat.shortDebugString(flush) + " because its sequence id " + replaySeqId
          + " is smaller than this regions " + "lastReplayedOpenRegionSeqId of "
          + lastReplayedOpenRegionSeqId);
        return;
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    synchronized (writestate) {
      try {
        if (flush.getFlushSequenceNumber() < lastReplayedOpenRegionSeqId) {
          LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying flush event :"
            + TextFormat.shortDebugString(flush)
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            + TextFormat.shortDebugString(flush)
            + " because its sequence id is smaller than this regions lastReplayedOpenRegionSeqId "
            + " of " + lastReplayedOpenRegionSeqId);
          return;
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // than this. We are updating lastReplayedOpenRegionSeqId so that we can skip all edits
        // that we have picked the flush files for
        if (this.lastReplayedOpenRegionSeqId < smallestSeqIdInStores) {
          this.lastReplayedOpenRegionSeqId = smallestSeqIdInStores;
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // that we have picked the flush files for
        if (this.lastReplayedOpenRegionSeqId < smallestSeqIdInStores) {
          this.lastReplayedOpenRegionSeqId = smallestSeqIdInStores;
        }
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `this.lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        if (
          bulkLoadEvent.getBulkloadSeqNum() >= 0
            && this.lastReplayedOpenRegionSeqId >= bulkLoadEvent.getBulkloadSeqNum()
        ) {
          LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying bulkload event :"
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            + TextFormat.shortDebugString(bulkLoadEvent)
            + " because its sequence id is smaller than this region's lastReplayedOpenRegionSeqId"
            + " =" + lastReplayedOpenRegionSeqId);

          return;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

    synchronized (writestate) {
      if (replaySeqId < lastReplayedOpenRegionSeqId) {
        LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying compaction event :"
          + TextFormat.shortDebugString(compaction) + " because its sequence id " + replaySeqId
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          + TextFormat.shortDebugString(compaction) + " because its sequence id " + replaySeqId
          + " is smaller than this regions " + "lastReplayedOpenRegionSeqId of "
          + lastReplayedOpenRegionSeqId);
        return;
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedCompactionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        return;
      }
      if (replaySeqId < lastReplayedCompactionSeqId) {
        LOG.warn(getRegionInfo().getEncodedName() + " : " + "Skipping replaying compaction event :"
          + TextFormat.shortDebugString(compaction) + " because its sequence id " + replaySeqId
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedCompactionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          + TextFormat.shortDebugString(compaction) + " because its sequence id " + replaySeqId
          + " is smaller than this regions " + "lastReplayedCompactionSeqId of "
          + lastReplayedCompactionSeqId);
        return;
      } else {
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedCompactionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        return;
      } else {
        lastReplayedCompactionSeqId = replaySeqId;
      }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `lastReplayedOpenRegionSeqId` accessed in synchronized context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        LOG.debug(getRegionInfo().getEncodedName() + " : " + "Replaying compaction marker "
          + TextFormat.shortDebugString(compaction) + " with seqId=" + replaySeqId
          + " and lastReplayedOpenRegionSeqId=" + lastReplayedOpenRegionSeqId);
      }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sum` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java

      public synchronized Pair<S, Long> getAvgArgs() {
        return new Pair<>(sum, rowCount);
      }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `rowCount` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java

      public synchronized Pair<S, Long> getAvgArgs() {
        return new Pair<>(sum, rowCount);
      }

```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized Pair<List<S>, Long> getStdParams() {
        List<S> l = new ArrayList<>(2);
        l.add(sumVal);
        l.add(sumSqVal);
        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumSqVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        List<S> l = new ArrayList<>(2);
        l.add(sumVal);
        l.add(sumSqVal);
        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);
        return p;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `rowCountVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        l.add(sumVal);
        l.add(sumSqVal);
        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);
        return p;
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams() {
        List<S> l = new ArrayList<>(2);
        l.add(sumVal);
        l.add(sumWeights);
        Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumWeights` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        List<S> l = new ArrayList<>(2);
        l.add(sumVal);
        l.add(sumWeights);
        Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);
        return p;
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized void update(byte[] region, byte[] row, Pair<List<S>, Long> result) {
        if (result.getFirst().size() > 0) {
          sumVal = ci.add(sumVal, result.getFirst().get(0));
          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));
          rowCountVal += result.getSecond();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized void update(byte[] region, byte[] row, Pair<List<S>, Long> result) {
        if (result.getFirst().size() > 0) {
          sumVal = ci.add(sumVal, result.getFirst().get(0));
          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));
          rowCountVal += result.getSecond();
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumSqVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        if (result.getFirst().size() > 0) {
          sumVal = ci.add(sumVal, result.getFirst().get(0));
          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));
          rowCountVal += result.getSecond();
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumSqVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        if (result.getFirst().size() > 0) {
          sumVal = ci.add(sumVal, result.getFirst().get(0));
          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));
          rowCountVal += result.getSecond();
        }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `rowCountVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
          sumVal = ci.add(sumVal, result.getFirst().get(0));
          sumSqVal = ci.add(sumSqVal, result.getFirst().get(1));
          rowCountVal += result.getSecond();
        }
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `max` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, R result) {
        max = (max == null || (result != null && ci.compare(max, result) < 0)) ? result : max;
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `max` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, R result) {
        max = (max == null || (result != null && ci.compare(max, result) < 0)) ? result : max;
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `max` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, R result) {
        max = (max == null || (result != null && ci.compare(max, result) < 0)) ? result : max;
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `max` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, R result) {
        max = (max == null || (result != null && ci.compare(max, result) < 0)) ? result : max;
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sum` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, Pair<S, Long> result) {
        sum = ci.add(sum, result.getFirst());
        rowCount += result.getSecond();
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sum` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, Pair<S, Long> result) {
        sum = ci.add(sum, result.getFirst());
        rowCount += result.getSecond();
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `rowCount` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized void update(byte[] region, byte[] row, Pair<S, Long> result) {
        sum = ci.add(sum, result.getFirst());
        rowCount += result.getSecond();
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, S result) {
        sumVal = ci.add(sumVal, result);
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      @Override
      public synchronized void update(byte[] region, byte[] row, S result) {
        sumVal = ci.add(sumVal, result);
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized void update(byte[] region, byte[] row, List<S> result) {
        map.put(row, result);
        sumVal = ci.add(sumVal, result.get(0));
        sumWeights = ci.add(sumWeights, result.get(1));
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumVal` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      public synchronized void update(byte[] region, byte[] row, List<S> result) {
        map.put(row, result);
        sumVal = ci.add(sumVal, result.get(0));
        sumWeights = ci.add(sumWeights, result.get(1));
      }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumWeights` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        map.put(row, result);
        sumVal = ci.add(sumVal, result.get(0));
        sumWeights = ci.add(sumWeights, result.get(1));
      }
    }
```

### PublicFieldAccessedInSynchronizedContext
Non-private field `sumWeights` accessed in synchronized context
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        map.put(row, result);
        sumVal = ci.add(sumVal, result.get(0));
        sumWeights = ci.add(sumWeights, result.get(1));
      }
    }
```

## RuleId[id=RedundantSuppression]
### RedundantSuppression
Redundant suppression
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static ByteArrayComparable toComparator(ComparatorProtos.Comparator proto)
    throws IOException {
    String type = proto.getName();
```

### RedundantSuppression
Redundant suppression
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static Filter toFilter(FilterProtos.Filter proto) throws IOException {
    String type = proto.getName();
    final byte[] value = proto.getSerializedFilter().toByteArray();
```

### RedundantSuppression
Redundant suppression
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java`
#### Snippet
```java
   */
  @SuppressWarnings("serial")
  static class RoundRobinPool<R> implements Pool<R> {
    private final List<R> resources;
    private final int maxSize;
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java

        // yes, UnsafeComparer does implement Comparer<byte[]>
        @SuppressWarnings("unchecked")
        Converter converter = (Converter) theClass.getConstructor().newInstance();
        return converter;
      } catch (Throwable t) { // ensure we really catch *everything*
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static boolean equals(final byte[] left, final byte[] right) {
    // Could use Arrays.equals?
    // noinspection SimplifiableConditionalExpression
    if (left == right) return true;
    if (left == null || right == null) return false;
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
    LINKEDLIST_ENTRY = align(OBJECT + (2 * REFERENCE));

    // noinspection PointlessArithmeticExpression
    BYTE_BUFFER = JVM.getJVMSpecVersion() < 17
      ? align(OBJECT + REFERENCE + (5 * Bytes.SIZEOF_INT) + (3 * Bytes.SIZEOF_BOOLEAN)
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  private static int[] getSizeCoefficients(Class cl, boolean debug) {
    int primitives = 0;
    int arrays = 0;
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static long estimateBase(Class cl, boolean debug) {
    return estimateBaseFromCoefficients(getSizeCoefficients(cl, debug), debug);
  }
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java`
#### Snippet
```java
  @Override
  @SuppressWarnings("unchecked")
  public boolean equals(Object other) {
    return other instanceof Pair && equals(first, ((Pair) other).first)
      && equals(second, ((Pair) other).second);
```

### RedundantSuppression
Redundant suppression
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java`
#### Snippet
```java
  @Override
  @SuppressWarnings("unchecked")
  public boolean equals(Object other) {
    return other instanceof PairOfSameType && equals(first, ((PairOfSameType) other).first)
      && equals(second, ((PairOfSameType) other).second);
```

### RedundantSuppression
Redundant suppression
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java

  public static HttpServletRequest toLowerCase(final HttpServletRequest request) {
    @SuppressWarnings("unchecked")
    final Map<String, String[]> original = (Map<String, String[]>) request.getParameterMap();
    if (!containsUpperCase(original.keySet())) {
      return request;
```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static void initJob(String table, String columns, String groupColumns,
    Class<? extends TableMap> mapper, JobConf job) {

```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/IdentityTableMap.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static void initJob(String table, String columns, Class<? extends TableMap> mapper,
    JobConf job) {
    TableMapReduceUtil.initTableMapJob(table, columns, mapper, ImmutableBytesWritable.class,
```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableReduce.java`
#### Snippet
```java
@InterfaceAudience.Public
@SuppressWarnings("unchecked")
public interface TableReduce<K extends WritableComparable, V>
  extends Reducer<K, V, ImmutableBytesWritable, Put> {

```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static void initJob(String table, Scan scan, String groupColumns,
    Class<? extends TableMapper> mapper, Job job) throws IOException {
    TableMapReduceUtil.initTableMapperJob(table, scan, mapper, ImmutableBytesWritable.class,
```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java

  @SuppressWarnings("deprecation")
  static byte[] getStartKey(Configuration conf) {
    return getKeyFromConf(conf, START_BASE64, START);
  }
```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java

  @SuppressWarnings("deprecation")
  static byte[] getEndKey(Configuration conf) {
    return getKeyFromConf(conf, END_BASE64, END);
  }
```

### RedundantSuppression
Redundant suppression
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java`
#### Snippet
```java

  @SuppressWarnings("hiding")
  private static final Logger LOG = LoggerFactory.getLogger(TableInputFormat.class);

  /** Job parameter that specifies the input table. */
```

### RedundantSuppression
Redundant suppression
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java
   */
  protected <T extends Procedure<TEnvironment>> void
    addChildProcedure(@SuppressWarnings("unchecked") T... subProcedure) {
    if (subProcedure == null) {
      return;
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
  @Override
  @SuppressWarnings("deprecation")
  public FSDataOutputStream createNonRecursive(Path f, boolean overwrite, int bufferSize,
    short replication, long blockSize, Progressable progress) throws IOException {
    return fs.createNonRecursive(f, overwrite, bufferSize, replication, blockSize, progress);
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
   */
  @SuppressWarnings({ "rawtypes" })
  public void unbuffer() {
    FSDataInputStream stream = this.getStream(this.shouldUseHBaseChecksum());
    if (stream != null) {
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java
    }

    @SuppressWarnings("unchecked")
    List<String> remainingArgs = cmd.getArgList();
    if (remainingArgs.size() != 1) {
      usage(null);
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java

  @SuppressWarnings("resource")
  private int stopMaster() {
    Configuration conf = getConf();
    // Don't try more than once
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java`
#### Snippet
```java

    // the skipTableStateCheck is false so we still need to set it...
    @SuppressWarnings("deprecation")
    MasterProcedureProtos.EnableTableStateData.Builder enableTableMsg =
      MasterProcedureProtos.EnableTableStateData.newBuilder()
        .setUserInfo(MasterProcedureUtil.toProtoUserInfo(getUser()))
        .setTableName(ProtobufUtil.toProtoTableName(tableName)).setSkipTableStateCheck(false);

    serializer.serialize(enableTableMsg.build());
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellSet.java`
#### Snippet
```java
  @Override
  public boolean contains(Object o) {
    // noinspection SuspiciousMethodCalls
    return this.delegatee.containsKey(o);
  }
```

### RedundantSuppression
Redundant suppression
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   */
  @SuppressWarnings("deprecation")
  private void adoptHdfsOrphan(HbckRegionInfo hi) throws IOException {
    Path p = hi.getHdfsRegionDir();
    FileSystem fs = p.getFileSystem(getConf());
```

### RedundantSuppression
Redundant suppression
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.TOOLS)
@SuppressWarnings({ "rawtypes", "unchecked" })
public class ThriftServer extends org.apache.hadoop.hbase.thrift.ThriftServer {
  private static final Logger log = LoggerFactory.getLogger(ThriftServer.class);

```

## RuleId[id=OptionalUsedAsFieldOrParameterType]
### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'cursor'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncScanSingleRegionRpcRetryingCaller.java`
#### Snippet
```java
    private final Thread callerThread;

    private final Optional<Cursor> cursor;

    // INITIALIZED -> SUSPENDED -> DESTROYED
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'cursor'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncScanSingleRegionRpcRetryingCaller.java`
#### Snippet
```java
    private ScanResumerImpl resumer;

    public ScanControllerImpl(Optional<Cursor> cursor) {
      this.callerThread = Thread.currentThread();
      this.cursor = cursor;
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'metrics'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
   */
  private static <T> void connect(CompletableFuture<T> srcFuture, CompletableFuture<T> dstFuture,
    Optional<MetricsConnection> metrics) {
    addListener(srcFuture, (r, e) -> {
      if (e != null) {
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'metrics'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    TableName tableName, Query query, byte[] row, RegionLocateType locateType,
    Function<Integer, CompletableFuture<T>> requestReplica, long rpcTimeoutNs,
    long primaryCallTimeoutNs, Timer retryTimer, Optional<MetricsConnection> metrics) {
    if (query.getConsistency() != Consistency.TIMELINE) {
      return requestReplica.apply(RegionReplicaUtil.DEFAULT_REPLICA_ID);
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'optStats'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  }

  static void updateStats(Optional<ServerStatisticTracker> optStats,
    Optional<MetricsConnection> optMetrics, ServerName serverName, MultiResponse resp) {
    if (!optStats.isPresent() && !optMetrics.isPresent()) {
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'optMetrics'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java

  static void updateStats(Optional<ServerStatisticTracker> optStats,
    Optional<MetricsConnection> optMetrics, ServerName serverName, MultiResponse resp) {
    if (!optStats.isPresent() && !optMetrics.isPresent()) {
      // ServerStatisticTracker and MetricsConnection are both not present, just return
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'metrics'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  private static <T> void sendRequestsToSecondaryReplicas(
    Function<Integer, CompletableFuture<T>> requestReplica, RegionLocations locs,
    CompletableFuture<T> future, Optional<MetricsConnection> metrics) {
    if (future.isDone()) {
      // do not send requests to secondary replicas if the future is done, i.e, the primary request
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'metrics'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java

  private final String metricsScope;
  private final Optional<MetricsConnection> metrics;

  private final ClusterStatusListener clusterStatusListener;
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'stats'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java
    new AtomicReference<>();

  private final Optional<ServerStatisticTracker> stats;
  private final ClientBackoffPolicy backoffPolicy;

```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'policy'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshot.java`
#### Snippet
```java
  public static class SpaceQuotaStatus implements SpaceQuotaStatusView {
    private static final SpaceQuotaStatus NOT_IN_VIOLATION = new SpaceQuotaStatus(null, false);
    final Optional<SpaceViolationPolicy> policy;
    final boolean inViolation;

```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'trustStorePassword'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
   * @throws ClientTrustStoreInitializationException if the trust store file can not be loaded
   */
  public Client(Cluster cluster, String trustStorePath, Optional<String> trustStorePassword,
    Optional<String> trustStoreType) {
    this(cluster, HBaseConfiguration.create(), trustStorePath, trustStorePassword, trustStoreType);
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'trustStoreType'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
   */
  public Client(Cluster cluster, String trustStorePath, Optional<String> trustStorePassword,
    Optional<String> trustStoreType) {
    this(cluster, HBaseConfiguration.create(), trustStorePath, trustStorePassword, trustStoreType);
  }
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'trustStorePassword'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
   */
  public Client(Cluster cluster, Configuration conf, String trustStorePath,
    Optional<String> trustStorePassword, Optional<String> trustStoreType) {

    char[] password = trustStorePassword.map(String::toCharArray).orElse(null);
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'trustStoreType'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
   */
  public Client(Cluster cluster, Configuration conf, String trustStorePath,
    Optional<String> trustStorePassword, Optional<String> trustStoreType) {

    char[] password = trustStorePassword.map(String::toCharArray).orElse(null);
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'trustStore'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java

  private void initialize(Cluster cluster, Configuration conf, boolean sslEnabled,
    Optional<KeyStore> trustStore) {
    this.cluster = cluster;
    this.conf = conf;
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'firstKey'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
  private final Cell splitCell;

  private Optional<Cell> firstKey = Optional.empty();

  private boolean firstKeySeeked = false;
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'optGroupName'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java
  }

  public static RSGroupInfo checkGroupExists(RSGroupGetter getter, Optional<String> optGroupName,
    Supplier<String> forWhom) throws IOException {
    if (optGroupName.isPresent()) {
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'firstKey'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java

  // firstKey, lastkey and cellComparator will be set when openReader.
  private Optional<Cell> firstKey;

  private Optional<Cell> lastKey;
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'lastKey'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  private Optional<Cell> firstKey;

  private Optional<Cell> lastKey;

  private CellComparator comparator;
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for parameter 'rpcCall'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
  }

  private void checkClientDisconnect(Optional<RpcCall> rpcCall) throws CallerDisconnectedException {
    if (rpcCall.isPresent()) {
      // If a user specifies a too-restrictive or too-slow scanner, the
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'completionAction'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java`
#### Snippet
```java
     * {@link MultiVersionConcurrencyControl#writeQueue}.
     */
    private Optional<Runnable> completionAction = Optional.empty();

    private WriteEntry(long writeNumber) {
```

### OptionalUsedAsFieldOrParameterType
`Optional` used as type for field 'regionReplicationSink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private final StoreHotnessProtector storeHotnessProtector;

  protected Optional<RegionReplicationSink> regionReplicationSink = Optional.empty();

  /**
```

## RuleId[id=NonStrictComparisonCanBeEquality]
### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java`
#### Snippet
```java
   */
  public static byte[] isLegalFullyQualifiedTableName(final byte[] tableName) {
    if (tableName == null || tableName.length <= 0) {
      throw new IllegalArgumentException("Name is null or empty");
    }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
    do {
      tailName = tailPath.getName();
      if (tailName == null || tailName.length() <= 0) {
        result = true;
        break;
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
      }
      toSearchName = toSearch.getName();
      if (toSearchName == null || toSearchName.length() <= 0) {
        break;
      }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
     */
    public ModifyableTableDescriptor modifyColumnFamily(final ColumnFamilyDescriptor family) {
      if (family.getName() == null || family.getName().length <= 0) {
        throw new IllegalArgumentException("Family name cannot be null or empty");
      }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
     */
    public ModifyableTableDescriptor setColumnFamily(final ColumnFamilyDescriptor family) {
      if (family.getName() == null || family.getName().length <= 0) {
        throw new IllegalArgumentException("Family name cannot be null or empty");
      }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      return convert(peer);
    } else {
      if (bytes == null || bytes.length <= 0) {
        throw new DeserializationException("Bytes to deserialize should not be empty.");
      }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  public void submitProcedures(Procedure<TEnvironment>[] procs) {
    Preconditions.checkArgument(lastProcId.get() >= 0);
    if (procs == null || procs.length <= 0) {
      return;
    }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static ServerName toServerName(final byte[] data) throws DeserializationException {
    if (data == null || data.length <= 0) {
      return null;
    }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static ServerName parseServerNameFrom(final byte[] data) throws DeserializationException {
    if (data == null || data.length <= 0) return null;
    if (ProtobufMagic.isPBMagicPrefix(data)) {
      int prefixLen = ProtobufMagic.lengthOfPBMagic();
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
  public static void deleteNodeRecursivelyMultiOrSequential(ZKWatcher zkw,
    boolean runSequentialOnMultiFailure, String... pathRoots) throws KeeperException {
    if (pathRoots == null || pathRoots.length <= 0) {
      LOG.warn("Given path is not valid!");
      return;
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
  public static void deleteChildrenRecursivelyMultiOrSequential(ZKWatcher zkw,
    boolean runSequentialOnMultiFailure, String... pathRoots) throws KeeperException {
    if (pathRoots == null || pathRoots.length <= 0) {
      LOG.warn("Given path is not valid!");
      return;
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java`
#### Snippet
```java
  public void setConf(Configuration otherConf) {
    String tableName = otherConf.get(OUTPUT_TABLE);
    if (tableName == null || tableName.length() <= 0) {
      throw new IllegalArgumentException("Must specify table name");
    }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java`
#### Snippet
```java
    this.conf = configuration;
    String[] rawScans = conf.getStrings(SCANS);
    if (rawScans.length <= 0) {
      throw new IllegalArgumentException(
        "There must be at least 1 scan configuration set to : " + SCANS);
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
    // Check for L2. ioengine name must be non-null.
    String bucketCacheIOEngineName = c.get(BUCKET_CACHE_IOENGINE_KEY, null);
    if (bucketCacheIOEngineName == null || bucketCacheIOEngineName.length() <= 0) {
      return null;
    }
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        return;
      }
      if (peers == null || peers.size() <= 0) {
        future.completeExceptionally(
          new IllegalArgumentException("Found no peer cluster for replication."));
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        return;
      }
      if (tableNames == null || tableNames.size() <= 0) {
        future.complete(Collections.emptyList());
        return;
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ZKDataMigrator.java`
#### Snippet
```java
      ZNodePaths.joinZNode(zkw.getZNodePaths().tableZNode, tableName.getNameAsString());
    byte[] data = ZKUtil.getData(zkw, znode);
    if (data == null || data.length <= 0) return null;
    try {
      ProtobufUtil.expectPBMagicPrefix(data);
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      }
    }
    if (regionNames.size() <= 0) {
      errors.reportError(ERROR_CODE.INVALID_TABLE,
        "there is no barriers of this table: " + cleanReplicationBarrierTable);
```

### NonStrictComparisonCanBeEquality
Can be replaced with equality
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
      // check whether there is no sync futures offered, and no in-flight sync futures that is being
      // processed.
      return syncFutures.size() <= 0 && takeSyncFuture == null;
    }

```

## RuleId[id=CharsetObjectCanBeUsed]
### CharsetObjectCanBeUsed
StandardCharsets.UTF_8 can be used instead
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
   */

  public static final Charset UTF8 = Charset.forName("UTF-8");
  private static final byte TERM = 0x00;

```

### CharsetObjectCanBeUsed
StandardCharsets.UTF_8 can be used instead
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
  // Bytes.UTF8_CHARSET should be updated if this changed
  /** When we encode strings, we always specify UTF8 encoding */
  public static final Charset UTF8_CHARSET = Charset.forName(UTF8_ENCODING);
  /**
   * Default block size for an HFile.
```

### CharsetObjectCanBeUsed
StandardCharsets.UTF_8 can be used instead
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java`
#### Snippet
```java
   */
  static class JavaRegexEngine implements Engine {
    private Charset charset = Charset.forName("UTF-8");
    private Pattern pattern;

```

### CharsetObjectCanBeUsed
StandardCharsets.UTF_8 can be used instead
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java`
#### Snippet
```java
   */
  public static byte[] createCompositeKey(String tableName, ImmutableBytesWritable suffix) {
    return combineTableNameSuffix(tableName.getBytes(Charset.forName("UTF-8")), suffix.get());
  }

```

## RuleId[id=SystemOutErr]
### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java`
#### Snippet
```java
   */
  public static void main(String[] args) throws Exception {
    HBaseConfiguration.create().writeXml(System.out);
  }
}
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    }
    for (Algorithm algo : Algorithm.class.getEnumConstants()) {
      System.out.println(algo.name() + ":");
      System.out.println("    name: " + algo.getName());
      System.out.println("    confKey: " + algo.confKey);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    for (Algorithm algo : Algorithm.class.getEnumConstants()) {
      System.out.println(algo.name() + ":");
      System.out.println("    name: " + algo.getName());
      System.out.println("    confKey: " + algo.confKey);
      System.out.println("    confDefault: " + algo.confDefault);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      System.out.println(algo.name() + ":");
      System.out.println("    name: " + algo.getName());
      System.out.println("    confKey: " + algo.confKey);
      System.out.println("    confDefault: " + algo.confDefault);
      CompressionCodec codec = implMap.get(algo.name());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      System.out.println("    name: " + algo.getName());
      System.out.println("    confKey: " + algo.confKey);
      System.out.println("    confDefault: " + algo.confDefault);
      CompressionCodec codec = implMap.get(algo.name());
      System.out.println(
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      System.out.println("    confDefault: " + algo.confDefault);
      CompressionCodec codec = implMap.get(algo.name());
      System.out.println(
        "    implClass: " + (codec != null ? codec.getClass().getCanonicalName() : "<none>"));
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JenkinsHash.java`
#### Snippet
```java
  public static void main(String[] args) throws IOException {
    if (args.length != 1) {
      System.err.println("Usage: JenkinsHash filename");
      System.exit(-1);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JenkinsHash.java`
#### Snippet
```java
      in.close();
    }
    System.out.println(Math.abs(value));
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Random64.java`
#### Snippet
```java
    final long startTime = EnvironmentEdgeManager.currentTime();

    System.out.println("Do collision test, totalTestCnt=" + totalTestCnt);

    Random64 rand = new Random64();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Random64.java`
#### Snippet
```java

      if (longSet.contains(randLong)) {
        System.err.println("Conflict! count=" + cnt);
        System.exit(1);
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Random64.java`
#### Snippet
```java
      if (cnt % precision == 0) {
        if (!longSet.add(randLong)) {
          System.err.println("Conflict! count=" + cnt);
          System.exit(1);
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Random64.java`
#### Snippet
```java
          long cost = EnvironmentEdgeManager.currentTime() - startTime;
          long remainingMs = (long) (1.0 * (totalTestCnt - cnt) * cost / cnt);
          System.out.println(String.format("Progress: %.3f%%, remaining %d minutes",
            100.0 * cnt / totalTestCnt, remainingMs / 60000));
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Random64.java`
#### Snippet
```java
    }

    System.out.println("No collision!");
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java`
#### Snippet
```java

  public static void main(String[] args) {
    writeTo(System.out);
  }
}
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Threads.java`
#### Snippet
```java
      t.join(60 * 1000);
      if (t.isAlive()) {
        printThreadInfo(System.out,
          "Automatic Stack Trace every 60 seconds waiting on " + t.getName());
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/RefreshHFilesClient.java`
#### Snippet
```java
        + "use this tool manually sync the flushed HFiles from the source cluster.";
      message += "\nUsage: " + this.getClass().getName() + " tableName";
      System.out.println(message);
      return -1;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/util/ClientUtils.java`
#### Snippet
```java
      rowStr.append("; ");
    }
    System.out.println("row: " + utf8(rowResult.row.array()) + ", cols: " + rowStr);

  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java
      final long totalOutputRows = result.values().stream().mapToLong(v -> v.getRowCount()).sum();
      final long totalOutputCells = result.values().stream().mapToLong(v -> v.getCellCount()).sum();
      System.out.println("table:" + tableName);
      System.out.println("output:" + output);
      System.out.println("total rows:" + totalOutputRows);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java
      final long totalOutputCells = result.values().stream().mapToLong(v -> v.getCellCount()).sum();
      System.out.println("table:" + tableName);
      System.out.println("output:" + output);
      System.out.println("total rows:" + totalOutputRows);
      System.out.println("total cells:" + totalOutputCells);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java
      System.out.println("table:" + tableName);
      System.out.println("output:" + output);
      System.out.println("total rows:" + totalOutputRows);
      System.out.println("total cells:" + totalOutputCells);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java
      System.out.println("output:" + output);
      System.out.println("total rows:" + totalOutputRows);
      System.out.println("total cells:" + totalOutputCells);
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/MultiThreadedClientExample.java`
#### Snippet
```java

    if (args.length < 1 || args.length > 2) {
      System.out.println("Usage: " + this.getClass().getName() + " tableName [num_operations]");
      return -1;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/AsyncClientExample.java`
#### Snippet
```java
  public int run(String[] args) throws Exception {
    if (args.length < 1 || args.length > 2) {
      System.out.println("Usage: " + this.getClass().getName() + " tableName [num_operations]");
      return -1;
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/SampleUploader.java`
#### Snippet
```java
  public int run(String[] otherArgs) throws Exception {
    if (otherArgs.length != 2) {
      System.err.println("Wrong number of arguments: " + otherArgs.length);
      System.err.println("Usage: " + NAME + " <input> <tablename>");
      return -1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/SampleUploader.java`
#### Snippet
```java
    if (otherArgs.length != 2) {
      System.err.println("Wrong number of arguments: " + otherArgs.length);
      System.err.println("Usage: " + NAME + " <input> <tablename>");
      return -1;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
    TResult result = client.get(table, get);

    System.out.print("row = " + ClientUtils.utf8(result.getRow()));
    for (TColumnValue resultColumnValue : result.getColumnValues()) {
      System.out.print("family = " + ClientUtils.utf8(resultColumnValue.getFamily()));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
    System.out.print("row = " + ClientUtils.utf8(result.getRow()));
    for (TColumnValue resultColumnValue : result.getColumnValues()) {
      System.out.print("family = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("qualifier = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("value = " + ClientUtils.utf8(resultColumnValue.getValue()));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
    for (TColumnValue resultColumnValue : result.getColumnValues()) {
      System.out.print("family = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("qualifier = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("value = " + ClientUtils.utf8(resultColumnValue.getValue()));
      System.out.print("timestamp = " + resultColumnValue.getTimestamp());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
      System.out.print("family = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("qualifier = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("value = " + ClientUtils.utf8(resultColumnValue.getValue()));
      System.out.print("timestamp = " + resultColumnValue.getTimestamp());
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
      System.out.print("qualifier = " + ClientUtils.utf8(resultColumnValue.getFamily()));
      System.out.print("value = " + ClientUtils.utf8(resultColumnValue.getValue()));
      System.out.print("timestamp = " + resultColumnValue.getTimestamp());
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java

  public static void main(String[] args) throws Exception {
    System.out.println("Thrift2 Demo");
    System.out.println("Usage: DemoClient [host=localhost] [port=9090] [secure=false]");
    System.out.println("This demo assumes you have a table called \"example\" with a column "
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
  public static void main(String[] args) throws Exception {
    System.out.println("Thrift2 Demo");
    System.out.println("Usage: DemoClient [host=localhost] [port=9090] [secure=false]");
    System.out.println("This demo assumes you have a table called \"example\" with a column "
      + "family called \"family1\"");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
    System.out.println("Thrift2 Demo");
    System.out.println("Usage: DemoClient [host=localhost] [port=9090] [secure=false]");
    System.out.println("This demo assumes you have a table called \"example\" with a column "
      + "family called \"family1\"");

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java`
#### Snippet
```java
    Configuration conf = HBaseConfiguration.create(getConf());
    if (args.length < 3) {
      System.err.println("Only " + args.length + " arguments supplied, required: 3");
      System.err.println("Usage: IndexBuilder <TABLE_NAME> <COLUMN_FAMILY> <ATTR> [<ATTR> ...]");
      System.exit(-1);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java`
#### Snippet
```java
    if (args.length < 3) {
      System.err.println("Only " + args.length + " arguments supplied, required: 3");
      System.err.println("Usage: IndexBuilder <TABLE_NAME> <COLUMN_FAMILY> <ATTR> [<ATTR> ...]");
      System.exit(-1);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java`
#### Snippet
```java
    String tableName = args[0];
    String columnFamily = args[1];
    System.out.println("****" + tableName);
    conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(new Scan()));
    conf.set(TableInputFormat.INPUT_TABLE, tableName);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    // Scan all tables, look for the demo table and delete it.
    //
    System.out.println("scanning tables...");
    for (ByteBuffer name : refresh(client, httpClient).getTableNames()) {
      System.out.println("  found: " + ClientUtils.utf8(name));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    System.out.println("scanning tables...");
    for (ByteBuffer name : refresh(client, httpClient).getTableNames()) {
      System.out.println("  found: " + ClientUtils.utf8(name));
      if (ClientUtils.utf8(name).equals(ClientUtils.utf8(t))) {
        if (refresh(client, httpClient).isTableEnabled(name)) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
      if (ClientUtils.utf8(name).equals(ClientUtils.utf8(t))) {
        if (refresh(client, httpClient).isTableEnabled(name)) {
          System.out.println("    disabling table: " + ClientUtils.utf8(name));
          refresh(client, httpClient).disableTable(name);
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
          refresh(client, httpClient).disableTable(name);
        }
        System.out.println("    deleting table: " + ClientUtils.utf8(name));
        refresh(client, httpClient).deleteTable(name);
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    columns.add(col);

    System.out.println("creating table: " + ClientUtils.utf8(t));
    try {

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
      refresh(client, httpClient).createTable(ByteBuffer.wrap(t), columns);
    } catch (AlreadyExists ae) {
      System.out.println("WARN: " + ae.message);
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    }

    System.out.println("column families in " + ClientUtils.utf8(t) + ": ");
    Map<ByteBuffer, ColumnDescriptor> columnMap =
      refresh(client, httpClient).getColumnDescriptors(ByteBuffer.wrap(t));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
      refresh(client, httpClient).getColumnDescriptors(ByteBuffer.wrap(t));
    for (ColumnDescriptor col2 : columnMap.values()) {
      System.out
        .println("  column: " + ClientUtils.utf8(col2.name) + ", maxVer: " + col2.maxVersions);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  public static void main(String[] args) throws Exception {
    if (args.length < 3 || args.length > 6) {
      System.out.println("Invalid arguments!");
      System.out.println(
        "Usage: HttpDoAsClient host port doAsUserName [security=true] [principal] [keytab]");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    if (args.length < 3 || args.length > 6) {
      System.out.println("Invalid arguments!");
      System.out.println(
        "Usage: HttpDoAsClient host port doAsUserName [security=true] [principal] [keytab]");
      System.exit(-1);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
        keyTab = args[5];
        if (!new File(keyTab).exists()) {
          System.err.printf("ERROR: KeyTab File %s not found %n", keyTab);
          System.exit(-1);
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    outputBuffer.append("Negotiate ");
    outputBuffer.append(Bytes.toString(Base64.getEncoder().encode(outToken)));
    System.out.print("Ticket is: " + outputBuffer);
    return outputBuffer.toString();
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    }

    System.out.println("row: " + ClientUtils.utf8(row) + ", values: " + rowStr);
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

    // Scan all tables, look for the demo table and delete it.
    System.out.println("scanning tables...");

    for (ByteBuffer name : client.getTableNames()) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

    for (ByteBuffer name : client.getTableNames()) {
      System.out.println("  found: " + ClientUtils.utf8(name));

      if (name.equals(demoTable) || name.equals(disabledTable)) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      if (name.equals(demoTable) || name.equals(disabledTable)) {
        if (client.isTableEnabled(name)) {
          System.out.println("    disabling table: " + ClientUtils.utf8(name));
          client.disableTable(name);
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
        }

        System.out.println("    deleting table: " + ClientUtils.utf8(name));
        client.deleteTable(name);
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    columns.add(col);

    System.out.println("creating table: " + ClientUtils.utf8(demoTable.array()));

    try {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      client.createTable(disabledTable, columns);
    } catch (AlreadyExists ae) {
      System.out.println("WARN: " + ae.message);
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    }

    System.out.println("column families in " + ClientUtils.utf8(demoTable.array()) + ": ");
    Map<ByteBuffer, ColumnDescriptor> columnMap = client.getColumnDescriptors(demoTable);

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

    for (ColumnDescriptor col2 : columnMap.values()) {
      System.out.println(
        "  column: " + ClientUtils.utf8(col2.name.array()) + ", maxVer: " + col2.maxVersions);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

    if (client.isTableEnabled(disabledTable)) {
      System.out.println("disabling table: " + ClientUtils.utf8(disabledTable.array()));
      client.disableTable(disabledTable);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    }

    System.out.println("list tables with enabled statuses : ");
    Map<ByteBuffer, Boolean> statusMap = client.getTableNamesWithIsTableEnabled();
    for (Map.Entry<ByteBuffer, Boolean> entry : statusMap.entrySet()) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    Map<ByteBuffer, Boolean> statusMap = client.getTableNamesWithIsTableEnabled();
    for (Map.Entry<ByteBuffer, Boolean> entry : statusMap.entrySet()) {
      System.out.println(" Table: " + ClientUtils.utf8(entry.getKey().array()) + ", is enabled: "
        + entry.getValue());
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    columnNames.add(ByteBuffer.wrap(bytes("entry:")));

    System.out.println("Starting scanner...");
    int scanner =
      client.scannerOpen(demoTable, ByteBuffer.wrap(bytes("")), columnNames, dummyAttributes);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

      if (versions.isEmpty()) {
        System.out.println("FATAL: wrong # of versions");
        System.exit(-1);
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

      if (!result.isEmpty()) {
        System.out.println("FATAL: shouldn't get here");
        System.exit(-1);
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      }

      System.out.println("");
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

    for (ColumnDescriptor col2 : client.getColumnDescriptors(demoTable).values()) {
      System.out.println("column with name: " + ClientUtils.utf8(col2.name));
      System.out.println(col2.toString());

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    for (ColumnDescriptor col2 : client.getColumnDescriptors(demoTable).values()) {
      System.out.println("column with name: " + ClientUtils.utf8(col2.name));
      System.out.println(col2.toString());

      columnNames.add(col2.name);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    }

    System.out.println("Starting scanner...");
    scanner = client.scannerOpenWithStop(demoTable, ByteBuffer.wrap(bytes("00020")),
      ByteBuffer.wrap(bytes("00040")), columnNames, dummyAttributes);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

      if (entry.isEmpty()) {
        System.out.println("Scanner finished");
        break;
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
  public static void main(String[] args) throws Exception {
    if (args.length < 2 || args.length > 4 || (args.length > 2 && !isBoolean(args[2]))) {
      System.out.println("Invalid arguments!");
      System.out.println("Usage: DemoClient host port [secure=false [server-principal=hbase] ]");

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    if (args.length < 2 || args.length > 4 || (args.length > 2 && !isBoolean(args[2]))) {
      System.out.println("Invalid arguments!");
      System.out.println("Usage: DemoClient host port [secure=false [server-principal=hbase] ]");

      System.exit(-1);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java

      if (commandLine.hasOption("outputFieldNames")) {
        initialMode.getFieldInfos().forEach(f -> System.out.println(f.getField().getHeader()));
        return 0;
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
  private void printUsage(Options opts) {
    new HelpFormatter().printHelp("hbase hbtop [opts] [-D<property=value>]*", opts);
    System.out.println("");
    System.out.println(" Note: -D properties will be applied to the conf used.");
    System.out.println("  For example:");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
    new HelpFormatter().printHelp("hbase hbtop [opts] [-D<property=value>]*", opts);
    System.out.println("");
    System.out.println(" Note: -D properties will be applied to the conf used.");
    System.out.println("  For example:");
    System.out.println("   -Dhbase.client.zookeeper.quorum=<zookeeper quorum>");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
    System.out.println("");
    System.out.println(" Note: -D properties will be applied to the conf used.");
    System.out.println("  For example:");
    System.out.println("   -Dhbase.client.zookeeper.quorum=<zookeeper quorum>");
    System.out.println("   -Dzookeeper.znode.parent=<znode parent>");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
    System.out.println(" Note: -D properties will be applied to the conf used.");
    System.out.println("  For example:");
    System.out.println("   -Dhbase.client.zookeeper.quorum=<zookeeper quorum>");
    System.out.println("   -Dzookeeper.znode.parent=<znode parent>");
    System.out.println("");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
    System.out.println("  For example:");
    System.out.println("   -Dhbase.client.zookeeper.quorum=<zookeeper quorum>");
    System.out.println("   -Dzookeeper.znode.parent=<znode parent>");
    System.out.println("");
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java
    System.out.println("   -Dhbase.client.zookeeper.quorum=<zookeeper quorum>");
    System.out.println("   -Dzookeeper.znode.parent=<znode parent>");
    System.out.println("");
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/batch/BatchTerminalPrinter.java`
#### Snippet
```java
  @Override
  public void endOfLine() {
    System.out.println();
  }
}
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/batch/BatchTerminalPrinter.java`
#### Snippet
```java
  @Override
  public TerminalPrinter print(String value) {
    System.out.print(value);
    return this;
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/TerminalImpl.java`
#### Snippet
```java

  public TerminalImpl(@Nullable String title) {
    output = new PrintWriter(new OutputStreamWriter(System.out, StandardCharsets.UTF_8));
    sttyRaw();

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java

      if (md5HashBytes.length != MD5_HEX_LENGTH) {
        System.out.println(
          "MD5-hash length mismatch: Expected=" + MD5_HEX_LENGTH + "; Got=" + md5HashBytes.length);
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java`
#### Snippet
```java

  public ProcedureWALPrettyPrinter() {
    out = System.out;
  }

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java`
#### Snippet
```java
    FileSystem fs = p.getFileSystem(conf);
    if (!fs.exists(p)) {
      System.err.println("ERROR, file doesnt exist: " + p);
      return;
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALPrettyPrinter.java`
#### Snippet
```java
    }
    if (!fs.isFile(p)) {
      System.err.println(p + " is not a file");
      return;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java

  public void dump() {
    System.out.printf("%06d:%06d min=%d max=%d%n", getStart(), getEnd(), getActiveMinProcId(),
      getActiveMaxProcId());
    System.out.println("Modified:");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
    System.out.printf("%06d:%06d min=%d max=%d%n", getStart(), getEnd(), getActiveMinProcId(),
      getActiveMaxProcId());
    System.out.println("Modified:");
    for (int i = 0; i < modified.length; ++i) {
      for (int j = 0; j < BITS_PER_WORD; ++j) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
    for (int i = 0; i < modified.length; ++i) {
      for (int j = 0; j < BITS_PER_WORD; ++j) {
        System.out.print((modified[i] & (1L << j)) != 0 ? "1" : "0");
      }
      System.out.println(" " + i);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
        System.out.print((modified[i] & (1L << j)) != 0 ? "1" : "0");
      }
      System.out.println(" " + i);
    }
    System.out.println();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
      System.out.println(" " + i);
    }
    System.out.println();
    System.out.println("Delete:");
    for (int i = 0; i < deleted.length; ++i) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
    }
    System.out.println();
    System.out.println("Delete:");
    for (int i = 0; i < deleted.length; ++i) {
      for (int j = 0; j < BITS_PER_WORD; ++j) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
    for (int i = 0; i < deleted.length; ++i) {
      for (int j = 0; j < BITS_PER_WORD; ++j) {
        System.out.print((deleted[i] & (1L << j)) != 0 ? "1" : "0");
      }
      System.out.println(" " + i);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
        System.out.print((deleted[i] & (1L << j)) != 0 ? "1" : "0");
      }
      System.out.println(" " + i);
    }
    System.out.println();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
      System.out.println(" " + i);
    }
    System.out.println();
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
    Configuration conf = HBaseConfiguration.create();
    if (args == null || args.length != 1) {
      System.out.println("ERROR: Empty arguments list; pass path to MASTERPROCWALS_DIR.");
      System.out.println("Usage: WALProcedureStore MASTERPROCWALS_DIR");
      System.exit(-1);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
    if (args == null || args.length != 1) {
      System.out.println("ERROR: Empty arguments list; pass path to MASTERPROCWALS_DIR.");
      System.out.println("Usage: WALProcedureStore MASTERPROCWALS_DIR");
      System.exit(-1);
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java

  private void printUsageAndExit() {
    System.err.printf("Usage: hbase %s [options]%n", getClass().getName());
    System.err.println(" where [options] are:");
    System.err.println("  -h|-help                Show this help and exit.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
  private void printUsageAndExit() {
    System.err.printf("Usage: hbase %s [options]%n", getClass().getName());
    System.err.println(" where [options] are:");
    System.err.println("  -h|-help                Show this help and exit.");
    System.err.println("  -set-acls               Setup the hbase znode ACLs for a secure cluster");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.printf("Usage: hbase %s [options]%n", getClass().getName());
    System.err.println(" where [options] are:");
    System.err.println("  -h|-help                Show this help and exit.");
    System.err.println("  -set-acls               Setup the hbase znode ACLs for a secure cluster");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println(" where [options] are:");
    System.err.println("  -h|-help                Show this help and exit.");
    System.err.println("  -set-acls               Setup the hbase znode ACLs for a secure cluster");
    System.err.println();
    System.err.println("Examples:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println("  -h|-help                Show this help and exit.");
    System.err.println("  -set-acls               Setup the hbase znode ACLs for a secure cluster");
    System.err.println();
    System.err.println("Examples:");
    System.err.println("  To reset the ACLs to the unsecure cluster behavior:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println("  -set-acls               Setup the hbase znode ACLs for a secure cluster");
    System.err.println();
    System.err.println("Examples:");
    System.err.println("  To reset the ACLs to the unsecure cluster behavior:");
    System.err.println("  hbase " + getClass().getName());
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Examples:");
    System.err.println("  To reset the ACLs to the unsecure cluster behavior:");
    System.err.println("  hbase " + getClass().getName());
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println("Examples:");
    System.err.println("  To reset the ACLs to the unsecure cluster behavior:");
    System.err.println("  hbase " + getClass().getName());
    System.err.println();
    System.err.println("  To reset the ACLs to the secure cluster behavior:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println("  To reset the ACLs to the unsecure cluster behavior:");
    System.err.println("  hbase " + getClass().getName());
    System.err.println();
    System.err.println("  To reset the ACLs to the secure cluster behavior:");
    System.err.println("  hbase " + getClass().getName() + " -set-acls");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println("  hbase " + getClass().getName());
    System.err.println();
    System.err.println("  To reset the ACLs to the secure cluster behavior:");
    System.err.println("  hbase " + getClass().getName() + " -set-acls");
    System.exit(1);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    System.err.println();
    System.err.println("  To reset the ACLs to the secure cluster behavior:");
    System.err.println("  hbase " + getClass().getName() + " -set-acls");
    System.exit(1);
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java`
#### Snippet
```java
    for (ServerName server : readZKNodes(HBaseConfiguration.create())) {
      // bin/zookeeper.sh relies on the "ZK host" string for grepping which is case sensitive.
      System.out.println("ZK host: " + server.getHostname());
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      // Runs a 'stat' against the servers.
      if (!waitForServerUp(currentClientPort, connectionTimeout)) {
        Threads.printThreadInfo(System.out, "Why is zk standalone server not coming up?");
        throw new IOException(
          "Waiting for startup of standalone server; " + "server isRunning=" + server.isRunning());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java

  public void dump() {
    System.out.println("map " + map.size());
    System.out.println("isAllModified " + isAllModified());
    System.out.println("isEmpty " + isEmpty());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
  public void dump() {
    System.out.println("map " + map.size());
    System.out.println("isAllModified " + isAllModified());
    System.out.println("isEmpty " + isEmpty());
    for (Map.Entry<Long, BitSetNode> entry : map.entrySet()) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
    System.out.println("map " + map.size());
    System.out.println("isAllModified " + isAllModified());
    System.out.println("isEmpty " + isEmpty());
    for (Map.Entry<Long, BitSetNode> entry : map.entrySet()) {
      entry.getValue().dump();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
   */
  static void deleteRow(final Table table) throws IOException {
    System.out.println("Deleting row [" + Bytes.toString(MY_ROW_ID) + "] from Table ["
      + table.getName().getNameAsString() + "].");
    table.delete(new Delete(MY_ROW_ID));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java

    if (!namespaceExists(admin, MY_NAMESPACE_NAME)) {
      System.out.println("Creating Namespace [" + MY_NAMESPACE_NAME + "].");

      admin.createNamespace(NamespaceDescriptor.create(MY_NAMESPACE_NAME).build());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
    }
    if (!admin.tableExists(MY_TABLE_NAME)) {
      System.out.println("Creating Table [" + MY_TABLE_NAME.getNameAsString()
        + "], with one Column Family [" + Bytes.toString(MY_COLUMN_FAMILY_NAME) + "].");
      TableDescriptor desc = TableDescriptorBuilder.newBuilder(MY_TABLE_NAME)
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
  static void deleteNamespaceAndTable(final Admin admin) throws IOException {
    if (admin.tableExists(MY_TABLE_NAME)) {
      System.out.println("Disabling/deleting Table [" + MY_TABLE_NAME.getNameAsString() + "].");
      admin.disableTable(MY_TABLE_NAME); // Disable a table before deleting it.
      admin.deleteTable(MY_TABLE_NAME);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
    }
    if (namespaceExists(admin, MY_NAMESPACE_NAME)) {
      System.out.println("Deleting Namespace [" + MY_NAMESPACE_NAME + "].");
      admin.deleteNamespace(MY_NAMESPACE_NAME);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
      Admin admin = connection.getAdmin()) {
      admin.getClusterMetrics(); // assure connection successfully established
      System.out
        .println("\n*** Hello HBase! -- Connection has been " + "established via ZooKeeper!!\n");

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
      createNamespaceAndTable(admin);

      System.out.println("Getting a Table object for [" + MY_TABLE_NAME
        + "] with which to perform CRUD operations in HBase.");
      try (Table table = connection.getTable(MY_TABLE_NAME)) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
    Result row = table.get(new Get(MY_ROW_ID));

    System.out.println("Row [" + Bytes.toString(row.getRow()) + "] was retrieved from Table ["
      + table.getName().getNameAsString() + "] in HBase, with the following content:");

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
      String columnFamilyName = Bytes.toString(colFamilyEntry.getKey());

      System.out.println("  Columns in Column Family [" + columnFamilyName + "]:");

      for (Entry<byte[], byte[]> columnNameAndValueMap : colFamilyEntry.getValue().entrySet()) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
      for (Entry<byte[], byte[]> columnNameAndValueMap : colFamilyEntry.getValue().entrySet()) {

        System.out.println("    Value of Column [" + columnFamilyName + ":"
          + Bytes.toString(columnNameAndValueMap.getKey()) + "] == "
          + Bytes.toString(columnNameAndValueMap.getValue()));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
      .addColumn(MY_COLUMN_FAMILY_NAME, MY_SECOND_COLUMN_QUALIFIER, Bytes.toBytes("World!")));

    System.out.println("Row [" + Bytes.toString(MY_ROW_ID) + "] was put into Table ["
      + table.getName().getNameAsString() + "] in HBase;\n"
      + "  the row's two columns (created 'on the fly') are: ["
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/util/MapreduceDependencyClasspathTool.java`
#### Snippet
```java
  public int run(String[] args) throws Exception {
    if (args.length > 0) {
      System.err.println("Usage: hbase mapredcp [-Dtmpjars=...]");
      System.err
        .println("  Construct a CLASSPATH containing dependency jars required to run a mapreduce");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/util/MapreduceDependencyClasspathTool.java`
#### Snippet
```java
    if (args.length > 0) {
      System.err.println("Usage: hbase mapredcp [-Dtmpjars=...]");
      System.err
        .println("  Construct a CLASSPATH containing dependency jars required to run a mapreduce");
      System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/util/MapreduceDependencyClasspathTool.java`
#### Snippet
```java
      System.err
        .println("  Construct a CLASSPATH containing dependency jars required to run a mapreduce");
      System.err
        .println("  job. By default, includes any jars detected by TableMapReduceUtils. Provide");
      System.err.println("  additional entries by specifying a comma-separated list in tmpjars.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/util/MapreduceDependencyClasspathTool.java`
#### Snippet
```java
      System.err
        .println("  job. By default, includes any jars detected by TableMapReduceUtils. Provide");
      System.err.println("  additional entries by specifying a comma-separated list in tmpjars.");
      return 0;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/util/MapreduceDependencyClasspathTool.java`
#### Snippet
```java

    TableMapReduceUtil.addHBaseDependencyJars(getConf());
    System.out.println(TableMapReduceUtil.buildDependencyClasspath(getConf()));
    return 0;
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
        LOG.info("Finished creating report for '{}', family='{}'", tn, familyName);
      } else {
        System.err.println("Job was not successful");
        return 3;
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java

    } catch (ClassNotFoundException | RuntimeException | IOException | InterruptedException e) {
      System.err.println("Job aborted due to exception " + e);
      return 2; // job failed
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java

  private void printUsage() {
    System.err.println("Usage:\n" + "--------------------------\n" + MobRefReporter.class.getName()
      + " output-dir tableName familyName");
    System.err.println(" output-dir       Where to write output report.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
    System.err.println("Usage:\n" + "--------------------------\n" + MobRefReporter.class.getName()
      + " output-dir tableName familyName");
    System.err.println(" output-dir       Where to write output report.");
    System.err.println(" tableName        The table name");
    System.err.println(" familyName       The column family name");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
      + " output-dir tableName familyName");
    System.err.println(" output-dir       Where to write output report.");
    System.err.println(" tableName        The table name");
    System.err.println(" familyName       The column family name");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
    System.err.println(" output-dir       Where to write output report.");
    System.err.println(" tableName        The table name");
    System.err.println(" familyName       The column family name");
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/RowCounter.java`
#### Snippet
```java

  static int printUsage() {
    System.out.println(NAME + " <outputdir> <tablename> <column1> [<column2>...]");
    return -1;
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/RowCounter.java`
#### Snippet
```java
    // Make sure there are at least 3 parameters
    if (args.length < 3) {
      System.err.println("ERROR: Wrong number of parameters: " + args.length);
      return printUsage();
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RoundRobinTableInputFormat.java`
#### Snippet
```java
    List<InputSplit> splits = tif.getSplits(new JobContextImpl(configuration, new JobID()));
    for (InputSplit split : splits) {
      System.out.println(split);
    }
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java`
#### Snippet
```java
      | InvalidLabelException badLine) {
      if (logBadLines) {
        System.err.println(value);
      }
      System.err.println("Bad line at offset: " + offset.get() + ":\n" + badLine.getMessage());
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java`
#### Snippet
```java
        System.err.println(value);
      }
      System.err.println("Bad line at offset: " + offset.get() + ":\n" + badLine.getMessage());
      if (skipBadLines) {
        incrementBadLineCount(1);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
  public static void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: Export [-D <property=value>]* <tablename> <outputdir> [<versions> "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: Export [-D <property=value>]* <tablename> <outputdir> [<versions> "
      + "[<starttime> [<endtime>]] [^[regex pattern] or [Prefix] to filter]]\n");
    System.err.println("  Note: -D properties will be applied to the conf used. ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("Usage: Export [-D <property=value>]* <tablename> <outputdir> [<versions> "
      + "[<starttime> [<endtime>]] [^[regex pattern] or [Prefix] to filter]]\n");
    System.err.println("  Note: -D properties will be applied to the conf used. ");
    System.err.println("  For example: ");
    System.err.println("   -D " + FileOutputFormat.COMPRESS + "=true");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
      + "[<starttime> [<endtime>]] [^[regex pattern] or [Prefix] to filter]]\n");
    System.err.println("  Note: -D properties will be applied to the conf used. ");
    System.err.println("  For example: ");
    System.err.println("   -D " + FileOutputFormat.COMPRESS + "=true");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("  Note: -D properties will be applied to the conf used. ");
    System.err.println("  For example: ");
    System.err.println("   -D " + FileOutputFormat.COMPRESS + "=true");
    System.err.println(
      "   -D " + FileOutputFormat.COMPRESS_CODEC + "=org.apache.hadoop.io.compress.GzipCodec");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("  For example: ");
    System.err.println("   -D " + FileOutputFormat.COMPRESS + "=true");
    System.err.println(
      "   -D " + FileOutputFormat.COMPRESS_CODEC + "=org.apache.hadoop.io.compress.GzipCodec");
    System.err.println("   -D " + FileOutputFormat.COMPRESS_TYPE + "=BLOCK");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println(
      "   -D " + FileOutputFormat.COMPRESS_CODEC + "=org.apache.hadoop.io.compress.GzipCodec");
    System.err.println("   -D " + FileOutputFormat.COMPRESS_TYPE + "=BLOCK");
    System.err.println("  Additionally, the following SCAN properties can be specified");
    System.err.println("  to control/limit what is exported..");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
      "   -D " + FileOutputFormat.COMPRESS_CODEC + "=org.apache.hadoop.io.compress.GzipCodec");
    System.err.println("   -D " + FileOutputFormat.COMPRESS_TYPE + "=BLOCK");
    System.err.println("  Additionally, the following SCAN properties can be specified");
    System.err.println("  to control/limit what is exported..");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("   -D " + FileOutputFormat.COMPRESS_TYPE + "=BLOCK");
    System.err.println("  Additionally, the following SCAN properties can be specified");
    System.err.println("  to control/limit what is exported..");
    System.err
      .println("   -D " + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("  Additionally, the following SCAN properties can be specified");
    System.err.println("  to control/limit what is exported..");
    System.err
      .println("   -D " + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D " + RAW_SCAN + "=true");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err
      .println("   -D " + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D " + RAW_SCAN + "=true");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_START + "=<ROWSTART>");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_STOP + "=<ROWSTOP>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
      .println("   -D " + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D " + RAW_SCAN + "=true");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_START + "=<ROWSTART>");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_STOP + "=<ROWSTOP>");
    System.err.println("   -D " + HConstants.HBASE_CLIENT_SCANNER_CACHING + "=100");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("   -D " + RAW_SCAN + "=true");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_START + "=<ROWSTART>");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_STOP + "=<ROWSTOP>");
    System.err.println("   -D " + HConstants.HBASE_CLIENT_SCANNER_CACHING + "=100");
    System.err.println("   -D " + EXPORT_VISIBILITY_LABELS + "=<labels>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_START + "=<ROWSTART>");
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_STOP + "=<ROWSTOP>");
    System.err.println("   -D " + HConstants.HBASE_CLIENT_SCANNER_CACHING + "=100");
    System.err.println("   -D " + EXPORT_VISIBILITY_LABELS + "=<labels>");
    System.err.println("For tables with very wide rows consider setting the batch size as below:\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("   -D " + TableInputFormat.SCAN_ROW_STOP + "=<ROWSTOP>");
    System.err.println("   -D " + HConstants.HBASE_CLIENT_SCANNER_CACHING + "=100");
    System.err.println("   -D " + EXPORT_VISIBILITY_LABELS + "=<labels>");
    System.err.println("For tables with very wide rows consider setting the batch size as below:\n"
      + "   -D " + EXPORT_BATCHING + "=10\n" + "   -D " + EXPORT_CACHING + "=100");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    System.err.println("   -D " + HConstants.HBASE_CLIENT_SCANNER_CACHING + "=100");
    System.err.println("   -D " + EXPORT_VISIBILITY_LABELS + "=<labels>");
    System.err.println("For tables with very wide rows consider setting the batch size as below:\n"
      + "   -D " + EXPORT_BATCHING + "=10\n" + "   -D " + EXPORT_CACHING + "=100");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java`
#### Snippet
```java
    if (!ExportUtils.isValidArguements(args)) {
      ExportUtils.usage("Wrong number of arguments: " + ArrayUtils.getLength(args));
      System.err.println("   -D " + JOB_NAME_CONF_KEY
        + "=jobName - use the specified mapreduce job name for the export");
      System.err.println("For MR performance consider the following properties:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java`
#### Snippet
```java
      System.err.println("   -D " + JOB_NAME_CONF_KEY
        + "=jobName - use the specified mapreduce job name for the export");
      System.err.println("For MR performance consider the following properties:");
      System.err.println("   -D mapreduce.map.speculative=false");
      System.err.println("   -D mapreduce.reduce.speculative=false");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java`
#### Snippet
```java
        + "=jobName - use the specified mapreduce job name for the export");
      System.err.println("For MR performance consider the following properties:");
      System.err.println("   -D mapreduce.map.speculative=false");
      System.err.println("   -D mapreduce.reduce.speculative=false");
      return -1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java`
#### Snippet
```java
      System.err.println("For MR performance consider the following properties:");
      System.err.println("   -D mapreduce.map.speculative=false");
      System.err.println("   -D mapreduce.reduce.speculative=false");
      return -1;
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java

  private void printUsage(int parameterCount) {
    System.err.println("ERROR: Wrong number of parameters: " + parameterCount);
    System.err.println("Usage: hbase cellcounter <tablename> <outputDir> [reportSeparator] "
      + "[^[regex pattern] or [Prefix]] [--starttime=<starttime> --endtime=<endtime>]");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
  private void printUsage(int parameterCount) {
    System.err.println("ERROR: Wrong number of parameters: " + parameterCount);
    System.err.println("Usage: hbase cellcounter <tablename> <outputDir> [reportSeparator] "
      + "[^[regex pattern] or [Prefix]] [--starttime=<starttime> --endtime=<endtime>]");
    System.err.println("  Note: -D properties will be applied to the conf used.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("Usage: hbase cellcounter <tablename> <outputDir> [reportSeparator] "
      + "[^[regex pattern] or [Prefix]] [--starttime=<starttime> --endtime=<endtime>]");
    System.err.println("  Note: -D properties will be applied to the conf used.");
    System.err.println("  Additionally, all of the SCAN properties from TableInputFormat can be "
      + "specified to get fine grained control on what is counted.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
      + "[^[regex pattern] or [Prefix]] [--starttime=<starttime> --endtime=<endtime>]");
    System.err.println("  Note: -D properties will be applied to the conf used.");
    System.err.println("  Additionally, all of the SCAN properties from TableInputFormat can be "
      + "specified to get fine grained control on what is counted.");
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_START + "=<rowkey>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("  Additionally, all of the SCAN properties from TableInputFormat can be "
      + "specified to get fine grained control on what is counted.");
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_START + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_STOP + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMNS + "=\"<col1> <col2>...\"");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
      + "specified to get fine grained control on what is counted.");
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_START + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_STOP + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMNS + "=\"<col1> <col2>...\"");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_START + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_STOP + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMNS + "=\"<col1> <col2>...\"");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMESTAMP + "=<timestamp>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_ROW_STOP + "=<rowkey>");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMNS + "=\"<col1> <col2>...\"");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMESTAMP + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_START + "=<timestamp>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMNS + "=\"<col1> <col2>...\"");
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMESTAMP + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_START + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_END + "=<timestamp>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_COLUMN_FAMILY + "=<family1>,<family2>, ...");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMESTAMP + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_START + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_END + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_MAXVERSIONS + "=<count>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_TIMESTAMP + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_START + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_END + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_MAXVERSIONS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_CACHEDROWS + "=<count>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_START + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_END + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_MAXVERSIONS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_CACHEDROWS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_BATCHSIZE + "=<count>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_TIMERANGE_END + "=<timestamp>");
    System.err.println("   -D" + TableInputFormat.SCAN_MAXVERSIONS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_CACHEDROWS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_BATCHSIZE + "=<count>");
    System.err.println(" <reportSeparator> parameter can be used to override the default report "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_MAXVERSIONS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_CACHEDROWS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_BATCHSIZE + "=<count>");
    System.err.println(" <reportSeparator> parameter can be used to override the default report "
      + "separator string : used to separate the rowId/column family name and qualifier name.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println("   -D" + TableInputFormat.SCAN_CACHEDROWS + "=<count>");
    System.err.println("   -D" + TableInputFormat.SCAN_BATCHSIZE + "=<count>");
    System.err.println(" <reportSeparator> parameter can be used to override the default report "
      + "separator string : used to separate the rowId/column family name and qualifier name.");
    System.err.println(" [^[regex pattern] or [Prefix] parameter can be used to limit the cell "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    System.err.println(" <reportSeparator> parameter can be used to override the default report "
      + "separator string : used to separate the rowId/column family name and qualifier name.");
    System.err.println(" [^[regex pattern] or [Prefix] parameter can be used to limit the cell "
      + "counter count operation to a limited subset of rows from the table based on regex or "
      + "prefix pattern.");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java

    for (int i = 1; i < args.length; i++) {
      System.out.println("i:" + i + "arg[i]" + args[i]);
      if (args[i].startsWith(startTimeArgKey)) {
        startTime = Long.parseLong(args[i].substring(startTimeArgKey.length()));
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java`
#### Snippet
```java
          | InvalidLabelException badLine) {
          if (skipBadLines) {
            System.err.println("Bad line." + badLine.getMessage());
            incrementBadLineCount(1);
            continue;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  private void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: " + NAME + " [options] <WAL inputdir> [<tables> <tableMappings>]");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: " + NAME + " [options] <WAL inputdir> [<tables> <tableMappings>]");
    System.err.println(" <WAL inputdir>   directory of WALs to replay.");
    System.err.println(" <tables>         comma separated list of tables. If no tables specified,");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    }
    System.err.println("Usage: " + NAME + " [options] <WAL inputdir> [<tables> <tableMappings>]");
    System.err.println(" <WAL inputdir>   directory of WALs to replay.");
    System.err.println(" <tables>         comma separated list of tables. If no tables specified,");
    System.err.println("                  all are imported (even hbase:meta if present).");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println("Usage: " + NAME + " [options] <WAL inputdir> [<tables> <tableMappings>]");
    System.err.println(" <WAL inputdir>   directory of WALs to replay.");
    System.err.println(" <tables>         comma separated list of tables. If no tables specified,");
    System.err.println("                  all are imported (even hbase:meta if present).");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" <WAL inputdir>   directory of WALs to replay.");
    System.err.println(" <tables>         comma separated list of tables. If no tables specified,");
    System.err.println("                  all are imported (even hbase:meta if present).");
    System.err.println(
      " <tableMappings>  WAL entries can be mapped to a new set of tables by " + "passing");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" <tables>         comma separated list of tables. If no tables specified,");
    System.err.println("                  all are imported (even hbase:meta if present).");
    System.err.println(
      " <tableMappings>  WAL entries can be mapped to a new set of tables by " + "passing");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(
      " <tableMappings>  WAL entries can be mapped to a new set of tables by " + "passing");
    System.err
      .println("                  <tableMappings>, a comma separated list of target " + "tables.");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err
      .println("                  <tableMappings>, a comma separated list of target " + "tables.");
    System.err
      .println("                  If specified, each table in <tables> must have a " + "mapping.");
    System.err.println("To generate HFiles to bulk load instead of loading HBase directly, pass:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err
      .println("                  If specified, each table in <tables> must have a " + "mapping.");
    System.err.println("To generate HFiles to bulk load instead of loading HBase directly, pass:");
    System.err.println(" -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println(" Only one table can be specified, and no mapping allowed!");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      .println("                  If specified, each table in <tables> must have a " + "mapping.");
    System.err.println("To generate HFiles to bulk load instead of loading HBase directly, pass:");
    System.err.println(" -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println(" Only one table can be specified, and no mapping allowed!");
    System.err.println("To specify a time range, pass:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println("To generate HFiles to bulk load instead of loading HBase directly, pass:");
    System.err.println(" -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println(" Only one table can be specified, and no mapping allowed!");
    System.err.println("To specify a time range, pass:");
    System.err.println(" -D" + WALInputFormat.START_TIME_KEY + "=[date|ms]");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println(" Only one table can be specified, and no mapping allowed!");
    System.err.println("To specify a time range, pass:");
    System.err.println(" -D" + WALInputFormat.START_TIME_KEY + "=[date|ms]");
    System.err.println(" -D" + WALInputFormat.END_TIME_KEY + "=[date|ms]");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" Only one table can be specified, and no mapping allowed!");
    System.err.println("To specify a time range, pass:");
    System.err.println(" -D" + WALInputFormat.START_TIME_KEY + "=[date|ms]");
    System.err.println(" -D" + WALInputFormat.END_TIME_KEY + "=[date|ms]");
    System.err.println(" The start and the end date of timerange (inclusive). The dates can be");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println("To specify a time range, pass:");
    System.err.println(" -D" + WALInputFormat.START_TIME_KEY + "=[date|ms]");
    System.err.println(" -D" + WALInputFormat.END_TIME_KEY + "=[date|ms]");
    System.err.println(" The start and the end date of timerange (inclusive). The dates can be");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" -D" + WALInputFormat.START_TIME_KEY + "=[date|ms]");
    System.err.println(" -D" + WALInputFormat.END_TIME_KEY + "=[date|ms]");
    System.err.println(" The start and the end date of timerange (inclusive). The dates can be");
    System.err
      .println(" expressed in milliseconds-since-epoch or yyyy-MM-dd'T'HH:mm:ss.SS " + "format.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" -D" + WALInputFormat.END_TIME_KEY + "=[date|ms]");
    System.err.println(" The start and the end date of timerange (inclusive). The dates can be");
    System.err
      .println(" expressed in milliseconds-since-epoch or yyyy-MM-dd'T'HH:mm:ss.SS " + "format.");
    System.err.println(" E.g. 1234567890120 or 2009-02-13T23:32:30.12");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err
      .println(" expressed in milliseconds-since-epoch or yyyy-MM-dd'T'HH:mm:ss.SS " + "format.");
    System.err.println(" E.g. 1234567890120 or 2009-02-13T23:32:30.12");
    System.err.println("Other options:");
    System.err.println(" -D" + JOB_NAME_CONF_KEY + "=jobName");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      .println(" expressed in milliseconds-since-epoch or yyyy-MM-dd'T'HH:mm:ss.SS " + "format.");
    System.err.println(" E.g. 1234567890120 or 2009-02-13T23:32:30.12");
    System.err.println("Other options:");
    System.err.println(" -D" + JOB_NAME_CONF_KEY + "=jobName");
    System.err.println(" Use the specified mapreduce job name for the wal player");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" E.g. 1234567890120 or 2009-02-13T23:32:30.12");
    System.err.println("Other options:");
    System.err.println(" -D" + JOB_NAME_CONF_KEY + "=jobName");
    System.err.println(" Use the specified mapreduce job name for the wal player");
    System.err.println(" -Dwal.input.separator=' '");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println("Other options:");
    System.err.println(" -D" + JOB_NAME_CONF_KEY + "=jobName");
    System.err.println(" Use the specified mapreduce job name for the wal player");
    System.err.println(" -Dwal.input.separator=' '");
    System.err.println(" Change WAL filename separator (WAL dir names use default ','.)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" -D" + JOB_NAME_CONF_KEY + "=jobName");
    System.err.println(" Use the specified mapreduce job name for the wal player");
    System.err.println(" -Dwal.input.separator=' '");
    System.err.println(" Change WAL filename separator (WAL dir names use default ','.)");
    System.err.println("For performance also consider the following options:\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" Use the specified mapreduce job name for the wal player");
    System.err.println(" -Dwal.input.separator=' '");
    System.err.println(" Change WAL filename separator (WAL dir names use default ','.)");
    System.err.println("For performance also consider the following options:\n"
      + "  -Dmapreduce.map.speculative=false\n" + "  -Dmapreduce.reduce.speculative=false");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    System.err.println(" -Dwal.input.separator=' '");
    System.err.println(" Change WAL filename separator (WAL dir names use default ','.)");
    System.err.println("For performance also consider the following options:\n"
      + "  -Dmapreduce.map.speculative=false\n" + "  -Dmapreduce.reduce.speculative=false");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] "
      + "[--new.name=NEW] [--peer.adr=ADR] <tablename | snapshotName>");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] "
      + "[--new.name=NEW] [--peer.adr=ADR] <tablename | snapshotName>");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" rs.class     hbase.regionserver.class of the peer cluster");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      + "[--new.name=NEW] [--peer.adr=ADR] <tablename | snapshotName>");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" rs.class     hbase.regionserver.class of the peer cluster");
    System.err.println("              specify if different from current cluster");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Options:");
    System.err.println(" rs.class     hbase.regionserver.class of the peer cluster");
    System.err.println("              specify if different from current cluster");
    System.err.println(" rs.impl      hbase.regionserver.impl of the peer cluster");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("Options:");
    System.err.println(" rs.class     hbase.regionserver.class of the peer cluster");
    System.err.println("              specify if different from current cluster");
    System.err.println(" rs.impl      hbase.regionserver.impl of the peer cluster");
    System.err.println(" startrow     the start row");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" rs.class     hbase.regionserver.class of the peer cluster");
    System.err.println("              specify if different from current cluster");
    System.err.println(" rs.impl      hbase.regionserver.impl of the peer cluster");
    System.err.println(" startrow     the start row");
    System.err.println(" stoprow      the stop row");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("              specify if different from current cluster");
    System.err.println(" rs.impl      hbase.regionserver.impl of the peer cluster");
    System.err.println(" startrow     the start row");
    System.err.println(" stoprow      the stop row");
    System.err.println(" starttime    beginning of the time range (unixtime in millis)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" rs.impl      hbase.regionserver.impl of the peer cluster");
    System.err.println(" startrow     the start row");
    System.err.println(" stoprow      the stop row");
    System.err.println(" starttime    beginning of the time range (unixtime in millis)");
    System.err.println("              without endtime means from starttime to forever");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" startrow     the start row");
    System.err.println(" stoprow      the stop row");
    System.err.println(" starttime    beginning of the time range (unixtime in millis)");
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range.  Ignored if no starttime specified.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" stoprow      the stop row");
    System.err.println(" starttime    beginning of the time range (unixtime in millis)");
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range.  Ignored if no starttime specified.");
    System.err.println(" versions     number of cell versions to copy");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" starttime    beginning of the time range (unixtime in millis)");
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range.  Ignored if no starttime specified.");
    System.err.println(" versions     number of cell versions to copy");
    System.err.println(" new.name     new table's name");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range.  Ignored if no starttime specified.");
    System.err.println(" versions     number of cell versions to copy");
    System.err.println(" new.name     new table's name");
    System.err.println(" peer.adr     Address of the peer cluster given in the format");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" endtime      end of the time range.  Ignored if no starttime specified.");
    System.err.println(" versions     number of cell versions to copy");
    System.err.println(" new.name     new table's name");
    System.err.println(" peer.adr     Address of the peer cluster given in the format");
    System.err.println("              hbase.zookeeper.quorum:hbase.zookeeper.client"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" versions     number of cell versions to copy");
    System.err.println(" new.name     new table's name");
    System.err.println(" peer.adr     Address of the peer cluster given in the format");
    System.err.println("              hbase.zookeeper.quorum:hbase.zookeeper.client"
      + ".port:zookeeper.znode.parent");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" new.name     new table's name");
    System.err.println(" peer.adr     Address of the peer cluster given in the format");
    System.err.println("              hbase.zookeeper.quorum:hbase.zookeeper.client"
      + ".port:zookeeper.znode.parent");
    System.err.println(" families     comma-separated list of families to copy");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("              hbase.zookeeper.quorum:hbase.zookeeper.client"
      + ".port:zookeeper.znode.parent");
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println("              To copy from cf1 to cf2, give sourceCfName:destCfName. ");
    System.err.println("              To keep the same name, just give \"cfName\"");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      + ".port:zookeeper.znode.parent");
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println("              To copy from cf1 to cf2, give sourceCfName:destCfName. ");
    System.err.println("              To keep the same name, just give \"cfName\"");
    System.err.println(" all.cells    also copy delete markers and deleted cells");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println("              To copy from cf1 to cf2, give sourceCfName:destCfName. ");
    System.err.println("              To keep the same name, just give \"cfName\"");
    System.err.println(" all.cells    also copy delete markers and deleted cells");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("              To copy from cf1 to cf2, give sourceCfName:destCfName. ");
    System.err.println("              To keep the same name, just give \"cfName\"");
    System.err.println(" all.cells    also copy delete markers and deleted cells");
    System.err
      .println(" bulkload     Write input into HFiles and bulk load to the destination " + "table");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("              To keep the same name, just give \"cfName\"");
    System.err.println(" all.cells    also copy delete markers and deleted cells");
    System.err
      .println(" bulkload     Write input into HFiles and bulk load to the destination " + "table");
    System.err.println(" snapshot     Copy the data from snapshot to destination table.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err
      .println(" bulkload     Write input into HFiles and bulk load to the destination " + "table");
    System.err.println(" snapshot     Copy the data from snapshot to destination table.");
    System.err.println();
    System.err.println("Args:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      .println(" bulkload     Write input into HFiles and bulk load to the destination " + "table");
    System.err.println(" snapshot     Copy the data from snapshot to destination table.");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" tablename    Name of the table to copy");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" snapshot     Copy the data from snapshot to destination table.");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" tablename    Name of the table to copy");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Args:");
    System.err.println(" tablename    Name of the table to copy");
    System.err.println();
    System.err.println("Examples:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println("Args:");
    System.err.println(" tablename    Name of the table to copy");
    System.err.println();
    System.err.println("Examples:");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" tablename    Name of the table to copy");
    System.err.println();
    System.err.println("Examples:");
    System.err
      .println(" To copy 'TestTable' to a cluster that uses replication for a 1 hour window:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Examples:");
    System.err
      .println(" To copy 'TestTable' to a cluster that uses replication for a 1 hour window:");
    System.err.println(" $ hbase "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err
      .println(" To copy 'TestTable' to a cluster that uses replication for a 1 hour window:");
    System.err.println(" $ hbase "
      + "org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 "
      + "--peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      + "org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 "
      + "--peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable ");
    System.err.println(" To copy data from 'sourceTableSnapshot' to 'destTable': ");
    System.err.println(" $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable "
      + "--snapshot --new.name=destTable sourceTableSnapshot");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      + "--peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable ");
    System.err.println(" To copy data from 'sourceTableSnapshot' to 'destTable': ");
    System.err.println(" $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable "
      + "--snapshot --new.name=destTable sourceTableSnapshot");
    System.err.println(" To copy data from 'sourceTableSnapshot' and bulk load to 'destTable': ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable "
      + "--snapshot --new.name=destTable sourceTableSnapshot");
    System.err.println(" To copy data from 'sourceTableSnapshot' and bulk load to 'destTable': ");
    System.err.println(" $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable "
      + "--new.name=destTable --snapshot --bulkload sourceTableSnapshot");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      + "--snapshot --new.name=destTable sourceTableSnapshot");
    System.err.println(" To copy data from 'sourceTableSnapshot' and bulk load to 'destTable': ");
    System.err.println(" $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable "
      + "--new.name=destTable --snapshot --bulkload sourceTableSnapshot");
    System.err.println("For performance consider the following general option:\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
    System.err.println(" $ hbase org.apache.hadoop.hbase.mapreduce.CopyTable "
      + "--new.name=destTable --snapshot --bulkload sourceTableSnapshot");
    System.err.println("For performance consider the following general option:\n"
      + "  It is recommended that you set the following to >=100. A higher value uses more memory but\n"
      + "  decreases the round trip time to the server and may increase performance.\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
   */
  private static void printUsage(String errorMessage) {
    System.err.println("ERROR: " + errorMessage);
    System.err.println(
      "Usage: hbase rowcounter [options] <tablename> " + "[--starttime=<start> --endtime=<end>] "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
  private static void printUsage(String errorMessage) {
    System.err.println("ERROR: " + errorMessage);
    System.err.println(
      "Usage: hbase rowcounter [options] <tablename> " + "[--starttime=<start> --endtime=<end>] "
        + "[--range=[startKey],[endKey][;[startKey],[endKey]...]] [<column1> <column2>...]");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
      "Usage: hbase rowcounter [options] <tablename> " + "[--starttime=<start> --endtime=<end>] "
        + "[--range=[startKey],[endKey][;[startKey],[endKey]...]] [<column1> <column2>...]");
    System.err.println("For performance consider the following options:\n"
      + "-Dhbase.client.scanner.caching=100\n" + "-Dmapreduce.map.speculative=false");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  private static void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    String usage = "Usage: " + NAME + " -D" + COLUMNS_CONF_KEY + "=a,b,c <tablename> <inputdir>\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
      + "  -Dmapreduce.map.speculative=false\n" + "  -Dmapreduce.reduce.speculative=false";

    System.err.println(usage);
  }

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
      System.err.println();
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
      System.err.println();
    }
    System.err.println("Usage: HashTable [options] <tablename> <outputpath>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
      System.err.println();
    }
    System.err.println("Usage: HashTable [options] <tablename> <outputpath>");
    System.err.println();
    System.err.println("Options:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    }
    System.err.println("Usage: HashTable [options] <tablename> <outputpath>");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" batchsize         the target amount of bytes to hash in each batch");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("Usage: HashTable [options] <tablename> <outputpath>");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" batchsize         the target amount of bytes to hash in each batch");
    System.err.println("                   rows are added to the batch until this size is reached");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Options:");
    System.err.println(" batchsize         the target amount of bytes to hash in each batch");
    System.err.println("                   rows are added to the batch until this size is reached");
    System.err.println("                   (defaults to " + DEFAULT_BATCH_SIZE + " bytes)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("Options:");
    System.err.println(" batchsize         the target amount of bytes to hash in each batch");
    System.err.println("                   rows are added to the batch until this size is reached");
    System.err.println("                   (defaults to " + DEFAULT_BATCH_SIZE + " bytes)");
    System.err.println(" numhashfiles      the number of hash files to create");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" batchsize         the target amount of bytes to hash in each batch");
    System.err.println("                   rows are added to the batch until this size is reached");
    System.err.println("                   (defaults to " + DEFAULT_BATCH_SIZE + " bytes)");
    System.err.println(" numhashfiles      the number of hash files to create");
    System.err.println("                   if set to fewer than number of regions then");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   rows are added to the batch until this size is reached");
    System.err.println("                   (defaults to " + DEFAULT_BATCH_SIZE + " bytes)");
    System.err.println(" numhashfiles      the number of hash files to create");
    System.err.println("                   if set to fewer than number of regions then");
    System.err.println("                   the job will create this number of reducers");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   (defaults to " + DEFAULT_BATCH_SIZE + " bytes)");
    System.err.println(" numhashfiles      the number of hash files to create");
    System.err.println("                   if set to fewer than number of regions then");
    System.err.println("                   the job will create this number of reducers");
    System.err.println("                   (defaults to 1/100 of regions -- at least 1)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" numhashfiles      the number of hash files to create");
    System.err.println("                   if set to fewer than number of regions then");
    System.err.println("                   the job will create this number of reducers");
    System.err.println("                   (defaults to 1/100 of regions -- at least 1)");
    System.err.println(" startrow          the start row");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   if set to fewer than number of regions then");
    System.err.println("                   the job will create this number of reducers");
    System.err.println("                   (defaults to 1/100 of regions -- at least 1)");
    System.err.println(" startrow          the start row");
    System.err.println(" stoprow           the stop row");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   the job will create this number of reducers");
    System.err.println("                   (defaults to 1/100 of regions -- at least 1)");
    System.err.println(" startrow          the start row");
    System.err.println(" stoprow           the stop row");
    System.err.println(" starttime         beginning of the time range (unixtime in millis)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   (defaults to 1/100 of regions -- at least 1)");
    System.err.println(" startrow          the start row");
    System.err.println(" stoprow           the stop row");
    System.err.println(" starttime         beginning of the time range (unixtime in millis)");
    System.err.println("                   without endtime means from starttime to forever");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" startrow          the start row");
    System.err.println(" stoprow           the stop row");
    System.err.println(" starttime         beginning of the time range (unixtime in millis)");
    System.err.println("                   without endtime means from starttime to forever");
    System.err.println(" endtime           end of the time range.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" stoprow           the stop row");
    System.err.println(" starttime         beginning of the time range (unixtime in millis)");
    System.err.println("                   without endtime means from starttime to forever");
    System.err.println(" endtime           end of the time range.");
    System.err.println("                   Ignored if no starttime specified.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" starttime         beginning of the time range (unixtime in millis)");
    System.err.println("                   without endtime means from starttime to forever");
    System.err.println(" endtime           end of the time range.");
    System.err.println("                   Ignored if no starttime specified.");
    System.err.println(" scanbatch         scanner batch size to support intra row scans");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   without endtime means from starttime to forever");
    System.err.println(" endtime           end of the time range.");
    System.err.println("                   Ignored if no starttime specified.");
    System.err.println(" scanbatch         scanner batch size to support intra row scans");
    System.err.println(" versions          number of cell versions to include");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" endtime           end of the time range.");
    System.err.println("                   Ignored if no starttime specified.");
    System.err.println(" scanbatch         scanner batch size to support intra row scans");
    System.err.println(" versions          number of cell versions to include");
    System.err.println(" rawScan           performs a raw scan (false if omitted)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   Ignored if no starttime specified.");
    System.err.println(" scanbatch         scanner batch size to support intra row scans");
    System.err.println(" versions          number of cell versions to include");
    System.err.println(" rawScan           performs a raw scan (false if omitted)");
    System.err.println(" families          comma-separated list of families to include");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" scanbatch         scanner batch size to support intra row scans");
    System.err.println(" versions          number of cell versions to include");
    System.err.println(" rawScan           performs a raw scan (false if omitted)");
    System.err.println(" families          comma-separated list of families to include");
    System.err.println(" ignoreTimestamps  if true, ignores cell timestamps");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" versions          number of cell versions to include");
    System.err.println(" rawScan           performs a raw scan (false if omitted)");
    System.err.println(" families          comma-separated list of families to include");
    System.err.println(" ignoreTimestamps  if true, ignores cell timestamps");
    System.err.println("                   when calculating hashes");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" rawScan           performs a raw scan (false if omitted)");
    System.err.println(" families          comma-separated list of families to include");
    System.err.println(" ignoreTimestamps  if true, ignores cell timestamps");
    System.err.println("                   when calculating hashes");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" families          comma-separated list of families to include");
    System.err.println(" ignoreTimestamps  if true, ignores cell timestamps");
    System.err.println("                   when calculating hashes");
    System.err.println();
    System.err.println("Args:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" ignoreTimestamps  if true, ignores cell timestamps");
    System.err.println("                   when calculating hashes");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" tablename     Name of the table to hash");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("                   when calculating hashes");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" tablename     Name of the table to hash");
    System.err.println(" outputpath    Filesystem path to put the output data");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Args:");
    System.err.println(" tablename     Name of the table to hash");
    System.err.println(" outputpath    Filesystem path to put the output data");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("Args:");
    System.err.println(" tablename     Name of the table to hash");
    System.err.println(" outputpath    Filesystem path to put the output data");
    System.err.println();
    System.err.println("Examples:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" tablename     Name of the table to hash");
    System.err.println(" outputpath    Filesystem path to put the output data");
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" To hash 'TestTable' in 32kB batches for a 1 hour window into 50 files:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println(" outputpath    Filesystem path to put the output data");
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" To hash 'TestTable' in 32kB batches for a 1 hour window into 50 files:");
    System.err.println(" $ hbase "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" To hash 'TestTable' in 32kB batches for a 1 hour window into 50 files:");
    System.err.println(" $ hbase "
      + "org.apache.hadoop.hbase.mapreduce.HashTable --batchsize=32000 --numhashfiles=50"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    System.err.println("Examples:");
    System.err.println(" To hash 'TestTable' in 32kB batches for a 1 hour window into 50 files:");
    System.err.println(" $ hbase "
      + "org.apache.hadoop.hbase.mapreduce.HashTable --batchsize=32000 --numhashfiles=50"
      + " --starttime=1265875194289 --endtime=1265878794289 --families=cf2,cf3"
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  protected void printUsage() {
    super.printUsage();
    System.out.println("\n" + "Examples:\n" + "  hbase snapshot export \\\n"
      + "    --snapshot MySnapshot --copy-to hdfs://srv2:8082/hbase \\\n"
      + "    --chuser MyUser --chgroup MyGroup --chmod 700 --mappers 16\n" + "\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    // Check user options
    if (snapshotName == null) {
      System.err.println("Snapshot name not provided.");
      LOG.error("Use -h or --help for usage instructions.");
      return 0;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java

    if (outputRoot == null) {
      System.err
        .println("Destination file-system (--" + Options.COPY_TO.getLongOpt() + ") not provided.");
      LOG.error("Use -h or --help for usage instructions.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      if (overwrite) {
        if (!outputFs.delete(outputSnapshotDir, true)) {
          System.err.println("Unable to remove existing snapshot directory: " + outputSnapshotDir);
          return 1;
        }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
        }
      } else {
        System.err.println("The snapshot '" + targetName + "' already exists in the destination: "
          + outputSnapshotDir);
        return 1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
        if (overwrite) {
          if (!outputFs.delete(snapshotTmpDir, true)) {
            System.err
              .println("Unable to remove existing snapshot tmp directory: " + snapshotTmpDir);
            return 1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
          }
        } else {
          System.err
            .println("A snapshot with the same name '" + targetName + "' may be in-progress");
          System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
          System.err
            .println("A snapshot with the same name '" + targetName + "' may be in-progress");
          System.err
            .println("Please check " + snapshotTmpDir + ". If the snapshot has completed, ");
          System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
          System.err
            .println("Please check " + snapshotTmpDir + ". If the snapshot has completed, ");
          System.err
            .println("consider removing " + snapshotTmpDir + " by using the -overwrite option");
          return 1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterTextMapper.java`
#### Snippet
```java
    } catch (ImportTsv.TsvParser.BadTsvLineException | IllegalArgumentException badLine) {
      if (logBadLines) {
        System.err.println(value);
      }
      System.err.println("Bad line at offset: " + offset.get() + ":\n" + badLine.getMessage());
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterTextMapper.java`
#### Snippet
```java
        System.err.println(value);
      }
      System.err.println("Bad line at offset: " + offset.get() + ":\n" + badLine.getMessage());
      if (skipBadLines) {
        incrementBadLineCount(1);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
      System.err.println();
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
      System.err.println();
    }
    System.err.println("Usage: SyncTable [options] <sourcehashdir> <sourcetable> <targettable>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
      System.err.println();
    }
    System.err.println("Usage: SyncTable [options] <sourcehashdir> <sourcetable> <targettable>");
    System.err.println();
    System.err.println("Options:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    }
    System.err.println("Usage: SyncTable [options] <sourcehashdir> <sourcetable> <targettable>");
    System.err.println();
    System.err.println("Options:");

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("Usage: SyncTable [options] <sourcehashdir> <sourcetable> <targettable>");
    System.err.println();
    System.err.println("Options:");

    System.err.println(" sourcezkcluster  ZK cluster key of the source table");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("Options:");

    System.err.println(" sourcezkcluster  ZK cluster key of the source table");
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" targetzkcluster  ZK cluster key of the target table");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java

    System.err.println(" sourcezkcluster  ZK cluster key of the source table");
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" targetzkcluster  ZK cluster key of the target table");
    System.err.println("                  (defaults to cluster in classpath's config)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" sourcezkcluster  ZK cluster key of the source table");
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" targetzkcluster  ZK cluster key of the target table");
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" dryrun           if true, output counters but no writes");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" targetzkcluster  ZK cluster key of the target table");
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" dryrun           if true, output counters but no writes");
    System.err.println("                  (defaults to false)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" targetzkcluster  ZK cluster key of the target table");
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" dryrun           if true, output counters but no writes");
    System.err.println("                  (defaults to false)");
    System.err.println(" doDeletes        if false, does not perform deletes");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (defaults to cluster in classpath's config)");
    System.err.println(" dryrun           if true, output counters but no writes");
    System.err.println("                  (defaults to false)");
    System.err.println(" doDeletes        if false, does not perform deletes");
    System.err.println("                  (defaults to true)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" dryrun           if true, output counters but no writes");
    System.err.println("                  (defaults to false)");
    System.err.println(" doDeletes        if false, does not perform deletes");
    System.err.println("                  (defaults to true)");
    System.err.println(" doPuts           if false, does not perform puts");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (defaults to false)");
    System.err.println(" doDeletes        if false, does not perform deletes");
    System.err.println("                  (defaults to true)");
    System.err.println(" doPuts           if false, does not perform puts");
    System.err.println("                  (defaults to true)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" doDeletes        if false, does not perform deletes");
    System.err.println("                  (defaults to true)");
    System.err.println(" doPuts           if false, does not perform puts");
    System.err.println("                  (defaults to true)");
    System.err.println(" ignoreTimestamps if true, ignores cells timestamps while comparing ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (defaults to true)");
    System.err.println(" doPuts           if false, does not perform puts");
    System.err.println("                  (defaults to true)");
    System.err.println(" ignoreTimestamps if true, ignores cells timestamps while comparing ");
    System.err.println("                  cell values. Any missing cell on target then gets");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" doPuts           if false, does not perform puts");
    System.err.println("                  (defaults to true)");
    System.err.println(" ignoreTimestamps if true, ignores cells timestamps while comparing ");
    System.err.println("                  cell values. Any missing cell on target then gets");
    System.err.println("                  added with current time as timestamp ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (defaults to true)");
    System.err.println(" ignoreTimestamps if true, ignores cells timestamps while comparing ");
    System.err.println("                  cell values. Any missing cell on target then gets");
    System.err.println("                  added with current time as timestamp ");
    System.err.println("                  (defaults to false)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" ignoreTimestamps if true, ignores cells timestamps while comparing ");
    System.err.println("                  cell values. Any missing cell on target then gets");
    System.err.println("                  added with current time as timestamp ");
    System.err.println("                  (defaults to false)");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  cell values. Any missing cell on target then gets");
    System.err.println("                  added with current time as timestamp ");
    System.err.println("                  (defaults to false)");
    System.err.println();
    System.err.println("Args:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  added with current time as timestamp ");
    System.err.println("                  (defaults to false)");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" sourcehashdir    path to HashTable output dir for source table");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (defaults to false)");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" sourcehashdir    path to HashTable output dir for source table");
    System.err.println("                  (see org.apache.hadoop.hbase.mapreduce.HashTable)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Args:");
    System.err.println(" sourcehashdir    path to HashTable output dir for source table");
    System.err.println("                  (see org.apache.hadoop.hbase.mapreduce.HashTable)");
    System.err.println(" sourcetable      Name of the source table to sync from");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("Args:");
    System.err.println(" sourcehashdir    path to HashTable output dir for source table");
    System.err.println("                  (see org.apache.hadoop.hbase.mapreduce.HashTable)");
    System.err.println(" sourcetable      Name of the source table to sync from");
    System.err.println(" targettable      Name of the target table to sync to");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" sourcehashdir    path to HashTable output dir for source table");
    System.err.println("                  (see org.apache.hadoop.hbase.mapreduce.HashTable)");
    System.err.println(" sourcetable      Name of the source table to sync from");
    System.err.println(" targettable      Name of the target table to sync to");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("                  (see org.apache.hadoop.hbase.mapreduce.HashTable)");
    System.err.println(" sourcetable      Name of the source table to sync from");
    System.err.println(" targettable      Name of the target table to sync to");
    System.err.println();
    System.err.println("Examples:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" sourcetable      Name of the source table to sync from");
    System.err.println(" targettable      Name of the target table to sync to");
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" For a dry run SyncTable of tableA from a remote source cluster");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" targettable      Name of the target table to sync to");
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" For a dry run SyncTable of tableA from a remote source cluster");
    System.err.println(" to a local target cluster:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" For a dry run SyncTable of tableA from a remote source cluster");
    System.err.println(" to a local target cluster:");
    System.err.println(" $ hbase " + "org.apache.hadoop.hbase.mapreduce.SyncTable --dryrun=true"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println("Examples:");
    System.err.println(" For a dry run SyncTable of tableA from a remote source cluster");
    System.err.println(" to a local target cluster:");
    System.err.println(" $ hbase " + "org.apache.hadoop.hbase.mapreduce.SyncTable --dryrun=true"
      + " --sourcezkcluster=zk1.example.com,zk2.example.com,zk3.example.com:2181:/hbase"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    System.err.println(" For a dry run SyncTable of tableA from a remote source cluster");
    System.err.println(" to a local target cluster:");
    System.err.println(" $ hbase " + "org.apache.hadoop.hbase.mapreduce.SyncTable --dryrun=true"
      + " --sourcezkcluster=zk1.example.com,zk2.example.com,zk3.example.com:2181:/hbase"
      + " hdfs://nn:9000/hashes/tableA tableA tableA");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
   */
  static void deleteRow(final Table table) throws IOException {
    System.out.println("Deleting row [" + Bytes.toString(MY_ROW_ID) + "] from Table ["
      + table.getName().getNameAsString() + "].");
    table.delete(new Delete(MY_ROW_ID));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
      Admin admin = connection.getAdmin()) {
      admin.getClusterMetrics(); // assure connection successfully established
      System.out
        .println("\n*** Hello HBase! -- Connection has been " + "established via ZooKeeper!!\n");

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
      createNamespaceAndTable(admin);

      System.out.println("Getting a Table object for [" + MY_TABLE_NAME
        + "] with which to perform CRUD operations in HBase.");
      try (Table table = connection.getTable(MY_TABLE_NAME)) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
    Result row = table.get(new Get(MY_ROW_ID));

    System.out.println("Row [" + Bytes.toString(row.getRow()) + "] was retrieved from Table ["
      + table.getName().getNameAsString() + "] in HBase, with the following content:");

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
      String columnFamilyName = Bytes.toString(colFamilyEntry.getKey());

      System.out.println("  Columns in Column Family [" + columnFamilyName + "]:");

      for (Entry<byte[], byte[]> columnNameAndValueMap : colFamilyEntry.getValue().entrySet()) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
      for (Entry<byte[], byte[]> columnNameAndValueMap : colFamilyEntry.getValue().entrySet()) {

        System.out.println("    Value of Column [" + columnFamilyName + ":"
          + Bytes.toString(columnNameAndValueMap.getKey()) + "] == "
          + Bytes.toString(columnNameAndValueMap.getValue()));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java

    if (!namespaceExists(admin, MY_NAMESPACE_NAME)) {
      System.out.println("Creating Namespace [" + MY_NAMESPACE_NAME + "].");

      admin.createNamespace(NamespaceDescriptor.create(MY_NAMESPACE_NAME).build());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
    }
    if (!admin.tableExists(MY_TABLE_NAME)) {
      System.out.println("Creating Table [" + MY_TABLE_NAME.getNameAsString()
        + "], with one Column Family [" + Bytes.toString(MY_COLUMN_FAMILY_NAME) + "].");

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
      .addColumn(MY_COLUMN_FAMILY_NAME, MY_SECOND_COLUMN_QUALIFIER, Bytes.toBytes("World!")));

    System.out.println("Row [" + Bytes.toString(MY_ROW_ID) + "] was put into Table ["
      + table.getName().getNameAsString() + "] in HBase;\n"
      + "  the row's two columns (created 'on the fly') are: ["
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
  static void deleteNamespaceAndTable(final Admin admin) throws IOException {
    if (admin.tableExists(MY_TABLE_NAME)) {
      System.out.println("Disabling/deleting Table [" + MY_TABLE_NAME.getNameAsString() + "].");
      admin.disableTable(MY_TABLE_NAME); // Disable a table before deleting it.
      admin.deleteTable(MY_TABLE_NAME);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
    }
    if (namespaceExists(admin, MY_NAMESPACE_NAME)) {
      System.out.println("Deleting Namespace [" + MY_NAMESPACE_NAME + "].");
      admin.deleteNamespace(MY_NAMESPACE_NAME);
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    long outputRecords = job.getCounters().findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getValue();
    if (outputRecords < inputRecords) {
      System.err.println("Warning, not all records were imported (maybe filtered out).");
      if (outputRecords == 0) {
        System.err.println("If the data was exported from HBase 0.94 "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      System.err.println("Warning, not all records were imported (maybe filtered out).");
      if (outputRecords == 0) {
        System.err.println("If the data was exported from HBase 0.94 "
          + "consider using -Dhbase.import.version=0.94.");
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  private static void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: Import [options] <tablename> <inputdir>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: Import [options] <tablename> <inputdir>");
    System.err.println("By default Import will load data directly into HBase. To instead generate");
    System.err.println("HFiles of data to prepare for a bulk data load, pass the option:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    }
    System.err.println("Usage: Import [options] <tablename> <inputdir>");
    System.err.println("By default Import will load data directly into HBase. To instead generate");
    System.err.println("HFiles of data to prepare for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("Usage: Import [options] <tablename> <inputdir>");
    System.err.println("By default Import will load data directly into HBase. To instead generate");
    System.err.println("HFiles of data to prepare for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("If there is a large result that includes too much Cell "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("By default Import will load data directly into HBase. To instead generate");
    System.err.println("HFiles of data to prepare for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("If there is a large result that includes too much Cell "
      + "whitch can occur OOME caused by the memery sort in reducer, pass the option:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("HFiles of data to prepare for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("If there is a large result that includes too much Cell "
      + "whitch can occur OOME caused by the memery sort in reducer, pass the option:");
    System.err.println("  -D" + HAS_LARGE_RESULT + "=true");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("If there is a large result that includes too much Cell "
      + "whitch can occur OOME caused by the memery sort in reducer, pass the option:");
    System.err.println("  -D" + HAS_LARGE_RESULT + "=true");
    System.err
      .println(" To apply a generic org.apache.hadoop.hbase.filter.Filter to the input, use");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      + "whitch can occur OOME caused by the memery sort in reducer, pass the option:");
    System.err.println("  -D" + HAS_LARGE_RESULT + "=true");
    System.err
      .println(" To apply a generic org.apache.hadoop.hbase.filter.Filter to the input, use");
    System.err.println("  -D" + FILTER_CLASS_CONF_KEY + "=<name of filter class>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err
      .println(" To apply a generic org.apache.hadoop.hbase.filter.Filter to the input, use");
    System.err.println("  -D" + FILTER_CLASS_CONF_KEY + "=<name of filter class>");
    System.err.println("  -D" + FILTER_ARGS_CONF_KEY + "=<comma separated list of args for filter");
    System.err.println(" NOTE: The filter will be applied BEFORE doing key renames via the "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      .println(" To apply a generic org.apache.hadoop.hbase.filter.Filter to the input, use");
    System.err.println("  -D" + FILTER_CLASS_CONF_KEY + "=<name of filter class>");
    System.err.println("  -D" + FILTER_ARGS_CONF_KEY + "=<comma separated list of args for filter");
    System.err.println(" NOTE: The filter will be applied BEFORE doing key renames via the "
      + CF_RENAME_PROP + " property. Futher, filters will only use the"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("  -D" + FILTER_CLASS_CONF_KEY + "=<name of filter class>");
    System.err.println("  -D" + FILTER_ARGS_CONF_KEY + "=<comma separated list of args for filter");
    System.err.println(" NOTE: The filter will be applied BEFORE doing key renames via the "
      + CF_RENAME_PROP + " property. Futher, filters will only use the"
      + " Filter#filterRowKey(byte[] buffer, int offset, int length) method to identify "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      + " Filter.ReturnCode#INCLUDE and #INCLUDE_AND_NEXT_COL will be considered as including"
      + " the Cell.");
    System.err.println("To import data exported from HBase 0.94, use");
    System.err.println("  -Dhbase.import.version=0.94");
    System.err.println("  -D " + JOB_NAME_CONF_KEY
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      + " the Cell.");
    System.err.println("To import data exported from HBase 0.94, use");
    System.err.println("  -Dhbase.import.version=0.94");
    System.err.println("  -D " + JOB_NAME_CONF_KEY
      + "=jobName - use the specified mapreduce job name for the import");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("To import data exported from HBase 0.94, use");
    System.err.println("  -Dhbase.import.version=0.94");
    System.err.println("  -D " + JOB_NAME_CONF_KEY
      + "=jobName - use the specified mapreduce job name for the import");
    System.err.println("For performance consider the following options:\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    System.err.println("  -D " + JOB_NAME_CONF_KEY
      + "=jobName - use the specified mapreduce job name for the import");
    System.err.println("For performance consider the following options:\n"
      + "  -Dmapreduce.map.speculative=false\n" + "  -Dmapreduce.reduce.speculative=false\n"
      + "  -D" + WAL_DURABILITY + "=<Used while writing data to hbase."
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  private static void printUsage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: verifyrep [--starttime=X]"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: verifyrep [--starttime=X]"
      + " [--endtime=Y] [--families=A] [--row-prefixes=B] [--delimiter=] [--recomparesleep=] "
      + "[--batch=] [--verbose] [--peerTableName=] [--sourceSnapshotName=P] "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      + "[--sourceSnapshotTmpDir=Q] [--peerSnapshotName=R] [--peerSnapshotTmpDir=S] "
      + "[--peerFSAddress=T] [--peerHBaseRootAddress=U] <peerid|peerQuorumAddress> <tablename>");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" starttime    beginning of the time range");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      + "[--peerFSAddress=T] [--peerHBaseRootAddress=U] <peerid|peerQuorumAddress> <tablename>");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" starttime    beginning of the time range");
    System.err.println("              without endtime means from starttime to forever");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Options:");
    System.err.println(" starttime    beginning of the time range");
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println("Options:");
    System.err.println(" starttime    beginning of the time range");
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range");
    System.err.println(" versions     number of cell versions to verify");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" starttime    beginning of the time range");
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range");
    System.err.println(" versions     number of cell versions to verify");
    System.err.println(" batch        batch count for scan, note that"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println("              without endtime means from starttime to forever");
    System.err.println(" endtime      end of the time range");
    System.err.println(" versions     number of cell versions to verify");
    System.err.println(" batch        batch count for scan, note that"
      + " result row counts will no longer be actual number of rows when you use this option");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" endtime      end of the time range");
    System.err.println(" versions     number of cell versions to verify");
    System.err.println(" batch        batch count for scan, note that"
      + " result row counts will no longer be actual number of rows when you use this option");
    System.err.println(" raw          includes raw scan if given in options");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" batch        batch count for scan, note that"
      + " result row counts will no longer be actual number of rows when you use this option");
    System.err.println(" raw          includes raw scan if given in options");
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println(" row-prefixes comma-separated list of row key prefixes to filter on ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      + " result row counts will no longer be actual number of rows when you use this option");
    System.err.println(" raw          includes raw scan if given in options");
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println(" row-prefixes comma-separated list of row key prefixes to filter on ");
    System.err.println(" delimiter    the delimiter used in display around rowkey");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" raw          includes raw scan if given in options");
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println(" row-prefixes comma-separated list of row key prefixes to filter on ");
    System.err.println(" delimiter    the delimiter used in display around rowkey");
    System.err.println(" recomparesleep   milliseconds to sleep before recompare row, "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" families     comma-separated list of families to copy");
    System.err.println(" row-prefixes comma-separated list of row key prefixes to filter on ");
    System.err.println(" delimiter    the delimiter used in display around rowkey");
    System.err.println(" recomparesleep   milliseconds to sleep before recompare row, "
      + "default value is 0 which disables the recompare.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" row-prefixes comma-separated list of row key prefixes to filter on ");
    System.err.println(" delimiter    the delimiter used in display around rowkey");
    System.err.println(" recomparesleep   milliseconds to sleep before recompare row, "
      + "default value is 0 which disables the recompare.");
    System.err.println(" verbose      logs row keys of good rows");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" recomparesleep   milliseconds to sleep before recompare row, "
      + "default value is 0 which disables the recompare.");
    System.err.println(" verbose      logs row keys of good rows");
    System.err.println(" peerTableName  Peer Table Name");
    System.err.println(" sourceSnapshotName  Source Snapshot Name");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      + "default value is 0 which disables the recompare.");
    System.err.println(" verbose      logs row keys of good rows");
    System.err.println(" peerTableName  Peer Table Name");
    System.err.println(" sourceSnapshotName  Source Snapshot Name");
    System.err.println(" sourceSnapshotTmpDir Tmp location to restore source table snapshot");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" verbose      logs row keys of good rows");
    System.err.println(" peerTableName  Peer Table Name");
    System.err.println(" sourceSnapshotName  Source Snapshot Name");
    System.err.println(" sourceSnapshotTmpDir Tmp location to restore source table snapshot");
    System.err.println(" peerSnapshotName  Peer Snapshot Name");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerTableName  Peer Table Name");
    System.err.println(" sourceSnapshotName  Source Snapshot Name");
    System.err.println(" sourceSnapshotTmpDir Tmp location to restore source table snapshot");
    System.err.println(" peerSnapshotName  Peer Snapshot Name");
    System.err.println(" peerSnapshotTmpDir Tmp location to restore peer table snapshot");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" sourceSnapshotName  Source Snapshot Name");
    System.err.println(" sourceSnapshotTmpDir Tmp location to restore source table snapshot");
    System.err.println(" peerSnapshotName  Peer Snapshot Name");
    System.err.println(" peerSnapshotTmpDir Tmp location to restore peer table snapshot");
    System.err.println(" peerFSAddress      Peer cluster Hadoop FS address");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" sourceSnapshotTmpDir Tmp location to restore source table snapshot");
    System.err.println(" peerSnapshotName  Peer Snapshot Name");
    System.err.println(" peerSnapshotTmpDir Tmp location to restore peer table snapshot");
    System.err.println(" peerFSAddress      Peer cluster Hadoop FS address");
    System.err.println(" peerHBaseRootAddress  Peer cluster HBase root location");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerSnapshotName  Peer Snapshot Name");
    System.err.println(" peerSnapshotTmpDir Tmp location to restore peer table snapshot");
    System.err.println(" peerFSAddress      Peer cluster Hadoop FS address");
    System.err.println(" peerHBaseRootAddress  Peer cluster HBase root location");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerSnapshotTmpDir Tmp location to restore peer table snapshot");
    System.err.println(" peerFSAddress      Peer cluster Hadoop FS address");
    System.err.println(" peerHBaseRootAddress  Peer cluster HBase root location");
    System.err.println();
    System.err.println("Args:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerFSAddress      Peer cluster Hadoop FS address");
    System.err.println(" peerHBaseRootAddress  Peer cluster HBase root location");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" peerid       Id of the peer used for verification,"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerHBaseRootAddress  Peer cluster HBase root location");
    System.err.println();
    System.err.println("Args:");
    System.err.println(" peerid       Id of the peer used for verification,"
      + " must match the one given for replication");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Args:");
    System.err.println(" peerid       Id of the peer used for verification,"
      + " must match the one given for replication");
    System.err.println(" peerQuorumAddress   quorumAdress of the peer used for verification. The "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerid       Id of the peer used for verification,"
      + " must match the one given for replication");
    System.err.println(" peerQuorumAddress   quorumAdress of the peer used for verification. The "
      + "format is zk_quorum:zk_port:zk_hbase_path");
    System.err.println(" tablename    Name of the table to verify");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" peerQuorumAddress   quorumAdress of the peer used for verification. The "
      + "format is zk_quorum:zk_port:zk_hbase_path");
    System.err.println(" tablename    Name of the table to verify");
    System.err.println();
    System.err.println("Examples:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      + "format is zk_quorum:zk_port:zk_hbase_path");
    System.err.println(" tablename    Name of the table to verify");
    System.err.println();
    System.err.println("Examples:");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" tablename    Name of the table to verify");
    System.err.println();
    System.err.println("Examples:");
    System.err
      .println(" To verify the data replicated from TestTable for a 1 hour window with peer #5 ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Examples:");
    System.err
      .println(" To verify the data replicated from TestTable for a 1 hour window with peer #5 ");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err
      .println(" To verify the data replicated from TestTable for a 1 hour window with peer #5 ");
    System.err
      .println(" $ hbase " + "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication"
        + " --starttime=1265875194289 --endtime=1265878794289 5 TestTable ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      .println(" $ hbase " + "org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication"
        + " --starttime=1265875194289 --endtime=1265878794289 5 TestTable ");
    System.err.println();
    System.err.println(
      " To verify the data in TestTable between the cluster runs VerifyReplication and cluster-b");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + " --starttime=1265875194289 --endtime=1265878794289 5 TestTable ");
    System.err.println();
    System.err.println(
      " To verify the data in TestTable between the cluster runs VerifyReplication and cluster-b");
    System.err.println(" Assume quorum address for cluster-b is"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(
      " To verify the data in TestTable between the cluster runs VerifyReplication and cluster-b");
    System.err.println(" Assume quorum address for cluster-b is"
      + " cluster-b-1.example.com,cluster-b-2.example.com,cluster-b-3.example.com:2181:/cluster-b");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" Assume quorum address for cluster-b is"
      + " cluster-b-1.example.com,cluster-b-2.example.com,cluster-b-3.example.com:2181:/cluster-b");
    System.err
      .println(" $ hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication \\\n"
        + "     cluster-b-1.example.com,cluster-b-2.example.com,cluster-b-3.example.com:"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + "     cluster-b-1.example.com,cluster-b-2.example.com,cluster-b-3.example.com:"
        + "2181:/cluster-b \\\n" + "     TestTable");
    System.err.println();
    System.err
      .println(" To verify the data in TestTable between the secured cluster runs VerifyReplication"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + "2181:/cluster-b \\\n" + "     TestTable");
    System.err.println();
    System.err
      .println(" To verify the data in TestTable between the secured cluster runs VerifyReplication"
        + " and insecure cluster-b");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      .println(" To verify the data in TestTable between the secured cluster runs VerifyReplication"
        + " and insecure cluster-b");
    System.err
      .println(" $ hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication \\\n"
        + "     -D verifyrep.peer.hbase.security.authentication=simple \\\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + "     cluster-b-1.example.com,cluster-b-2.example.com,cluster-b-3.example.com:"
        + "2181:/cluster-b \\\n" + "     TestTable");
    System.err.println();
    System.err.println(" To verify the data in TestTable between"
      + " the secured cluster runs VerifyReplication and secured cluster-b");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + "2181:/cluster-b \\\n" + "     TestTable");
    System.err.println();
    System.err.println(" To verify the data in TestTable between"
      + " the secured cluster runs VerifyReplication and secured cluster-b");
    System.err.println(" Assume cluster-b uses different kerberos principal, cluster-b/_HOST@E"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" To verify the data in TestTable between"
      + " the secured cluster runs VerifyReplication and secured cluster-b");
    System.err.println(" Assume cluster-b uses different kerberos principal, cluster-b/_HOST@E"
      + ", for master and regionserver kerberos principal from another cluster");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    System.err.println(" Assume cluster-b uses different kerberos principal, cluster-b/_HOST@E"
      + ", for master and regionserver kerberos principal from another cluster");
    System.err
      .println(" $ hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication \\\n"
        + "     -D verifyrep.peer.hbase.regionserver.kerberos.principal="
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + "     cluster-b-1.example.com,cluster-b-2.example.com,cluster-b-3.example.com:"
        + "2181:/cluster-b \\\n" + "     TestTable");
    System.err.println();
    System.err.println(
      " To verify the data in TestTable between the insecure cluster runs VerifyReplication"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
        + "2181:/cluster-b \\\n" + "     TestTable");
    System.err.println();
    System.err.println(
      " To verify the data in TestTable between the insecure cluster runs VerifyReplication"
        + " and secured cluster-b");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
      " To verify the data in TestTable between the insecure cluster runs VerifyReplication"
        + " and secured cluster-b");
    System.err
      .println(" $ hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication \\\n"
        + "     -D verifyrep.peer.hbase.security.authentication=kerberos \\\n"
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
    }
    for (String arg : args) {
      System.out.println("Original: " + arg);
      String quoted = quoteHtmlChars(arg);
      System.out.println("Quoted: " + quoted);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
      System.out.println("Original: " + arg);
      String quoted = quoteHtmlChars(arg);
      System.out.println("Quoted: " + quoted);
      String unquoted = unquoteHtmlChars(quoted);
      System.out.println("Unquoted: " + unquoted);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
      System.out.println("Quoted: " + quoted);
      String unquoted = unquoteHtmlChars(quoted);
      System.out.println("Unquoted: " + unquoted);
      System.out.println();
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
      String unquoted = unquoteHtmlChars(quoted);
      System.out.println("Unquoted: " + unquoted);
      System.out.println();
    }
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
  private void printUsage(final String message) {
    if (message != null && message.length() > 0) {
      System.err.println(message);
    }
    System.err.println("Usage: java " + this.getClass().getName() + " \\");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
      System.err.println(message);
    }
    System.err.println("Usage: java " + this.getClass().getName() + " \\");
    System.err.println("  [-compactOnce] [-major] [-mapred] [-D<property=value>]* files...");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    }
    System.err.println("Usage: java " + this.getClass().getName() + " \\");
    System.err.println("  [-compactOnce] [-major] [-mapred] [-D<property=value>]* files...");
    System.err.println();
    System.err.println("Options:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println("Usage: java " + this.getClass().getName() + " \\");
    System.err.println("  [-compactOnce] [-major] [-mapred] [-D<property=value>]* files...");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" mapred         Use MapReduce to run compaction.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println("  [-compactOnce] [-major] [-mapred] [-D<property=value>]* files...");
    System.err.println();
    System.err.println("Options:");
    System.err.println(" mapred         Use MapReduce to run compaction.");
    System.err.println(" compactOnce    Execute just one compaction step. (default: while needed)");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Options:");
    System.err.println(" mapred         Use MapReduce to run compaction.");
    System.err.println(" compactOnce    Execute just one compaction step. (default: while needed)");
    System.err.println(" major          Trigger major compaction.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println("Options:");
    System.err.println(" mapred         Use MapReduce to run compaction.");
    System.err.println(" compactOnce    Execute just one compaction step. (default: while needed)");
    System.err.println(" major          Trigger major compaction.");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println(" mapred         Use MapReduce to run compaction.");
    System.err.println(" compactOnce    Execute just one compaction step. (default: while needed)");
    System.err.println(" major          Trigger major compaction.");
    System.err.println();
    System.err.println("Note: -D properties will be applied to the conf used. ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println(" compactOnce    Execute just one compaction step. (default: while needed)");
    System.err.println(" major          Trigger major compaction.");
    System.err.println();
    System.err.println("Note: -D properties will be applied to the conf used. ");
    System.err.println("For example: ");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println(" major          Trigger major compaction.");
    System.err.println();
    System.err.println("Note: -D properties will be applied to the conf used. ");
    System.err.println("For example: ");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Note: -D properties will be applied to the conf used. ");
    System.err.println("For example: ");
    System.err
      .println(" To stop delete of compacted file, pass -D" + CONF_DELETE_COMPACTED + "=false");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println("Note: -D properties will be applied to the conf used. ");
    System.err.println("For example: ");
    System.err
      .println(" To stop delete of compacted file, pass -D" + CONF_DELETE_COMPACTED + "=false");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err
      .println(" To stop delete of compacted file, pass -D" + CONF_DELETE_COMPACTED + "=false");
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" To compact the full 'TestTable' using MapReduce:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
      .println(" To stop delete of compacted file, pass -D" + CONF_DELETE_COMPACTED + "=false");
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" To compact the full 'TestTable' using MapReduce:");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println();
    System.err.println("Examples:");
    System.err.println(" To compact the full 'TestTable' using MapReduce:");
    System.err.println(
      " $ hbase " + this.getClass().getName() + " -mapred hdfs://hbase/data/default/TestTable");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println("Examples:");
    System.err.println(" To compact the full 'TestTable' using MapReduce:");
    System.err.println(
      " $ hbase " + this.getClass().getName() + " -mapred hdfs://hbase/data/default/TestTable");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println(
      " $ hbase " + this.getClass().getName() + " -mapred hdfs://hbase/data/default/TestTable");
    System.err.println();
    System.err.println(" To compact column family 'x' of the table 'TestTable' region 'abc':");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
      " $ hbase " + this.getClass().getName() + " -mapred hdfs://hbase/data/default/TestTable");
    System.err.println();
    System.err.println(" To compact column family 'x' of the table 'TestTable' region 'abc':");
    System.err.println(
      " $ hbase " + this.getClass().getName() + " hdfs://hbase/data/default/TestTable/abc/x");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    System.err.println();
    System.err.println(" To compact column family 'x' of the table 'TestTable' region 'abc':");
    System.err.println(
      " $ hbase " + this.getClass().getName() + " hdfs://hbase/data/default/TestTable/abc/x");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java`
#### Snippet
```java

  private static void printUsage() {
    System.err.println(USAGES);
    System.exit(-1);
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java`
#### Snippet
```java
    private void process(String urlString) throws Exception {
      URL url = new URL(urlString);
      System.out.println("Connecting to " + url);

      HttpURLConnection connection = connect(url);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java`
#### Snippet
```java
        BufferedReader bufferedReader = new BufferedReader(streamReader)) {
        bufferedReader.lines().filter(Objects::nonNull).filter(line -> line.startsWith(MARKER))
          .forEach(line -> System.out.println(TAG.matcher(line).replaceAll("")));
      } catch (IOException ioe) {
        System.err.println("" + ioe);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java`
#### Snippet
```java
          .forEach(line -> System.out.println(TAG.matcher(line).replaceAll("")));
      } catch (IOException ioe) {
        System.err.println("" + ioe);
      }
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
  public static void dumpAllBeans() throws IOException, MalformedObjectNameException {
    try (PrintWriter writer =
      new PrintWriter(new OutputStreamWriter(System.out, StandardCharsets.UTF_8))) {
      JSONBean dumper = new JSONBean();
      try (JSONBean.Writer jsonBeanWriter = dumper.open(writer)) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    for (ServerName addr : serverSet) {
      if (i++ % 3 == 0) {
        System.out.print("\n\t\t\t");
      }
      System.out.print(addr.getAddress() + " ; ");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        System.out.print("\n\t\t\t");
      }
      System.out.print(addr.getAddress() + " ; ");
    }
    System.out.println("\n");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      System.out.print(addr.getAddress() + " ; ");
    }
    System.out.println("\n");
  }
}
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  public void print(boolean isDetailMode) {
    if (!isFilledUp) {
      System.err.println("[Error] Region assignment verification report" + "hasn't been filled up");
    }
    DecimalFormat df = new java.text.DecimalFormat("#.##");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

    // Print some basic information
    System.out.println("Region Assignment Verification for Table: " + tableName
      + "\n\tTotal regions : " + totalRegions);

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

    // Print the number of regions on each kinds of the favored nodes
    System.out.println("\tTotal regions on favored nodes " + totalFavoredAssignments);
    for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {
      System.out.println(
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    System.out.println("\tTotal regions on favored nodes " + totalFavoredAssignments);
    for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {
      System.out.println(
        "\t\tTotal regions on " + p.toString() + " region servers: " + favoredNodes[p.ordinal()]);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    }
    // Print the number of regions in each kinds of invalid assignment
    System.out.println("\tTotal unassigned regions: " + unAssignedRegionsList.size());
    if (isDetailMode) {
      for (RegionInfo region : unAssignedRegionsList) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    if (isDetailMode) {
      for (RegionInfo region : unAssignedRegionsList) {
        System.out.println("\t\t" + region.getRegionNameAsString());
      }
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    }

    System.out
      .println("\tTotal regions NOT on favored nodes: " + nonFavoredAssignedRegionList.size());
    if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    if (isDetailMode) {
      for (RegionInfo region : nonFavoredAssignedRegionList) {
        System.out.println("\t\t" + region.getRegionNameAsString());
      }
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    }

    System.out
      .println("\tTotal regions without favored nodes: " + regionsWithoutValidFavoredNodes.size());
    if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    if (isDetailMode) {
      for (RegionInfo region : regionsWithoutValidFavoredNodes) {
        System.out.println("\t\t" + region.getRegionNameAsString());
      }
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      // Print the actual locality for this table
      float actualLocality = 100 * this.actualLocalitySummary / (float) totalRegions;
      System.out.println("\n\tThe actual avg locality is " + df.format(actualLocality) + " %");

      // Print the expected locality if regions are placed on the each kinds of
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      for (FavoredNodesPlan.Position p : FavoredNodesPlan.Position.values()) {
        float avgLocality = 100 * (favoredNodesLocalitySummary[p.ordinal()] / (float) totalRegions);
        System.out.println("\t\tThe expected avg locality if all regions" + " on the "
          + p.toString() + " region servers: " + df.format(avgLocality) + " %");
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

    // Print the region balancing information
    System.out.println("\n\tTotal hosting region servers: " + totalRegionServers);
    // Print the region balance information
    if (totalRegionServers != 0) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    // Print the region balance information
    if (totalRegionServers != 0) {
      System.out.println("\tAvg dispersion num: " + df.format(avgDispersionNum)
        + " hosts;\tMax dispersion num: " + df.format(maxDispersionNum)
        + " hosts;\tMin dispersion num: " + df.format(minDispersionNum) + " hosts;");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        + " hosts;\tMin dispersion num: " + df.format(minDispersionNum) + " hosts;");

      System.out.println("\t\tThe number of the region servers with the max" + " dispersion num: "
        + this.maxDispersionNumServerSet.size());
      if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      }

      System.out.println("\t\tThe number of the region servers with the min" + " dispersion num: "
        + this.minDispersionNumServerSet.size());
      if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      }

      System.out.println("\tAvg dispersion score: " + df.format(avgDispersionScore)
        + ";\tMax dispersion score: " + df.format(maxDispersionScore) + ";\tMin dispersion score: "
        + df.format(minDispersionScore) + ";");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        + df.format(minDispersionScore) + ";");

      System.out.println("\t\tThe number of the region servers with the max" + " dispersion score: "
        + this.maxDispersionScoreServerSet.size());
      if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      }

      System.out.println("\t\tThe number of the region servers with the min" + " dispersion score: "
        + this.minDispersionScoreServerSet.size());
      if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      }

      System.out.println("\tAvg regions/region server: " + df.format(avgRegionsOnRS)
        + ";\tMax regions/region server: " + maxRegionsOnRS + ";\tMin regions/region server: "
        + minRegionsOnRS + ";");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

      // Print the details about the most loaded region servers
      System.out
        .println("\t\tThe number of the most loaded region servers: " + mostLoadedRSSet.size());
      if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

      // Print the details about the least loaded region servers
      System.out
        .println("\t\tThe number of the least loaded region servers: " + leastLoadedRSSet.size());
      if (isDetailMode) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      }
    }
    System.out.println("==============================");
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
  private static final int FOUND_MOB_FILES_CACHE_CAPACITY = 50;
  private static final int MISSING_MOB_FILES_CACHE_CAPACITY = 20;
  private PrintStream out = System.out;
  private PrintStream err = System.err;

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
  private static final int MISSING_MOB_FILES_CACHE_CAPACITY = 20;
  private PrintStream out = System.out;
  private PrintStream err = System.err;

  /**
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
        Optional<TableName> tn = MobUtils.getTableName(cell);
        if (!tn.isPresent()) {
          System.err.println(
            "ERROR, wrong tag format in mob reference cell " + CellUtil.getCellKeyAsString(cell));
        } else if (!MobUtils.hasValidMobRefCellValue(cell)) {
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
            "ERROR, wrong tag format in mob reference cell " + CellUtil.getCellKeyAsString(cell));
        } else if (!MobUtils.hasValidMobRefCellValue(cell)) {
          System.err.println(
            "ERROR, wrong value format in mob reference cell " + CellUtil.getCellKeyAsString(cell));
        } else {
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
          if (!exist) {
            // report error
            System.err.println("ERROR, the mob file [" + mobFileName
              + "] is missing referenced by cell " + CellUtil.getCellKeyAsString(cell));
          }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java

      private Builder() {
        this.output = System.out;
        this.locale = Locale.getDefault();
        this.timeZone = TimeZone.getDefault();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    if (checkMobIntegrity) {
      if (verbose) {
        System.out.println("checkMobIntegrity is enabled");
      }
      mobFileLocations = new HashMap<>();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/ExpiredMobFileCleaner.java`
#### Snippet
```java

  private void printUsage() {
    System.err.println("Usage:\n" + "--------------------------\n"
      + ExpiredMobFileCleaner.class.getName() + " tableName familyName");
    System.err.println(" tableName        The table name");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/ExpiredMobFileCleaner.java`
#### Snippet
```java
    System.err.println("Usage:\n" + "--------------------------\n"
      + ExpiredMobFileCleaner.class.getName() + " tableName familyName");
    System.err.println(" tableName        The table name");
    System.err.println(" familyName       The column family name");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/ExpiredMobFileCleaner.java`
#### Snippet
```java
      + ExpiredMobFileCleaner.class.getName() + " tableName familyName");
    System.err.println(" tableName        The table name");
    System.err.println(" familyName       The column family name");
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java`
#### Snippet
```java
    persistentOutput = false;
    firstTxn = true;
    out = System.out;
  }

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java`
#### Snippet
```java
      FileSystem fs = file.getFileSystem(conf);
      if (!fs.exists(file)) {
        System.err.println("ERROR, file doesnt exist: " + file);
        return;
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java

  private void printUsage() {
    System.out.println("usage: hbase " + TOOL_NAME + " command ...");
    System.out.println("Available commands:");
    System.out.printf(" %-15s Validate co-processors are compatible with HBase%n",
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java
  private void printUsage() {
    System.out.println("usage: hbase " + TOOL_NAME + " command ...");
    System.out.println("Available commands:");
    System.out.printf(" %-15s Validate co-processors are compatible with HBase%n",
      VALIDATE_CP_NAME);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java
    System.out.println("usage: hbase " + TOOL_NAME + " command ...");
    System.out.println("Available commands:");
    System.out.printf(" %-15s Validate co-processors are compatible with HBase%n",
      VALIDATE_CP_NAME);
    System.out.printf(" %-15s Validate DataBlockEncodings are compatible with HBase%n",
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java
    System.out.printf(" %-15s Validate co-processors are compatible with HBase%n",
      VALIDATE_CP_NAME);
    System.out.printf(" %-15s Validate DataBlockEncodings are compatible with HBase%n",
      VALIDATE_DBE_NAME);
    System.out.printf(" %-15s Validate HFile contents are readable%n", VALIDATE_HFILE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java
    System.out.printf(" %-15s Validate DataBlockEncodings are compatible with HBase%n",
      VALIDATE_DBE_NAME);
    System.out.printf(" %-15s Validate HFile contents are readable%n", VALIDATE_HFILE);
    System.out.println("For further information, please use command -h");
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java
      VALIDATE_DBE_NAME);
    System.out.printf(" %-15s Validate HFile contents are readable%n", VALIDATE_HFILE);
    System.out.println("For further information, please use command -h");
  }

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/PreUpgradeValidator.java`
#### Snippet
```java
        return AbstractHBaseTool.EXIT_FAILURE;
      default:
        System.err.println("Unknown command: " + args[0]);
        printUsage();
        return AbstractHBaseTool.EXIT_FAILURE;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

  private void printUsageAndExit() {
    System.err.println(
      "Usage: canary [OPTIONS] [<TABLE1> [<TABLE2]...] | [<REGIONSERVER1> [<REGIONSERVER2]..]");
    System.err.println("Where [OPTIONS] are:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      "Usage: canary [OPTIONS] [<TABLE1> [<TABLE2]...] | [<REGIONSERVER1> [<REGIONSERVER2]..]");
    System.err.println("Where [OPTIONS] are:");
    System.err.println(" -h,-help        show this help and exit.");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      "Usage: canary [OPTIONS] [<TABLE1> [<TABLE2]...] | [<REGIONSERVER1> [<REGIONSERVER2]..]");
    System.err.println("Where [OPTIONS] are:");
    System.err.println(" -h,-help        show this help and exit.");
    System.err.println(
      " -regionserver   set 'regionserver mode'; gets row from random region on " + "server");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println("Where [OPTIONS] are:");
    System.err.println(" -h,-help        show this help and exit.");
    System.err.println(
      " -regionserver   set 'regionserver mode'; gets row from random region on " + "server");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      " -regionserver   set 'regionserver mode'; gets row from random region on " + "server");
    System.err.println(
      " -allRegions     get from ALL regions when 'regionserver mode', not just " + "random one.");
    System.err.println(" -zookeeper      set 'zookeeper mode'; grab zookeeper.znode.parent on "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      " -allRegions     get from ALL regions when 'regionserver mode', not just " + "random one.");
    System.err.println(" -zookeeper      set 'zookeeper mode'; grab zookeeper.znode.parent on "
      + "each ensemble member");
    System.err.println(" -daemon         continuous check at defined intervals.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -zookeeper      set 'zookeeper mode'; grab zookeeper.znode.parent on "
      + "each ensemble member");
    System.err.println(" -daemon         continuous check at defined intervals.");
    System.err.println(" -interval <N>   interval between checks in seconds");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      + "each ensemble member");
    System.err.println(" -daemon         continuous check at defined intervals.");
    System.err.println(" -interval <N>   interval between checks in seconds");
    System.err
      .println(" -e              consider table/regionserver argument as regular " + "expression");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -daemon         continuous check at defined intervals.");
    System.err.println(" -interval <N>   interval between checks in seconds");
    System.err
      .println(" -e              consider table/regionserver argument as regular " + "expression");
    System.err.println(" -f <B>          exit on first error; default=true");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err
      .println(" -e              consider table/regionserver argument as regular " + "expression");
    System.err.println(" -f <B>          exit on first error; default=true");
    System.err.println(" -failureAsError treat read/write failure as error");
    System.err.println(" -t <N>          timeout for canary-test run; default=600000ms");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      .println(" -e              consider table/regionserver argument as regular " + "expression");
    System.err.println(" -f <B>          exit on first error; default=true");
    System.err.println(" -failureAsError treat read/write failure as error");
    System.err.println(" -t <N>          timeout for canary-test run; default=600000ms");
    System.err.println(" -writeSniffing  enable write sniffing");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -f <B>          exit on first error; default=true");
    System.err.println(" -failureAsError treat read/write failure as error");
    System.err.println(" -t <N>          timeout for canary-test run; default=600000ms");
    System.err.println(" -writeSniffing  enable write sniffing");
    System.err.println(" -writeTable     the table used for write sniffing; default=hbase:canary");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -failureAsError treat read/write failure as error");
    System.err.println(" -t <N>          timeout for canary-test run; default=600000ms");
    System.err.println(" -writeSniffing  enable write sniffing");
    System.err.println(" -writeTable     the table used for write sniffing; default=hbase:canary");
    System.err.println(" -writeTableTimeout <N>  timeout for writeTable; default=600000ms");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -t <N>          timeout for canary-test run; default=600000ms");
    System.err.println(" -writeSniffing  enable write sniffing");
    System.err.println(" -writeTable     the table used for write sniffing; default=hbase:canary");
    System.err.println(" -writeTableTimeout <N>  timeout for writeTable; default=600000ms");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -writeSniffing  enable write sniffing");
    System.err.println(" -writeTable     the table used for write sniffing; default=hbase:canary");
    System.err.println(" -writeTableTimeout <N>  timeout for writeTable; default=600000ms");
    System.err.println(
      " -readTableTimeouts <tableName>=<read timeout>," + "<tableName>=<read timeout>,...");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -writeTable     the table used for write sniffing; default=hbase:canary");
    System.err.println(" -writeTableTimeout <N>  timeout for writeTable; default=600000ms");
    System.err.println(
      " -readTableTimeouts <tableName>=<read timeout>," + "<tableName>=<read timeout>,...");
    System.err
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      " -readTableTimeouts <tableName>=<read timeout>," + "<tableName>=<read timeout>,...");
    System.err
      .println("                comma-separated list of table read timeouts " + "(no spaces);");
    System.err.println("                logs 'ERROR' if takes longer. default=600000ms");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err
      .println("                comma-separated list of table read timeouts " + "(no spaces);");
    System.err.println("                logs 'ERROR' if takes longer. default=600000ms");
    System.err.println(" -permittedZookeeperFailures <N>  Ignore first N failures attempting to ");
    System.err.println("                connect to individual zookeeper nodes in ensemble");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      .println("                comma-separated list of table read timeouts " + "(no spaces);");
    System.err.println("                logs 'ERROR' if takes longer. default=600000ms");
    System.err.println(" -permittedZookeeperFailures <N>  Ignore first N failures attempting to ");
    System.err.println("                connect to individual zookeeper nodes in ensemble");
    System.err.println("");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println("                logs 'ERROR' if takes longer. default=600000ms");
    System.err.println(" -permittedZookeeperFailures <N>  Ignore first N failures attempting to ");
    System.err.println("                connect to individual zookeeper nodes in ensemble");
    System.err.println("");
    System.err.println(" -D<configProperty>=<value> to assign or override configuration params");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -permittedZookeeperFailures <N>  Ignore first N failures attempting to ");
    System.err.println("                connect to individual zookeeper nodes in ensemble");
    System.err.println("");
    System.err.println(" -D<configProperty>=<value> to assign or override configuration params");
    System.err.println(" -Dhbase.canary.read.raw.enabled=<true/false> Set to enable/disable "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println("                connect to individual zookeeper nodes in ensemble");
    System.err.println("");
    System.err.println(" -D<configProperty>=<value> to assign or override configuration params");
    System.err.println(" -Dhbase.canary.read.raw.enabled=<true/false> Set to enable/disable "
      + "raw scan; default=false");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println("");
    System.err.println(" -D<configProperty>=<value> to assign or override configuration params");
    System.err.println(" -Dhbase.canary.read.raw.enabled=<true/false> Set to enable/disable "
      + "raw scan; default=false");
    System.err.println(
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(" -Dhbase.canary.read.raw.enabled=<true/false> Set to enable/disable "
      + "raw scan; default=false");
    System.err.println(
      " -Dhbase.canary.info.port=PORT_NUMBER  Set for a Canary UI; " + "default=-1 (None)");
    System.err.println("");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      " -Dhbase.canary.info.port=PORT_NUMBER  Set for a Canary UI; " + "default=-1 (None)");
    System.err.println("");
    System.err.println(
      "Canary runs in one of three modes: region (default), regionserver, or " + "zookeeper.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      " -Dhbase.canary.info.port=PORT_NUMBER  Set for a Canary UI; " + "default=-1 (None)");
    System.err.println("");
    System.err.println(
      "Canary runs in one of three modes: region (default), regionserver, or " + "zookeeper.");
    System.err.println("To sniff/probe all regions, pass no arguments.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println(
      "Canary runs in one of three modes: region (default), regionserver, or " + "zookeeper.");
    System.err.println("To sniff/probe all regions, pass no arguments.");
    System.err.println("To sniff/probe all regions of a table, pass tablename.");
    System.err.println("To sniff/probe regionservers, pass -regionserver, etc.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      "Canary runs in one of three modes: region (default), regionserver, or " + "zookeeper.");
    System.err.println("To sniff/probe all regions, pass no arguments.");
    System.err.println("To sniff/probe all regions of a table, pass tablename.");
    System.err.println("To sniff/probe regionservers, pass -regionserver, etc.");
    System.err.println("See http://hbase.apache.org/book.html#_canary for Canary documentation.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println("To sniff/probe all regions, pass no arguments.");
    System.err.println("To sniff/probe all regions of a table, pass tablename.");
    System.err.println("To sniff/probe regionservers, pass -regionserver, etc.");
    System.err.println("See http://hbase.apache.org/book.html#_canary for Canary documentation.");
    System.exit(USAGE_EXIT_CODE);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    System.err.println("To sniff/probe all regions of a table, pass tablename.");
    System.err.println("To sniff/probe regionservers, pass -regionserver, etc.");
    System.err.println("See http://hbase.apache.org/book.html#_canary for Canary documentation.");
    System.exit(USAGE_EXIT_CODE);
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        if (index >= 0) {
          // command line args must be in the form: [opts] [table 1 [table 2 ...]]
          System.err.println("Invalid command line options");
          printUsageAndExit();
        }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

          if (i == args.length) {
            System.err.println("-interval takes a numeric seconds value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
            interval = Long.parseLong(args[i]) * 1000;
          } catch (NumberFormatException e) {
            System.err.println("-interval needs a numeric value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

          if (i == args.length) {
            System.err.println("-t takes a numeric milliseconds value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
            timeout = Long.parseLong(args[i]);
          } catch (NumberFormatException e) {
            System.err.println("-t takes a numeric milliseconds value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

          if (i == args.length) {
            System.err.println("-writeTableTimeout takes a numeric milliseconds value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
            configuredWriteTableTimeout = Long.parseLong(args[i]);
          } catch (NumberFormatException e) {
            System.err.println("-writeTableTimeout takes a numeric milliseconds value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

          if (i == args.length) {
            System.err.println("-writeTable takes a string tablename value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          i++;
          if (i == args.length) {
            System.err.println("-f needs a boolean value argument (true|false).");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          i++;
          if (i == args.length) {
            System.err.println("-readTableTimeouts needs a comma-separated list of read "
              + "millisecond timeouts per table (without spaces).");
            printUsageAndExit();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

          if (i == args.length) {
            System.err.println("-permittedZookeeperFailures needs a numeric value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
            permittedFailures = Long.parseLong(args[i]);
          } catch (NumberFormatException e) {
            System.err.println("-permittedZookeeperFailures needs a numeric value argument.");
            printUsageAndExit();
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        } else {
          // no options match
          System.err.println(cmd + " options is invalid.");
          printUsageAndExit();
        }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    }
    if (regionServerAllRegions && !this.regionServerMode) {
      System.err.println("-allRegions can only be specified in regionserver mode.");
      printUsageAndExit();
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    if (this.zookeeperMode) {
      if (this.regionServerMode || regionServerAllRegions || writeSniffing) {
        System.err.println("-zookeeper is exclusive and cannot be combined with " + "other modes.");
        printUsageAndExit();
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    }
    if (permittedFailures != 0 && !this.zookeeperMode) {
      System.err.println("-permittedZookeeperFailures requires -zookeeper mode.");
      printUsageAndExit();
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    }
    if (readTableTimeoutsStr != null && (this.regionServerMode || this.zookeeperMode)) {
      System.err.println("-readTableTimeouts can only be configured in region mode.");
      printUsageAndExit();
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

      if (foundTableNames.size() > 0) {
        System.err.println("Cannot pass a tablename when using the -regionserver "
          + "option, tablenames:" + foundTableNames.toString());
        this.errorCode = USAGE_EXIT_CODE;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java`
#### Snippet
```java
  protected void usage(String message) {
    if (message != null) {
      System.err.println(message);
      System.err.println("");
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java`
#### Snippet
```java
    if (message != null) {
      System.err.println(message);
      System.err.println("");
    }

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java`
#### Snippet
```java
    }

    System.err.println(getUsage());
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
              + "if > 3 attempts: i=" + i);
            if (i > 3) {
              Threads.printThreadInfo(System.out, "Thread dump " + t.getName());
            }
            t.interrupt();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
      if (System.nanoTime() > end) {
        String msg = "Master not " + action + " after " + millis + "ms";
        Threads.printThreadInfo(System.out, "Thread dump because: " + msg);
        throw new RuntimeException(msg);
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseConfTool.java`
#### Snippet
```java
  public static void main(String args[]) {
    if (args.length < 1) {
      System.err.println("Usage: HBaseConfTool <CONFIGURATION_KEY>");
      System.exit(1);
      return;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseConfTool.java`
#### Snippet
```java

    Configuration conf = HBaseConfiguration.create();
    System.out.println(conf.get(args[0]));
  }
}
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/GetJavaProperty.java`
#### Snippet
```java
    if (args.length == 0) {
      for (Object prop : System.getProperties().keySet()) {
        System.out.println(prop + "=" + System.getProperty((String) prop, ""));
      }
    } else {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/GetJavaProperty.java`
#### Snippet
```java
    } else {
      for (String prop : args) {
        System.out.println(System.getProperty(prop, ""));
      }
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
  public static void usage() {

    System.err.println("Usage: CompressionTest <path> "
      + StringUtils.join(Compression.Algorithm.values(), "|").toLowerCase(Locale.ROOT) + "\n"
      + "For example:\n" + "  hbase " + CompressionTest.class + " file:///tmp/testfile gz\n");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
    FileSystem fs = path.getFileSystem(conf);
    if (fs.exists(path)) {
      System.err.println("The specified path exists, aborting!");
      System.exit(1);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
      fs.delete(path, false);
    }
    System.out.println("SUCCESS");
  }
}
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactorTTL.java`
#### Snippet
```java
      commandLine = cmdLineParser.parse(options, args);
    } catch (ParseException parseException) {
      System.out.println("ERROR: Unable to parse command-line arguments " + Arrays.toString(args)
        + " due to: " + parseException);
      printUsage(options);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactorTTL.java`
#### Snippet
```java
    }
    if (commandLine == null) {
      System.out.println("ERROR: Failed parse, empty commandLine; " + Arrays.toString(args));
      printUsage(options);
      return -1;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    @Override
    public synchronized int summarize() {
      System.out.println(Integer.toString(errorCount) + " inconsistencies detected.");
      if (errorCount == 0) {
        System.out.println("Status: OK");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      System.out.println(Integer.toString(errorCount) + " inconsistencies detected.");
      if (errorCount == 0) {
        System.out.println("Status: OK");
        return 0;
      } else {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        return 0;
      } else {
        System.out.println("Status: INCONSISTENT");
        return -1;
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void print(String message) {
      if (!summary) {
        System.out.println(message);
      }
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void reportError(ERROR_CODE errorCode, String message) {
      if (errorCode == ERROR_CODE.WRONG_USAGE) {
        System.err.println(message);
        return;
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      errorList.add(errorCode);
      if (!summary) {
        System.out.println("ERROR: " + message);
      }
      errorCount++;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      if (numOfSkippedRegions > 0) {
        Set<String> skippedRegionStrings = skippedRegions.get(tInfo.getName());
        System.out.println("    Number of skipped regions: " + numOfSkippedRegions);
        System.out.println("      List of skipped regions:");
        for (String sr : skippedRegionStrings) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        Set<String> skippedRegionStrings = skippedRegions.get(tInfo.getName());
        System.out.println("    Number of skipped regions: " + numOfSkippedRegions);
        System.out.println("      List of skipped regions:");
        for (String sr : skippedRegionStrings) {
          System.out.println("        " + sr);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        System.out.println("      List of skipped regions:");
        for (String sr : skippedRegionStrings) {
          System.out.println("        " + sr);
        }
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      if (showProgress++ == progressThreshold) {
        if (!summary) {
          System.out.print(".");
        }
        showProgress = 0;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void report(String message) {
      if (!summary) {
        System.out.println("ERROR: " + message);
      }
      showProgress = 0;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    public synchronized void detail(String message) {
      if (details) {
        System.out.println(message);
      }
      showProgress = 0;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
      commandLine = cmdLineParser.parse(options, args);
    } catch (ParseException parseException) {
      System.out.println("ERROR: Unable to parse command-line arguments " + Arrays.toString(args)
        + " due to: " + parseException);
      printUsage(options);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
    }
    if (commandLine == null) {
      System.out.println("ERROR: Failed parse, empty commandLine; " + Arrays.toString(args));
      printUsage(options);
      return -1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java
        return stopMaster();
      }
      System.err.println("To shutdown the master run "
        + "hbase-daemon.sh stop master or send a kill signal to the HMaster pid, "
        + "and to stop HBase Cluster run \"stop-hbase.sh\" or \"hbase master "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java
              + " ZK servers in local mode deployment. Aborting as clients (e.g. shell) will not "
              + "be able to find this ZK quorum.";
            System.err.println(errorMsg);
            throw new IOException(errorMsg);
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java
            + ".  ZK was started at port: " + clientPort
            + ".  Aborting as clients (e.g. shell) will not be able to find " + "this ZK quorum.";
          System.err.println(errorMsg);
          throw new IOException(errorMsg);
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterInitializationMonitor.java`
#### Snippet
```java
          if (haltOnTimeout) {
            LOG.error("Zombie Master exiting. Thread dump to stdout");
            Threads.printThreadInfo(System.out, "Zombie HMaster");
            System.exit(-1);
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java

  private void usage() {
    System.err.println("Usage: " + "bin/hbase completebulkload [OPTIONS] "
      + "</PATH/TO/HFILEOUTPUTFORMAT-OUTPUT> <TABLENAME>\n"
      + "Loads directory of hfiles -- a region dir or product of HFileOutputFormat -- "
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          Map<TableName, Integer> movesPerTable = rp.getRegionsMovement(newPlan);
          rp.checkDifferencesWithOldPlan(movesPerTable, locality, newPlan);
          System.out.println("Do you want to update the assignment plan? [y/n]");
          Scanner s = new Scanner(System.in);
          String input = s.nextLine().trim();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          String input = s.nextLine().trim();
          if (input.equals("y")) {
            System.out.println("Updating assignment plan...");
            rp.updateAssignmentPlan(newPlan);
          }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
      }
      List<RegionInfo> regions = tableToRegionsMap.get(table);
      System.out.println("==================================================");
      System.out.println("Assignment Plan Projection Report For Table: " + table);
      System.out.println("\t Total regions: " + regions.size());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
      List<RegionInfo> regions = tableToRegionsMap.get(table);
      System.out.println("==================================================");
      System.out.println("Assignment Plan Projection Report For Table: " + table);
      System.out.println("\t Total regions: " + regions.size());
      System.out.println(
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
      System.out.println("==================================================");
      System.out.println("Assignment Plan Projection Report For Table: " + table);
      System.out.println("\t Total regions: " + regions.size());
      System.out.println(
        "\t" + movesPerTable.get(table) + " primaries will move due to their primary has changed");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
      System.out.println("Assignment Plan Projection Report For Table: " + table);
      System.out.println("\t Total regions: " + regions.size());
      System.out.println(
        "\t" + movesPerTable.get(table) + " primaries will move due to their primary has changed");
      for (RegionInfo currentRegion : regions) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
      DecimalFormat df = new java.text.DecimalFormat("#.##");
      for (int i = 0; i < deltaLocality.length; i++) {
        System.out.print("\t\t Baseline locality for ");
        if (i == 0) {
          System.out.print("primary ");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        System.out.print("\t\t Baseline locality for ");
        if (i == 0) {
          System.out.print("primary ");
        } else if (i == 1) {
          System.out.print("secondary ");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          System.out.print("primary ");
        } else if (i == 1) {
          System.out.print("secondary ");
        } else if (i == 2) {
          System.out.print("tertiary ");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          System.out.print("secondary ");
        } else if (i == 2) {
          System.out.print("tertiary ");
        }
        System.out.println(df.format(100 * locality[i] / regions.size()) + "%");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          System.out.print("tertiary ");
        }
        System.out.println(df.format(100 * locality[i] / regions.size()) + "%");
        System.out.print("\t\t Locality will change with the new plan: ");
        System.out.println(df.format(100 * deltaLocality[i] / regions.size()) + "%");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        }
        System.out.println(df.format(100 * locality[i] / regions.size()) + "%");
        System.out.print("\t\t Locality will change with the new plan: ");
        System.out.println(df.format(100 * deltaLocality[i] / regions.size()) + "%");
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        System.out.println(df.format(100 * locality[i] / regions.size()) + "%");
        System.out.print("\t\t Locality will change with the new plan: ");
        System.out.println(df.format(100 * deltaLocality[i] / regions.size()) + "%");
      }
      System.out.println("\t Baseline dispersion");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        System.out.println(df.format(100 * deltaLocality[i] / regions.size()) + "%");
      }
      System.out.println("\t Baseline dispersion");
      printDispersionScores(table, snapshot, regions.size(), null, true);
      System.out.println("\t Projected dispersion");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
      System.out.println("\t Baseline dispersion");
      printDispersionScores(table, snapshot, regions.size(), null, true);
      System.out.println("\t Projected dispersion");
      printDispersionScores(table, snapshot, regions.size(), newPlan, true);
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
    if (simplePrint) {
      DecimalFormat df = new java.text.DecimalFormat("#.##");
      System.out.println("\tAvg dispersion score: " + df.format(dispersion.get(0))
        + " hosts;\tMax dispersion score: " + df.format(dispersion.get(1))
        + " hosts;\tMin dispersion score: " + df.format(dispersion.get(2)) + " hosts;");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
  public List<AssignmentVerificationReport> verifyRegionPlacement(boolean isDetailMode)
    throws IOException {
    System.out
      .println("Start to verify the region assignment and " + "generate the verification report");
    // Get the region assignment snapshot
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/RegionVisualizer.java`
#### Snippet
```java
    final AsyncConnection conn = ConnectionFactory.createAsyncConnection(conf).get();
    final RegionVisualizer viz = new RegionVisualizer(conn.getAdmin());
    System.out.println(viz.renderRegionDetails());
    return 0;
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/CatalogJanitor.java`
#### Snippet
```java
        new Properties().load(inStream);
      } else {
        System.out.println("No " + filename + " on classpath; Add one else no logging output!");
      }
    } catch (IOException e) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    // Output on stdout so user sees it in terminal.
    if (message) {
      System.out.println("WARNING! " + msg);
    }
    throw new FileSystemVersionException(msg);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupMajorCompactionTTL.java`
#### Snippet
```java
      commandLine = cmdLineParser.parse(options, args);
    } catch (ParseException parseException) {
      System.out.println("ERROR: Unable to parse command-line arguments " + Arrays.toString(args)
        + " due to: " + parseException);
      printUsage(options);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupMajorCompactionTTL.java`
#### Snippet
```java
    }
    if (commandLine == null) {
      System.out.println("ERROR: Failed parse, empty commandLine; " + Arrays.toString(args));
      printUsage(options);
      return -1;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/CreateSnapshot.java`
#### Snippet
```java
      admin.snapshot(new SnapshotDescription(snapshotName, tableName, snapshotType));
    } catch (Exception e) {
      System.err.println("failed to take the snapshot: " + e.getMessage());
      return -1;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    if (listSnapshots) {
      SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss");
      System.out.printf("%-20s | %-20s | %-20s | %s%n", "SNAPSHOT", "CREATION TIME", "TTL IN SEC",
        "TABLE NAME");
      for (SnapshotDescription desc : getSnapshotList(conf)) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
        "TABLE NAME");
      for (SnapshotDescription desc : getSnapshotList(conf)) {
        System.out.printf("%-20s | %20s | %20s | %s%n", desc.getName(),
          df.format(new Date(desc.getCreationTime())), desc.getTtl(), desc.getTableNameAsString());
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    // Load snapshot information
    if (!loadSnapshotInfo(snapshotName)) {
      System.err.println("Snapshot '" + snapshotName + "' not found!");
      return 1;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    SnapshotProtos.SnapshotDescription snapshotDesc = snapshotManifest.getSnapshotDescription();
    SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss");
    System.out.println("Snapshot Info");
    System.out.println("----------------------------------------");
    System.out.println("   Name: " + snapshotDesc.getName());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss");
    System.out.println("Snapshot Info");
    System.out.println("----------------------------------------");
    System.out.println("   Name: " + snapshotDesc.getName());
    System.out.println("   Type: " + snapshotDesc.getType());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("Snapshot Info");
    System.out.println("----------------------------------------");
    System.out.println("   Name: " + snapshotDesc.getName());
    System.out.println("   Type: " + snapshotDesc.getType());
    System.out.println("  Table: " + snapshotDesc.getTable());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("----------------------------------------");
    System.out.println("   Name: " + snapshotDesc.getName());
    System.out.println("   Type: " + snapshotDesc.getType());
    System.out.println("  Table: " + snapshotDesc.getTable());
    System.out.println(" Format: " + snapshotDesc.getVersion());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("   Name: " + snapshotDesc.getName());
    System.out.println("   Type: " + snapshotDesc.getType());
    System.out.println("  Table: " + snapshotDesc.getTable());
    System.out.println(" Format: " + snapshotDesc.getVersion());
    System.out.println("Created: " + df.format(new Date(snapshotDesc.getCreationTime())));
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("   Type: " + snapshotDesc.getType());
    System.out.println("  Table: " + snapshotDesc.getTable());
    System.out.println(" Format: " + snapshotDesc.getVersion());
    System.out.println("Created: " + df.format(new Date(snapshotDesc.getCreationTime())));
    System.out.println("    Ttl: " + snapshotDesc.getTtl());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("  Table: " + snapshotDesc.getTable());
    System.out.println(" Format: " + snapshotDesc.getVersion());
    System.out.println("Created: " + df.format(new Date(snapshotDesc.getCreationTime())));
    System.out.println("    Ttl: " + snapshotDesc.getTtl());
    System.out.println("  Owner: " + snapshotDesc.getOwner());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println(" Format: " + snapshotDesc.getVersion());
    System.out.println("Created: " + df.format(new Date(snapshotDesc.getCreationTime())));
    System.out.println("    Ttl: " + snapshotDesc.getTtl());
    System.out.println("  Owner: " + snapshotDesc.getOwner());
    System.out.println();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("Created: " + df.format(new Date(snapshotDesc.getCreationTime())));
    System.out.println("    Ttl: " + snapshotDesc.getTtl());
    System.out.println("  Owner: " + snapshotDesc.getOwner());
    System.out.println();
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("    Ttl: " + snapshotDesc.getTtl());
    System.out.println("  Owner: " + snapshotDesc.getOwner());
    System.out.println();
  }

```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  protected void printUsage() {
    printUsage("hbase snapshot info [options]", "Options:", "");
    System.err.println("Examples:");
    System.err.println("  hbase snapshot info --snapshot MySnapshot --files");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    printUsage("hbase snapshot info [options]", "Options:", "");
    System.err.println("Examples:");
    System.err.println("  hbase snapshot info --snapshot MySnapshot --files");
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
          if (showFiles) {
            String state = info.getStateToString();
            System.out.printf("%8s %s/%s/%s/%s %s%n",
              (info.isMissing() ? "-" : fileSizeToString(info.getSize())), table,
              regionInfo.getEncodedName(), family, storeFile.getName(),
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private void printFiles(final boolean showFiles, final boolean showStats) throws IOException {
    if (showFiles) {
      System.out.println("Snapshot Files");
      System.out.println("----------------------------------------");
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    if (showFiles) {
      System.out.println("Snapshot Files");
      System.out.println("----------------------------------------");
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java

    // Dump the stats
    System.out.println();
    if (stats.isSnapshotCorrupted()) {
      System.out.println("**************************************************************");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println();
    if (stats.isSnapshotCorrupted()) {
      System.out.println("**************************************************************");
      System.out.printf("BAD SNAPSHOT: %d hfile(s) and %d log(s) missing.%n",
        stats.getMissingStoreFilesCount(), stats.getMissingLogsCount());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    if (stats.isSnapshotCorrupted()) {
      System.out.println("**************************************************************");
      System.out.printf("BAD SNAPSHOT: %d hfile(s) and %d log(s) missing.%n",
        stats.getMissingStoreFilesCount(), stats.getMissingLogsCount());
      System.out.printf("              %d hfile(s) corrupted.%n",
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
      System.out.printf("BAD SNAPSHOT: %d hfile(s) and %d log(s) missing.%n",
        stats.getMissingStoreFilesCount(), stats.getMissingLogsCount());
      System.out.printf("              %d hfile(s) corrupted.%n",
        stats.getCorruptedStoreFilesCount());
      System.out.println("**************************************************************");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
      System.out.printf("              %d hfile(s) corrupted.%n",
        stats.getCorruptedStoreFilesCount());
      System.out.println("**************************************************************");
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java

    if (showStats) {
      System.out.printf(
        "%d HFiles (%d in archive, %d in mob storage), total size %s "
          + "(%.2f%% %s shared with the source table, %.2f%% %s in mob dir)%n",
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
        stats.getSharedStoreFilePercentage(), fileSizeToString(stats.getSharedStoreFilesSize()),
        stats.getMobStoreFilePercentage(), fileSizeToString(stats.getMobStoreFilesSize()));
      System.out.printf("%d Logs, total size %s%n", stats.getLogsCount(),
        fileSizeToString(stats.getLogsSize()));
      System.out.println();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
      System.out.printf("%d Logs, total size %s%n", stats.getLogsCount(),
        fileSizeToString(stats.getLogsSize()));
      System.out.println();
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
   */
  private void printSchema() {
    System.out.println("Table Descriptor");
    System.out.println("----------------------------------------");
    System.out.println(snapshotManifest.getTableDescriptor().toString());
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private void printSchema() {
    System.out.println("Table Descriptor");
    System.out.println("----------------------------------------");
    System.out.println(snapshotManifest.getTableDescriptor().toString());
    System.out.println();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("Table Descriptor");
    System.out.println("----------------------------------------");
    System.out.println(snapshotManifest.getTableDescriptor().toString());
    System.out.println();
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
    System.out.println("----------------------------------------");
    System.out.println(snapshotManifest.getTableDescriptor().toString());
    System.out.println();
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/WALProcedurePrettyPrinter.java`
#### Snippet
```java

  public WALProcedurePrettyPrinter() {
    this(System.out);
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/HFileProcedurePrettyPrinter.java`
#### Snippet
```java

  public HFileProcedurePrettyPrinter() {
    this(System.out);
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
      if (!liveRegionServers.contains(sn)) {
        List<String> replicationQueues = mgr.getQueueStorage().getAllQueues(sn);
        System.out.println(sn + " is dead, claim its replication queues: " + replicationQueues);
        for (String queue : replicationQueues) {
          mgr.claimQueue(sn, queue);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
      Path logDir = new Path(walRootDir, HConstants.HREGION_LOGDIR_NAME);

      System.out.println("Start Replication Server start");
      Replication replication = new Replication();
      replication.initialize(new DummyServer(zkw), fs, logDir, oldLogDir,
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
      manager.join();
    } catch (InterruptedException e) {
      System.err.println("didn't wait long enough:" + e);
      return -1;
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
  protected static void printUsage(final String className, final String message) {
    if (message != null && message.length() > 0) {
      System.err.println(message);
    }
    System.err.println("Usage: hbase " + className + " \\");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      System.err.println(message);
    }
    System.err.println("Usage: hbase " + className + " \\");
    System.err.println("  <OPTIONS> [-D<property=value>]*");
    System.err.println();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    }
    System.err.println("Usage: hbase " + className + " \\");
    System.err.println("  <OPTIONS> [-D<property=value>]*");
    System.err.println();
    System.err.println("General Options:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    System.err.println("Usage: hbase " + className + " \\");
    System.err.println("  <OPTIONS> [-D<property=value>]*");
    System.err.println();
    System.err.println("General Options:");
    System.err.println(" -h|--h|--help  Show this help and exit.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    System.err.println("  <OPTIONS> [-D<property=value>]*");
    System.err.println();
    System.err.println("General Options:");
    System.err.println(" -h|--h|--help  Show this help and exit.");
    System.err.println(" --distributed  Poll each RS and print its own replication queue. "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    System.err.println();
    System.err.println("General Options:");
    System.err.println(" -h|--h|--help  Show this help and exit.");
    System.err.println(" --distributed  Poll each RS and print its own replication queue. "
      + "Default only polls ZooKeeper");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    System.err.println("General Options:");
    System.err.println(" -h|--h|--help  Show this help and exit.");
    System.err.println(" --distributed  Poll each RS and print its own replication queue. "
      + "Default only polls ZooKeeper");
    System.err.println(" --hdfs         Use HDFS to calculate usage of WALs by replication."
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    System.err.println(" --distributed  Poll each RS and print its own replication queue. "
      + "Default only polls ZooKeeper");
    System.err.println(" --hdfs         Use HDFS to calculate usage of WALs by replication."
      + " It could be overestimated if replicating to multiple peers."
      + " --distributed flag is also needed.");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      }

      System.out.println("Dumping replication peers and configurations:");
      System.out.println(dumpPeersState(peers));

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java

      System.out.println("Dumping replication peers and configurations:");
      System.out.println(dumpPeersState(peers));

      if (opts.isDistributed()) {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        Set<String> peerIds =
          peers.stream().map((peer) -> peer.getPeerId()).collect(Collectors.toSet());
        System.out.println(dumpQueues(zkw, peerIds, opts.isHdfs()));
        System.out.println(dumpReplicationSummary());
      } else {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
          peers.stream().map((peer) -> peer.getPeerId()).collect(Collectors.toSet());
        System.out.println(dumpQueues(zkw, peerIds, opts.isHdfs()));
        System.out.println(dumpReplicationSummary());
      } else {
        // use ZK instead
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      } else {
        // use ZK instead
        System.out.print("Dumping replication znodes via ZooKeeper:");
        System.out.println(ZKDump.getReplicationZnodesDump(zkw));
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        // use ZK instead
        System.out.print("Dumping replication znodes via ZooKeeper:");
        System.out.println(ZKDump.getReplicationZnodesDump(zkw));
      }
      return (0);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerIdGenerator.java`
#### Snippet
```java
    ScannerIdGenerator sig = new ScannerIdGenerator(ServerName.valueOf("a.example.org,1234,5678"));
    for (int i = 0; i < 10; i++) {
      System.out.println(sig.generateNewScannerId());
    }
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java`
#### Snippet
```java
      return start();
    } else if ("stop".equals(cmd)) {
      System.err.println("To shutdown the regionserver run "
        + "hbase-daemon.sh stop regionserver or send a kill signal to " + "the regionserver pid");
      return 1;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DumpRegionServerMetrics.java`
#### Snippet
```java
  public static void main(String[] args) throws IOException, MalformedObjectNameException {
    String str = dumpMetrics();
    System.out.println(str);
  }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        + " and does not wait for any running shutdown hooks or finalizers to finish their work."
        + " Thread dump to stdout.");
      Threads.printThreadInfo(System.out, "Zombie HRegionServer");
      Runtime.getRuntime().halt(1);
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java

  private static void printHelp() {
    System.err.println("usage: Compressor <input> <output>");
    System.err.println("If <input> WAL is compressed, <output> will be decompressed.");
    System.err.println("If <input> WAL is uncompressed, <output> will be compressed.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
  private static void printHelp() {
    System.err.println("usage: Compressor <input> <output>");
    System.err.println("If <input> WAL is compressed, <output> will be decompressed.");
    System.err.println("If <input> WAL is uncompressed, <output> will be compressed.");
    return;
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
    System.err.println("usage: Compressor <input> <output>");
    System.err.println("If <input> WAL is compressed, <output> will be decompressed.");
    System.err.println("If <input> WAL is uncompressed, <output> will be compressed.");
    return;
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
    try {
      if (!(in instanceof ReaderBase)) {
        System.err.println("Cannot proceed, invalid reader type: " + in.getClass().getName());
        return;
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
          split(conf, logPath);
        } catch (IOException t) {
          t.printStackTrace(System.err);
          System.exit(-1);
        }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java

  private static void usage() {
    System.err.println("Usage: AbstractFSWAL <ARGS>");
    System.err.println("Arguments:");
    System.err.println(" --dump  Dump textual representation of passed one or more files");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
  private static void usage() {
    System.err.println("Usage: AbstractFSWAL <ARGS>");
    System.err.println("Arguments:");
    System.err.println(" --dump  Dump textual representation of passed one or more files");
    System.err.println("         For example: "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    System.err.println("Usage: AbstractFSWAL <ARGS>");
    System.err.println("Arguments:");
    System.err.println(" --dump  Dump textual representation of passed one or more files");
    System.err.println("         For example: "
      + "AbstractFSWAL --dump hdfs://example.com:9000/hbase/WALs/MACHINE/LOGFILE");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    System.err.println("Arguments:");
    System.err.println(" --dump  Dump textual representation of passed one or more files");
    System.err.println("         For example: "
      + "AbstractFSWAL --dump hdfs://example.com:9000/hbase/WALs/MACHINE/LOGFILE");
    System.err.println(" --split Split the passed directory of WAL logs");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    System.err.println("         For example: "
      + "AbstractFSWAL --dump hdfs://example.com:9000/hbase/WALs/MACHINE/LOGFILE");
    System.err.println(" --split Split the passed directory of WAL logs");
    System.err.println(
      "         For example: AbstractFSWAL --split hdfs://example.com:9000/hbase/WALs/DIR");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
      + "AbstractFSWAL --dump hdfs://example.com:9000/hbase/WALs/MACHINE/LOGFILE");
    System.err.println(" --split Split the passed directory of WAL logs");
    System.err.println(
      "         For example: AbstractFSWAL --split hdfs://example.com:9000/hbase/WALs/DIR");
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
  private String tableName;
  private Path path;
  private PrintStream err = System.err;
  private PrintStream out = System.out;

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
  private Path path;
  private PrintStream err = System.err;
  private PrintStream out = System.out;

  public StoreFileListFilePrettyPrinter() {
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
    // Check if backup is enabled
    if (!BackupManager.isBackupEnabled(getConf())) {
      System.err.println(BackupRestoreConstants.ENABLE_BACKUP);
      return -1;
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java

    if (cmd.hasOption(OPTION_SET) && cmd.hasOption(OPTION_TABLE)) {
      System.err.println(
        "Options -s and -t are mutaully exclusive," + " you can not specify both of them.");
      printToolUsage();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java

    if (!cmd.hasOption(OPTION_SET) && !cmd.hasOption(OPTION_TABLE)) {
      System.err.println("You have to specify either set name or table list to restore");
      printToolUsage();
      return -1;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
          tables = getTablesForSet(conn, setName);
        } catch (IOException e) {
          System.out.println("ERROR: " + e.getMessage() + " for setName=" + setName);
          printToolUsage();
          return -2;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
        }
        if (tables == null) {
          System.out
            .println("ERROR: Backup set '" + setName + "' is either empty or does not exist");
          printToolUsage();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
        sTableArray != null && tTableArray != null && (sTableArray.length != tTableArray.length)
      ) {
        System.out.println("ERROR: table mapping mismatch: " + tables + " : " + tableMapping);
        printToolUsage();
        return -4;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
      cmdLineArgs = args;
    } catch (Exception e) {
      System.out.println("Error when parsing command-line arguments: " + e.getMessage());
      printToolUsage();
      return EXIT_FAILURE;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java

  protected void printToolUsage() {
    System.out.println(USAGE_STRING);
    HelpFormatter helpFormatter = new HelpFormatter();
    helpFormatter.setLeftPadding(2);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java
    helpFormatter.setSyntaxPrefix("Options:");
    helpFormatter.printHelp(" ", null, options, USAGE_FOOTER);
    System.out.println(BackupRestoreConstants.VERIFY_BACKUP);
  }
}
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
      cmdLineArgs = args;
    } catch (Exception e) {
      System.err.println("Error when parsing command-line arguments: " + e.getMessage());
      printToolUsage();
      return EXIT_FAILURE;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java

  protected void printToolUsage() throws IOException {
    System.out.println(BackupCommands.USAGE);
    System.out.println(BackupRestoreConstants.VERIFY_BACKUP);
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
  protected void printToolUsage() throws IOException {
    System.out.println(BackupCommands.USAGE);
    System.out.println(BackupRestoreConstants.VERIFY_BACKUP);
  }
}
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
    // Check if backup is enabled
    if (!BackupManager.isBackupEnabled(getConf())) {
      System.err.println(BackupRestoreConstants.ENABLE_BACKUP);
      return -1;
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
      type = BackupCommand.MERGE;
    } else {
      System.out.println("Unsupported command for backup: " + cmd);
      printToolUsage();
      return -1;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(REPAIR_CMD_USAGE);
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(PROGRESS_CMD_USAGE);
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(MERGE_CMD_USAGE);
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        if (fs.exists(path)) {
          if (!fs.delete(path, true)) {
            System.out.println("MERGE repair removing: " + path + " - FAILED");
          } else {
            System.out.println("MERGE repair removing: " + path + " - OK");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
            System.out.println("MERGE repair removing: " + path + " - FAILED");
          } else {
            System.out.println("MERGE repair removing: " + path + " - OK");
          }
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        String[] backupIds = convertToBackupIds(history);
        int deleted = admin.deleteBackups(backupIds);
        System.out.println("Deleted " + deleted + " backups. Total older than " + days + " days: "
          + backupIds.length);
      } catch (IOException e) {
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          + backupIds.length);
      } catch (IOException e) {
        System.err.println("Delete command FAILED. Please run backup repair tool to restore backup "
          + "system integrity");
        throw e;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(DESCRIBE_CMD_USAGE);
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      }
      for (BackupInfo info : history) {
        System.out.println(info.getShortDescription());
      }
    }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        BackupInfo info = sysTable.readBackupInfo(backupId);
        if (info == null) {
          System.out.println("ERROR: " + backupId + " does not exist");
          printUsage();
          throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          throw new IOException(INCORRECT_USAGE);
        }
        System.out.println(info.getShortDescription());
      }
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

          if (sessions.size() > 0) {
            System.err.println("Found backup session in a RUNNING state: ");
            System.err.println(sessions.get(0));
            System.err.println("This may indicate that a previous session has failed abnormally.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          if (sessions.size() > 0) {
            System.err.println("Found backup session in a RUNNING state: ");
            System.err.println(sessions.get(0));
            System.err.println("This may indicate that a previous session has failed abnormally.");
            System.err.println("In this case, backup recovery is recommended.");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
            System.err.println("Found backup session in a RUNNING state: ");
            System.err.println(sessions.get(0));
            System.err.println("This may indicate that a previous session has failed abnormally.");
            System.err.println("In this case, backup recovery is recommended.");
            throw new IOException("Active session found, aborted command execution");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
            System.err.println(sessions.get(0));
            System.err.println("This may indicate that a previous session has failed abnormally.");
            System.err.println("In this case, backup recovery is recommended.");
            throw new IOException("Active session found, aborted command execution");
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

          if (ids != null && ids.length > 0) {
            System.err.println("Found failed backup DELETE coommand. ");
            System.err.println("Backup system recovery is required.");
            throw new IOException("Failed backup DELETE found, aborted command execution");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          if (ids != null && ids.length > 0) {
            System.err.println("Found failed backup DELETE coommand. ");
            System.err.println("Backup system recovery is required.");
            throw new IOException("Failed backup DELETE found, aborted command execution");
          }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          ids = table.getListOfBackupIdsFromMergeOperation();
          if (ids != null && ids.length > 0) {
            System.err.println("Found failed backup MERGE coommand. ");
            System.err.println("Backup system recovery is required.");
            throw new IOException("Failed backup MERGE found, aborted command execution");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          if (ids != null && ids.length > 0) {
            System.err.println("Found failed backup MERGE coommand. ");
            System.err.println("Backup system recovery is required.");
            throw new IOException("Failed backup MERGE found, aborted command execution");
          }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(HISTORY_CMD_USAGE);
      Options options = new Options();
      options.addOption(OPTION_RECORD_NUMBER, true, OPTION_RECORD_NUMBER_DESC);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        List<BackupSet> list = admin.listBackupSets();
        for (BackupSet bs : list) {
          System.out.println(bs);
        }
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        return TableName.valueOf(value);
      } catch (IllegalArgumentException e) {
        System.out.println("Illegal argument for table name: " + value);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        return new Path(value);
      } catch (IllegalArgumentException e) {
        System.out.println("ERROR: Illegal argument for backup root path: " + value);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

      if (args.length != 2) {
        System.out.println("ERROR: Only supports help message of a single command type");
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

      if (BackupCommand.CREATE.name().equalsIgnoreCase(type)) {
        System.out.println(CREATE_CMD_USAGE);
      } else if (BackupCommand.DESCRIBE.name().equalsIgnoreCase(type)) {
        System.out.println(DESCRIBE_CMD_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println(CREATE_CMD_USAGE);
      } else if (BackupCommand.DESCRIBE.name().equalsIgnoreCase(type)) {
        System.out.println(DESCRIBE_CMD_USAGE);
      } else if (BackupCommand.HISTORY.name().equalsIgnoreCase(type)) {
        System.out.println(HISTORY_CMD_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println(DESCRIBE_CMD_USAGE);
      } else if (BackupCommand.HISTORY.name().equalsIgnoreCase(type)) {
        System.out.println(HISTORY_CMD_USAGE);
      } else if (BackupCommand.PROGRESS.name().equalsIgnoreCase(type)) {
        System.out.println(PROGRESS_CMD_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println(HISTORY_CMD_USAGE);
      } else if (BackupCommand.PROGRESS.name().equalsIgnoreCase(type)) {
        System.out.println(PROGRESS_CMD_USAGE);
      } else if (BackupCommand.DELETE.name().equalsIgnoreCase(type)) {
        System.out.println(DELETE_CMD_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println(PROGRESS_CMD_USAGE);
      } else if (BackupCommand.DELETE.name().equalsIgnoreCase(type)) {
        System.out.println(DELETE_CMD_USAGE);
      } else if (BackupCommand.SET.name().equalsIgnoreCase(type)) {
        System.out.println(SET_CMD_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println(DELETE_CMD_USAGE);
      } else if (BackupCommand.SET.name().equalsIgnoreCase(type)) {
        System.out.println(SET_CMD_USAGE);
      } else {
        System.out.println("Unknown command : " + type);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println(SET_CMD_USAGE);
      } else {
        System.out.println("Unknown command : " + type);
        printUsage();
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(SET_CMD_USAGE);
    }
  }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      String[] args = cmdline == null ? null : cmdline.getArgs();
      if (args == null || (args.length != 2)) {
        System.err
          .println("ERROR: wrong number of arguments: " + (args == null ? null : args.length));
        printUsage();
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        String msg = "ERROR: can not merge a single backup image. "
          + "Number of images must be greater than 1.";
        System.err.println(msg);
        throw new IOException(msg);

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      String[] backupIds = sysTable.getListOfBackupIdsFromDeleteOperation();
      if (backupIds == null || backupIds.length == 0) {
        System.out.println("No failed backup DELETE operation found");
        // Delete backup table snapshot if exists
        BackupSystemTable.deleteSnapshot(conn);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        return;
      }
      System.out.println("Found failed DELETE operation for: " + StringUtils.join(backupIds));
      System.out.println("Running DELETE again ...");
      // Restore table from snapshot
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      }
      System.out.println("Found failed DELETE operation for: " + StringUtils.join(backupIds));
      System.out.println("Running DELETE again ...");
      // Restore table from snapshot
      BackupSystemTable.restoreFromSnapshot(conn);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        admin.deleteBackups(backupIds);
      }
      System.out.println("DELETE operation finished OK: " + StringUtils.join(backupIds));
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

      if (cmdline == null || cmdline.getArgs() == null || cmdline.getArgs().length == 1) {
        System.out.println(
          "No backup id was specified, " + "will retrieve the most recent (ongoing) session");
      }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      String[] args = cmdline == null ? null : cmdline.getArgs();
      if (args != null && args.length > 2) {
        System.err.println("ERROR: wrong number of arguments: " + args.length);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
            info = infos.get(0);
            backupId = info.getBackupId();
            System.out.println("Found ongoing session with backupId=" + backupId);
          }
        }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        if (progress < 0) {
          if (backupId != null) {
            System.out.println(NO_INFO_FOUND + backupId);
          } else {
            System.err.println(NO_ACTIVE_SESSION_FOUND);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
            System.out.println(NO_INFO_FOUND + backupId);
          } else {
            System.err.println(NO_ACTIVE_SESSION_FOUND);
          }
        } else {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          }
        } else {
          System.out.println(backupId + " progress=" + progress + "%");
        }
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(CREATE_CMD_USAGE);
      Options options = new Options();
      options.addOption(OPTION_WORKERS, true, OPTION_WORKERS_DESC);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          && !BackupType.INCREMENTAL.toString().equalsIgnoreCase(args[1])
      ) {
        System.out.println("ERROR: invalid backup type: " + args[1]);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      }
      if (!verifyPath(args[2])) {
        System.out.println("ERROR: invalid backup destination: " + args[2]);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      // Check if we have both: backup set and list of tables
      if (cmdline.hasOption(OPTION_TABLE) && cmdline.hasOption(OPTION_SET)) {
        System.out
          .println("ERROR: You can specify either backup set or list" + " of tables, but not both");
        printUsage();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

        if (tables == null) {
          System.out
            .println("ERROR: Backup set '" + setName + "' is either empty or does not exist");
          printUsage();
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          .withBandwidthPerTasks(bandwidth).withBackupSetName(setName).build();
        String backupId = admin.backupTables(request);
        System.out.println("Backup session " + backupId + " finished. Status: SUCCESS");
      } catch (IOException e) {
        System.out.println("Backup session finished. Status: FAILURE");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println("Backup session " + backupId + " finished. Status: SUCCESS");
      } catch (IOException e) {
        System.out.println("Backup session finished. Status: FAILURE");
        throw e;
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          return BackupCommand.SET_LIST;
        default:
          System.out.println("ERROR: Unknown command for 'set' :" + cmdStr);
          printUsage();
          throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        return Integer.parseInt(value);
      } catch (NumberFormatException e) {
        System.out.println("Illegal argument for history length: " + value);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(USAGE);
    }
  }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        boolean result = admin.deleteBackupSet(setName);
        if (result) {
          System.out.println("Delete set " + setName + " OK.");
        } else {
          System.out.println("Set " + setName + " does not exist");
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          System.out.println("Delete set " + setName + " OK.");
        } else {
          System.out.println("Set " + setName + " does not exist");
        }
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        BackupSet set = tables == null ? null : new BackupSet(setName, tables);
        if (set == null) {
          System.out.println("Set '" + setName + "' does not exist.");
        } else {
          System.out.println(set);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          System.out.println("Set '" + setName + "' does not exist.");
        } else {
          System.out.println(set);
        }
      }
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      try (BackupAdminImpl admin = new BackupAdminImpl(conn)) {
        int deleted = admin.deleteBackups(backupIds);
        System.out.println("Deleted " + deleted + " backups. Total requested: " + backupIds.length);
      } catch (IOException e) {
        System.err.println("Delete command FAILED. Please run backup repair tool to restore backup "
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        System.out.println("Deleted " + deleted + " backups. Total requested: " + backupIds.length);
      } catch (IOException e) {
        System.err.println("Delete command FAILED. Please run backup repair tool to restore backup "
          + "system integrity");
        throw e;
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      String[] backupIds = sysTable.getListOfBackupIdsFromMergeOperation();
      if (backupIds == null || backupIds.length == 0) {
        System.out.println("No failed backup MERGE operation found");
        // Delete backup table snapshot if exists
        BackupSystemTable.deleteSnapshot(conn);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        return;
      }
      System.out.println("Found failed MERGE operation for: " + StringUtils.join(backupIds));
      // Check if backup .tmp exists
      BackupInfo bInfo = sysTable.readBackupInfo(backupIds[0]);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        Path destPath = HBackupFileSystem.getBackupPath(backupRoot, backupId);
        if (!fs.delete(destPath, true)) {
          System.out.println("Failed to delete " + destPath);
        }
        boolean res = fs.rename(tmpPath, destPath);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
            "MERGE repair: failed  to rename from " + tmpPath + " to " + destPath);
        }
        System.out
          .println("MERGE repair: renamed from " + tmpPath + " to " + destPath + " res=" + res);
      } else {
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      sysTable.finishMergeOperation();

      System.out.println("MERGE repair operation finished OK: " + StringUtils.join(backupIds));
    }

```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    @Override
    protected void printUsage() {
      System.out.println(DELETE_CMD_USAGE);
      Options options = new Options();
      options.addOption(OPTION_KEEP, true, OPTION_KEEP_DESC);
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      String[] args = cmdline == null ? null : cmdline.getArgs();
      if (args != null && args.length > 1) {
        System.err.println("ERROR: wrong number of arguments: " + args.length);
        printUsage();
        throw new IOException(INCORRECT_USAGE);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        if (list.size() == 0) {
          // No failed sessions found
          System.out.println("REPAIR status: no failed sessions found."
            + " Checking failed delete backup operation ...");
          repairFailedBackupDeletionIfAny(conn, sysTable);
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
          + backupInfo.getStartTs() + ",failedts=" + backupInfo.getCompleteTs() + ",failedphase="
          + backupInfo.getPhase() + ",failedmessage=" + backupInfo.getFailedMsg();
        System.out.println(backupFailedData);
        TableBackupClient.cleanupAndRestoreBackupSystem(conn, backupInfo, conf);
        // If backup session is updated to FAILED state - means we
```

### SystemOutErr
Uses of `System.out` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        sysTable.updateBackupInfo(backupInfo);
        sysTable.finishBackupExclusiveOperation();
        System.out.println("REPAIR status: finished repair failed session:\n " + backupInfo);
      }
    }
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
  private void usage(final String errorMsg) {
    if (errorMsg != null && errorMsg.length() > 0) {
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: " + NAME + " [options] <HFile inputdir(s)> <table>");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
      System.err.println("ERROR: " + errorMsg);
    }
    System.err.println("Usage: " + NAME + " [options] <HFile inputdir(s)> <table>");
    System.err.println("Read all HFile's for <table> and split them to <table> region boundaries.");
    System.err.println("<table>  table to load.\n");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    }
    System.err.println("Usage: " + NAME + " [options] <HFile inputdir(s)> <table>");
    System.err.println("Read all HFile's for <table> and split them to <table> region boundaries.");
    System.err.println("<table>  table to load.\n");
    System.err.println("To generate HFiles for a bulk data load, pass the option:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    System.err.println("Usage: " + NAME + " [options] <HFile inputdir(s)> <table>");
    System.err.println("Read all HFile's for <table> and split them to <table> region boundaries.");
    System.err.println("<table>  table to load.\n");
    System.err.println("To generate HFiles for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    System.err.println("Read all HFile's for <table> and split them to <table> region boundaries.");
    System.err.println("<table>  table to load.\n");
    System.err.println("To generate HFiles for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("Other options:");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    System.err.println("<table>  table to load.\n");
    System.err.println("To generate HFiles for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("Other options:");
    System.err.println("   -D " + JOB_NAME_CONF_KEY
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    System.err.println("To generate HFiles for a bulk data load, pass the option:");
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("Other options:");
    System.err.println("   -D " + JOB_NAME_CONF_KEY
      + "=jobName - use the specified mapreduce job name for the HFile splitter");
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    System.err.println("  -D" + BULK_OUTPUT_CONF_KEY + "=/path/for/output");
    System.err.println("Other options:");
    System.err.println("   -D " + JOB_NAME_CONF_KEY
      + "=jobName - use the specified mapreduce job name for the HFile splitter");
    System.err.println("For performance also consider the following options:\n"
```

### SystemOutErr
Uses of `System.err` should probably be replaced with more robust logging
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    System.err.println("   -D " + JOB_NAME_CONF_KEY
      + "=jobName - use the specified mapreduce job name for the HFile splitter");
    System.err.println("For performance also consider the following options:\n"
      + "  -Dmapreduce.map.speculative=false\n" + "  -Dmapreduce.reduce.speculative=false");
  }
```

## RuleId[id=DynamicRegexReplaceableByCompiledPattern]
### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java
  static String[] getDefaultCipherSuitesForJavaVersion(String javaVersion) {
    Objects.requireNonNull(javaVersion);
    if (javaVersion.matches("\\d+")) {
      // Must be Java 9 or later
      LOG.debug("Using Java9+ optimized cipher suites for Java version {}", javaVersion);
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Addressing.java`
#### Snippet
```java
  public static String inetSocketAddress2String(InetSocketAddress address) {
    return address.isUnresolved()
      ? address.toString().replace("/<unresolved>", "")
      : address.toString();
  }
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java

    if (partsArray.length == 3) {
      if (!partsArray[2].matches("/.*[^/]")) {
        throw new IOException("Cluster key passed " + key + " is invalid, the format should be:"
          + HConstants.ZOOKEEPER_QUORUM + ":" + HConstants.ZOOKEEPER_CLIENT_PORT + ":"
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
      // The quorum could contain client port in server:clientport format, try to transform more.
      String zNodeParent = partsArray[partsArray.length - 1];
      if (!zNodeParent.matches("/.*[^/]")) {
        throw new IOException("Cluster key passed " + key + " is invalid, the format should be:"
          + HConstants.ZOOKEEPER_QUORUM + ":" + HConstants.ZOOKEEPER_CLIENT_PORT + ":"
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
   */
  public static String getZooKeeperClusterKey(Configuration conf, String name) {
    String ensemble = conf.get(HConstants.ZOOKEEPER_QUORUM).replaceAll("[\\t\\n\\x0B\\f\\r]", "");
    StringBuilder builder = new StringBuilder(ensemble);
    builder.append(":");
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java`
#### Snippet
```java

    public static String fuzzyNormalize(String name) {
      return name != null ? name.replaceAll("-", "").trim().toLowerCase(Locale.ROOT) : "";
    }

```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
      StringBuilder kvString = new StringBuilder();
      for (Map.Entry<String, String> e : cp.getProperties().entrySet()) {
        if (!e.getKey().matches(CP_HTD_ATTR_VALUE_PARAM_KEY_PATTERN)) {
          throw new IOException("Illegal parameter key = " + e.getKey());
        }
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
          throw new IOException("Illegal parameter key = " + e.getKey());
        }
        if (!e.getValue().matches(CP_HTD_ATTR_VALUE_PARAM_VALUE_PATTERN)) {
          throw new IOException("Illegal parameter (" + e.getKey() + ") value = " + e.getValue());
        }
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
  public MultiRowResource getMultipleRowResource(final @QueryParam("v") String versions,
    @PathParam("multiget") String path) throws IOException {
    return new MultiRowResource(this, versions, path.replace("multiget", "").replace("/", ""));
  }

```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
  public MultiRowResource getMultipleRowResource(final @QueryParam("v") String versions,
    @PathParam("multiget") String path) throws IOException {
    return new MultiRowResource(this, versions, path.replace("multiget", "").replace("/", ""));
  }

```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/WrapperAsyncFSOutput.java`
#### Snippet
```java
    this.out = out;
    this.executor = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true)
      .setNameFormat("AsyncFSOutputFlusher-" + file.toString().replace("%", "%%")).build());
  }

```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
      }

      String id = getClass().getSimpleName() + UUID.randomUUID().toString().replace("-", "");
      Job job = null;
      Scan scan = new Scan();
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/JarFinder.java`
#### Snippet
```java
    ClassLoader loader = klass.getClassLoader();
    if (loader != null) {
      String class_file = klass.getName().replaceAll("\\.", "/") + ".class";
      try {
        for (Enumeration itr = loader.getResources(class_file); itr.hasMoreElements();) {
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/JarFinder.java`
#### Snippet
```java
          if ("jar".equals(url.getProtocol())) {
            path = URLDecoder.decode(path, "UTF-8");
            return path.replaceAll("!.*$", "");
          } else if ("file".equals(url.getProtocol())) {
            String klassName = klass.getName();
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/JarFinder.java`
#### Snippet
```java
          } else if ("file".equals(url.getProtocol())) {
            String klassName = klass.getName();
            klassName = klassName.replace(".", "/") + ".class";
            path = path.substring(0, path.length() - klassName.length());
            File baseDir = new File(path);
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
    ClassLoader loader = my_class.getClassLoader();

    String class_file = my_class.getName().replaceAll("\\.", "/") + ".class";

    if (loader != null) {
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
          // either unencoded or encoded as "%20"). Replace +s first, so
          // that they are kept sacred during the decoding process.
          toReturn = toReturn.replaceAll("\\+", "%2B");
          toReturn = URLDecoder.decode(toReturn, "UTF-8");
          return toReturn.replaceAll("!.*$", "");
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java`
#### Snippet
```java
      return null;
    }
    if (callbackName.matches("[A-Za-z0-9_]+")) {
      return callbackName;
    }
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
          toReturn = toReturn.replaceAll("\\+", "%2B");
          toReturn = URLDecoder.decode(toReturn, "UTF-8");
          return toReturn.replaceAll("!.*$", "");
        }
      }
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java`
#### Snippet
```java
  // knowledge should be contained
  private static final Pattern prefetchPathExclude =
    Pattern.compile("(" + Path.SEPARATOR_CHAR + HConstants.HBASE_TEMP_DIRECTORY.replace(".", "\\.")
      + Path.SEPARATOR_CHAR + ")|(" + Path.SEPARATOR_CHAR
      + HConstants.HREGION_COMPACTIONDIR_NAME.replace(".", "\\.") + Path.SEPARATOR_CHAR + ")");
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java`
#### Snippet
```java
    Pattern.compile("(" + Path.SEPARATOR_CHAR + HConstants.HBASE_TEMP_DIRECTORY.replace(".", "\\.")
      + Path.SEPARATOR_CHAR + ")|(" + Path.SEPARATOR_CHAR
      + HConstants.HREGION_COMPACTIONDIR_NAME.replace(".", "\\.") + Path.SEPARATOR_CHAR + ")");

  public static void request(Path path, Runnable runnable) {
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
   */
  private static String asSeparateLines(String keyValueStr) {
    return keyValueStr.replaceAll(", ([a-zA-Z]+=)", ",\n" + FOUR_SPACES + "$1");
  }

```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    if (bloomFilter != null) {
      out.println(FOUR_SPACES
        + bloomFilter.toString().replaceAll(BloomFilterUtil.STATS_RECORD_SEP, "\n" + FOUR_SPACES));
    } else {
      out.println(FOUR_SPACES + "Not present");
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    if (bloomFilter != null) {
      out.println(FOUR_SPACES
        + bloomFilter.toString().replaceAll(BloomFilterUtil.STATS_RECORD_SEP, "\n" + FOUR_SPACES));
    } else {
      out.println(FOUR_SPACES + "Not present");
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          HConstants.DEFAULT_SNAPSHOT_RESTORE_FAILSAFE_NAME);
      final String failSafeSnapshotSnapshotName =
        failSafeSnapshotSnapshotNameFormat.replace("{snapshot.name}", snapshotName)
          .replace("{table.name}", tableName.toString().replace(TableName.NAMESPACE_DELIM, '.'))
          .replace("{restore.timestamp}", String.valueOf(EnvironmentEdgeManager.currentTime()));
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      final String failSafeSnapshotSnapshotName =
        failSafeSnapshotSnapshotNameFormat.replace("{snapshot.name}", snapshotName)
          .replace("{table.name}", tableName.toString().replace(TableName.NAMESPACE_DELIM, '.'))
          .replace("{restore.timestamp}", String.valueOf(EnvironmentEdgeManager.currentTime()));
      LOG.info("Taking restore-failsafe snapshot: " + failSafeSnapshotSnapshotName);
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        failSafeSnapshotSnapshotNameFormat.replace("{snapshot.name}", snapshotName)
          .replace("{table.name}", tableName.toString().replace(TableName.NAMESPACE_DELIM, '.'))
          .replace("{restore.timestamp}", String.valueOf(EnvironmentEdgeManager.currentTime()));
      LOG.info("Taking restore-failsafe snapshot: " + failSafeSnapshotSnapshotName);
      addListener(snapshot(failSafeSnapshotSnapshotName, tableName), (ret, err) -> {
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java
    Encryption.Context cryptoContext, boolean isCompaction, String regionName) throws IOException {
    MobFileName mobFileName = MobFileName.create(startKey, date,
      UUID.randomUUID().toString().replaceAll("-", ""), regionName);
    return createWriter(conf, fs, family, mobFileName, basePath, maxKeyCount, compression,
      cacheConfig, cryptoContext, isCompaction);
```

### DynamicRegexReplaceableByCompiledPattern
`split()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java
      // TODO avoid turning the tablename pieces in to strings.
      String s = Bytes.toString(bytes);
      String[] tables = s.split("//");
      for (String tableEnc : tables) {
        final int delim = tableEnc.indexOf('/');
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java`
#### Snippet
```java
              msg != null && (msg.contains("Cannot obtain block length")
                || msg.contains("Could not obtain the last block")
                || msg.matches("Blocklist for [^ ]* has changed.*"))
            ) {
              if (++nbAttempt == 1) {
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          final String encodedName = regionDir.getPath().getName();
          // ignore directories that aren't hexadecimal
          if (!encodedName.toLowerCase(Locale.ROOT).matches("[0-9a-f]+")) {
            continue;
          }
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  // unique file name for the table
  private String getUniqueName() {
    return UUID.randomUUID().toString().replaceAll("-", "");
  }

```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
   */
  static int calculatePoolSize(String poolSize) {
    if (poolSize.matches("[1-9][0-9]*")) {
      // If poolSize is an integer, return it directly,
      // but upmost to the number of available processors.
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
      }
      return size;
    } else if (poolSize.matches("0.[0-9]+|1.0")) {
      // if poolSize is a double, return poolSize * availableProcessors;
      // Ensure that we always return at least one.
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
        }

        if (!regionName.toLowerCase(Locale.ROOT).matches("[0-9a-f]+")) {
          return false;
        }
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorType.java`
#### Snippet
```java
  /** Returns Conflation of the executor type and the passed {@code serverName}. */
  String getExecutorName(String serverName) {
    return this.toString() + "-" + serverName.replace("%", "%%");
  }
}
```

### DynamicRegexReplaceableByCompiledPattern
`matches()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java

  private void checkGroupName(String groupName) throws ConstraintException {
    if (!groupName.matches("[a-zA-Z0-9_]+")) {
      throw new ConstraintException("RSGroup name should only contain alphanumeric characters");
    }
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java`
#### Snippet
```java

  private Path createStagingDir(Path baseDir, User user, TableName tableName) throws IOException {
    String tblName = tableName.getNameAsString().replace(":", UNDERSCORE);
    int RANDOM_WIDTH = 320;
    int RANDOM_RADIX = 32;
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
    Consumer<Path> writerCreationTracker) throws IOException {
    MobFileName mobFileName =
      MobFileName.create(startKey, date, UUID.randomUUID().toString().replaceAll("-", ""),
        getHRegion().getRegionInfo().getEncodedName());
    return createWriterInTmp(mobFileName, basePath, maxKeyCount, compression, isCompaction,
```

### DynamicRegexReplaceableByCompiledPattern
`replaceAll()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
   */
  private static String generateUniqueName(final String suffix) {
    String name = UUID.randomUUID().toString().replaceAll("-", "");
    if (suffix != null) name += suffix;
    return name;
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java

  private Path createStagingDir(Path baseDir, User user, TableName tableName) throws IOException {
    String tblName = tableName.getNameAsString().replace(":", "_");
    String randomDir = user.getShortName() + "__" + tblName + "__"
      + (new BigInteger(RANDOM_WIDTH, random).toString(RANDOM_RADIX));
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
        new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>(),
          new ThreadFactoryBuilder().setNameFormat("AsyncFSWAL-%d-" + rootDir.toString()
            + "-prefix:" + (prefix == null ? "default" : prefix).replace("%", "%%")).setDaemon(true)
            .build());
      hasConsumerTask = () -> threadPool.getQueue().peek() == consumer;
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHttpServlet.java`
#### Snippet
```java
        byte[] res = gssContext.acceptSecContext(inToken, 0, inToken.length);
        if (res != null) {
          outToken = Base64.getEncoder().encodeToString(res).replace("\n", "");
        }
        // Authenticate or deny based on its context completion
```

### DynamicRegexReplaceableByCompiledPattern
`replace()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  public static String getSnapshotName(Configuration conf) {
    return "snapshot_" + getTableNameAsString(conf).replace(":", "_");
  }

```

### DynamicRegexReplaceableByCompiledPattern
`replaceFirst()` could be replaced with compiled 'java.util.regex.Pattern' construct
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
                      r.getValue(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER);
                    ServerName serverName =
                      ServerName.valueOf(Bytes.toString(server).replaceFirst(":", ",") + ","
                        + Bytes.toLong(startCode));
                    if (
```

## RuleId[id=ComparatorMethodParameterNotUsed]
### ComparatorMethodParameterNotUsed
Comparator never returns negative value
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java`
#### Snippet
```java

  @Override
  public int compareTo(byte[] value) {
    return value != null ? 1 : 0;
  }
```

### ComparatorMethodParameterNotUsed
Comparator does not return 0 for equal elements
in `hbase-server/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java`
#### Snippet
```java
   */
  @Override
  public int compareTo(EventHandler o) {
    if (o == null) {
      return 1;
```

### ComparatorMethodParameterNotUsed
Comparator never returns positive value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java

    @Override
    public int compareTo(Delayed o) {
      return -1;
    }
```

### ComparatorMethodParameterNotUsed
'compareTo()' parameter `o` is not used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java

    @Override
    public int compareTo(Delayed o) {
      return -1;
    }
```

### ComparatorMethodParameterNotUsed
Comparator does not return 0 for equal elements
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
    @Override
    public int compareTo(Delayed o) {
      return -1;
    }

```

## RuleId[id=NonProtectedConstructorInAbstractClass]
### NonProtectedConstructorInAbstractClass
Constructor `ScheduledChore()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
   * @param unit         The unit that is used to measure period and initialDelay
   */
  public ScheduledChore(final String name, Stoppable stopper, final int period,
    final long initialDelay, final TimeUnit unit) {
    this.name = name;
```

### NonProtectedConstructorInAbstractClass
Constructor `ScheduledChore()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
   * @param period  Period in millis with which this Chore repeats execution when scheduled.
   */
  public ScheduledChore(final String name, Stoppable stopper, final int period) {
    this(name, stopper, period, DEFAULT_INITIAL_DELAY);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `ScheduledChore()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
   *                     delays are invalid and will be corrected to a value of 0.
   */
  public ScheduledChore(final String name, Stoppable stopper, final int period,
    final long initialDelay) {
    this(name, stopper, period, initialDelay, DEFAULT_TIME_UNIT);
```

### NonProtectedConstructorInAbstractClass
Constructor `Cipher()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Cipher.java`
#### Snippet
```java
  private final CipherProvider provider;

  public Cipher(CipherProvider provider) {
    this.provider = provider;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `AbstractEncodedSeeker()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/AbstractDataBlockEncoder.java`
#### Snippet
```java
    protected HFileBlockDecodingContext decodingCtx;

    public AbstractEncodedSeeker(HFileBlockDecodingContext decodingCtx) {
      this.decodingCtx = decodingCtx;
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `BufferedEncodedSeeker()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
    protected STATE current, previous;

    public BufferedEncodedSeeker(HFileBlockDecodingContext decodingCtx) {
      super(decodingCtx);
      if (decodingCtx.getHFileContext().isCompressTags()) {
```

### NonProtectedConstructorInAbstractClass
Constructor `CellHashKey()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CellHashKey.java`
#### Snippet
```java
  protected static final byte MAX_TYPE = KeyValue.Type.Maximum.getCode();

  public CellHashKey(Cell cell) {
    super(cell);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `ObjectPool()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java`
#### Snippet
```java
   *                                  {@code concurrencyLevel} is non-positive
   */
  public ObjectPool(ObjectFactory<K, V> objectFactory, int initialCapacity, int concurrencyLevel) {

    this.objectFactory = Objects.requireNonNull(objectFactory, "Object factory cannot be null");
```

### NonProtectedConstructorInAbstractClass
Constructor `ObjectPool()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java`
#### Snippet
```java
   * @throws IllegalArgumentException if {@code initialCapacity} is negative
   */
  public ObjectPool(ObjectFactory<K, V> objectFactory, int initialCapacity) {
    this(objectFactory, initialCapacity, DEFAULT_CONCURRENCY_LEVEL);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `ObjectPool()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java`
#### Snippet
```java
   * @throws NullPointerException if {@code objectFactory} is {@code null}
   */
  public ObjectPool(ObjectFactory<K, V> objectFactory) {
    this(objectFactory, DEFAULT_INITIAL_CAPACITY, DEFAULT_CONCURRENCY_LEVEL);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `HashKey()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/HashKey.java`
#### Snippet
```java
  protected final T t;

  public HashKey(T t) {
    this.t = t;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `BaseDecoder()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java`
#### Snippet
```java
  }

  public BaseDecoder(final InputStream in) {
    this.in = new PBIS(in, 1);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `BaseEncoder()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseEncoder.java`
#### Snippet
```java
  protected boolean flushed = false;

  public BaseEncoder(final OutputStream out) {
    this.out = out;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `Union3()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union3.java`
#### Snippet
```java
   * Create an instance of {@code Union3} over the set of specified types.
   */
  public Union3(DataType<A> typeA, DataType<B> typeB, DataType<C> typeC) {
    super(typeA, typeB);
    this.typeC = typeC;
```

### NonProtectedConstructorInAbstractClass
Constructor `Union4()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union4.java`
#### Snippet
```java
   * Create an instance of {@code Union4} over the set of specified types.
   */
  public Union4(DataType<A> typeA, DataType<B> typeB, DataType<C> typeC, DataType<D> typeD) {
    super(typeA, typeB, typeC);
    this.typeD = typeD;
```

### NonProtectedConstructorInAbstractClass
Constructor `Union2()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union2.java`
#### Snippet
```java
   * Create an instance of {@code Union2} over the set of specified types.
   */
  public Union2(DataType<A> typeA, DataType<B> typeB) {
    this.typeA = typeA;
    this.typeB = typeB;
```

### NonProtectedConstructorInAbstractClass
Constructor `OrderedBytesBase()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/OrderedBytesBase.java`
#### Snippet
```java
  protected final Order order;

  public OrderedBytesBase(Order order) {
    this.order = order;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `ByteArrayComparable()` of an abstract class should not be declared 'public'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/filter/ByteArrayComparable.java`
#### Snippet
```java
   * @param value the value to compare against
   */
  public ByteArrayComparable(byte[] value) {
    this.value = value;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `MutableRangeHistogram()` of an abstract class should not be declared 'public'
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableRangeHistogram.java`
#### Snippet
```java
  }

  public MutableRangeHistogram(String name, String description) {
    super(name, description);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `MutableRangeHistogram()` of an abstract class should not be declared 'public'
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableRangeHistogram.java`
#### Snippet
```java
public abstract class MutableRangeHistogram extends MutableHistogram implements MetricHistogram {

  public MutableRangeHistogram(MetricsInfo info) {
    this(info.name(), info.description());
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `AbstractScreenView()` of an abstract class should not be declared 'public'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/AbstractScreenView.java`
#### Snippet
```java
  private final Terminal terminal;

  public AbstractScreenView(Screen screen, Terminal terminal) {
    this.screen = Objects.requireNonNull(screen);
    this.terminal = Objects.requireNonNull(terminal);
```

### NonProtectedConstructorInAbstractClass
Constructor `AbstractRpcClient()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
   * @param metrics   the connection metrics
   */
  public AbstractRpcClient(Configuration conf, String clusterId, SocketAddress localAddr,
    MetricsConnection metrics) {
    this.userProvider = UserProvider.instantiate(conf);
```

### NonProtectedConstructorInAbstractClass
Constructor `AsyncRpcRetryingCaller()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java`
#### Snippet
```java
  protected final HBaseRpcController controller;

  public AsyncRpcRetryingCaller(Timer retryTimer, AsyncConnectionImpl conn, int priority,
    long pauseNs, long pauseNsForServerOverloaded, int maxAttempts, long operationTimeoutNs,
    long rpcTimeoutNs, int startLogErrorsCnt) {
```

### NonProtectedConstructorInAbstractClass
Constructor `CompareFilter()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java`
#### Snippet
```java
   * @param comparator the comparator for row matching
   */
  public CompareFilter(final CompareOperator op, final ByteArrayComparable comparator) {
    this.op = op;
    this.comparator = comparator;
```

### NonProtectedConstructorInAbstractClass
Constructor `BasicRowRange()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    }

    public BasicRowRange(byte[] startRow, boolean startRowInclusive, byte[] stopRow,
      boolean stopRowInclusive) {
      this.startRow = (startRow == null) ? HConstants.EMPTY_BYTE_ARRAY : startRow;
```

### NonProtectedConstructorInAbstractClass
Constructor `BasicRowRange()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    protected boolean stopRowInclusive = false;

    public BasicRowRange() {
    }

```

### NonProtectedConstructorInAbstractClass
Constructor `BasicRowRange()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
     * HConstants.EMPTY_BYTE_ARRAY, means end of the last row of table.
     */
    public BasicRowRange(String startRow, boolean startRowInclusive, String stopRow,
      boolean stopRowInclusive) {
      this(
```

### NonProtectedConstructorInAbstractClass
Constructor `FilterListBase()` of an abstract class should not be declared 'public'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListBase.java`
#### Snippet
```java
  protected ArrayList<Boolean> subFiltersIncludedCell;

  public FilterListBase(List<Filter> filters) {
    reversed = checkAndGetReversed(filters, reversed);
    this.filters = new ArrayList<>(filters);
```

### NonProtectedConstructorInAbstractClass
Constructor `StoppableThread()` of an abstract class should not be declared 'public'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StoppableThread.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(StoppableThread.class);

  public StoppableThread(final ThreadGroup group, final String name) {
    super(group, name);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `DelayedContainer()` of an abstract class should not be declared 'public'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/util/DelayedUtil.java`
#### Snippet
```java
    private final T object;

    public DelayedContainer(final T object) {
      this.object = object;
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `ZKNodeTracker()` of an abstract class should not be declared 'public'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
   * @param abortable used to abort if a fatal error occurs
   */
  public ZKNodeTracker(ZKWatcher watcher, String node, Abortable abortable) {
    super(watcher);
    this.node = node;
```

### NonProtectedConstructorInAbstractClass
Constructor `ZKListener()` of an abstract class should not be declared 'public'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKListener.java`
#### Snippet
```java
   * Construct a ZooKeeper event listener.
   */
  public ZKListener(ZKWatcher watcher) {
    this.watcher = watcher;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `HBaseServerBase()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
  }

  public HBaseServerBase(Configuration conf, String name) throws IOException {
    super(name); // thread name
    final Span span = TraceUtil.createSpan("HBaseServerBase.cxtor");
```

### NonProtectedConstructorInAbstractClass
Constructor `HFileReaderImpl()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
   */
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "URF_UNREAD_PUBLIC_OR_PROTECTED_FIELD")
  public HFileReaderImpl(ReaderContext context, HFileInfo fileInfo, CacheConfig cacheConf,
    Configuration conf) throws IOException {
    this.cacheConf = cacheConf;
```

### NonProtectedConstructorInAbstractClass
Constructor `PersistentIOEngine()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/PersistentIOEngine.java`
#### Snippet
```java
  protected final String[] filePaths;

  public PersistentIOEngine(String... filePaths) {
    this.filePaths = filePaths;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `FileMmapIOEngine()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.java`
#### Snippet
```java
  private RandomAccessFile raf = null;

  public FileMmapIOEngine(String filePath, long capacity) throws IOException {
    super(filePath);
    this.path = filePath;
```

### NonProtectedConstructorInAbstractClass
Constructor `PluggableBlockingQueue()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/PluggableBlockingQueue.java`
#### Snippet
```java
  protected final Configuration conf;

  public PluggableBlockingQueue(final int maxQueueLength, final PriorityFunction priority,
    final Configuration conf) {
    this.maxQueueLength = maxQueueLength;
```

### NonProtectedConstructorInAbstractClass
Constructor `RpcExecutor()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcExecutor.java`
#### Snippet
```java
  }

  public RpcExecutor(final String name, final int handlerCount, final String callQueueType,
    final int maxQueueLength, final PriorityFunction priority, final Configuration conf,
    final Abortable abortable) {
```

### NonProtectedConstructorInAbstractClass
Constructor `RpcExecutor()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcExecutor.java`
#### Snippet
```java
  private final Abortable abortable;

  public RpcExecutor(final String name, final int handlerCount, final int maxQueueLength,
    final PriorityFunction priority, final Configuration conf, final Abortable abortable) {
    this(name, handlerCount, conf.get(CALL_QUEUE_TYPE_CONF_KEY, CALL_QUEUE_TYPE_CONF_DEFAULT),
```

### NonProtectedConstructorInAbstractClass
Constructor `AnnotationReadingPriorityFunction()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/AnnotationReadingPriorityFunction.java`
#### Snippet
```java
   * @param rpcServices The RPC server implementation
   */
  public AnnotationReadingPriorityFunction(final T rpcServices) {
    Map<String, Integer> qosMap = new HashMap<>();
    for (Method m : rpcServices.getClass().getMethods()) {
```

### NonProtectedConstructorInAbstractClass
Constructor `ServerRpcConnection()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected SaslServerAuthenticationProviders saslProviders = null;

  public ServerRpcConnection(RpcServer rpcServer) {
    this.rpcServer = rpcServer;
    this.callCleanup = null;
```

### NonProtectedConstructorInAbstractClass
Constructor `RpcServer()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @param reservoirEnabled Enable ByteBufferPool or not.
   */
  public RpcServer(final Server server, final String name,
    final List<BlockingServiceAndInterface> services, final InetSocketAddress bindAddress,
    Configuration conf, RpcScheduler scheduler, boolean reservoirEnabled) throws IOException {
```

### NonProtectedConstructorInAbstractClass
Constructor `AbstractRecoveredEditsOutputSink()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
  private final ConcurrentMap<String, Long> regionMaximumEditLogSeqNum = new ConcurrentHashMap<>();

  public AbstractRecoveredEditsOutputSink(WALSplitter walSplitter,
    WALSplitter.PipelineController controller, EntryBuffers entryBuffers, int numWriters) {
    super(controller, entryBuffers, numWriters);
```

### NonProtectedConstructorInAbstractClass
Constructor `OutputSink()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
  protected final CompletionService<Void> closeCompletionService;

  public OutputSink(WALSplitter.PipelineController controller, EntryBuffers entryBuffers,
    int numWriters) {
    this.numThreads = numWriters;
```

### NonProtectedConstructorInAbstractClass
Constructor `BloomContext()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomContext.java`
#### Snippet
```java
  protected CellComparator comparator;

  public BloomContext(BloomFilterWriter bloomFilterWriter, CellComparator comparator) {
    this.bloomFilterWriter = bloomFilterWriter;
    this.comparator = comparator;
```

### NonProtectedConstructorInAbstractClass
Constructor `FileConverter()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
    protected final FileSystem fs;

    public FileConverter(FileSystem fs) {
      this.fs = fs;
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `File()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
    protected final FileSystem fs;

    public File(FileSystem fs) {
      this.fs = fs;
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `CleanerChore()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
   * @param params      members could be used in cleaner
   */
  public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Configuration conf,
    FileSystem fs, Path oldFileDir, String confKey, DirScanPool pool, Map<String, Object> params,
    List<Path> excludePaths) {
```

### NonProtectedConstructorInAbstractClass
Constructor `CleanerChore()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
  private boolean sortDirectories;

  public CleanerChore(String name, final int sleepPeriod, final Stoppable s, Configuration conf,
    FileSystem fs, Path oldFileDir, String confKey, DirScanPool pool) {
    this(name, sleepPeriod, s, conf, fs, oldFileDir, confKey, pool, null, null);
```

### NonProtectedConstructorInAbstractClass
Constructor `TakeSnapshotHandler()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java`
#### Snippet
```java
   *                                  determined
   */
  public TakeSnapshotHandler(SnapshotDescription snapshot, final MasterServices masterServices,
    final SnapshotManager snapshotManager) throws IOException {
    super(masterServices, EventType.C_M_SNAPSHOT_TABLE);
```

### NonProtectedConstructorInAbstractClass
Constructor `ClientZKSyncer()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
  private final ConcurrentMap<String, ZKData> queues;

  public ClientZKSyncer(ZKWatcher watcher, ZKWatcher clientZkWatcher, Server server) {
    super(watcher);
    this.server = server;
```

### NonProtectedConstructorInAbstractClass
Constructor `NonceProcedureRunnable()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java
    private Long procId;

    public NonceProcedureRunnable(final MasterServices master, final long nonceGroup,
      final long nonce) {
      this.master = master;
```

### NonProtectedConstructorInAbstractClass
Constructor `MasterObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java`
#### Snippet
```java
    }

    public MasterObserverOperation(User user) {
      super(masterObserverGetter, user);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `MasterObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java`
#### Snippet
```java
    }

    public MasterObserverOperation(User user, boolean bypassable) {
      super(masterObserverGetter, user, bypassable);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `MasterObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java`
#### Snippet
```java
    }

    public MasterObserverOperation(boolean bypassable) {
      this(null, bypassable);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `MasterObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java`
#### Snippet
```java

  abstract class MasterObserverOperation extends ObserverOperationWithoutResult<MasterObserver> {
    public MasterObserverOperation() {
      super(masterObserverGetter);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionTransitionProcedure()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.java`
#### Snippet
```java

  // Required by the Procedure framework to create the procedure on replay
  public RegionTransitionProcedure() {
  }

```

### NonProtectedConstructorInAbstractClass
Constructor `RegionTransitionProcedure()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.java`
#### Snippet
```java
  }

  public RegionTransitionProcedure(final RegionInfo regionInfo) {
    this.regionInfo = regionInfo;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `EventHandler()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java`
#### Snippet
```java
   * Default base class constructor.
   */
  public EventHandler(Server server, EventType eventType) {
    this.parent = Span.current();
    this.server = server;
```

### NonProtectedConstructorInAbstractClass
Constructor `ZKProcedureUtil()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java`
#### Snippet
```java
   * @throws KeeperException when the procedure znodes cannot be created
   */
  public ZKProcedureUtil(ZKWatcher watcher, String procDescription) throws KeeperException {
    super(watcher);
    // make sure we are listening for events
```

### NonProtectedConstructorInAbstractClass
Constructor `Subprocedure()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
   *                      completed
   */
  public Subprocedure(ProcedureMember member, String procName, ForeignExceptionDispatcher monitor,
    long wakeFrequency, long timeout) {
    // Asserts should be caught during unit testing
```

### NonProtectedConstructorInAbstractClass
Constructor `ObserverOperationWithResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    }

    public ObserverOperationWithResult(ObserverGetter<C, O> observerGetter, R result,
      boolean bypassable) {
      this(observerGetter, result, null, bypassable);
```

### NonProtectedConstructorInAbstractClass
Constructor `CoprocessorHost()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
  protected AtomicInteger loadSequence = new AtomicInteger();

  public CoprocessorHost(Abortable abortable) {
    this.abortable = abortable;
    this.pathPrefix = UUID.randomUUID().toString();
```

### NonProtectedConstructorInAbstractClass
Constructor `ObserverOperationWithResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    }

    public ObserverOperationWithResult(ObserverGetter<C, O> observerGetter, R result, User user) {
      this(observerGetter, result, user, false);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `ObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    }

    public ObserverOperationWithoutResult(ObserverGetter<C, O> observerGetter, User user) {
      super(observerGetter, user);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `ObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    protected abstract void call(O observer) throws IOException;

    public ObserverOperationWithoutResult(ObserverGetter<C, O> observerGetter) {
      super(observerGetter);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `ObserverOperationWithResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    private R result;

    public ObserverOperationWithResult(ObserverGetter<C, O> observerGetter, R result) {
      this(observerGetter, result, false);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `ObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    }

    public ObserverOperationWithoutResult(ObserverGetter<C, O> observerGetter, User user,
      boolean bypassable) {
      super(observerGetter, user, bypassable);
```

### NonProtectedConstructorInAbstractClass
Constructor `Chunk()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Chunk.java`
#### Snippet
```java
   * @param id   the chunk id
   */
  public Chunk(int size, int id, ChunkType chunkType) {
    this(size, id, chunkType, false);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `Chunk()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Chunk.java`
#### Snippet
```java
   * @param fromPool if the chunk is formed by pool
   */
  public Chunk(int size, int id, ChunkType chunkType, boolean fromPool) {
    this.size = size;
    this.id = id;
```

### NonProtectedConstructorInAbstractClass
Constructor `StoreFlusher()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java`
#### Snippet
```java
  protected HStore store;

  public StoreFlusher(Configuration conf, HStore store) {
    this.conf = conf;
    this.store = store;
```

### NonProtectedConstructorInAbstractClass
Constructor `MemStoreSegmentsIterator()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreSegmentsIterator.java`
#### Snippet
```java

  // C-tor
  public MemStoreSegmentsIterator(int compactionKVMax) throws IOException {
    this.scannerContext = ScannerContext.newBuilder().setBatchLimit(compactionKVMax).build();
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `StripeMultiFileWriter()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
  private boolean doWriteStripeMetadata = true;

  public StripeMultiFileWriter(CellComparator comparator) {
    this.comparator = comparator;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionServerObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java`
#### Snippet
```java
  abstract class RegionServerObserverOperation
    extends ObserverOperationWithoutResult<RegionServerObserver> {
    public RegionServerObserverOperation() {
      super(rsObserverGetter);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionServerObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java`
#### Snippet
```java
    }

    public RegionServerObserverOperation(User user) {
      super(rsObserverGetter, user);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `MemStoreCompactionStrategy()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactionStrategy.java`
#### Snippet
```java
  protected final int pipelineThreshold;

  public MemStoreCompactionStrategy(Configuration conf, String cfName) {
    this.cfName = cfName;
    if (conf == null) {
```

### NonProtectedConstructorInAbstractClass
Constructor `CellFlatMap()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java

  /* C-tor */
  public CellFlatMap(Comparator<? super Cell> comparator, int min, int max, boolean d) {
    this.comparator = comparator;
    this.minCellIdx = min;
```

### NonProtectedConstructorInAbstractClass
Constructor `WALObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.java`
#### Snippet
```java

  abstract class WALObserverOperation extends ObserverOperationWithoutResult<WALObserver> {
    public WALObserverOperation() {
      super(walObserverGetter);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `ReaderBase()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ReaderBase.java`
#### Snippet
```java
   * Default constructor.
   */
  public ReaderBase() {
  }

```

### NonProtectedConstructorInAbstractClass
Constructor `CompactionPolicy()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionPolicy.java`
#### Snippet
```java
  protected StoreConfigInformation storeConfigInfo;

  public CompactionPolicy(Configuration conf, StoreConfigInformation storeConfigInfo) {
    this.storeConfigInfo = storeConfigInfo;
    this.comConf = new CompactionConfiguration(conf, this.storeConfigInfo);
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }

    public RegionObserverOperationWithoutResult(User user) {
      super(regionObserverGetter, user);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
  abstract class RegionObserverOperationWithoutResult
    extends ObserverOperationWithoutResult<RegionObserver> {
    public RegionObserverOperationWithoutResult() {
      super(regionObserverGetter);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }

    public RegionObserverOperationWithoutResult(boolean bypassable) {
      super(regionObserverGetter, null, bypassable);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `RegionObserverOperationWithoutResult()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }

    public RegionObserverOperationWithoutResult(User user, boolean bypassable) {
      super(regionObserverGetter, user, bypassable);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `BulkLoadObserverOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
  abstract class BulkLoadObserverOperation
    extends ObserverOperationWithoutResult<BulkLoadObserver> {
    public BulkLoadObserverOperation(User user) {
      super(RegionCoprocessor::getBulkLoadObserver, user);
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `AbstractMultiOutputCompactor()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/AbstractMultiOutputCompactor.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(AbstractMultiOutputCompactor.class);

  public AbstractMultiOutputCompactor(Configuration conf, HStore store) {
    super(conf, store);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `SortedCompactionPolicy()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java`
#### Snippet
```java
  private static final Random RNG = new Random();

  public SortedCompactionPolicy(Configuration conf, StoreConfigInformation storeConfigInfo) {
    super(conf, storeConfigInfo);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `StripeCompactionRequest()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
      ThroughputController throughputController, User user) throws IOException;

    public StripeCompactionRequest(CompactionRequestImpl request) {
      this.request = request;
    }
```

### NonProtectedConstructorInAbstractClass
Constructor `TableBackupClient()` of an abstract class should not be declared 'public'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
  }

  public TableBackupClient(final Connection conn, final String backupId, BackupRequest request)
    throws IOException {
    init(conn, backupId, request);
```

### NonProtectedConstructorInAbstractClass
Constructor `TableBackupClient()` of an abstract class should not be declared 'public'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
  protected FileSystem fs;

  public TableBackupClient() {
  }

```

### NonProtectedConstructorInAbstractClass
Constructor `HBaseServiceHandler()` of an abstract class should not be declared 'public'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/HBaseServiceHandler.java`
#### Snippet
```java
  protected final ConnectionCache connectionCache;

  public HBaseServiceHandler(final Configuration c, final UserProvider userProvider)
    throws IOException {
    this.conf = c;
```

### NonProtectedConstructorInAbstractClass
Constructor `ThriftClientBuilder()` of an abstract class should not be declared 'public'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftClientBuilder.java`
#### Snippet
```java
  protected ThriftConnection connection;

  public ThriftClientBuilder(ThriftConnection connection) {
    this.connection = connection;
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `HBaseCluster()` of an abstract class should not be declared 'public'
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCluster.java`
#### Snippet
```java
   * @param conf Configuration to be used for cluster
   */
  public HBaseCluster(Configuration conf) {
    setConf(conf);
  }
```

### NonProtectedConstructorInAbstractClass
Constructor `BatchOperation()` of an abstract class should not be declared 'public'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    protected boolean atomic = false;

    public BatchOperation(final HRegion region, T[] operations) {
      this.operations = operations;
      this.retCodeDetails = new OperationStatus[operations.length];
```

## RuleId[id=AssignmentToMethodParameter]
### AssignmentToMethodParameter
Assignment to method parameter `corePoolSize`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java`
#### Snippet
```java
    this.coreThreadPoolPrefix = coreThreadPoolPrefix;
    if (corePoolSize < MIN_CORE_POOL_SIZE) {
      corePoolSize = MIN_CORE_POOL_SIZE;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffInputStream.java`
#### Snippet
```java

    if (len > avail) {
      len = avail;
    }
    this.buf.get(b, off, len);
```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferInputStream.java`
#### Snippet
```java

    if (len > avail) {
      len = avail;
    }
    if (len <= 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `pos`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    }
    short tagsLen = Bytes.toShort(buf, pos);
    pos += Bytes.SIZEOF_SHORT;
    if (tagsLen < 0 || pos + tagsLen > endOffset) {
      String msg = "Invalid tags length in KeyValue at position=" + (pos - Bytes.SIZEOF_SHORT)
```

### AssignmentToMethodParameter
Assignment to method parameter `pos`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
      }
      short tagLen = Bytes.toShort(buf, pos);
      pos += Tag.TAG_LENGTH_SIZE;
      // tagLen contains one byte tag type, so must be not less than 1.
      if (tagLen < 1 || pos + tagLen > endOffset) {
```

### AssignmentToMethodParameter
Assignment to method parameter `pos`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
        throw new IllegalArgumentException(msg);
      }
      pos += tagLen;
    }
    return pos;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java

  public static int appendKeyTo(Cell cell, ByteBuffer buf, int offset) {
    offset = ByteBufferUtils.putShort(buf, offset, cell.getRowLength());// RK length
    offset = CellUtil.copyRowTo(cell, buf, offset);// Row bytes
    offset = ByteBufferUtils.putByte(buf, offset, cell.getFamilyLength());// CF length
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
  public static int appendKeyTo(Cell cell, ByteBuffer buf, int offset) {
    offset = ByteBufferUtils.putShort(buf, offset, cell.getRowLength());// RK length
    offset = CellUtil.copyRowTo(cell, buf, offset);// Row bytes
    offset = ByteBufferUtils.putByte(buf, offset, cell.getFamilyLength());// CF length
    offset = CellUtil.copyFamilyTo(cell, buf, offset);// CF bytes
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = ByteBufferUtils.putShort(buf, offset, cell.getRowLength());// RK length
    offset = CellUtil.copyRowTo(cell, buf, offset);// Row bytes
    offset = ByteBufferUtils.putByte(buf, offset, cell.getFamilyLength());// CF length
    offset = CellUtil.copyFamilyTo(cell, buf, offset);// CF bytes
    offset = CellUtil.copyQualifierTo(cell, buf, offset);// Qualifier bytes
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = CellUtil.copyRowTo(cell, buf, offset);// Row bytes
    offset = ByteBufferUtils.putByte(buf, offset, cell.getFamilyLength());// CF length
    offset = CellUtil.copyFamilyTo(cell, buf, offset);// CF bytes
    offset = CellUtil.copyQualifierTo(cell, buf, offset);// Qualifier bytes
    offset = ByteBufferUtils.putLong(buf, offset, cell.getTimestamp());// TS
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = ByteBufferUtils.putByte(buf, offset, cell.getFamilyLength());// CF length
    offset = CellUtil.copyFamilyTo(cell, buf, offset);// CF bytes
    offset = CellUtil.copyQualifierTo(cell, buf, offset);// Qualifier bytes
    offset = ByteBufferUtils.putLong(buf, offset, cell.getTimestamp());// TS
    offset = ByteBufferUtils.putByte(buf, offset, cell.getTypeByte());// Type
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = CellUtil.copyFamilyTo(cell, buf, offset);// CF bytes
    offset = CellUtil.copyQualifierTo(cell, buf, offset);// Qualifier bytes
    offset = ByteBufferUtils.putLong(buf, offset, cell.getTimestamp());// TS
    offset = ByteBufferUtils.putByte(buf, offset, cell.getTypeByte());// Type
    return offset;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = CellUtil.copyQualifierTo(cell, buf, offset);// Qualifier bytes
    offset = ByteBufferUtils.putLong(buf, offset, cell.getTimestamp());// TS
    offset = ByteBufferUtils.putByte(buf, offset, cell.getTypeByte());// Type
    return offset;
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
   */
  public static int appendTo(Cell cell, ByteBuffer buf, int offset, boolean withTags) {
    offset = ByteBufferUtils.putInt(buf, offset, keyLength(cell));// Key length
    offset = ByteBufferUtils.putInt(buf, offset, cell.getValueLength());// Value length
    offset = appendKeyTo(cell, buf, offset);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
  public static int appendTo(Cell cell, ByteBuffer buf, int offset, boolean withTags) {
    offset = ByteBufferUtils.putInt(buf, offset, keyLength(cell));// Key length
    offset = ByteBufferUtils.putInt(buf, offset, cell.getValueLength());// Value length
    offset = appendKeyTo(cell, buf, offset);
    offset = CellUtil.copyValueTo(cell, buf, offset);// Value bytes
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = ByteBufferUtils.putInt(buf, offset, keyLength(cell));// Key length
    offset = ByteBufferUtils.putInt(buf, offset, cell.getValueLength());// Value length
    offset = appendKeyTo(cell, buf, offset);
    offset = CellUtil.copyValueTo(cell, buf, offset);// Value bytes
    int tagsLength = cell.getTagsLength();
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    offset = ByteBufferUtils.putInt(buf, offset, cell.getValueLength());// Value length
    offset = appendKeyTo(cell, buf, offset);
    offset = CellUtil.copyValueTo(cell, buf, offset);// Value bytes
    int tagsLength = cell.getTagsLength();
    if (withTags && (tagsLength > 0)) {
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    int tagsLength = cell.getTagsLength();
    if (withTags && (tagsLength > 0)) {
      offset = ByteBufferUtils.putAsShort(buf, offset, tagsLength);// Tags length
      offset = PrivateCellUtil.copyTagsTo(cell, buf, offset);// Tags bytes
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    if (withTags && (tagsLength > 0)) {
      offset = ByteBufferUtils.putAsShort(buf, offset, tagsLength);// Tags length
      offset = PrivateCellUtil.copyTagsTo(cell, buf, offset);// Tags bytes
    }
    return offset;
```

### AssignmentToMethodParameter
Assignment to method parameter `off`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
      toWrite = Math.min(len, this.curBuf.remaining());
      ByteBufferUtils.copyFromArrayToBuffer(this.curBuf, b, off, toWrite);
      off += toWrite;
      len -= toWrite;
      if (len > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
      ByteBufferUtils.copyFromArrayToBuffer(this.curBuf, b, off, toWrite);
      off += toWrite;
      len -= toWrite;
      if (len > 0) {
        allocateNewBuffer();// The curBuf is over. Let us move to the next one
```

### AssignmentToMethodParameter
Assignment to method parameter `off`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
      toWrite = Math.min(len, this.curBuf.remaining());
      ByteBufferUtils.copyFromBufferToBuffer(b, this.curBuf, off, toWrite);
      off += toWrite;
      len -= toWrite;
      if (len > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
      ByteBufferUtils.copyFromBufferToBuffer(b, this.curBuf, off, toWrite);
      off += toWrite;
      len -= toWrite;
      if (len > 0) {
        allocateNewBuffer();// The curBuf is over. Let us move to the next one
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
      if (status == Dictionary.NOT_IN_DICTIONARY) {
        tagLen = StreamUtils.readRawVarint32(src);
        offset = Bytes.putAsShort(dest, offset, tagLen);
        src.get(dest, offset, tagLen);
        tagDict.addEntry(dest, offset, tagLen);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
        src.get(dest, offset, tagLen);
        tagDict.addEntry(dest, offset, tagLen);
        offset += tagLen;
      } else {
        short dictIdx = StreamUtils.toShort(status, src.get());
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
        }
        tagLen = entry.length;
        offset = Bytes.putAsShort(dest, offset, tagLen);
        System.arraycopy(entry, 0, dest, offset, tagLen);
        offset += tagLen;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
        offset = Bytes.putAsShort(dest, offset, tagLen);
        System.arraycopy(entry, 0, dest, offset, tagLen);
        offset += tagLen;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
      if (status == Dictionary.NOT_IN_DICTIONARY) {
        int tagLen = StreamUtils.readRawVarint32(src);
        offset = Bytes.putAsShort(dest, offset, tagLen);
        IOUtils.readFully(src, dest, offset, tagLen);
        tagDict.addEntry(dest, offset, tagLen);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
        IOUtils.readFully(src, dest, offset, tagLen);
        tagDict.addEntry(dest, offset, tagLen);
        offset += tagLen;
      } else {
        short dictIdx = StreamUtils.toShort(status, StreamUtils.readByte(src));
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
          throw new IOException("Missing dictionary entry for index " + dictIdx);
        }
        offset = Bytes.putAsShort(dest, offset, entry.length);
        System.arraycopy(entry, 0, dest, offset, entry.length);
        offset += entry.length;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
        offset = Bytes.putAsShort(dest, offset, entry.length);
        System.arraycopy(entry, 0, dest, offset, entry.length);
        offset += entry.length;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `value`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
      } else {
        output.write((value & 0x7F) | 0x80);
        value >>>= 7;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `num`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/CryptoAES.java`
#### Snippet
```java
      for (int i = 3; i >= 0; i--) {
        seqNum[i] = (byte) (num & 0xff);
        num >>>= 8;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/BlockIOUtils.java`
#### Snippet
```java
      cur.put(buf, offset, copyLen);
      remain -= copyLen;
      offset += copyLen;
    }
    return len;
```

### AssignmentToMethodParameter
Assignment to method parameter `bufOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/BlockIOUtils.java`
#### Snippet
```java
            + " extra bytes, " + "successfully read " + (necessaryLen + extraLen - bytesRemaining));
        }
        bufOffset += ret;
        bytesRemaining -= ret;
        heapBytesRead += ret;
```

### AssignmentToMethodParameter
Assignment to method parameter `v`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java`
#### Snippet
```java
      }
      sum = v + (iv[i] & 0xFF);
      v = sum / 256;
      iv[i] = (byte) (sum % 256);
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `host`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/HBaseHostnameVerifier.java`
#### Snippet
```java

  private static Optional<InetAddress> parseIpAddress(String host) {
    host = host.trim();
    // Uri strings only work for ipv6 and are wrapped with brackets
    // Unfortunately InetAddresses can't handle a mixed input, so we
```

### AssignmentToMethodParameter
Assignment to method parameter `trustStorePassword`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java

    if (trustStorePassword == null) {
      trustStorePassword = EMPTY_CHAR_ARRAY;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `keyStorePassword`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java

    if (keyStorePassword == null) {
      keyStorePassword = EMPTY_CHAR_ARRAY;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `commonPrefix`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    } else if (commonPrefix == 1) {
      out.writeByte((byte) rLen);
      commonPrefix--;
    } else {
      commonPrefix -= KeyValue.ROW_LENGTH_SIZE;
```

### AssignmentToMethodParameter
Assignment to method parameter `commonPrefix`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      commonPrefix--;
    } else {
      commonPrefix -= KeyValue.ROW_LENGTH_SIZE;
    }
    if (rLen > commonPrefix) {
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    @Override
    public void write(ByteBuffer buf, int offset) {
      offset = KeyValueUtil.appendTo(this.cell, buf, offset, false);
      int tagsLen = this.tags == null ? 0 : this.tags.length;
      if (tagsLen > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      int tagsLen = this.tags == null ? 0 : this.tags.length;
      if (tagsLen > 0) {
        offset = ByteBufferUtils.putAsShort(buf, offset, tagsLen);
        ByteBufferUtils.copyFromArrayToBuffer(buf, offset, this.tags, 0, tagsLen);
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    @Override
    public void write(ByteBuffer buf, int offset) {
      offset = KeyValueUtil.appendTo(this.cell, buf, offset, false);
      int tagsLen = this.tags == null ? 0 : this.tags.length;
      if (tagsLen > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      int tagsLen = this.tags == null ? 0 : this.tags.length;
      if (tagsLen > 0) {
        offset = ByteBufferUtils.putAsShort(buf, offset, tagsLen);
        ByteBufferUtils.copyFromArrayToBuffer(buf, offset, this.tags, 0, tagsLen);
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
     */
    static void write(ByteBuffer buf, int offset, Cell cell, byte[] value, byte[] tags) {
      offset = ByteBufferUtils.putInt(buf, offset, KeyValueUtil.keyLength(cell));// Key length
      offset = ByteBufferUtils.putInt(buf, offset, value.length);// Value length
      offset = KeyValueUtil.appendKeyTo(cell, buf, offset);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    static void write(ByteBuffer buf, int offset, Cell cell, byte[] value, byte[] tags) {
      offset = ByteBufferUtils.putInt(buf, offset, KeyValueUtil.keyLength(cell));// Key length
      offset = ByteBufferUtils.putInt(buf, offset, value.length);// Value length
      offset = KeyValueUtil.appendKeyTo(cell, buf, offset);
      ByteBufferUtils.copyFromArrayToBuffer(buf, offset, value, 0, value.length);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      offset = ByteBufferUtils.putInt(buf, offset, KeyValueUtil.keyLength(cell));// Key length
      offset = ByteBufferUtils.putInt(buf, offset, value.length);// Value length
      offset = KeyValueUtil.appendKeyTo(cell, buf, offset);
      ByteBufferUtils.copyFromArrayToBuffer(buf, offset, value, 0, value.length);
      offset += value.length;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      offset = KeyValueUtil.appendKeyTo(cell, buf, offset);
      ByteBufferUtils.copyFromArrayToBuffer(buf, offset, value, 0, value.length);
      offset += value.length;
      int tagsLen = tags == null ? 0 : tags.length;
      if (tagsLen > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      int tagsLen = tags == null ? 0 : tags.length;
      if (tagsLen > 0) {
        offset = ByteBufferUtils.putAsShort(buf, offset, tagsLen);
        ByteBufferUtils.copyFromArrayToBuffer(buf, offset, tags, 0, tagsLen);
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `v`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/CompressionUtil.java`
#### Snippet
```java
   */
  public static int roundInt2(int v) {
    v = Integer.highestOneBit(v) << 1;
    if (v < 0) {
      return Integer.MAX_VALUE;
```

### AssignmentToMethodParameter
Assignment to method parameter `commonPrefix`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java`
#### Snippet
```java
      // The full row key part is common. CF part will be common for sure as we deal with Cells in
      // same family. Just need write the differing part in q, ts and type
      commonPrefix = commonPrefix - (rLen + KeyValue.ROW_LENGTH_SIZE)
        - (cell.getFamilyLength() + KeyValue.FAMILY_LENGTH_SIZE);
      int qLen = cell.getQualifierLength();
```

### AssignmentToMethodParameter
Assignment to method parameter `commonPrefix`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java`
#### Snippet
```java
        PrivateCellUtil.writeQualifierSkippingBytes(out, cell, qLen, commonQualPrefix);
      }
      commonPrefix -= commonQualPrefix;
      // Common part in TS also?
      if (commonPrefix > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `commonPrefix`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java`
#### Snippet
```java
            KeyValue.TIMESTAMP_SIZE - commonTimestampPrefix);
        }
        commonPrefix -= commonTimestampPrefix;
        if (commonPrefix == 0) {
          out.writeByte(cell.getTypeByte());
```

### AssignmentToMethodParameter
Assignment to method parameter `onDiskSizeWithoutHeader`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java`
#### Snippet
```java
          dataInputStream = decryptor.createDecryptionStream(dataInputStream);
        }
        onDiskSizeWithoutHeader -= Bytes.SIZEOF_BYTE + ivLength;
      }

```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java`
#### Snippet
```java
      int len = channel.write(buf, offset);
      total += len;
      offset += len;
    }
    return total;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    int itemIndex = getItemIndex(offset);
    ByteBuffer item = this.items[itemIndex];
    offset = offset - this.itemBeginPos[itemIndex];
    if (item.limit() - offset >= length) {
      pair.setFirst(item);
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      int toRead = Math.min(length, item.limit() - offset);
      ByteBufferUtils.copyFromBufferToArray(dst, item, offset, destOffset, toRead);
      length -= toRead;
      if (length == 0) break;
      itemIndex++;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      item = this.items[itemIndex];
      destOffset += toRead;
      offset = 0;
    }
    pair.setFirst(ByteBuffer.wrap(dst));
```

### AssignmentToMethodParameter
Assignment to method parameter `sourceOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    int itemIndex = getItemIndex(sourceOffset);
    ByteBuffer in = this.items[itemIndex];
    sourceOffset = sourceOffset - this.itemBeginPos[itemIndex];
    while (length > 0) {
      int toRead = Math.min(in.limit() - sourceOffset, length);
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      int toRead = Math.min(in.limit() - sourceOffset, length);
      ByteBufferUtils.copyFromBufferToBuffer(in, out, sourceOffset, toRead);
      length -= toRead;
      if (length == 0) {
        break;
```

### AssignmentToMethodParameter
Assignment to method parameter `sourceOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      itemIndex++;
      in = this.items[itemIndex];
      sourceOffset = 0;
    }
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      }
      this.curItem.position(this.curItem.position() + jump);
      length -= jump;
      this.curItemIndex++;
      this.curItem = this.items[this.curItemIndex];
```

### AssignmentToMethodParameter
Assignment to method parameter `sourceOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    int itemIndex = getItemIndex(sourceOffset);
    ByteBuffer item = this.items[itemIndex];
    sourceOffset = sourceOffset - this.itemBeginPos[itemIndex];
    while (length > 0) {
      int toRead = Math.min((item.limit() - sourceOffset), length);
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      int toRead = Math.min((item.limit() - sourceOffset), length);
      ByteBufferUtils.copyFromBufferToArray(dst, item, sourceOffset, offset, toRead);
      length -= toRead;
      if (length == 0) break;
      itemIndex++;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      itemIndex++;
      item = this.items[itemIndex];
      offset += toRead;
      sourceOffset = 0;
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `sourceOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      item = this.items[itemIndex];
      offset += toRead;
      sourceOffset = 0;
    }
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      if (len > 0) {
        total += len;
        offset += len;
      }
      if (buffer.hasRemaining()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        toRead);
      this.curItem.position(this.curItem.position() + toRead);
      length -= toRead;
      if (length == 0) break;
      this.curItemIndex++;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      this.curItemIndex++;
      this.curItem = this.items[this.curItemIndex];
      offset += toRead;
    }
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
    while (length != 0) {
      if (length > curItem.position()) {
        length -= curItem.position();
        this.curItem.position(0);
        this.curItemIndex--;
```

### AssignmentToMethodParameter
Assignment to method parameter `destOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java

    ByteBuffer destItem = this.items[destItemIndex];
    destOffset = this.getRelativeOffset(destOffset, destItemIndex);

    ByteBuffer srcItem = getItemByteBuffer(src, srcItemIndex);
```

### AssignmentToMethodParameter
Assignment to method parameter `srcOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java

    ByteBuffer srcItem = getItemByteBuffer(src, srcItemIndex);
    srcOffset = getRelativeOffsetForByteBuff(src, srcOffset, srcItemIndex);

    while (length > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      int toMove = Math.min(length, Math.min(toRead, toWrite));
      ByteBufferUtils.copyFromBufferToBuffer(srcItem, destItem, srcOffset, destOffset, toMove);
      length -= toMove;
      if (length == 0) {
        break;
```

### AssignmentToMethodParameter
Assignment to method parameter `srcOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        }
        srcItem = getItemByteBuffer(src, srcItemIndex);
        srcOffset = 0;
        destOffset += toMove;
      } else if (toRead > toWrite) {
```

### AssignmentToMethodParameter
Assignment to method parameter `destOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        srcItem = getItemByteBuffer(src, srcItemIndex);
        srcOffset = 0;
        destOffset += toMove;
      } else if (toRead > toWrite) {
        if (++destItemIndex >= this.items.length) {
```

### AssignmentToMethodParameter
Assignment to method parameter `destOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        }
        destItem = this.items[destItemIndex];
        destOffset = 0;
        srcOffset += toMove;
      } else {
```

### AssignmentToMethodParameter
Assignment to method parameter `srcOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        destItem = this.items[destItemIndex];
        destOffset = 0;
        srcOffset += toMove;
      } else {
        // toRead = toWrite case
```

### AssignmentToMethodParameter
Assignment to method parameter `srcOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        }
        srcItem = getItemByteBuffer(src, srcItemIndex);
        srcOffset = 0;
        if (++destItemIndex >= this.items.length) {
          throw new BufferOverflowException();
```

### AssignmentToMethodParameter
Assignment to method parameter `destOffset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        }
        destItem = this.items[destItemIndex];
        destOffset = 0;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
        int len = channel.write(buffer, offset);
        total += len;
        offset += len;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
      ByteBufferUtils.copyFromBufferToArray(dupB, locCurItem, locCurItem.position(), offset,
        toRead);
      length -= toRead;
      if (length == 0) break;
      locCurItemIndex++;
```

### AssignmentToMethodParameter
Assignment to method parameter `head`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
        node.iterPrev.iterNext = node.iterNext;
        node.iterNext.iterPrev = node.iterPrev;
        head = (head == node) ? (TNode) node.iterNext : head;
      } else {
        head = null;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimplePositionedMutableByteRange.java`
#### Snippet
```java
    // one can get the same result.
    bytes[offset + index + 1] = (byte) val;
    val >>= 8;
    bytes[offset + index] = (byte) val;
    clearHashCache();
```

### AssignmentToMethodParameter
Assignment to method parameter `head`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
        head = (head == node) ? (TNode) node.iterNext : head;
      } else {
        head = null;
      }
      node.iterNext = null;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimplePositionedMutableByteRange.java`
#### Snippet
```java
    for (int i = Bytes.SIZEOF_LONG - 1; i > 0; i--) {
      bytes[offset + index + i] = (byte) val;
      val >>>= 8;
    }
    bytes[offset + index] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `root`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
      if (root != null) {
        while (root.avlLeft != null) {
          root = (TNode) root.avlLeft;
        }
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimplePositionedMutableByteRange.java`
#### Snippet
```java
    for (int i = Bytes.SIZEOF_INT - 1; i > 0; i--) {
      bytes[offset + index + i] = (byte) val;
      val >>>= 8;
    }
    bytes[offset + index] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `root`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
      if (root != null) {
        while (root.avlRight != null) {
          root = (TNode) root.avlRight;
        }
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimplePositionedMutableByteRange.java`
#### Snippet
```java
      } else {
        bytes[offset + index + rPos] = (byte) ((val & 0x7F) | 0x80);
        val >>>= 7;
      }
      rPos++;
```

### AssignmentToMethodParameter
Assignment to method parameter `root`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
        int cmp = keyComparator.compareKey(root, key);
        if (cmp > 0) {
          root = (TNode) root.avlLeft;
        } else if (cmp < 0) {
          root = (TNode) root.avlRight;
```

### AssignmentToMethodParameter
Assignment to method parameter `root`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
          root = (TNode) root.avlLeft;
        } else if (cmp < 0) {
          root = (TNode) root.avlRight;
        } else {
          return (TNode) root;
```

### AssignmentToMethodParameter
Assignment to method parameter `root`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
        TNode left = (TNode) root.avlLeft;
        TNode right = (TNode) root.avlRight;
        root = insertOrReplace.replace(key, root);
        root.avlLeft = left;
        root.avlRight = right;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java
        int ioSize = Math.min(buf.remaining(), NIO_BUFFER_LIMIT);
        buf.limit(buf.position() + ioSize);
        offset += ret;
        ret = reader.read(channel, buf, offset);
        if (ret < ioSize) {
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMutableByteRange.java`
#### Snippet
```java
      } else {
        bytes[offset + index + rPos] = (byte) ((val & 0x7F) | 0x80);
        val >>>= 7;
      }
      rPos++;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMutableByteRange.java`
#### Snippet
```java
    // one can get the same result.
    bytes[offset + index + 1] = (byte) val;
    val >>= 8;
    bytes[offset + index] = (byte) val;
    clearHashCache();
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMutableByteRange.java`
#### Snippet
```java
    for (int i = Bytes.SIZEOF_INT - 1; i > 0; i--) {
      bytes[offset + index + i] = (byte) val;
      val >>>= 8;
    }
    bytes[offset + index] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMutableByteRange.java`
#### Snippet
```java
    for (int i = Bytes.SIZEOF_LONG - 1; i > 0; i--) {
      bytes[offset + index + i] = (byte) val;
      val >>>= 8;
    }
    bytes[offset + index] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `cl`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
    int index = 0;

    for (; null != cl; cl = cl.getSuperclass()) {
      Field[] field = cl.getDeclaredFields();
      if (null != field) {
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  public static int putInt(byte[] bytes, int offset, int val) {
    if (LITTLE_ENDIAN) {
      val = Integer.reverseBytes(val);
    }
    HBasePlatformDependent.putInt(bytes, offset + BYTE_ARRAY_BASE_OFFSET, val);
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  public static int putLong(ByteBuffer buf, int offset, long val) {
    if (LITTLE_ENDIAN) {
      val = Long.reverseBytes(val);
    }
    if (buf.isDirect()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  public static int putShort(byte[] bytes, int offset, short val) {
    if (LITTLE_ENDIAN) {
      val = Short.reverseBytes(val);
    }
    HBasePlatformDependent.putShort(bytes, offset + BYTE_ARRAY_BASE_OFFSET, val);
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  public static int putInt(ByteBuffer buf, int offset, int val) {
    if (LITTLE_ENDIAN) {
      val = Integer.reverseBytes(val);
    }
    if (buf.isDirect()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  public static int putLong(byte[] bytes, int offset, long val) {
    if (LITTLE_ENDIAN) {
      val = Long.reverseBytes(val);
    }
    HBasePlatformDependent.putLong(bytes, offset + BYTE_ARRAY_BASE_OFFSET, val);
```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
      long size = (len > UNSAFE_COPY_THRESHOLD) ? UNSAFE_COPY_THRESHOLD : len;
      HBasePlatformDependent.copyMemory(src, srcAddr, dst, destAddr, size);
      len -= size;
      srcAddr += size;
      destAddr += size;
```

### AssignmentToMethodParameter
Assignment to method parameter `srcAddr`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
      HBasePlatformDependent.copyMemory(src, srcAddr, dst, destAddr, size);
      len -= size;
      srcAddr += size;
      destAddr += size;
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `destAddr`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
      len -= size;
      srcAddr += size;
      destAddr += size;
    }
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  public static int putShort(ByteBuffer buf, int offset, short val) {
    if (LITTLE_ENDIAN) {
      val = Short.reverseBytes(val);
    }
    if (buf.isDirect()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `index`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractByteRange.java`
#### Snippet
```java
    long result = 0;
    while (shift < 64) {
      final byte b = get(index++);
      result |= (long) (b & 0x7F) << shift;
      if ((b & 0x80) == 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractByteRange.java`
#### Snippet
```java
    int rPos = 0;
    while ((val & ~0x7F) != 0) {
      val >>>= 7;
      rPos++;
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
    }
    if (off + len > b.capacity()) {
      len = b.capacity() - off;
    }
    for (int i = off; i < off + len; ++i) {
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
    int len = -112;
    if (i < 0) {
      i ^= -1L; // take one's complement
      len = -120;
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  public static int putAsShort(ByteBuffer buf, int index, int val) {
    buf.put(index + 1, (byte) val);
    val >>= 8;
    buf.put(index, (byte) val);
    return index + Bytes.SIZEOF_SHORT;
```

### AssignmentToMethodParameter
Assignment to method parameter `index`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Struct.java`
#### Snippet
```java
    assert index >= 0;
    StructIterator it = iterator(src.shallowCopy());
    for (; index > 0; index--) {
      it.skip();
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
        for (int i = offset + 7; i > offset; i--) {
          bytes[i] = (byte) val;
          val >>>= 8;
        }
        bytes[offset] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
      int putShort(byte[] bytes, int offset, short val) {
        bytes[offset + 1] = (byte) val;
        val >>= 8;
        bytes[offset] = (byte) val;
        return offset + SIZEOF_SHORT;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
        for (int i = offset + 3; i > offset; i--) {
          bytes[i] = (byte) val;
          val >>>= 8;
        }
        bytes[offset] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    for (int i = 3; i > 0; i--) {
      b[i] = (byte) val;
      val >>>= 8;
    }
    b[0] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    byte[] b = new byte[SIZEOF_SHORT];
    b[1] = (byte) val;
    val >>= 8;
    b[0] = (byte) val;
    return b;
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    for (int i = 7; i > 0; i--) {
      b[i] = (byte) val;
      val >>>= 8;
    }
    b[0] = (byte) val;
```

### AssignmentToMethodParameter
Assignment to method parameter `len`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    // Just in case we are passed a 'len' that is > buffer length...
    if (off >= b.length) return result.toString();
    if (off + len > b.length) len = b.length - off;
    for (int i = off; i < off + len; ++i) {
      int ch = b[i] & 0xFF;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    byte[] valueBytes = val.unscaledValue().toByteArray();
    byte[] result = new byte[valueBytes.length + SIZEOF_INT];
    offset = putInt(result, offset, val.scale());
    return putBytes(result, offset, valueBytes, 0, valueBytes.length);
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `val`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    }
    bytes[offset + 1] = (byte) val;
    val >>= 8;
    bytes[offset] = (byte) val;
    return offset + SIZEOF_SHORT;
```

### AssignmentToMethodParameter
Assignment to method parameter `flength`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    }
    // Family length
    flength = family == null ? 0 : flength;
    if (flength > Byte.MAX_VALUE) {
      throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
```

### AssignmentToMethodParameter
Assignment to method parameter `namespace`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java`
#### Snippet
```java
  public static TableName valueOf(byte[] namespace, byte[] qualifier) {
    if (namespace == null || namespace.length < 1) {
      namespace = NamespaceDescriptor.DEFAULT_NAMESPACE_NAME;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `fullname`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java`
#### Snippet
```java
   */
  public static TableName valueOf(ByteBuffer fullname) {
    fullname = fullname.duplicate();
    fullname.mark();
    boolean miss = true;
```

### AssignmentToMethodParameter
Assignment to method parameter `namespaceAsString`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java`
#### Snippet
```java
  public static TableName valueOf(String namespaceAsString, String qualifierAsString) {
    if (namespaceAsString == null || namespaceAsString.length() < 1) {
      namespaceAsString = NamespaceDescriptor.DEFAULT_NAMESPACE_NAME_STR;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `percentString`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/field/FieldValue.java`
#### Snippet
```java
  private Float parsePercentString(String percentString) {
    if (percentString.endsWith("%")) {
      percentString = percentString.substring(0, percentString.length() - 1);
    }
    return Float.valueOf(percentString);
```

### AssignmentToMethodParameter
Assignment to method parameter `cellBlock`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
    // resources if the stream is not closed properly after we let it out.
    if (compressor != null) {
      cellBlock = decompress(compressor, cellBlock);
    }
    return codec.getDecoder(cellBlock);
```

### AssignmentToMethodParameter
Assignment to method parameter `os`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
        }
        poolCompressor = CodecPool.getCompressor(compressor);
        os = compressor.createOutputStream(os, poolCompressor);
      }
      Codec.Encoder encoder = codec.getEncoder(os);
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CompleteScanResultCache.java`
#### Snippet
```java
      partialResults.add(results[0]);
      start = 1;
      length--;
    } else {
      start = 0;
```

### AssignmentToMethodParameter
Assignment to method parameter `qualifier`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
    }
    if (qualifier == null) {
      qualifier = HConstants.EMPTY_BYTE_ARRAY;
    }
    set.add(qualifier);
```

### AssignmentToMethodParameter
Assignment to method parameter `maxCols`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
      if (entry.getValue() == null) {
        colCount++;
        --maxCols;
        familyList.add("ALL");
      } else {
```

### AssignmentToMethodParameter
Assignment to method parameter `maxCols`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
        }
        for (byte[] column : entry.getValue()) {
          if (--maxCols <= 0) {
            continue;
          }
```

### AssignmentToMethodParameter
Assignment to method parameter `error`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncScanSingleRegionRpcRetryingCaller.java`
#### Snippet
```java

  private void onError(Throwable error) {
    error = translateException(error);
    if (tries > startLogErrorsCnt) {
      LOG.warn("Call to " + loc.getServerName() + " for scanner id = " + scannerId + " for "
```

### AssignmentToMethodParameter
Assignment to method parameter `locs`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
        return oldLocs;
      }
      locs = mergedLocs;
    } else {
      // the region is different, here we trust the one we fetched. This maybe wrong but finally
```

### AssignmentToMethodParameter
Assignment to method parameter `qualifier`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
    }
    if (qualifier == null) {
      qualifier = HConstants.EMPTY_BYTE_ARRAY;
    }
    set.add(qualifier);
```

### AssignmentToMethodParameter
Assignment to method parameter `maxCols`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
      if (entry.getValue() == null) {
        colCount++;
        --maxCols;
        columns.add("ALL");
      } else {
```

### AssignmentToMethodParameter
Assignment to method parameter `maxCols`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
        }
        for (byte[] column : entry.getValue()) {
          if (--maxCols <= 0) {
            continue;
          }
```

### AssignmentToMethodParameter
Assignment to method parameter `results`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AllowPartialScanResultCache.java`
#### Snippet
```java
    recordLastResult(results[results.length - 1]);
    if (i > 0) {
      results = Arrays.copyOfRange(results, i, results.length);
    }
    for (Result result : results) {
```

### AssignmentToMethodParameter
Assignment to method parameter `row`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    // algorithm to locate it.
    if (locateType.equals(RegionLocateType.AFTER)) {
      row = createClosestRowAfter(row);
      locateType = RegionLocateType.CURRENT;
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `locateType`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    if (locateType.equals(RegionLocateType.AFTER)) {
      row = createClosestRowAfter(row);
      locateType = RegionLocateType.CURRENT;
    }
    return getRegionLocationsInternal(tableName, row, replicaId, locateType, reload);
```

### AssignmentToMethodParameter
Assignment to method parameter `maxCols`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
      // add details for each cell
      for (Cell cell : entry.getValue()) {
        if (--maxCols <= 0) {
          continue;
        }
```

### AssignmentToMethodParameter
Assignment to method parameter `data`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
      return null;
    }
    data = removeMetaData(data);
    return ClusterId.parseFrom(data).toString();
  }
```

### AssignmentToMethodParameter
Assignment to method parameter `data`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
      return null;
    }
    data = removeMetaData(data);
    int prefixLen = lengthOfPBMagic();
    return ZooKeeperProtos.Master.parser().parseFrom(data, prefixLen, data.length - prefixLen);
```

### AssignmentToMethodParameter
Assignment to method parameter `data`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
      return null;
    }
    data = removeMetaData(data);
    int prefixLen = lengthOfPBMagic();
    return ZooKeeperProtos.MetaRegionServer.parser().parseFrom(data, prefixLen,
```

### AssignmentToMethodParameter
Assignment to method parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  static Throwable translateException(Throwable t) {
    if (t instanceof UndeclaredThrowableException && t.getCause() != null) {
      t = t.getCause();
    }
    if (t instanceof RemoteException) {
```

### AssignmentToMethodParameter
Assignment to method parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    }
    if (t instanceof RemoteException) {
      t = ((RemoteException) t).unwrapRemoteException();
    }
    if (t instanceof ServiceException && t.getCause() != null) {
```

### AssignmentToMethodParameter
Assignment to method parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    }
    if (t instanceof ServiceException && t.getCause() != null) {
      t = translateException(t.getCause());
    }
    return t;
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java`
#### Snippet
```java
  public int compareTo(ByteBuffer value, int offset, int length) {
    if (this.value.length <= length) {
      length = this.value.length;
    }
    return ByteBufferUtils.compareTo(this.value, 0, this.value.length, value, offset, length);
```

### AssignmentToMethodParameter
Assignment to method parameter `start`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  private List<byte[]> getStartKeysInRange(byte[] start, byte[] end) throws IOException {
    if (start == null) {
      start = HConstants.EMPTY_START_ROW;
    }
    if (end == null) {
```

### AssignmentToMethodParameter
Assignment to method parameter `end`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
    }
    if (end == null) {
      end = HConstants.EMPTY_END_ROW;
    }
    return getKeysAndRegionsInRange(start, end, true).getFirst();
```

### AssignmentToMethodParameter
Assignment to method parameter `length`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
      return SatisfiesCode.YES;
    }
    length = Math.min(length, fuzzyKeyBytes.length);
    int numWords = length / Bytes.SIZEOF_LONG;

```

### AssignmentToMethodParameter
Assignment to method parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java

    if (t instanceof UndeclaredThrowableException) {
      t = t.getCause();
    }
    if (t instanceof RemoteException) {
```

### AssignmentToMethodParameter
Assignment to method parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
    }
    if (t instanceof RemoteException) {
      t = ((RemoteException) t).unwrapRemoteException();
    }
    if (t instanceof DoNotRetryIOException) {
```

### AssignmentToMethodParameter
Assignment to method parameter `cur`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java

  public static boolean isMetaClearingException(Throwable cur) {
    cur = findException(cur);

    if (cur == null) {
```

### AssignmentToMethodParameter
Assignment to method parameter `proc`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
    Procedure<T> proc) {
    while (proc.hasParent()) {
      proc = procedures.get(proc.getParentProcId());
      if (proc == null) {
        return null;
```

### AssignmentToMethodParameter
Assignment to method parameter `minCapacity`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/util/ByteSlot.java`
#### Snippet
```java

  private void ensureCapacity(int minCapacity) {
    minCapacity = (minCapacity + (GROW_ALIGN - 1)) & -GROW_ALIGN;
    if (buf == null) {
      buf = new byte[minCapacity];
```

### AssignmentToMethodParameter
Assignment to method parameter `maxRows`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java
        // specified number of rows
        if (maxRows > 0) {
          if (--maxRows == 0) {
            generator.putBack(value);
            break;
```

### AssignmentToMethodParameter
Assignment to method parameter `model`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
    if (model == null) {
      try {
        model = new NamespacesInstanceModel(namespace);
      } catch (IOException ioe) {
        servlet.getMetrics().incrementFailedPutRequests(1);
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      throw new RuntimeException(e);
    }
    i += query.length();
    int j = 0;
    while (j < query.length()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
          this.columns.add(Bytes.toBytes(s));
          column.setLength(0);
          i++;
          continue;
        }
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
        }
        column.append(c);
        i++;
      }
      i++;
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
        i++;
      }
      i++;
      // trailing list entry
      if (column.length() > 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
        }
        stamp.append(c);
        i++;
      }
      try {
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      if (c == ',') {
        stamp = new StringBuilder();
        i++;
        while (i < path.length() && ((c = path.charAt(i)) != '/')) {
          stamp.append(c);
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
        while (i < path.length() && ((c = path.charAt(i)) != '/')) {
          stamp.append(c);
          i++;
        }
        try {
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      }
      if (c == '/') {
        i++;
      }
    } catch (IndexOutOfBoundsException e) {
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
      while (i < path.length() && (c = path.charAt(i)) != '/') {
        sb.append(c);
        i++;
      }
      i++;
```

### AssignmentToMethodParameter
Assignment to method parameter `i`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java
        i++;
      }
      i++;
      String row = startRow = sb.toString();
      int idx = startRow.indexOf(',');
```

### AssignmentToMethodParameter
Assignment to method parameter `builder`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static MutationProto toMutation(final MutationType type, final Mutation mutation,
    MutationProto.Builder builder, long nonce) throws IOException {
    builder = getMutationBuilderAndSetCommonFields(type, mutation, builder);
    if (nonce != HConstants.NO_NONCE) {
      builder.setNonce(nonce);
```

### AssignmentToMethodParameter
Assignment to method parameter `timeRange`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static HBaseProtos.TimeRange toTimeRange(TimeRange timeRange) {
    if (timeRange == null) {
      timeRange = TimeRange.allTime();
    }
    return HBaseProtos.TimeRange.newBuilder().setFrom(timeRange.getMin()).setTo(timeRange.getMax())
```

### AssignmentToMethodParameter
Assignment to method parameter `method`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
          put.setEntity(((HttpPut) method).getEntity());
          put.setHeaders(method.getAllHeaders());
          method = put;
        } else if (method instanceof HttpGet) {
          method = new HttpGet(uri);
```

### AssignmentToMethodParameter
Assignment to method parameter `method`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
          method = put;
        } else if (method instanceof HttpGet) {
          method = new HttpGet(uri);
        } else if (method instanceof HttpHead) {
          method = new HttpHead(uri);
```

### AssignmentToMethodParameter
Assignment to method parameter `method`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
          method = new HttpGet(uri);
        } else if (method instanceof HttpHead) {
          method = new HttpHead(uri);
        } else if (method instanceof HttpDelete) {
          method = new HttpDelete(uri);
```

### AssignmentToMethodParameter
Assignment to method parameter `method`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
          method = new HttpHead(uri);
        } else if (method instanceof HttpDelete) {
          method = new HttpDelete(uri);
        } else if (method instanceof HttpPost) {
          HttpPost post = new HttpPost(uri);
```

### AssignmentToMethodParameter
Assignment to method parameter `method`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
          post.setEntity(((HttpPost) method).getEntity());
          post.setHeaders(method.getAllHeaders());
          method = post;
        }
        return executeURI(method, headers, uri.toString());
```

### AssignmentToMethodParameter
Assignment to method parameter `headers`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
      method.setEntity(new InputStreamEntity(new ByteArrayInputStream(content), content.length));
      HttpResponse resp = execute(cluster, method, headers, path);
      headers = resp.getAllHeaders();
      content = getResponseBody(resp);
      return new Response(resp.getStatusLine().getStatusCode(), headers, content);
```

### AssignmentToMethodParameter
Assignment to method parameter `content`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
      HttpResponse resp = execute(cluster, method, headers, path);
      headers = resp.getAllHeaders();
      content = getResponseBody(resp);
      return new Response(resp.getStatusLine().getStatusCode(), headers, content);
    } finally {
```

### AssignmentToMethodParameter
Assignment to method parameter `headers`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
      method.setEntity(new InputStreamEntity(new ByteArrayInputStream(content), content.length));
      HttpResponse resp = execute(cluster, method, headers, path);
      headers = resp.getAllHeaders();
      content = getResponseBody(resp);
      return new Response(resp.getStatusLine().getStatusCode(), headers, content);
```

### AssignmentToMethodParameter
Assignment to method parameter `content`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
      HttpResponse resp = execute(cluster, method, headers, path);
      headers = resp.getAllHeaders();
      content = getResponseBody(resp);
      return new Response(resp.getStatusLine().getStatusCode(), headers, content);
    } finally {
```

### AssignmentToMethodParameter
Assignment to method parameter `identifier`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java
    if (identifier == null || identifier.length() == 0) {
      // the identifier = processID@hostName
      identifier = ManagementFactory.getRuntimeMXBean().getName();
    }
    LOG.info("Process identifier={} connecting to ZooKeeper ensemble={}", identifier,
```

### AssignmentToMethodParameter
Assignment to method parameter `node`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java

  private BitSetNode update(BitSetNode node, long procId) {
    node = lookupClosestNode(node, procId);
    assert node != null : "expected node to update procId=" + procId;
    assert node.contains(procId) : "expected procId=" + procId + " in the node";
```

### AssignmentToMethodParameter
Assignment to method parameter `node`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java

  private BitSetNode delete(BitSetNode node, long procId) {
    node = lookupClosestNode(node, procId);
    if (node == null || !node.contains(procId)) {
      LOG.warn("The BitSetNode for procId={} does not exist, maybe a double deletion?", procId);
```

### AssignmentToMethodParameter
Assignment to method parameter `node`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
  private BitSetNode insert(BitSetNode node, long procId) {
    if (node == null || !node.contains(procId)) {
      node = getOrCreateNode(procId);
    }
    node.insertOrUpdate(procId);
```

### AssignmentToMethodParameter
Assignment to method parameter `fs`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/util/RecoverLeaseFSUtils.java`
#### Snippet
```java
    CancelableProgressable reporter) throws IOException {
    if (fs instanceof FilterFileSystem) {
      fs = ((FilterFileSystem) fs).getRawFileSystem();
    }
    // lease recovery not needed for local file system case.
```

### AssignmentToMethodParameter
Assignment to method parameter `overwrite`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
        }
        // overwrite the old broken file.
        overwrite = true;
        try {
          Thread.sleep(ConnectionUtils.getPauseTime(100, retry));
```

### AssignmentToMethodParameter
Assignment to method parameter `ofs`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ResultSerialization.java`
#### Snippet
```java
      int maxRead = 8192;

      for (; ofs < len; ofs += maxRead)
        in.readFully(dest, ofs, Math.min(len - ofs, maxRead));
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `n`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
    }
    // if n < 1, then still continue using n = 1
    n = n < 1 ? 1 : n;
    List<InputSplit> res = new ArrayList<>(n);
    if (n == 1) {
```

### AssignmentToMethodParameter
Assignment to method parameter `size`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      .setHfile(path.toString()).build();
    if (size == -1) {
      size = HFileLink.buildFromHFileLinkPattern(conf, path).getFileStatus(fs).getLen();
    }
    return new Pair<>(fileInfo, size);
```

### AssignmentToMethodParameter
Assignment to method parameter `sourceCell`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
    private Cell checkAndResetTimestamp(Cell sourceCell) {
      if (ignoreTimestamp) {
        sourceCell =
          CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setType(sourceCell.getType())
            .setRow(sourceCell.getRowArray(), sourceCell.getRowOffset(), sourceCell.getRowLength())
```

### AssignmentToMethodParameter
Assignment to method parameter `numTopsAtMost`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java

    if (numTopsAtMost < 1) { // invalid if numTopsAtMost < 1, correct it to be 1
      numTopsAtMost = 1;
    }
    int top = Math.min(numTopsAtMost, hostAndWeights.length);
```

### AssignmentToMethodParameter
Assignment to method parameter `restoreDir`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
    FileSystem fs = rootDir.getFileSystem(conf);

    restoreDir = new Path(restoreDir, UUID.randomUUID().toString());

    RestoreSnapshotHelper.copySnapshotForScanner(conf, fs, rootDir, restoreDir, snapshotName);
```

### AssignmentToMethodParameter
Assignment to method parameter `delete`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
        } else if (CellUtil.isDelete(kv)) {
          if (delete == null) {
            delete = new Delete(key.get());
          }
          delete.add(kv);
```

### AssignmentToMethodParameter
Assignment to method parameter `put`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
        } else {
          if (put == null) {
            put = new Put(key.get());
          }
          addPutToKv(put, kv);
```

### AssignmentToMethodParameter
Assignment to method parameter `kv`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
      if (newCfName != null) {
        List<Tag> tags = PrivateCellUtil.getTags(kv);
        kv = new KeyValue(kv.getRowArray(), // row buffer
          kv.getRowOffset(), // row offset
          kv.getRowLength(), // row length
```

### AssignmentToMethodParameter
Assignment to method parameter `name`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpRequestLog.java`
#### Snippet
```java
    String lookup = SERVER_TO_COMPONENT.get(name);
    if (lookup != null) {
      name = lookup;
    }
    String loggerName = "http.requests." + name;
```

### AssignmentToMethodParameter
Assignment to method parameter `request`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java

          final UserGroupInformation ugiF = requestUgi;
          request = new HttpServletRequestWrapper(request) {
            @Override
            public String getRemoteUser() {
```

### AssignmentToMethodParameter
Assignment to method parameter `attribute`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONBean.java`
#### Snippet
```java
              }
              // nullify the attribute
              attribute = null;
            } else {
              prs = attribute;
```

### AssignmentToMethodParameter
Assignment to method parameter `conf`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
    final FilterInitializer[] initializers = getFilterInitializers(conf);
    if (initializers != null) {
      conf = new Configuration(conf);
      conf.set(BIND_ADDRESS, hostName);
      for (FilterInitializer c : initializers) {
```

### AssignmentToMethodParameter
Assignment to method parameter `exception`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
  private MultiException addMultiException(MultiException exception, Exception e) {
    if (exception == null) {
      exception = new MultiException();
    }
    exception.add(e);
```

### AssignmentToMethodParameter
Assignment to method parameter `conf`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
    // their header magic numbers. See HBASE-5885
    if (useHBaseChecksum && !(fs instanceof LocalFileSystem)) {
      conf = new Configuration(conf);
      conf.setBoolean("dfs.client.read.shortcircuit.skip.checksum", true);
      this.noChecksumFs = maybeWrapFileSystem(newInstanceFileSystem(conf), conf);
```

### AssignmentToMethodParameter
Assignment to method parameter `newException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
    IOException newException, IOException previousException) throws IOException {
    if (newException instanceof RemoteException) {
      newException = ((RemoteException) newException)
        .unwrapRemoteException(FileNotFoundException.class, AccessControlException.class);
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `previousException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
      // Try another file location
      if (previousException == null) {
        previousException = new FileNotFoundException(fileLink.toString());
      }
    } else if (newException instanceof AccessControlException) {
```

### AssignmentToMethodParameter
Assignment to method parameter `previousException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
    } else if (newException instanceof AccessControlException) {
      // Try another file location
      previousException = newException;
    } else {
      throw newException;
```

### AssignmentToMethodParameter
Assignment to method parameter `unassignedRegions`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
    RegionHDFSBlockLocationFinder regionFinder, RackManager rackManager) {
    if (unassignedRegions == null) {
      unassignedRegions = Collections.emptyList();
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
    @Override
    protected long calculateHeapSizeForBlockKeys(long heapSize) {
      heapSize = super.calculateHeapSizeForBlockKeys(heapSize);
      if (seeker != null) {
        heapSize += ClassSize.REFERENCE;
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      heapSize = super.calculateHeapSizeForBlockKeys(heapSize);
      if (seeker != null) {
        heapSize += ClassSize.REFERENCE;
        heapSize += ClassSize.align(seeker.heapSize());
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      if (seeker != null) {
        heapSize += ClassSize.REFERENCE;
        heapSize += ClassSize.align(seeker.heapSize());
      }
      return heapSize;
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      // Calculating the size of blockKeys
      if (blockKeys != null) {
        heapSize += ClassSize.REFERENCE;
        // Adding array + references overhead
        heapSize += ClassSize.align(ClassSize.ARRAY + blockKeys.length * ClassSize.REFERENCE);
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
        heapSize += ClassSize.REFERENCE;
        // Adding array + references overhead
        heapSize += ClassSize.align(ClassSize.ARRAY + blockKeys.length * ClassSize.REFERENCE);

        // Adding bytes
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
        // Adding bytes
        for (byte[] key : blockKeys) {
          heapSize += ClassSize.align(ClassSize.ARRAY + key.length);
        }
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
    protected long calculateHeapSizeForBlockKeys(long heapSize) {
      if (blockKeys != null) {
        heapSize += ClassSize.REFERENCE;
        // Adding array + references overhead
        heapSize += ClassSize.align(ClassSize.ARRAY + blockKeys.length * ClassSize.REFERENCE);
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
        heapSize += ClassSize.REFERENCE;
        // Adding array + references overhead
        heapSize += ClassSize.align(ClassSize.ARRAY + blockKeys.length * ClassSize.REFERENCE);

        // Adding blockKeys
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
        // Adding blockKeys
        for (Cell key : blockKeys) {
          heapSize += ClassSize.align(key.heapSize());
        }
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `heapSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      }
      // Add comparator and the midkey atomicreference
      heapSize += 2 * ClassSize.REFERENCE;
      return heapSize;
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `offsetFromPos`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
        int remaining = len - 1;
        long i = 0;
        offsetFromPos++;
        if (remaining >= Bytes.SIZEOF_INT) {
          // The int read has to be converted to unsigned long so the & op
```

### AssignmentToMethodParameter
Assignment to method parameter `offsetFromPos`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
          i = (blockBuffer.getIntAfterPosition(offsetFromPos) & 0x00000000ffffffffL);
          remaining -= Bytes.SIZEOF_INT;
          offsetFromPos += Bytes.SIZEOF_INT;
        }
        if (remaining >= Bytes.SIZEOF_SHORT) {
```

### AssignmentToMethodParameter
Assignment to method parameter `offsetFromPos`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
          i = i | (s & 0xFFFF);
          remaining -= Bytes.SIZEOF_SHORT;
          offsetFromPos += Bytes.SIZEOF_SHORT;
        }
        for (int idx = 0; idx < remaining; idx++) {
```

### AssignmentToMethodParameter
Assignment to method parameter `cacheBlock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
        new BlockCacheKey(name, metaBlockOffset, this.isPrimaryReplicaReader(), BlockType.META);

      cacheBlock &= cacheConf.shouldCacheBlockOnRead(BlockType.META.getCategory());
      HFileBlock cachedBlock =
        getCachedBlock(cacheKey, cacheBlock, false, true, BlockType.META, null);
```

### AssignmentToMethodParameter
Assignment to method parameter `cacheBlock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    // we can really only check for that if have an expectedBlockType
    if (expectedBlockType != null) {
      cacheBlock &= cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory());
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `value`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
      }
    } else {
      value = asReferencedHeapBlock(value);
      cache.put(key, value);
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `buf`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    }
    // Ensure that the block is an heap one.
    buf = asReferencedHeapBlock(buf);
    cb = new LruCachedBlock(cacheKey, buf, count.incrementAndGet(), inMemory);
    long newSize = updateSizeMetrics(cb, false);
```

### AssignmentToMethodParameter
Assignment to method parameter `buf`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    }
    // Ensure that the block is an heap one.
    buf = asReferencedHeapBlock(buf);
    cb = new LruCachedBlock(cacheKey, buf, count.incrementAndGet(), inMemory);
    long newSize = updateSizeMetrics(cb, false);
```

### AssignmentToMethodParameter
Assignment to method parameter `heavyEvictionOverheadCoefficient`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    this.heavyEvictionMbSizeLimit = Math.max(heavyEvictionMbSizeLimit, 1);
    this.cacheDataBlockPercent = 100;
    heavyEvictionOverheadCoefficient = Math.min(heavyEvictionOverheadCoefficient, 1.0f);
    heavyEvictionOverheadCoefficient = Math.max(heavyEvictionOverheadCoefficient, 0.001f);
    this.heavyEvictionOverheadCoefficient = heavyEvictionOverheadCoefficient;
```

### AssignmentToMethodParameter
Assignment to method parameter `heavyEvictionOverheadCoefficient`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    this.cacheDataBlockPercent = 100;
    heavyEvictionOverheadCoefficient = Math.min(heavyEvictionOverheadCoefficient, 1.0f);
    heavyEvictionOverheadCoefficient = Math.max(heavyEvictionOverheadCoefficient, 0.001f);
    this.heavyEvictionOverheadCoefficient = heavyEvictionOverheadCoefficient;

```

### AssignmentToMethodParameter
Assignment to method parameter `bb`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
   */
  public ByteBuffer getMetaData(ByteBuffer bb) {
    bb = addMetaData(bb, true);
    bb.flip();
    return bb;
```

### AssignmentToMethodParameter
Assignment to method parameter `destination`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
  public void serialize(ByteBuffer destination, boolean includeNextBlockMetadata) {
    this.buf.get(destination, 0, getSerializedLength() - BLOCK_METADATA_SPACE);
    destination = addMetaData(destination, includeNextBlockMetadata);

    // Make it ready for reading. flip sets position to zero and limit to current position which
```

### AssignmentToMethodParameter
Assignment to method parameter `fileContext`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      onDiskDataSizeWithHeader = onDiskSizeWithoutHeader + headerSize(usesHBaseChecksum);
    }
    fileContext = fileContextBuilder.build();
    assert usesHBaseChecksum == fileContext.isUseHBaseChecksum();
    return new HFileBlockBuilder().withBlockType(blockType)
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
    private void putHeader(byte[] dest, int offset, int onDiskSize, int uncompressedSize,
      int onDiskDataSize) {
      offset = blockType.put(dest, offset);
      offset = Bytes.putInt(dest, offset, onDiskSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putInt(dest, offset, uncompressedSize - HConstants.HFILEBLOCK_HEADER_SIZE);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      int onDiskDataSize) {
      offset = blockType.put(dest, offset);
      offset = Bytes.putInt(dest, offset, onDiskSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putInt(dest, offset, uncompressedSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putLong(dest, offset, prevOffset);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      offset = blockType.put(dest, offset);
      offset = Bytes.putInt(dest, offset, onDiskSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putInt(dest, offset, uncompressedSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putLong(dest, offset, prevOffset);
      offset = Bytes.putByte(dest, offset, fileContext.getChecksumType().getCode());
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      offset = Bytes.putInt(dest, offset, onDiskSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putInt(dest, offset, uncompressedSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putLong(dest, offset, prevOffset);
      offset = Bytes.putByte(dest, offset, fileContext.getChecksumType().getCode());
      offset = Bytes.putInt(dest, offset, fileContext.getBytesPerChecksum());
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      offset = Bytes.putInt(dest, offset, uncompressedSize - HConstants.HFILEBLOCK_HEADER_SIZE);
      offset = Bytes.putLong(dest, offset, prevOffset);
      offset = Bytes.putByte(dest, offset, fileContext.getChecksumType().getCode());
      offset = Bytes.putInt(dest, offset, fileContext.getBytesPerChecksum());
      Bytes.putInt(dest, offset, onDiskDataSize);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      offset = Bytes.putLong(dest, offset, prevOffset);
      offset = Bytes.putByte(dest, offset, fileContext.getChecksumType().getCode());
      offset = Bytes.putInt(dest, offset, fileContext.getBytesPerChecksum());
      Bytes.putInt(dest, offset, onDiskDataSize);
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `value`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketEntry.java`
#### Snippet
```java
  private void setOffset(long value) {
    assert (value & 0xFF) == 0;
    value >>= 8;
    offsetBase = (int) value;
    offset1 = (byte) (value >> 32);
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java

    private void free(long offset) {
      offset -= baseOffset;
      assert offset >= 0;
      assert offset < itemCount * itemAllocationSize;
```

### AssignmentToMethodParameter
Assignment to method parameter `offset`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java

    public void addAllocation(long offset) throws BucketAllocatorException {
      offset -= baseOffset;
      if (offset < 0 || offset % itemAllocationSize != 0)
        throw new BucketAllocatorException("Attempt to add allocation for bad offset: " + offset
```

### AssignmentToMethodParameter
Assignment to method parameter `bucketEntry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
    boolean existedInRamCache = removeFromRamCache(cacheKey);
    if (bucketEntry == null) {
      bucketEntry = backingMap.get(cacheKey);
    }
    final BucketEntry bucketEntryToUse = bucketEntry;
```

### AssignmentToMethodParameter
Assignment to method parameter `compression`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java
    Consumer<Path> writerCreationTracker) throws IOException {
    if (compression == null) {
      compression = HFile.DEFAULT_COMPRESSION_ALGORITHM;
    }
    final CacheConfig writerCacheConf;
```

### AssignmentToMethodParameter
Assignment to method parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
    private void logAndThrowWriterAppendFailure(WAL.Entry logEntry, IOException e)
      throws IOException {
      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
      final String errorMsg = "Failed to write log entry " + logEntry.toString() + " to log";
      LOG.error(HBaseMarkers.FATAL, errorMsg, e);
```

### AssignmentToMethodParameter
Assignment to method parameter `providerId`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java`
#### Snippet
```java
    }
    if (null == providerId) {
      providerId = "defaultDisabled";
    }
    disabled = new DisabledWAL(new Path(CommonFSUtils.getWALRootDir(conf), providerId), conf, null);
```

### AssignmentToMethodParameter
Assignment to method parameter `conf`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
  public void setConf(Configuration conf) {
    if (conf == null) {
      conf = HBaseConfiguration.create();
    }
    this.conf = conf;
```

### AssignmentToMethodParameter
Assignment to method parameter `start`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
    public byte[][] split(byte[] start, byte[] end, int numSplits, boolean inclusive) {
      if (Arrays.equals(start, HConstants.EMPTY_BYTE_ARRAY)) {
        start = firstRowBytes;
      }
      if (Arrays.equals(end, HConstants.EMPTY_BYTE_ARRAY)) {
```

### AssignmentToMethodParameter
Assignment to method parameter `end`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
      }
      if (Arrays.equals(end, HConstants.EMPTY_BYTE_ARRAY)) {
        end = lastRowBytes;
      }
      Preconditions.checkArgument(Bytes.compareTo(end, start) > 0,
```

### AssignmentToMethodParameter
Assignment to method parameter `end`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java`
#### Snippet
```java
        int cmp = comparator.compare(mitem, key);
        if (cmp > 0) {
          end = mid;
        } else {
          start = mid + 1;
```

### AssignmentToMethodParameter
Assignment to method parameter `start`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java`
#### Snippet
```java
          end = mid;
        } else {
          start = mid + 1;
        }
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `codec`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java

  public static boolean testCompression(String codec) {
    codec = codec.toLowerCase(Locale.ROOT);

    Compression.Algorithm a;
```

### AssignmentToMethodParameter
Assignment to method parameter `htd`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java`
#### Snippet
```java
    scan.setIsolationLevel(IsolationLevel.READ_UNCOMMITTED);

    htd = TableDescriptorBuilder.newBuilder(htd).setReadOnly(true).build();

    // open region from the snapshot directory
```

### AssignmentToMethodParameter
Assignment to method parameter `timeout`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/DirScanPool.java`
#### Snippet
```java
      try {
        wait(timeout);
        timeout = stopTime - EnvironmentEdgeManager.currentTime();
      } catch (InterruptedException ie) {
        Thread.currentThread().interrupt();
```

### AssignmentToMethodParameter
Assignment to method parameter `retries`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
            throw (InterruptedIOException) new InterruptedIOException().initCause(ie);
          }
          retries--;
        } else {
          throw e;
```

### AssignmentToMethodParameter
Assignment to method parameter `threadPoolSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java

    // lower the number of threads in case we have very few expected regions
    threadPoolSize = Math.min(threadPoolSize, statusList.length);

    // run in multiple threads
```

### AssignmentToMethodParameter
Assignment to method parameter `options`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    // we return all information to client if the list of Option is empty.
    if (options.isEmpty()) {
      options = EnumSet.allOf(Option.class);
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `destServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      LOG.info(Bytes.toString(encodedRegionName) + " can not move to "
        + Bytes.toString(destServerName) + " because the server is in exclude list");
      destServerName = null;
    }
    if (destServerName == null || destServerName.length == 0) {
```

### AssignmentToMethodParameter
Assignment to method parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
    private IOException unwrapException(IOException e) {
      if (e instanceof RemoteException) {
        e = ((RemoteException) e).unwrapRemoteException();
      }
      return e;
```

### AssignmentToMethodParameter
Assignment to method parameter `snapshotDir`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    // first create the snapshot root path and check to see if it exists
    FileSystem fs = master.getMasterFileSystem().getFileSystem();
    if (snapshotDir == null) snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);

    // if there are no snapshots, return an empty list
```

### AssignmentToMethodParameter
Assignment to method parameter `withCpCall`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
      new SnapshotDescriptionUtils.CompletedSnaphotDirectoriesFilter(fs));
    MasterCoprocessorHost cpHost = master.getMasterCoprocessorHost();
    withCpCall = withCpCall && cpHost != null;
    // loop through all the completed snapshots
    for (FileStatus snapshot : snapshots) {
```

### AssignmentToMethodParameter
Assignment to method parameter `snapshot`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java

    // set the snapshot to be a disabled snapshot, since the client doesn't know about that
    snapshot = snapshot.toBuilder().setType(Type.DISABLED).build();

    // Take the snapshot of the disabled table
```

### AssignmentToMethodParameter
Assignment to method parameter `snapshot`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    // Get snapshot info from file system. The one passed as parameter is a "fake" snapshotInfo with
    // just the "name" and it does not contains the "real" snapshot information
    snapshot = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);

    // call coproc pre hook
```

### AssignmentToMethodParameter
Assignment to method parameter `newRegions`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java`
#### Snippet
```java

    // 2. Create Regions
    newRegions = hdfsRegionHandler.createHdfsRegions(env, mfs.getRootDir(),
      tableDescriptor.getTableName(), newRegions);

```

### AssignmentToMethodParameter
Assignment to method parameter `treeMap`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
    while (treeMap != null) {
      Queue<T> node = AvlTree.getFirst(treeMap);
      treeMap = AvlTree.remove(treeMap, node.getKey(), comparator);
      if (fairq != null) {
        removeFromRunQueue(fairq, node, () -> "clear all queues");
```

### AssignmentToMethodParameter
Assignment to method parameter `newRegions`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java`
#### Snippet
```java

    // 2. Create Regions
    newRegions = hdfsRegionHandler.createHdfsRegions(env, mfs.getRootDir(),
      tableDescriptor.getTableName(), newRegions);

```

### AssignmentToMethodParameter
Assignment to method parameter `peerConfig`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java`
#### Snippet
```java
      return;
    }
    peerConfig = ReplicationPeerConfigUtil.updateReplicationBasePeerConfigs(conf, peerConfig);
    ReplicationPeerConfig copiedPeerConfig = ReplicationPeerConfig.newBuilder(peerConfig).build();
    SyncReplicationState syncReplicationState = copiedPeerConfig.isSyncReplication()
```

### AssignmentToMethodParameter
Assignment to method parameter `quotas`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java`
#### Snippet
```java
          @Override
          public void visitUserQuotas(String userName, Quotas quotas) {
            quotas = updateClusterQuotaToMachineQuota(quotas, factor);
            quotaInfo.setQuotas(quotas);
          }
```

### AssignmentToMethodParameter
Assignment to method parameter `quotas`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java`
#### Snippet
```java
          @Override
          public void visitUserQuotas(String userName, String namespace, Quotas quotas) {
            quotas = updateClusterQuotaToMachineQuota(quotas, factor);
            quotaInfo.setQuotas(namespace, quotas);
          }
```

### AssignmentToMethodParameter
Assignment to method parameter `quotas`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java`
#### Snippet
```java
          @Override
          public void visitUserQuotas(String userName, TableName table, Quotas quotas) {
            quotas = updateClusterQuotaToMachineQuota(quotas,
              tableMachineQuotaFactors.containsKey(table)
                ? tableMachineQuotaFactors.get(table)
```

### AssignmentToMethodParameter
Assignment to method parameter `limiters`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
    final Quotas quotas) {
    if (limiters == null) {
      limiters = new HashMap<>();
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `table`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
    }
    if (table == null) {
      table = PermissionStorage.ACL_TABLE_NAME;
    }
    if (authorizeUserNamespace(user, table.getNamespaceAsString(), action)) {
```

### AssignmentToMethodParameter
Assignment to method parameter `table`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
    }
    if (table == null) {
      table = PermissionStorage.ACL_TABLE_NAME;
    }
    if (authorizeUserNamespace(user, table.getNamespaceAsString(), action)) {
```

### AssignmentToMethodParameter
Assignment to method parameter `entryName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    Table t, byte[] cf, byte[] cq, String user, boolean hasFilterUser) throws IOException {
    if (entryName == null) {
      entryName = ACL_GLOBAL_NAME;
    }
    // for normal user tables, we just read the table row from _acl_
```

### AssignmentToMethodParameter
Assignment to method parameter `serversToExclude`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    ) {
      LOG.debug("Only one region server found and hence going ahead with the assignment");
      serversToExclude = null;
    }
    try {
```

### AssignmentToMethodParameter
Assignment to method parameter `index`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  private int skipSpaces(byte[] exp, int index) {
    while (index < exp.length - 1 && exp[index + 1] == SPACE) {
      index++;
    }
    return index;
```

### AssignmentToMethodParameter
Assignment to method parameter `expS`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java

  public ExpressionNode parse(String expS) throws ParseException {
    expS = expS.trim();
    Stack<ExpressionNode> expStack = new Stack<>();
    int index = 0;
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
      // Merge the single right node into the left side
      assert leftChild instanceof NonLeafExpressionNode;
      newNode = mergeChildNodes(newNode, outerOp, rightChild, (NonLeafExpressionNode) leftChild);
    } else if (leftChild.isSingleNode()) {
      // Merge the single left node into the right side
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
      // Merge the single left node into the right side
      assert rightChild instanceof NonLeafExpressionNode;
      newNode = mergeChildNodes(newNode, outerOp, leftChild, (NonLeafExpressionNode) rightChild);
    } else {
      // Both the child exp nodes are non single.
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
        NonLeafExpressionNode leftChildNLEClone = leftChildNLE.deepClone();
        leftChildNLEClone.addChildExps(rightChildNLE.getChildExps());
        newNode = leftChildNLEClone;
      } else {
        // (a | b) & (c & d) ...
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
          ) {
            leftChildNLE.addChildExp(rightChildNLE);
            newNode = leftChildNLE;
          } else if (
            leftChildNLE.getOperator() == Operator.AND && rightChildNLE.getOperator() == Operator.OR
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
            // (a & b) | (c | d)
            rightChildNLE.addChildExp(leftChildNLE);
            newNode = rightChildNLE;
          }
          // (a & b) | (c & d)
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
            leftChildNLE.getOperator() == Operator.OR && rightChildNLE.getOperator() == Operator.AND
          ) {
            newNode = new NonLeafExpressionNode(Operator.OR);
            for (ExpressionNode exp : leftChildNLE.getChildExps()) {
              NonLeafExpressionNode rightChildNLEClone = rightChildNLE.deepClone();
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
          ) {
            // (a & b) & (c | d) => (a & b & c) | (a & b & d)
            newNode = new NonLeafExpressionNode(Operator.OR);
            for (ExpressionNode exp : rightChildNLE.getChildExps()) {
              NonLeafExpressionNode leftChildNLEClone = leftChildNLE.deepClone();
```

### AssignmentToMethodParameter
Assignment to method parameter `newNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
          } else {
            // (a | b) & (c | d) => (a & c) | (a & d) | (b & c) | (b & d)
            newNode = new NonLeafExpressionNode(Operator.OR);
            for (ExpressionNode leftExp : leftChildNLE.getChildExps()) {
              for (ExpressionNode rightExp : rightChildNLE.getChildExps()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `newOuterNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
      NonLeafExpressionNode leftChildNLEClone = nlChild.deepClone();
      leftChildNLEClone.addChildExp(lChild);
      newOuterNode = leftChildNLEClone;
    } else if (outerOp == Operator.AND) {
      assert nlChild.getOperator() == Operator.OR;
```

### AssignmentToMethodParameter
Assignment to method parameter `newOuterNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
      // OR
      // c & (a | b) -> (c & a) | (c & b)
      newOuterNode = new NonLeafExpressionNode(Operator.OR);
      for (ExpressionNode exp : nlChild.getChildExps()) {
        newOuterNode.addChildExp(new NonLeafExpressionNode(Operator.AND, exp, lChild));
```

### AssignmentToMethodParameter
Assignment to method parameter `authorizations`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
        authLabels = scanLabelGenerator.getLabels(VisibilityUtils.getActiveUser(), authorizations);
        authLabels = (authLabels == null) ? new ArrayList<>() : authLabels;
        authorizations = new Authorizations(authLabels);
      } catch (Throwable t) {
        LOG.error(t.toString(), t);
```

### AssignmentToMethodParameter
Assignment to method parameter `snapshot`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java`
#### Snippet
```java
    });

    snapshot = builder.build();

    // set the acl to snapshot if security feature is enabled.
```

### AssignmentToMethodParameter
Assignment to method parameter `snapshot`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java`
#### Snippet
```java
    // set the acl to snapshot if security feature is enabled.
    if (isSecurityAvailable(conf)) {
      snapshot = writeAclToSnapshotDescription(snapshot, conf);
    }
    return snapshot;
```

### AssignmentToMethodParameter
Assignment to method parameter `pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    Pair<Boolean, Tag> pair) throws IOException {
    if (pair == null) {
      pair = new Pair<>(false, null);
    } else {
      pair.setFirst(false);
```

### AssignmentToMethodParameter
Assignment to method parameter `data`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
    // ProtobufUtil.prependPBMagic does not take care of null
    if (data == null) {
      data = new byte[0];
    }
    try {
```

### AssignmentToMethodParameter
Assignment to method parameter `type`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java`
#### Snippet
```java
          return ALL;
        }
        type = type.toLowerCase();
        for (TaskType taskType : values()) {
          if (taskType.toString().equals(type)) {
```

### AssignmentToMethodParameter
Assignment to method parameter `logPayloadList`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/LogHandlerUtils.java`
#### Snippet
```java
    int totalFilters = getTotalFiltersCount(request);
    if (totalFilters > 0) {
      logPayloadList = filterLogs(request, logPayloadList, totalFilters);
    }
    int limit = Math.min(request.getLimit(), logPayloadList.size());
```

### AssignmentToMethodParameter
Assignment to method parameter `entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
        return null;
      }
      entry = filter.filter(entry);
    }
    return entry;
```

### AssignmentToMethodParameter
Assignment to method parameter `cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
  private Cell filterCell(Entry entry, Cell cell) {
    for (WALCellFilter filter : cellFilters) {
      cell = filter.filterCell(entry, cell);
      if (cell == null) {
        break;
```

### AssignmentToMethodParameter
Assignment to method parameter `entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
  @Override
  public Entry filter(Entry entry) {
    entry = filterEntry(entry);
    if (entry == null) {
      return null;
```

### AssignmentToMethodParameter
Assignment to method parameter `entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEmptyEntryFilter.java`
#### Snippet
```java
  @Override
  public WAL.Entry filter(WAL.Entry entry) {
    entry = super.filter(entry);
    if (filterEmptyEntry && entry != null && entry.getEdit().isEmpty()) {
      return null;
```

### AssignmentToMethodParameter
Assignment to method parameter `io`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  public static boolean isNoSuchColumnFamilyException(Throwable io) {
    if (io instanceof RemoteException) {
      io = ((RemoteException) io).unwrapRemoteException();
    }
    if (io != null && io.getMessage().contains("NoSuchColumnFamilyException")) {
```

### AssignmentToMethodParameter
Assignment to method parameter `io`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
      return true;
    }
    for (; io != null; io = io.getCause()) {
      if (io instanceof NoSuchColumnFamilyException) {
        return true;
```

### AssignmentToMethodParameter
Assignment to method parameter `io`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  public static boolean isTableNotFoundException(Throwable io) {
    if (io instanceof RemoteException) {
      io = ((RemoteException) io).unwrapRemoteException();
    }
    if (io != null && io.getMessage().contains("TableNotFoundException")) {
```

### AssignmentToMethodParameter
Assignment to method parameter `io`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
      return true;
    }
    for (; io != null; io = io.getCause()) {
      if (io instanceof TableNotFoundException) {
        return true;
```

### AssignmentToMethodParameter
Assignment to method parameter `data`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      return;
    }
    data = ZKMetadata.removeMetaData(data);
    SplitLogTask slt = SplitLogTask.parseFrom(data);
    if (slt.isUnassigned()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `data`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
        return;
      }
      data = ZKMetadata.removeMetaData(data);
      getDataSetWatchSuccess(path, data);
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java
    // HBASE-15180). We must do below deep copy. Or else we will keep referring to the bigger
    // chunk of memory and prevent it from getting GCed.
    cell = deepCopyIfNeeded(cell);
    boolean sizeAddedPreOperation = sizeAddedPreOperation();
    currentActive.upsert(cell, readpoint, memstoreSizing, sizeAddedPreOperation);
```

### AssignmentToMethodParameter
Assignment to method parameter `newFiles`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
    }
    if (newFiles == null) {
      newFiles = Collections.emptySet();
    }
    if (compactedFiles == null) {
```

### AssignmentToMethodParameter
Assignment to method parameter `startKey`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
    Consumer<Path> writerCreationTracker) throws IOException {
    if (startKey == null) {
      startKey = HConstants.EMPTY_START_ROW;
    }
    Path path = getPath();
```

### AssignmentToMethodParameter
Assignment to method parameter `startKey`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
    Compression.Algorithm compression, byte[] startKey, boolean isCompaction) throws IOException {
    if (startKey == null) {
      startKey = HConstants.EMPTY_START_ROW;
    }
    Path path = getTempDir();
```

### AssignmentToMethodParameter
Assignment to method parameter `srcPath`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
    // Copy the file if it's on another filesystem
    FileSystem srcFs = srcPath.getFileSystem(conf);
    srcPath = srcFs.resolvePath(srcPath);
    FileSystem realSrcFs = srcPath.getFileSystem(conf);
    FileSystem desFs = fs instanceof HFileSystem ? ((HFileSystem) fs).getBackingFs() : fs;
```

### AssignmentToMethodParameter
Assignment to method parameter `srcPath`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
      FileUtil.copy(realSrcFs, srcPath, fs, tmpPath, false, conf);
      LOG.info("Copied " + srcPath + " to temporary path on destination filesystem: " + tmpPath);
      srcPath = tmpPath;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `useBloom`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
  public boolean requestSeek(Cell kv, boolean forward, boolean useBloom) throws IOException {
    if (kv.getFamilyLength() == 0) {
      useBloom = false;
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `cellsToReturn`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
            // Hard to guess the size here. Just make a rough guess.
            if (cellsToReturn == null) {
              cellsToReturn = new ArrayList<>();
            }
            cellsToReturn.add(r);
```

### AssignmentToMethodParameter
Assignment to method parameter `expiredStoreFiles`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
            + fileTs + ", which is below " + maxTs);
          if (expiredStoreFiles == null) {
            expiredStoreFiles = new ArrayList<>();
          }
          expiredStoreFiles.add(sf);
```

### AssignmentToMethodParameter
Assignment to method parameter `cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.java`
#### Snippet
```java
    long oldOffHeapSize = offHeapSizeChange(cell, true);
    long oldCellSize = getCellLength(cell);
    cell = maybeCloneWithAllocator(cell, true);
    long newHeapSize = heapSizeChange(cell, true);
    long newOffHeapSize = offHeapSizeChange(cell, true);
```

### AssignmentToMethodParameter
Assignment to method parameter `refresh`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
            previousLogTime = EnvironmentEdgeManager.currentTime();
          }
          refresh = true; // let's try pull it from ZK directly
          if (sleepInterrupted(200)) {
            interrupted = true;
```

### AssignmentToMethodParameter
Assignment to method parameter `regionLoadBldr`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    long blocksLocalWithSsdWeight = hdfsBd.getBlocksLocalWithSsdWeight(serverName.getHostname());
    if (regionLoadBldr == null) {
      regionLoadBldr = RegionLoad.newBuilder();
    }
    if (regionSpecifier == null) {
```

### AssignmentToMethodParameter
Assignment to method parameter `regionSpecifier`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    }
    if (regionSpecifier == null) {
      regionSpecifier = RegionSpecifier.newBuilder();
    }

```

### AssignmentToMethodParameter
Assignment to method parameter `stream`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
    long expectedPos = PB_WAL_MAGIC.length;
    if (stream == null) {
      stream = fs.open(path);
      stream.seek(expectedPos);
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `cellCodecClsName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java`
#### Snippet
```java
    CompressionContext compression) throws UnsupportedOperationException {
    if (cellCodecClsName == null) {
      cellCodecClsName = getWALCellCodecClass(conf).getName();
    }
    return ReflectionUtils.instantiateWithCustomCtor(cellCodecClsName,
```

### AssignmentToMethodParameter
Assignment to method parameter `os`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java`
#### Snippet
```java
  @Override
  public Encoder getEncoder(OutputStream os) {
    os = (os instanceof ByteBufferWriter) ? os : new ByteBufferWriterOutputStream(os);
    if (compression == null) {
      return new EnsureKvEncoder(os);
```

### AssignmentToMethodParameter
Assignment to method parameter `sequence`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
          // Set the sync number to current highwater mark; might be able to let go more
          // queued sync futures
          sequence = currentHighestSyncedSequence;
          break;
        }
```

### AssignmentToMethodParameter
Assignment to method parameter `endOfBatch`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
          // Force flush of syncs if we are carrying a full complement of syncFutures.
          if (this.syncFuturesCount.get() == this.syncFutures.length) {
            endOfBatch = true;
          }
        } else if (truck.type() == RingBufferTruck.Type.APPEND) {
```

### AssignmentToMethodParameter
Assignment to method parameter `scanners`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java

    // Filter the list of scanners using Bloom filters, time range, TTL, etc.
    scanners = selectScannersFrom(store, scanners);

    // Seek all scanners to the initial key
```

### AssignmentToMethodParameter
Assignment to method parameter `timestamp`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExponentialCompactionWindowFactory.java`
#### Snippet
```java
      if (timestamp < 0) {
        try {
          timestamp = LongMath.checkedSubtract(timestamp, windowMillis - 1);
        } catch (ArithmeticException ae) {
          timestamp = Long.MIN_VALUE;
```

### AssignmentToMethodParameter
Assignment to method parameter `timestamp`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExponentialCompactionWindowFactory.java`
#### Snippet
```java
          timestamp = LongMath.checkedSubtract(timestamp, windowMillis - 1);
        } catch (ArithmeticException ae) {
          timestamp = Long.MIN_VALUE;
        }
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `candidateSelection`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java`
#### Snippet
```java
    if (!tryingMajor) {
      filterBulk(candidateSelection);
      candidateSelection = applyCompactionPolicy(candidateSelection, mayUseOffPeak, mayBeStuck);
      candidateSelection =
        checkMinFilesCriteria(candidateSelection, comConf.getMinFilesToCompact());
```

### AssignmentToMethodParameter
Assignment to method parameter `candidateSelection`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java`
#### Snippet
```java
      filterBulk(candidateSelection);
      candidateSelection = applyCompactionPolicy(candidateSelection, mayUseOffPeak, mayBeStuck);
      candidateSelection =
        checkMinFilesCriteria(candidateSelection, comConf.getMinFilesToCompact());
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `matchCode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
      case INCLUDE_AND_NEXT_COL:
        if (matchCode == MatchCode.INCLUDE) {
          matchCode = MatchCode.INCLUDE_AND_SEEK_NEXT_COL;
        }
        break;
```

### AssignmentToMethodParameter
Assignment to method parameter `matchCode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
        break;
      case INCLUDE_AND_SEEK_NEXT_ROW:
        matchCode = MatchCode.INCLUDE_AND_SEEK_NEXT_ROW;
        break;
      default:
```

### AssignmentToMethodParameter
Assignment to method parameter `matchCode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
      // step between SEEK_NEXT_COL and INCLUDE_AND_SEEK_NEXT_ROW, which is SEEK_NEXT_ROW.
      if (matchCode == MatchCode.INCLUDE_AND_SEEK_NEXT_ROW) {
        matchCode = MatchCode.SEEK_NEXT_ROW;
      } else {
        matchCode = MatchCode.SEEK_NEXT_COL;
```

### AssignmentToMethodParameter
Assignment to method parameter `matchCode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
        matchCode = MatchCode.SEEK_NEXT_ROW;
      } else {
        matchCode = MatchCode.SEEK_NEXT_COL;
      }
    }
```

### AssignmentToMethodParameter
Assignment to method parameter `compression`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTrackerBase.java`
#### Snippet
```java
    boolean includeMVCCReadpoint, boolean includesTag, Encryption.Context encryptionContext) {
    if (compression == null) {
      compression = HFile.DEFAULT_COMPRESSION_ALGORITHM;
    }
    ColumnFamilyDescriptor family = ctx.getFamily();
```

### AssignmentToMethodParameter
Assignment to method parameter `splitCount`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
      }
      ratio = newRatio;
      splitCount += 1.0;
    }
    long kvCount = (long) (getTotalKvCount(files) / splitCount);
```

### AssignmentToMethodParameter
Assignment to method parameter `tableList`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
        throw new BackupException("No table exists for full backup of all tables.");
      } else {
        tableList = new ArrayList<>();
        for (TableDescriptor hTableDescriptor : htds) {
          TableName tn = hTableDescriptor.getTableName();
```

### AssignmentToMethodParameter
Assignment to method parameter `conf`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    Command(Configuration conf) {
      if (conf == null) {
        conf = HBaseConfiguration.create();
      }
      setConf(conf);
```

### AssignmentToMethodParameter
Assignment to method parameter `request`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
    // update table list
    BackupRequest.Builder builder = new BackupRequest.Builder();
    request = builder.withBackupType(request.getBackupType()).withTableList(tableList)
      .withTargetRootDir(request.getTargetRootDir()).withBackupSetName(request.getBackupSetName())
      .withTotalTasks(request.getTotalTasks()).withBandwidthPerTasks((int) request.getBandwidth())
```

### AssignmentToMethodParameter
Assignment to method parameter `newTableName`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
    Path tableBackupPath, boolean truncateIfExists, String lastIncrBackupId) throws IOException {
    if (newTableName == null) {
      newTableName = tableName;
    }
    FileSystem fileSys = tableBackupPath.getFileSystem(this.conf);
```

### AssignmentToMethodParameter
Assignment to method parameter `path`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
      while (count++ < level) {
        relPath = Path.SEPARATOR + path.getName() + relPath;
        path = path.getParent();
      }
      return new Text(relPath);
```

### AssignmentToMethodParameter
Assignment to method parameter `p`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java
      if (!name.equals(backupId)) {
        stack.push(name);
        p = p.getParent();
      } else {
        break;
```

### AssignmentToMethodParameter
Assignment to method parameter `t`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftMetrics.java`
#### Snippet
```java
    }
    if (t instanceof TIOError || t instanceof IOError) {
      t = t.getCause();
    }
    return ClientExceptionsUtil.findException(t);
```

### AssignmentToMethodParameter
Assignment to method parameter `masterClass`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
    try {
      if (masterClass == null) {
        masterClass = HMaster.class;
      }
      if (regionserverClass == null) {
```

### AssignmentToMethodParameter
Assignment to method parameter `regionserverClass`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
      }
      if (regionserverClass == null) {
        regionserverClass = MiniHBaseCluster.MiniHBaseClusterRegionServer.class;
      }

```

### AssignmentToMethodParameter
Assignment to method parameter `info`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    }
    if (info.getReplicaId() <= 0) {
      info = RegionReplicaUtil.getRegionInfoForReplica(info, 1);
    }
    HRegion r = HRegion.newHRegion(tableDir, null, fs, conf, info, htd, null);
```

### AssignmentToMethodParameter
Assignment to method parameter `writeEntry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      boolean newWriteEntry = false;
      if (writeEntry == null) {
        writeEntry = region.mvcc.begin();
        newWriteEntry = true;
      }
```

### AssignmentToMethodParameter
Assignment to method parameter `td`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      }
    }
    td = builder.build();
    int totalNumberOfRegions = 0;
    Connection unmanagedConnection = ConnectionFactory.createConnection(conf);
```

### AssignmentToMethodParameter
Assignment to method parameter `additionalMsg`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java

    if (additionalMsg == null) {
      additionalMsg = "";
    }
    if (!additionalMsg.isEmpty()) {
```

### AssignmentToMethodParameter
Assignment to method parameter `additionalMsg`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    }
    if (!additionalMsg.isEmpty()) {
      additionalMsg = ". " + additionalMsg;
    }

```

## RuleId[id=PrimitiveArrayArgumentToVariableArgMethod]
### PrimitiveArrayArgumentToVariableArgMethod
Confusing primitive array argument to varargs method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
        throw new IOException("Unable to locate function: " + funcName + " in type: " + type);
      }
      return (ByteArrayComparable) parseFrom.invoke(null, value);
    } catch (Exception e) {
      throw new IOException(e);
```

### PrimitiveArrayArgumentToVariableArgMethod
Confusing primitive array argument to varargs method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
        throw new IOException("Unable to locate function: " + funcName + " in type: " + type);
      }
      return (Filter) parseFrom.invoke(c, value);
    } catch (Exception e) {
      // Either we couldn't instantiate the method object, or "parseFrom" failed.
```

### PrimitiveArrayArgumentToVariableArgMethod
Confusing primitive array argument to varargs method
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
        try {
          // byteStringObject = new LiteralByteString(payload);
          byteStringObject = constructor.newInstance(payload);
          // builder.setPayload(byteStringObject);
          setPayloadMethod.invoke(builder, constructor.getDeclaringClass().cast(byteStringObject));
```

## RuleId[id=SynchronizationOnLocalVariableOrMethodParameter]
### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `chore`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java`
#### Snippet
```java
    }
    // always lock chore first to prevent dead lock
    synchronized (chore) {
      synchronized (this) {
        try {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `existing`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
    Entry existing;
    while ((existing = map.putIfAbsent(entry.id, entry)) != null) {
      synchronized (existing) {
        if (existing.locked) {
          ++existing.numWaiters; // Add ourselves to waiters.
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `existing`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
    long remaining = time;
    while ((existing = map.putIfAbsent(entry.id, entry)) != null) {
      synchronized (existing) {
        if (existing.locked) {
          ++existing.numWaiters; // Add ourselves to waiters.
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `entry`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
      entry = map.get(id);
      if (entry != null) {
        synchronized (entry) {
          if (entry.numWaiters >= numWaiters) {
            return;
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `entry`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
  public void releaseLockEntry(Entry entry) {
    Thread currentThread = Thread.currentThread();
    synchronized (entry) {
      if (entry.holder != currentThread) {
        LOG.warn("{} is trying to release lock entry {}, but it is not the holder.", currentThread,
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `entry`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
      return false;
    }
    synchronized (entry) {
      return currentThread.equals(entry.holder);
    }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `conf`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
    // Directly map all of the hbase.zookeeper.property.KEY properties.
    // Synchronize on conf so no loading of configs while we iterate
    synchronized (conf) {
      for (Entry<String, String> entry : conf) {
        String key = entry.getKey();
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `sourceNames`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystemHelper.java`
#### Snippet
```java
      Object sourceNames = sourceNamesField.get(DefaultMetricsSystem.INSTANCE);
      HashMap map = (HashMap) mapField.get(sourceNames);
      synchronized (sourceNames) {
        map.remove(name);
      }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `lastTimestamp`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/WriteHeavyIncrementObserver.java`
#### Snippet
```java
    MutableLong lastTimestamp = lastTimestamps[slot];
    long now = EnvironmentEdgeManager.currentTime();
    synchronized (lastTimestamp) {
      long pt = lastTimestamp.longValue() >> 10;
      if (now > pt) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `tableCache`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
            RegionLocations addedLocs = tableCache.regionLocationCache.add(locs);
            List<RegionLocationsFutureResult> futureResultList = new ArrayList<>();
            synchronized (tableCache) {
              futureResultList.addAll(tableCache.clearCompletedRequests(addedLocs));
            }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `tableCache`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    LocateRequest req;
    boolean sendRequest = false;
    synchronized (tableCache) {
      // check again
      if (!reload) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `saslServer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java`
#### Snippet
```java
    // synchronization may be needed since there can be multiple Handler
    // threads using saslServer or Crypto AES to wrap responses.
    synchronized (saslServer) {
      token = saslServer.wrap(responseBytes, 0, responseBytes.length);
    }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `readWriteLock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLock.java`
#### Snippet
```java
      readWriteLock = getLock(id);
      if (readWriteLock != null) {
        synchronized (readWriteLock) {
          if (readWriteLock.getQueueLength() >= numWaiters) {
            return;
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `hbi`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
                HbckRegionInfo hbi = HBaseFsck.this.getOrCreateInfo(encodedName);
                HbckRegionInfo.HdfsEntry he = new HbckRegionInfo.HdfsEntry();
                synchronized (hbi) {
                  if (hbi.getHdfsRegionDir() != null) {
                    errors
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `oldtask`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
    }
    // new task was not used.
    synchronized (oldtask) {
      if (oldtask.isOrphan()) {
        if (oldtask.status == SUCCESS) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on method parameter `batch`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java

  private void waitForSplittingCompletion(TaskBatch batch, MonitoredTask status) {
    synchronized (batch) {
      while ((batch.done + batch.error) != batch.installed) {
        try {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `serverNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  private void setServerState(ServerName serverName, ServerState state) {
    ServerStateNode serverNode = getOrCreateServer(serverName);
    synchronized (serverNode) {
      serverNode.setState(state);
    }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `usedWorkers`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java`
#### Snippet
```java
    throws ProcedureSuspendedException {
    UsedReplayWorkersForPeer usedWorkers = usedWorkersByPeer.get(peerId);
    synchronized (usedWorkers) {
      Optional<ServerName> worker = usedWorkers.acquire(serverManager);
      if (worker.isPresent()) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `usedWorkers`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java`
#### Snippet
```java
    MasterProcedureScheduler scheduler) {
    UsedReplayWorkersForPeer usedWorkers = usedWorkersByPeer.get(peerId);
    synchronized (usedWorkers) {
      usedWorkers.release(worker);
      usedWorkers.wake(scheduler);
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `usedWorkers`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java`
#### Snippet
```java
          services.getMasterProcedureExecutor().getEnvironment().getProcedureScheduler();
        for (UsedReplayWorkersForPeer usedWorkers : usedWorkersByPeer.values()) {
          synchronized (usedWorkers) {
            usedWorkers.wake(scheduler);
          }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `serverNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java

    ServerStateNode serverNode = regionStates.getOrCreateServer(serverName);
    synchronized (serverNode) {
      if (!serverNode.isInState(ServerState.ONLINE)) {
        LOG.warn("Got a report from a server result in state " + serverNode.getState());
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `balancer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    BalanceResponse.Builder responseBuilder = BalanceResponse.newBuilder();

    synchronized (balancer) {
      // If balance not true, don't run balancer.
      if (!masterServices.isBalancerOn() && !request.isDryRun()) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `task`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      return;
    }
    synchronized (task) {
      task.status = DELETED;
      task.notify();
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `task`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      }
    } else {
      synchronized (task) {
        if (task.status == IN_PROGRESS) {
          if (status == SUCCESS) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `file`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    final List<Long> storeFileSizes = new ArrayList<>(compactedfiles.size());
    for (final HStoreFile file : compactedfiles) {
      synchronized (file) {
        try {
          StoreFileReader r = file.getReader();
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `newResult`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java
    OperationContext newResult = nonces.get(nk);
    assert newResult != null;
    synchronized (newResult) {
      assert newResult.getState() == OperationContext.WAIT;
      // If we failed, other retries can proceed.
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `oldResult`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java

      // Collision with some operation - should be extremely rare.
      synchronized (oldResult) {
        int oldState = oldResult.getState();
        LOG.debug("Conflict detected by nonce: " + nk + ", " + oldResult);
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `result`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java
    OperationContext result = nonces.get(nk);
    assert result != null;
    synchronized (result) {
      result.setMvcc(mvcc);
    }
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `oc`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java
      OperationContext oc = entry.getValue();
      if (!oc.isExpired(cutoff)) continue;
      synchronized (oc) {
        if (oc.getState() == OperationContext.WAIT || !oc.isExpired(cutoff)) continue;
        nonces.remove(entry.getKey());
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `scanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
    try {
      int numOfResults = 0;
      synchronized (scanner) {
        boolean stale = (region.getRegionInfo().getReplicaId() != 0);
        boolean clientHandlesPartials =
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `sf`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    for (int i = 1; i < stripe.size(); ++i) {
      HStoreFile sf = stripe.get(i);
      synchronized (sf) {
        long fileTs = sf.getReader().getMaxTimestamp();
        if (fileTs < maxTs && !filesCompacting.contains(sf)) {
```

### SynchronizationOnLocalVariableOrMethodParameter
Synchronization on local variable `signaller`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java`
#### Snippet
```java
        && isRegionStillOpening()
    ) {
      synchronized (signaller) {
        try {
          // Wait for 10 seconds, so that server shutdown
```

## RuleId[id=ReturnNull]
### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TagUtil.java`
#### Snippet
```java
        justification = "Intentional")
    public Tag next() {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
      @Override
      public Class<?> getClassByName(String name) throws ClassNotFoundException {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
      public String get(String key) {
        Bytes ibw = new Bytes(Bytes.toBytes(key));
        if (!m.containsKey(ibw)) return null;
        Bytes value = m.get(ibw);
        if (value == null || value.get() == null) return null;
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
        if (!m.containsKey(ibw)) return null;
        Bytes value = m.get(ibw);
        if (value == null || value.get() == null) return null;
        return Bytes.toString(value.get());
      }
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
      @Override
      public Class<?> getClassByName(String name) throws ClassNotFoundException {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    }
    if (bb.remaining() < 1) {
      return null;
    }
    int underlyingArrayOffset = bb.arrayOffset() + bb.position();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java

    if (length <= 0) {
      if (length == 0) return null;
      throw new IOException("Failed read " + length + " bytes, stream corrupt?");
    }
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
  @Deprecated
  public static KeyValue ensureKeyValue(final Cell cell) {
    if (cell == null) return null;
    if (cell instanceof KeyValue) {
      if (cell.getClass().getName().equals(KeyValue.class.getName())) {
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
          maxPoolSizeInfoLevelLogged = true;
        }
        return null;
      }
      if (!this.usedBufCount.compareAndSet(c, c + 1)) {
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
      @Override
      public Cell current() {
        if (cells == null) return null;
        return (index < 0) ? null : this.cells[index];
      }
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
      public Cell current() {
        if (cells == null) return null;
        return (index < 0) ? null : this.cells[index];
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   */
  public static CellScanner createCellScanner(final Iterator<Cell> cells) {
    if (cells == null) return null;
    return new CellScanner() {
      private final Iterator<Cell> iterator = cells;
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
  /** Returns CellScanner interface over <code>cellIterable</code> */
  public static CellScanner createCellScanner(final Iterable<Cell> cellIterable) {
    if (cellIterable == null) return null;
    return createCellScanner(cellIterable.iterator());
  }
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
      @Override
      public Cell current() {
        return this.cellScanner != null ? this.cellScanner.current() : null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyStoreKeyProvider.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/HBaseHostnameVerifier.java`
#### Snippet
```java
  private static String extractCN(final String subjectPrincipal) throws SSLException {
    if (subjectPrincipal == null) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/HBaseHostnameVerifier.java`
#### Snippet
```java
        }
      }
      return null;
    } catch (final InvalidNameException e) {
      throw new SSLException(subjectPrincipal + " is not a valid X500 distinguished name");
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/KeyStoreFileType.java`
#### Snippet
```java
  public static KeyStoreFileType fromPropertyValue(String propertyValue) {
    if (propertyValue == null || propertyValue.length() == 0) {
      return null;
    }
    return KeyStoreFileType.valueOf(propertyValue.toUpperCase());
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java
    throws IOException {
    if (fileLocation == null || fileLocation.isEmpty() || resetContext == null) {
      return null;
    }
    final Path filePath = Paths.get(fileLocation).toAbsolutePath();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
          return tag;
        }
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
          return tag;
        }
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   */
  public static byte[] getCellKeySerializedAsKeyValueKey(final Cell cell) {
    if (cell == null) return null;
    byte[] b = new byte[KeyValueUtil.keyLength(cell)];
    KeyValueUtil.appendKeyTo(cell, b, 0);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/DictionaryCache.java`
#### Snippet
```java
    throws IOException {
    if (path == null || path.isEmpty()) {
      return null;
    }
    // Create the dictionary loading cache if we haven't already
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
        return compressor;
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      }

      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      @Override
      CompressionCodec getCodec(Configuration conf) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      @Override
      public CompressionCodec reload(Configuration conf) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java`
#### Snippet
```java
        return new Bytes(compressedByteStream.getBuffer(), 0, compressedByteStream.size());
      } else {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java`
#### Snippet
```java
  public static InetSocketAddress[] toSocketAddress(Address[] addrs) {
    if (addrs == null) {
      return null;
    }
    InetSocketAddress[] result = new InetSocketAddress[addrs.length];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ExceptionUtil.java`
#### Snippet
```java
  public static InterruptedIOException asInterrupt(Throwable t) {
    if (t instanceof SocketTimeoutException) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ExceptionUtil.java`
#### Snippet
```java
    }

    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    header = dsc ? DESCENDING.apply(header) : header;

    if (header == NULL) return null;
    if (header == NEG_LARGE) { /* Large negative number: 0x08, ~E, ~M */
      e = (int) getVaruint64(src, !dsc);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    byte header = src.get();
    if (header == NULL || header == DESCENDING.apply(NULL)) {
      return null;
    }
    assert header == BLOB_COPY || header == DESCENDING.apply(BLOB_COPY);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
  public static String decodeString(PositionedByteRange src) {
    final byte header = src.get();
    if (header == NULL || header == DESCENDING.apply(NULL)) return null;
    assert header == TEXT || header == DESCENDING.apply(TEXT);
    Order ord = header == TEXT ? ASCENDING : DESCENDING;
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    final byte header = src.get();
    if (header == NULL || header == DESCENDING.apply(NULL)) {
      return null;
    }
    assert header == BLOB_VAR || header == DESCENDING.apply(BLOB_VAR);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    if (isNull(src)) {
      src.get();
      return null;
    }
    if (!isNumeric(src)) throw unexpectedHeader(src.peek());
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
   */
  static BigDecimal normalize(BigDecimal val) {
    return null == val ? null : val.stripTrailingZeros().round(DEFAULT_MATH_CONTEXT);
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
    public static <TNode extends AvlNode> TNode remove(TNode root, Object key,
      final AvlKeyComparator<TNode> keyComparator, final AtomicBoolean removed) {
      if (root == null) return null;

      int cmp = keyComparator.compareKey(root, key);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
        }
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
                if (currentTS >= waitUtilTS) {
                  // time is up
                  return null;
                } else {
                  // our wait is waken, but the lock is still taken, this can happen
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Hash.java`
#### Snippet
```java
        return MurmurHash3.getInstance();
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java`
#### Snippet
```java
        e);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Strings.java`
#### Snippet
```java
  public static String domainNamePointerToHostName(String dnPtr) {
    if (dnPtr == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  public static BigDecimal toBigDecimal(ByteBuffer buffer, int offset, int length) {
    if (buffer == null || length < Bytes.SIZEOF_INT + 1 || (offset + length > buffer.limit())) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawFloat.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawByte.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union3.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union4.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawInteger.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union2.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawShort.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructIterator.java`
#### Snippet
```java
    DataType<?> t = types[idx++];
    if (src.getPosition() == src.getLength() && t.isNullable()) {
      return null;
    }
    return t.decode(src);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawLong.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawDouble.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Struct.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    int i = 0;
    Iterable<byte[]> iter = iterateOnSplits(a, b, inclusive, num);
    if (iter == null) return null;
    for (byte[] elem : iter) {
      ret[i++] = elem;
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static String toString(final byte[] b) {
    if (b == null) {
      return null;
    }
    return toString(b, 0, b.length);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
   */
  public static byte[] copy(byte[] bytes) {
    if (bytes == null) return null;
    byte[] result = new byte[bytes.length];
    System.arraycopy(bytes, 0, result, 0, bytes.length);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static byte[] head(final byte[] a, final int length) {
    if (a.length < length) {
      return null;
    }
    byte[] result = new byte[length];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    } catch (Exception e) {
      LOG.error("Exception caught during division", e);
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static String toString(final byte[] b, int off, int len) {
    if (b == null) {
      return null;
    }
    if (len == 0) {
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static byte[] tail(final byte[] a, final int length) {
    if (a.length < length) {
      return null;
    }
    byte[] result = new byte[length];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static BigDecimal toBigDecimal(byte[] bytes, int offset, final int length) {
    if (bytes == null || length < SIZEOF_INT + 1 || (offset + length > bytes.length)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static String toString(final byte[] b, int off) {
    if (b == null) {
      return null;
    }
    int len = b.length - off;
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
   */
  public static byte[] copy(byte[] bytes, final int offset, final int length) {
    if (bytes == null) return null;
    byte[] result = new byte[length];
    System.arraycopy(bytes, offset, result, 0, length);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
  public User create(UserGroupInformation ugi) {
    if (ugi == null) {
      return null;
    }
    return new User.SecureHadoopUser(ugi, groupCache);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
  public String getCurrentUserName() throws IOException {
    User user = getCurrent();
    return user == null ? null : user.getName();
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
  public static User create(UserGroupInformation ugi) {
    if (ugi == null) {
      return null;
    }
    return new SecureHadoopUser(ugi);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
    User user = new SecureHadoopUser();
    if (user.getUGI() == null) {
      return null;
    }
    return user;
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    int index = current.find(key);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    }
    if (index < current.startIndex || index >= current.endIndex) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

    if (index < current.startIndex || index >= current.endIndex) {
      return null;
    }
    return current.entries[index];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
      ArrayHolder<K, V> current = this.holder;
      if (current.getLength() == 0) {
        return null;
      }
      return current.entries[current.startIndex].getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    @Override
    public SortedSet<K> headSet(K toElement) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    int index = current.find(key);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

    if (index < current.startIndex || index >= current.endIndex) {
      return null;
    }
    return current.entries[index];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    Map.Entry<K, V> entry = floorEntry(key);
    if (entry == null) {
      return null;
    }
    return entry.getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    int index = current.find(key);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    }
    if (index < current.startIndex || index >= current.endIndex) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    return current.entries[current.endIndex - 1].getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    return current.entries[current.startIndex];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    Map.Entry<K, V> entry = higherEntry(key);
    if (entry == null) {
      return null;
    }
    return entry.getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    return current.entries[current.endIndex - 1];
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    Map.Entry<K, V> entry = lowerEntry(key);
    if (entry == null) {
      return null;
    }
    return entry.getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    @Override
    public SortedSet<K> tailSet(K fromElement) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
      ArrayHolder<K, V> current = this.holder;
      if (current.getLength() == 0) {
        return null;
      }
      return current.entries[current.endIndex - 1].getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    Map.Entry<K, V> entry = ceilingEntry(key);
    if (entry == null) {
      return null;
    }
    return entry.getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    ArrayHolder<K, V> current = this.holder;
    if (current.getLength() == 0) {
      return null;
    }
    return current.entries[current.startIndex].getKey();
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    @Override
    public SortedSet<K> subSet(K fromElement, K toElement) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
      return current.entries[index].getValue();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
      return current.entries[index].getValue();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
    String clientQuromServers = conf.get(HConstants.CLIENT_ZOOKEEPER_QUORUM);
    if (clientQuromServers == null) {
      return null;
    }
    int defaultClientPort =
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/AuthUtil.java`
#### Snippet
```java
  public static ScheduledChore getAuthChore(Configuration conf) throws IOException {
    if (!isAuthRenewalChoreEnabled(conf)) {
      return null;
    }
    User user = loginClientAsService(conf);
```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/AuthUtil.java`
#### Snippet
```java
    Configuration conf) {
    if (!user.hasKerberosCredentials() || !isAuthRenewalChoreEnabled(conf)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    if (length <= 0) {
      if (length == 0) {
        return null;
      }
      throw new IOException("Failed read " + length + " bytes, stream corrupt?");
```

### ReturnNull
Return of `null`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/types/PBType.java`
#### Snippet
```java
  @Override
  public Order getOrder() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/util/ForeignExceptionUtil.java`
#### Snippet
```java
    // if there is no stack trace, ignore it and just return the message
    if (trace == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeTokenIdentifier.java`
#### Snippet
```java
  public UserGroupInformation getUser() {
    if (username == null || "".equals(username)) {
      return null;
    }
    return UserGroupInformation.createRemoteUser(username);
```

### ReturnNull
Return of `null`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/SaslPlainServer.java`
#### Snippet
```java
  public Object getNegotiatedProperty(String propName) {
    throwIfNotComplete();
    return Sasl.QOP.equals(propName) ? "auth" : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/SaslPlainServer.java`
#### Snippet
```java
      completed = true;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/SaslPlainServer.java`
#### Snippet
```java
    public SaslServer createSaslServer(String mechanism, String protocol, String serverName,
      Map<String, ?> props, CallbackHandler cbh) throws SaslException {
      return "PLAIN".equals(mechanism) ? new SaslPlainServer(cbh) : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
      return p.toUri().toString();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
    }
    if (status == null || status.length < 1) {
      return null;
    }
    return status;
```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java

    if (fieldString.length() == 0 || filterString.length() == index) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
    Field field = getField(fields, fieldString.toString());
    if (field == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
    Operator operator = getOperator(operatorString.toString());
    if (operator == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
    FieldValue fieldValue = getFieldValue(field, value);
    if (fieldValue == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
      return field.newValue(value);
    } catch (Exception e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/top/TopScreenPresenter.java`
#### Snippet
```java
  private Record getSelectedRecord() {
    if (topScreenModel.getRecords().isEmpty()) {
      return null;
    }
    return topScreenModel.getRecords().get(paging.getCurrentPosition());
```

### ReturnNull
Return of `null`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/batch/BatchTerminal.java`
#### Snippet
```java
  @Override
  public CursorPosition getCursorPosition() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java
    HRegionLocation location, boolean checkForEquals, boolean force) {
    if (location == null) {
      return oldLocation == null ? null : oldLocation;
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java
  public HRegionLocation getRegionLocation(int replicaId) {
    if (replicaId >= locations.length) {
      return null;
    }
    return locations[replicaId];
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java`
#### Snippet
```java
      }
    }
    return hasNonNullElement ? new RegionLocations(newLocations) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java`
#### Snippet
```java
  public static byte[] getTableStartRowForMeta(TableName tableName, QueryType type) {
    if (tableName == null) {
      return null;
    }
    switch (type) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java`
#### Snippet
```java
  public static byte[] getTableStopRowForMeta(TableName tableName, QueryType type) {
    if (tableName == null) {
      return null;
    }
    final byte[] stopRow;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcController.java`
#### Snippet
```java
  /** Returns Target Region's RegionInfo or null if not available or pertinent. */
  default RegionInfo getRegionInfo() {
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
    CellScanner cellScanner, ByteBuffAllocator allocator) throws IOException {
    if (cellScanner == null) {
      return null;
    }
    if (codec == null) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
    if (bbos.size() == 0) {
      bbos.releaseResources();
      return null;
    }
    return bbos;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
      return supplier.buf;
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
      // If no cells, don't mess around. Just return null (could be a bunch of existence checking
      // gets or something -- stuff that does not return a cell).
      return bb.hasRemaining() ? bb : null;
    } else {
      return null;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
      return bb.hasRemaining() ? bb : null;
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcClientConfigHelper.java`
#### Snippet
```java
    }
    if (StringUtils.isBlank(name)) {
      return null;
    }
    return EVENT_LOOP_CONFIG_MAP.get(name);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
    String className = conf.get(HConstants.RPC_CODEC_CONF_KEY, getDefaultCodec(this.conf));
    if (className == null || className.length() == 0) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
    String className = conf.get("hbase.client.rpc.compressor", null);
    if (className == null || className.isEmpty()) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java`
#### Snippet
```java
        }
      }
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  public synchronized String errorText() {
    if (!done || exception == null) {
      return null;
    }
    return exception.getMessage();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  @Override
  public synchronized IOException getFailed() {
    return done ? exception : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
          // in question, it is okay
          Thread.sleep(ThreadLocalRandom.current().nextInt(reloginMaxBackoff) + 1);
          return null;
        } else {
          String msg =
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerStatisticTracker.java`
#### Snippet
```java
        HConstants.DEFAULT_ENABLE_CLIENT_BACKPRESSURE)
    ) {
      return null;
    }
    return new ServerStatisticTracker();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java`
#### Snippet
```java
    } else {
      // if all the locations are null, just return null
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java`
#### Snippet
```java
  public String getId() {
    byte[] attr = getAttribute(ID_ATRIBUTE);
    return attr == null ? null : Bytes.toString(attr);
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java`
#### Snippet
```java
  public byte[] getAttribute(String name) {
    if (attributes == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
      isEmptyStopRow ? cache.lastEntry() : cache.lowerEntry(row);
    if (entry == null) {
      return null;
    }
    RegionLocations locs = entry.getValue();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
    RegionLocations locs = entry.getValue();
    if (locs == null) {
      return null;
    }
    HRegionLocation loc = locs.getRegionLocation(replicaId);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
    HRegionLocation loc = locs.getRegionLocation(replicaId);
    if (loc == null) {
      return null;
    }
    if (
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
      return locs;
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
    Map.Entry<byte[], RegionLocations> entry = cache.floorEntry(row);
    if (entry == null) {
      return null;
    }
    RegionLocations locs = entry.getValue();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
    RegionLocations locs = entry.getValue();
    if (locs == null) {
      return null;
    }
    HRegionLocation loc = locs.getRegionLocation(replicaId);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
    HRegionLocation loc = locs.getRegionLocation(replicaId);
    if (loc == null) {
      return null;
    }
    byte[] endKey = loc.getRegion().getEndKey();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocationCache.java`
#### Snippet
```java
          Bytes.toStringBinary(row), Bytes.toStringBinary(endKey), locs);
      }
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Connection.java`
#### Snippet
```java
   */
  default String getClusterId() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
    while (queue.isEmpty()) {
      if (closed) {
        return null;
      }
      if (error != null) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultScanner.java`
#### Snippet
```java
        // if there is a next before proceeding.
        if (!hasNext()) {
          return null;
        }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    public String getValue(String key) {
      Bytes rval = values.get(new Bytes(Bytes.toBytes(key)));
      return rval == null ? null : Bytes.toString(rval.get(), rval.getOffset(), rval.getLength());
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
  public static byte[] isLegalColumnFamilyName(final byte[] b) {
    if (b == null) {
      return null;
    }
    Preconditions.checkArgument(b.length != 0, "Column Family name can not be empty");
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    public byte[] getValue(byte[] key) {
      Bytes value = values.get(new Bytes(key));
      return value == null ? null : value.get();
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    private static <T> Bytes toBytesOrNull(T t, Function<T, byte[]> f) {
      if (t == null) {
        return null;
      } else {
        return new Bytes(f.apply(t));
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
      return this.familyMap.keySet().toArray(new byte[0][0]);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    public byte[] getValue(byte[] key) {
      Bytes value = values.get(new Bytes(key));
      return value == null ? null : value.copyBytes();
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    public String getValue(String key) {
      Bytes rval = values.get(new Bytes(Bytes.toBytes(key)));
      return rval == null ? null : Bytes.toString(rval.get(), rval.getOffset(), rval.getLength());
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    private static <T> Bytes toBytesOrNull(T t, Function<T, byte[]> f) {
      if (t == null) {
        return null;
      } else {
        return new Bytes(f.apply(t));
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    public Bytes getValue(Bytes key) {
      Bytes rval = values.get(key);
      return rval == null ? null : new Bytes(rval.copyBytes());
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncMetaRegionLocator.java`
#### Snippet
```java
  private HRegionLocation getCacheLocation(HRegionLocation loc) {
    RegionLocations locs = metaRegionLocations.get();
    return locs != null ? locs.getRegionLocation(loc.getRegion().getReplicaId()) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientCoprocessorRpcController.java`
#### Snippet
```java
  @Override
  public String errorText() {
    return error != null ? error.getMessage() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
      .toMillis(conf.getLong(intervalSecsConfigName, PERIODIC_REFRESH_INTERVAL_SECS_DEFAULT));
    if (periodicRefreshMs <= 0) {
      return null;
    }
    long initialDelayMs = Math.max(1,
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    TableCache tableCache = cache.get(tableName);
    if (tableCache == null) {
      return null;
    }
    return locateRowInCache(tableCache, row, RegionReplicaUtil.DEFAULT_REPLICA_ID);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    TableCache tableCache = cache.get(loc.getRegion().getTable());
    if (tableCache == null) {
      return null;
    }
    RegionLocations locs = tableCache.regionLocationCache.get(loc.getRegion().getStartKey());
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    }
    RegionLocations locs = tableCache.regionLocationCache.get(loc.getRegion().getStartKey());
    return locs != null ? locs.getRegionLocation(loc.getRegion().getReplicaId()) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
  @InterfaceAudience.Private
  static RegionInfo parseFromOrNull(final byte[] bytes, int offset, int len) {
    if (bytes == null || len <= 0) return null;
    try {
      return parseFrom(bytes, offset, len);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
      return parseFrom(bytes, offset, len);
    } catch (DeserializationException e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
  @InterfaceAudience.Private
  static RegionInfo parseFrom(final byte[] bytes) throws DeserializationException {
    if (bytes == null) return null;
    return parseFrom(bytes, 0, bytes.length);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
    }
    if (offset == -1) {
      return null;
    }
    byte[] tableName = new byte[offset];
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
    }
    if (offset == -1) {
      return null;
    }
    byte[] startKey = HConstants.EMPTY_BYTE_ARRAY;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
  @InterfaceAudience.Private
  static RegionInfo parseFromOrNull(final byte[] bytes) {
    if (bytes == null) return null;
    return parseFromOrNull(bytes, 0, bytes.length);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
   */
  private static CellVisibility toCellVisibility(ClientProtos.CellVisibility proto) {
    if (proto == null) return null;
    return new CellVisibility(proto.getExpression());
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
  public CellVisibility getCellVisibility() throws DeserializationException {
    byte[] cellVisibilityBytes = this.getAttribute(VisibilityConstants.VISIBILITY_LABELS_ATTR_KEY);
    if (cellVisibilityBytes == null) return null;
    return toCellVisibility(cellVisibilityBytes);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
  private static CellVisibility toCellVisibility(byte[] protoBytes)
    throws DeserializationException {
    if (protoBytes == null) return null;
    ClientProtos.CellVisibility.Builder builder = ClientProtos.CellVisibility.newBuilder();
    ClientProtos.CellVisibility proto = null;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Query.java`
#### Snippet
```java
  public Authorizations getAuthorizations() throws DeserializationException {
    byte[] authorizationsBytes = this.getAttribute(VisibilityConstants.VISIBILITY_LABELS_ATTR_KEY);
    if (authorizationsBytes == null) return null;
    return ProtobufUtil.toAuthorizations(authorizationsBytes);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
        .thenApply(proto -> {
          if (proto == null) {
            return null;
          }
          HBaseProtos.ServerName snProto = proto.getMaster();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
  private static String getClusterId(byte[] data) throws DeserializationException {
    if (data == null || data.length == 0) {
      return null;
    }
    data = removeMetaData(data);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
  private static ZooKeeperProtos.Master getMasterProto(byte[] data) throws IOException {
    if (data == null || data.length == 0) {
      return null;
    }
    data = removeMetaData(data);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
  private static ZooKeeperProtos.MetaRegionServer getMetaProto(byte[] data) throws IOException {
    if (data == null || data.length == 0) {
      return null;
    }
    data = removeMetaData(data);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    } catch (UnknownHostException uhe) {
      LOG.error("cannot determine my address", uhe);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    }
    if (index == rawCells.length) {
      return null;
    }
    return Result.create(Arrays.copyOfRange(rawCells, index, rawCells.length), null,
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchScanResultCache.java`
#### Snippet
```java
    numCellsOfPartialResults += result.size();
    if (numCellsOfPartialResults < batch) {
      return null;
    }
    Cell[] cells = new Cell[batch];
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java
    if (conf.getBoolean(CLIENT_SIDE_METRICS_ENABLED_KEY, false)) {
      this.metrics =
        Optional.of(MetricsConnection.getMetricsConnection(metricsScope, () -> null, () -> null));
    } else {
      this.metrics = Optional.empty();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java
    if (conf.getBoolean(CLIENT_SIDE_METRICS_ENABLED_KEY, false)) {
      this.metrics =
        Optional.of(MetricsConnection.getMetricsConnection(metricsScope, () -> null, () -> null));
    } else {
      this.metrics = Optional.empty();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnectionImpl.java`
#### Snippet
```java
      LOG.error("Error fetching cluster ID: ", e);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    }
    if (isEmpty()) {
      return null;
    }
    NavigableMap<byte[], NavigableMap<byte[], byte[]>> returnMap =
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    Cell kv = getColumnLatestCell(family, qualifier);
    if (kv == null) {
      return null;
    }
    return CellUtil.cloneValue(kv);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
  public byte[] value() {
    if (isEmpty()) {
      return null;
    }
    return CellUtil.cloneValue(cells[0]);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java

    if (kv == null) {
      return null;
    }
    return ByteBuffer.wrap(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength())
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    }
    if (isEmpty()) {
      return null;
    }
    NavigableMap<byte[], byte[]> returnMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    Cell[] kvs = rawCells(); // side effect possibly.
    if (kvs == null || kvs.length == 0) {
      return null;
    }
    int pos = binarySearch(kvs, family, qualifier);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    int pos = binarySearch(kvs, family, qualifier);
    if (pos == -1) {
      return null;
    }
    if (CellUtil.matchingColumn(kvs[pos], family, qualifier)) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      return kvs[pos];
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    }
    if (isEmpty()) {
      return null;
    }
    this.familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java

    if (kv == null) {
      return null;
    }
    return ByteBuffer.wrap(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength())
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    if (
      isEmpty() || cellScannerIndex == INITIAL_CELLSCANNER_INDEX || cellScannerIndex >= cells.length
    ) return null;
    return this.cells[cellScannerIndex];
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    Cell[] kvs = rawCells(); // side effect possibly.
    if (kvs == null || kvs.length == 0) {
      return null;
    }
    int pos = binarySearch(kvs, family, foffset, flength, qualifier, qoffset, qlength);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
    int pos = binarySearch(kvs, family, foffset, flength, qualifier, qoffset, qlength);
    if (pos == -1) {
      return null;
    }
    if (
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      return kvs[pos];
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
   */
  public List<Cell> listCells() {
    return isEmpty() ? null : Arrays.asList(rawCells());
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientIdGenerator.java`
#### Snippet
```java
      LOG.warn("Don't know how to get PID from [" + name + "]");
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/TableOperationSpanBuilder.java`
#### Snippet
```java
  private static Operation valueFrom(final Row row) {
    if (row == null) {
      return null;
    }
    if (row instanceof Append) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/TableOperationSpanBuilder.java`
#### Snippet
```java
      return Operation.BATCH;
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/TableOperationSpanBuilder.java`
#### Snippet
```java
  private static Operation valueFrom(final Scan scan) {
    if (scan == null) {
      return null;
    }
    return Operation.SCAN;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Double multiply(Double d1, Double d2) {
    return (d1 == null || d2 == null) ? null : d1 * d2;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Double getValue(byte[] colFamily, byte[] colQualifier, Cell c) throws IOException {
    if (c == null || c.getValueLength() != Bytes.SIZEOF_DOUBLE) return null;
    return PrivateCellUtil.getValueAsDouble(c);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/DoubleColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Double increment(Double o) {
    return o == null ? null : (o + 1.00d);
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Long getValue(byte[] colFamily, byte[] colQualifier, Cell kv) throws IOException {
    if (kv == null || kv.getValueLength() != Bytes.SIZEOF_LONG) return null;
    return PrivateCellUtil.getValueAsLong(kv);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java`
#### Snippet
```java
      return (l1 == null) ? l2 : l1; // either of one is null.
    } else if (l1 == null) // both are null
      return null;
    return l1 + l2;
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Long multiply(Long l1, Long l2) {
    return (l1 == null || l2 == null) ? null : l1 * l2;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Long increment(Long o) {
    return o == null ? null : (o + 1l);
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java`
#### Snippet
```java
    }
    if (bd1 == null) {
      return null;
    }
    return bd1.add(bd2);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java`
#### Snippet
```java
  public BigDecimal multiply(BigDecimal bd1, BigDecimal bd2) {
    return (bd1 == null || bd2 == null)
      ? null
      : bd1.multiply(bd2).setScale(2, RoundingMode.HALF_EVEN);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java`
#### Snippet
```java
  @Override
  public BigDecimal increment(BigDecimal bd) {
    return bd == null ? null : bd.add(BigDecimal.ONE);
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java`
#### Snippet
```java
  public BigDecimal getValue(byte[] colFamily, byte[] colQualifier, Cell kv) throws IOException {
    if (kv == null || CellUtil.cloneValue(kv) == null) {
      return null;
    }
    return PrivateCellUtil.getValueAsBigDecimal(kv).setScale(2, RoundingMode.HALF_EVEN);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterBase.java`
#### Snippet
```java
  @Override
  public Cell getNextCellHint(Cell currentCell) throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
  public static ReplicationProtos.TableCF[] parseTableCFs(byte[] bytes) throws IOException {
    if (bytes == null) {
      return null;
    }
    return ReplicationPeerConfigUtil.convert(Bytes.toString(bytes));
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
    convert(Map<TableName, ? extends Collection<String>> tableCfs) {
    if (tableCfs == null) {
      return null;
    }
    List<ReplicationProtos.TableCF> tableCFList = new ArrayList<>(tableCfs.entrySet().size());
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
  public static ReplicationProtos.TableCF[] convert(String tableCFsConfig) {
    if (tableCFsConfig == null || tableCFsConfig.trim().length() == 0) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
  public static String convertToString(Map<TableName, ? extends Collection<String>> tableCfs) {
    if (tableCfs == null) {
      return null;
    }
    return convert(convert(tableCfs));
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
  public static String convertToString(Set<String> namespaces) {
    if (namespaces == null) {
      return null;
    }
    return StringUtils.join(namespaces, ';');
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
  public static Map<TableName, List<String>> convert2Map(ReplicationProtos.TableCF[] tableCFs) {
    if (tableCFs == null || tableCFs.length == 0) {
      return null;
    }
    Map<TableName, List<String>> tableCFsMap = new HashMap<>();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java`
#### Snippet
```java
  public Cell getNextCellHint(Cell currentCell) throws IOException {
    if (!canHint) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
      if (curKeyHint == null) {
        // If we ever don't have a hint and this is must-pass-one, then no hint
        return null;
      }
      // If this is the first hint we find, set it
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
    Cell prevCell, ReturnCode prevCode) throws IOException {
    if (prevCell == null || prevCode == null) {
      return null;
    }
    switch (prevCode) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
      case INCLUDE:
      case SKIP:
        return null;
      case SEEK_NEXT_USING_HINT:
        Cell nextHintCell = subFilter.getNextCellHint(prevCell);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
        return nextHintCell != null && compareCell(currentCell, nextHintCell) < 0
          ? ReturnCode.SEEK_NEXT_USING_HINT
          : null;
      case NEXT_COL:
      case INCLUDE_AND_NEXT_COL:
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
        // Once row changed, reset() will clear prevCells, so we need not to compare their rows
        // because rows are the same here.
        return CellUtil.matchingColumn(prevCell, currentCell) ? ReturnCode.NEXT_COL : null;
      case NEXT_ROW:
      case INCLUDE_AND_SEEK_NEXT_ROW:
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
      case INCLUDE_AND_SEEK_NEXT_ROW:
        // As described above, rows are definitely the same, so we only compare the family.
        return CellUtil.matchingFamily(prevCell, currentCell) ? ReturnCode.NEXT_ROW : null;
      default:
        throw new IllegalStateException("Received code is not valid.");
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettingsFactory.java`
#### Snippet
```java
  static QuotaSettings fromSpace(TableName table, String namespace, SpaceQuota protoQuota) {
    if (protoQuota == null) {
      return null;
    }
    if ((table == null && namespace == null) || (table != null && namespace != null)) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaRetriever.java`
#### Snippet
```java
      }
      if (result == null) {
        return null;
      }
      QuotaTableUtil.parseResultToCollection(result, cache);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
    if (!increased) {
      if (toInc < 0) {
        return null;
      }
      result[toInc] = order.inc(result[toInc]);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
    if (result == false) {
      done = true;
      return null;
    }
    byte[] nextRowKey = tracker.nextRow();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/ThrottleSettings.java`
#### Snippet
```java
      QuotaProtos.ThrottleRequest.Builder builder = proto.toBuilder();
      if (!otherThrottle.proto.hasType()) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/ThrottleSettings.java`
#### Snippet
```java
    return proto.hasTimedQuota()
      ? ProtobufUtil.toTimeUnit(proto.getTimedQuota().getTimeUnit())
      : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/ThrottleSettings.java`
#### Snippet
```java
    return proto.hasTimedQuota()
      ? ProtobufUtil.toQuotaScope(proto.getTimedQuota().getScope())
      : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java`
#### Snippet
```java
  public static Result[] getResults(CellScanner cellScanner, ScanResponse response)
    throws IOException {
    if (response == null) return null;
    // If cellscanner, then the number of Results to return is the count of elements in the
    // cellsPerResult list. Otherwise, it is how many results are embedded inside the response.
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java`
#### Snippet
```java
   */
  public static List<RegionInfo> getRegionInfos(final GetOnlineRegionResponse proto) {
    if (proto == null || proto.getRegionInfoCount() == 0) return null;
    return ProtobufUtil.getRegionInfos(proto);
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/SaslChallengeDecoder.java`
#### Snippet
```java
  private ByteBuf tryDecodeChallenge(ByteBuf in, int offset, int readableBytes) throws IOException {
    if (readableBytes < 4) {
      return null;
    }
    int len = in.getInt(offset);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/SaslChallengeDecoder.java`
#### Snippet
```java
    int totalLen = 4 + len;
    if (readableBytes < totalLen) {
      return null;
    }
    in.readerIndex(offset + totalLen);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/AuthMethod.java`
#### Snippet
```java
  public static AuthMethod valueOf(byte code) {
    final int i = (code & 0xff) - FIRST_CODE;
    return i < 0 || i >= values().length ? null : values()[i];
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufMessageConverter.java`
#### Snippet
```java
  private static Object toJavaObject(JsonElement element) {
    if (element.isJsonNull()) {
      return null;
    } else if (element.isJsonPrimitive()) {
      JsonPrimitive primitive = element.getAsJsonPrimitive();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufMessageConverter.java`
#### Snippet
```java
        return primitive.getAsString();
      } else {
        return null;
      }
    } else if (element.isJsonArray()) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufMessageConverter.java`
#### Snippet
```java
      return map;
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenIdentifier.java`
#### Snippet
```java
  public UserGroupInformation getUser() {
    if (username == null || "".equals(username)) {
      return null;
    }
    return UserGroupInformation.createRemoteUser(username);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java`
#### Snippet
```java
    }
    LOG.debug("No matching token found");
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
      // if we don't have any row corresponding to this get, return null
      if (result.isEmpty()) {
        return null;
      }
      // otherwise, extract quota snapshot in snapshots object
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
    Result result = doGet(connection, get);
    if (result.isEmpty()) {
      return null;
    }
    return quotasFromData(result.getValue(QUOTA_FAMILY_INFO, qualifier));
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/BuiltInProviderSelector.java`
#### Snippet
```java
    LOG.warn("No matching SASL authentication provider and supporting token found from providers"
      + " for user: {}", user);
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/SimpleSaslClientAuthenticationProvider.java`
#### Snippet
```java
    SecurityInfo securityInfo, Token<? extends TokenIdentifier> token, boolean fallbackAllowed,
    Map<String, String> saslProps) throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslClientAuthenticationProvider.java`
#### Snippet
```java
  public UserInformation getUserInfo(User user) {
    // Don't send user for token auth. Copied from RpcConnection.
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/snapshot/ClientSnapshotDescriptionUtils.java`
#### Snippet
```java
  public static String toString(SnapshotProtos.SnapshotDescription snapshot) {
    if (snapshot == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
  public static Throwable findException(Object exception) {
    if (exception == null || !(exception instanceof Throwable)) {
      return null;
    }
    Throwable cur = (Throwable) exception;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
    }

    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
    throws IOException {
    if (!resp.hasResult()) {
      return null;
    }
    return ProtobufUtil.toResult(resp.getResult(), controller.cellScanner());
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SimpleProcedureScheduler.java`
#### Snippet
```java
  @Override
  public LockedResource getLockResource(LockedResourceType resourceType, String resourceName) {
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java

      if (!hasMoreState() || isFailed()) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java
        return subProcedures;
      }
      return (isWaiting() || isFailed() || !hasMoreState()) ? null : new Procedure[] { this };
    } finally {
      updateTimestamp();
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java`
#### Snippet
```java
  protected synchronized long[] getSubprocedureIds() {
    if (subprocs == null) {
      return null;
    }
    return subprocs.stream().mapToLong(Procedure::getProcId).toArray();
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
      proc = procedures.get(proc.getParentProcId());
      if (proc == null) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
      if (bypass) {
        LOG.info("{} bypassed, returning null to finish it", this);
        return null;
      }
      return execute(env);
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
   */
  protected ProcedureMetrics getProcedureMetrics(TEnvironment env) {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/util/DelayedUtil.java`
#### Snippet
```java
    } catch (InterruptedException e) {
      Thread.currentThread().interrupt();
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java
      if (!running) {
        LOG.debug("the scheduler is not running");
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java
        if (!queueHasRunnables()) {
          nullPollCalls++;
          return null;
        }
      }
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java
      Thread.currentThread().interrupt();
      nullPollCalls++;
      return null;
    } finally {
      schedUnlock();
```

### ReturnNull
Return of `null`
in `hbase-metrics/src/main/java/org/apache/hadoop/hbase/metrics/impl/RefCountingMap.java`
#### Snippet
```java
  V remove(K k) {
    Payload<V> p = map.computeIfPresent(k, (k1, v) -> --v.refCount <= 0 ? null : v);
    return p == null ? null : p.v;
  }

```

### ReturnNull
Return of `null`
in `hbase-metrics/src/main/java/org/apache/hadoop/hbase/metrics/impl/RefCountingMap.java`
#### Snippet
```java
  V get(K k) {
    Payload<V> p = map.get(k);
    return p == null ? null : p.v;
  }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
    } catch (FileNotFoundException e) {
      LOG.warn("Log directory not found: " + e.getMessage());
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
      LOG.warn("Remove uninitialized log: {}", logFile);
      log.removeFile(walArchiveDir);
      return null;
    }
    LOG.debug("Opening Pv2 {}", logFile);
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
      LOG.warn("Remove uninitialized log: {}", logFile, e);
      log.removeFile(walArchiveDir);
      return null;
    } catch (IOException e) {
      String msg = "Unable to read state log: " + logFile;
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableScanResource.java`
#### Snippet
```java
      processException(exp);
      LOG.warn(exp.toString(), exp);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableScanResource.java`
#### Snippet
```java
            Result rs = itr.next();
            if ((rs == null) || (count <= 0)) {
              return null;
            }
            byte[] rowKey = rs.getRow();
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java`
#### Snippet
```java
    }
    if (valuesI == null) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java`
#### Snippet
```java
      return valuesI.next();
    } catch (NoSuchElementException e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
      processException(exp);
      LOG.warn(exp.toString(), exp);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java`
#### Snippet
```java
      }
    } while (loop);
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java`
#### Snippet
```java
    String filter = model.getFilter();
    if (filter == null || filter.length() == 0) {
      return null;
    }
    return buildFilter(filter);
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    CompletedProcedureRetainer<TEnvironment> retainer = completed.get(procId);
    if (retainer == null) {
      return null;
    } else {
      return retainer.getProcedure();
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
          this.procedures.forEachValue(1 /* Single-threaded */,
            // Transformer
            v -> v.getParentProcId() == procedure.getProcId() ? v : null,
            // Consumer
            v -> {
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
   */
  public NonceKey createNonceKey(final long nonceGroup, final long nonce) {
    return (nonce == HConstants.NO_NONCE) ? null : new NonceKey(nonceGroup, nonce);
  }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
        procedure
          .setFailure(new RemoteProcedureException(msg, new IllegalArgumentIOException(msg)));
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      return clazz.cast(proc);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java`
#### Snippet
```java
  public String getAttribute(String name) {
    Object o = attrs.get(new QName(name));
    return o != null ? o.toString() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Response.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java`
#### Snippet
```java
  public String getAttribute(String name) {
    Object o = attrs.get(new QName(name));
    return o != null ? o.toString() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java`
#### Snippet
```java
  @Override
  public JAXBContext getContext(Class<?> objectType) {
    return (types.contains(objectType)) ? context : null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static Authorizations toAuthorizations(byte[] protoBytes) throws DeserializationException {
    if (protoBytes == null) return null;
    ClientProtos.Authorizations.Builder builder = ClientProtos.Authorizations.newBuilder();
    ClientProtos.Authorizations proto = null;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static HBaseProtos.ServerName toServerName(final ServerName serverName) {
    if (serverName == null) {
      return null;
    }
    HBaseProtos.ServerName.Builder builder = HBaseProtos.ServerName.newBuilder();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static ServerName toServerName(final byte[] data) throws DeserializationException {
    if (data == null || data.length <= 0) {
      return null;
    }
    if (ProtobufMagic.isPBMagicPrefix(data)) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static SlowLogParams getSlowLogParams(Message message) {
    if (message == null) {
      return null;
    }
    if (message instanceof ScanRequest) {
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  @SuppressWarnings("unchecked")
  public static Throwable toException(final NameBytesPair parameter) throws IOException {
    if (parameter == null || !parameter.hasValue()) return null;
    String desc = parameter.getValue().toStringUtf8();
    String type = parameter.getName();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static Get toGet(final ClientProtos.Get proto) throws IOException {
    if (proto == null) return null;
    byte[] row = proto.getRow().toByteArray();
    Get get = new Get(row);
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    toRegionInfo(final HBaseProtos.RegionInfo proto) {
    if (proto == null) {
      return null;
    }
    TableName tableName = ProtobufUtil.toTableName(proto.getTableName());
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static CellVisibility toCellVisibility(byte[] protoBytes) throws DeserializationException {
    if (protoBytes == null) return null;
    ClientProtos.CellVisibility.Builder builder = ClientProtos.CellVisibility.newBuilder();
    ClientProtos.CellVisibility proto = null;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    toRegionInfo(final org.apache.hadoop.hbase.client.RegionInfo info) {
    if (info == null) {
      return null;
    }
    HBaseProtos.RegionInfo.Builder builder = HBaseProtos.RegionInfo.newBuilder();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static ServerName parseServerNameFrom(final byte[] data) throws DeserializationException {
    if (data == null || data.length <= 0) return null;
    if (ProtobufMagic.isPBMagicPrefix(data)) {
      int prefixLen = ProtobufMagic.lengthOfPBMagic();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static ServerName toServerName(final HBaseProtos.ServerName proto) {
    if (proto == null) return null;
    String hostName = proto.getHostName();
    long startCode = -1;
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static CellVisibility toCellVisibility(ClientProtos.CellVisibility proto) {
    if (proto == null) return null;
    return new CellVisibility(proto.getExpression());
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  public static Authorizations toAuthorizations(ClientProtos.Authorizations proto) {
    if (proto == null) return null;
    return new Authorizations(proto.getLabelList());
  }
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
  public static byte[] getResponseBody(HttpResponse resp) throws IOException {
    if (resp.getEntity() == null) {
      return null;
    }
    InputStream instream = resp.getEntity().getContent();
```

### ReturnNull
Return of `null`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
    InputStream instream = resp.getEntity().getContent();
    if (instream == null) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java`
#### Snippet
```java
  @Override
  public BlockCache[] getBlockCaches() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java`
#### Snippet
```java
        LOG.warn("Failed to deserialize data from memcached", e);
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java`
#### Snippet
```java
  public static ZooKeeperProtos.Master parse(byte[] data) throws DeserializationException {
    if (data == null) {
      return null;
    }
    int prefixLen = ProtobufUtil.lengthOfPBMagic();
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java`
#### Snippet
```java
    } catch (DeserializationException e) {
      LOG.warn("Failed parse", e);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
    String[] sp = server.split(":");
    if (sp.length == 0) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKClusterId.java`
#### Snippet
```java
  public static UUID getUUIDForCluster(ZKWatcher zkw) throws KeeperException {
    String uuid = readClusterIdZNode(zkw);
    return uuid == null ? null : UUID.fromString(uuid);
  }
}
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKClusterId.java`
#### Snippet
```java
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
        return null;
      }
      if (data != null) {
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKClusterId.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java`
#### Snippet
```java
    try {
      RegionState state = getMetaRegionState(zkw);
      return state.isOpened() ? state.getServerName() : null;
    } catch (KeeperException ke) {
      return null;
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java`
#### Snippet
```java
      return state.isOpened() ? state.getServerName() : null;
    } catch (KeeperException ke) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java`
#### Snippet
```java
    try {
      RegionState state = getMetaRegionState(zkw, replicaId);
      return state.isOpened() ? state.getServerName() : null;
    } catch (KeeperException ke) {
      return null;
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java`
#### Snippet
```java
      return state.isOpened() ? state.getServerName() : null;
    } catch (KeeperException ke) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java

  public synchronized States getState() {
    return zk == null ? null : zk.getState();
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java
  private Iterable<Op> prepareZKMulti(Iterable<Op> ops) throws UnsupportedOperationException {
    if (ops == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAuthentication.java`
#### Snippet
```java
      }

      return (null);
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java

    final Map.Entry<Long, BitSetNode> entry = map.floorEntry(procId);
    return entry != null ? entry.getValue() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
    List<String> children = listChildrenAndWatchForNewChildren(zkw, znode);
    if (children == null) {
      return null;
    }
    for (String child : children) {
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
    }

    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.debug(zkw.prefix("Unable to get data of znode " + znode + " "
        + "because node does not exist (not an error)"));
      return null;
    } catch (KeeperException e) {
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
      zkw.keeperException(e);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
  public static String getParent(String node) {
    int idx = node.lastIndexOf(ZNodePaths.ZNODE_PATH_SEPARATOR);
    return idx <= 0 ? null : node.substring(0, idx);
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
    } catch (InterruptedException e) {
      zkw.interruptedException(e);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      children = zkw.getRecoverableZooKeeper().getChildren(znode, null);
    } catch (KeeperException.NoNodeException nne) {
      return null;
    } catch (InterruptedException ie) {
      zkw.interruptedException(ie);
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.debug(zkw.prefix("Unable to get data of znode " + znode + " "
        + "because node does not exist (not necessarily an error)"));
      return null;
    } catch (KeeperException e) {
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
      zkw.keeperException(e);
      return null;
    } catch (InterruptedException e) {
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
      zkw.interruptedException(e);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
  private static Op toZooKeeperOp(ZKWatcher zkw, ZKUtilOp op) throws UnsupportedOperationException {
    if (op == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      return newNodes;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.trace(zkw.prefix("Unable to get data of znode " + znode + " "
        + "because node does not exist (not an error)"));
      return null;
    } catch (KeeperException e) {
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
      zkw.keeperException(e);
      return null;
    } catch (InterruptedException e) {
      LOG.warn(zkw.prefix("Unable to get data of znode " + znode), e);
```

### ReturnNull
Return of `null`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
        zkw.interruptedExceptionNoThrow(e, true);
      }
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java`
#### Snippet
```java
  protected ImmutableBytesWritable createGroupKey(byte[][] vals) {
    if (vals == null) {
      return null;
    }
    StringBuilder sb = new StringBuilder();
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/JarFinder.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    String[] args) throws IOException {
    if (!isValidArguements(args)) {
      return null;
    }
    return new Triple<>(TableName.valueOf(args[0]), getScanFromCommandLine(conf, args),
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    Filter exportFilter;
    String filterCriteria = (args.length > 5) ? args[5] : null;
    if (filterCriteria == null) return null;
    if (filterCriteria.startsWith("^")) {
      String regexPattern = filterCriteria.substring(1, filterCriteria.length());
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    }

    if (startTime == 0 && endTime == 0) return null;

    endTime = endTime == 0 ? HConstants.LATEST_TIMESTAMP : endTime;
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    Filter rowFilter = null;
    String filterCriteria = (args.length > 3) ? args[3] : null;
    if (filterCriteria == null) return null;
    if (filterCriteria.startsWith("^")) {
      String regexPattern = filterCriteria.substring(1, filterCriteria.length());
```

### ReturnNull
Return of `null`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
    FileEncryptionInfo feInfo = stat.getFileEncryptionInfo();
    if (feInfo == null) {
      return null;
    }
    return TRANSPARENT_CRYPTO_HELPER.createEncryptor(conf, feInfo, client);
```

### ReturnNull
Return of `null`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
        PBHelperClient.convertCipherOptionProtos(proto.getCipherOptionList());
      if (cipherOptions == null || cipherOptions.isEmpty()) {
        return null;
      }
      CipherOption cipherOption = cipherOptions.get(0);
```

### ReturnNull
Return of `null`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
      String cipherSuites = conf.get(DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY);
      if (StringUtils.isBlank(cipherSuites)) {
        return null;
      }
      if (!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName())) {
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  public Job createSubmittableJob(String[] args) throws IOException {
    if (!doCommandLine(args)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
            args[i].substring(args[1].indexOf(rangeSwitch) + rangeSwitch.length()));
        } catch (IllegalArgumentException e) {
          return null;
        }
        continue;
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
    if (endTime < startTime) {
      printUsage("--endtime=" + endTime + " needs to be greater than --starttime=" + startTime);
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java`
#### Snippet
```java
  protected ImmutableBytesWritable createGroupKey(byte[][] vals) {
    if (vals == null) {
      return null;
    }
    StringBuilder sb = new StringBuilder();
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
          return attributes.split(DEFAULT_MULTIPLE_ATTRIBUTES_SEPERATOR);
        } else {
          return null;
        }
      }
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
      public String getCellVisibility() {
        if (!hasCellVisibility()) {
          return null;
        } else {
          return Bytes.toString(lineBytes, getColumnOffset(cellVisibilityColumnIndex),
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
      private String getAttributes() {
        if (!hasAttributes()) {
          return null;
        } else {
          return Bytes.toString(lineBytes, getColumnOffset(attrKeyColumnIndex),
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java
    String oldStyleVal = conf.get(deprecatedKey);
    if (oldStyleVal == null) {
      return null;
    }
    LOG.warn("Using deprecated configuration " + deprecatedKey
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      } catch (IOException e) {
        LOG.warn("Unable to get checksum for file=" + path, e);
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
        if (currentRowResult == null) {
          // nothing left in current row
          return null;
        }

```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
            currentRowResult = null;
            currentRow = null;
            return null;
          }
        }
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java

    if (hostAndWeights.length == 0) { // no matter what numTopsAtMost is
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
    String splitAlgoClassName = conf.get(SPLIT_ALGO);
    if (splitAlgoClassName == null) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    if (filterClass == null) {
      LOG.debug("No configured filter class, accepting all keyvalues.");
      return null;
    }
    LOG.debug("Attempting to create filter:" + filterClass);
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
          || code.equals(Filter.ReturnCode.INCLUDE_AND_NEXT_COL))
      ) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  public Job createSubmittableJob(Configuration conf, String[] args) throws IOException {
    if (!doCommandLine(args)) {
      return null;
    }
    conf.set(NAME + ".tableName", tableName);
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
  public static String quoteHtmlChars(String item) {
    if (item == null) {
      return null;
    }
    byte[] bytes = Bytes.toBytes(item);
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HtmlQuoting.java`
#### Snippet
```java
  public static String unquoteHtmlChars(String item) {
    if (item == null) {
      return null;
    }
    int next = item.indexOf('&');
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
      public String getParameter(String name) {
        final List<String> a = m.get(name);
        return a == null ? null : a.get(0);
      }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
        return Long.valueOf(value);
      } catch (NumberFormatException e) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
      }

      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
        return Double.valueOf(value);
      } catch (NumberFormatException e) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java`
#### Snippet
```java
  private String checkCallbackName(String callbackName) throws IOException {
    if (null == callbackName) {
      return null;
    }
    if (callbackName.matches("[A-Za-z0-9_]+")) {
```

### ReturnNull
Return of `null`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java

    if (null == jar || jar.isEmpty()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/gson/GsonMessageBodyWriter.java`
#### Snippet
```java
        } catch (IllegalCharsetNameException e) {
          logger.debug("Client requested illegal Charset '{}'", c);
          return null;
        } catch (UnsupportedCharsetException e) {
          logger.debug("Client requested unsupported Charset '{}'", c);
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/gson/GsonMessageBodyWriter.java`
#### Snippet
```java
        } catch (UnsupportedCharsetException e) {
          logger.debug("Client requested unsupported Charset '{}'", c);
          return null;
        } catch (Exception e) {
          logger.debug("Error while resolving Charset '{}'", c, e);
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/gson/GsonMessageBodyWriter.java`
#### Snippet
```java
        } catch (Exception e) {
          logger.debug("Error while resolving Charset '{}'", c, e);
          return null;
        }
      }).orElse(StandardCharsets.UTF_8);
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/ProcessUtils.java`
#### Snippet
```java
      // ignore
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
    if (keys.length != values.length) {
      LOG.error("keys and values arrays must be same size");
      return null;
    }
    if (keys.length == 0 || values.length == 0) {
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
    if (keys.length == 0 || values.length == 0) {
      LOG.error("keys and values arrays can not be empty;");
      return null;
    }
    Hashtable<String, String> table = new Hashtable<>();
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/HeterogeneousRegionCountCostFunction.java`
#### Snippet
```java
  private List<String> readFile(final String filename) {
    if (null == filename) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/HeterogeneousRegionCountCostFunction.java`
#### Snippet
```java
    } catch (IOException e) {
      LOG.error("cannot read rules file located at ' " + filename + " ':" + e.getMessage());
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java

    if (index > webServer.getConnectors().length) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
      // -1 if the connector has not been opened
      // -2 if it has been closed
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
  private static FilterInitializer[] getFilterInitializers(Configuration conf) {
    if (conf == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
    Class<?>[] classes = conf.getClasses(FILTER_INITIALIZERS_PROPERTY);
    if (classes == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
        String[] unquoteValue = rawRequest.getParameterValues(unquoteName);
        if (unquoteValue == null) {
          return null;
        }
        String[] result = new String[unquoteValue.length];
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java
    ClusterInfoProvider service = this.provider;
    if (service == null) {
      return null;
    }
    return service.getTableDescriptor(tableName);
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
      LOG.error("At least one cost function needs a multiplier > 0. For example, set "
        + "hbase.master.balancer.stochastic.regionCountCost to a positive value or default");
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java

    if (!needsBalance(tableName, cluster)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
        + "{} ms, and did not find anything with an imbalance score less than {}",
      step, endTime - startTime, initCost / sumMultiplier);
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java`
#### Snippet
```java
        || favoredNodes.size() != FavoredNodeAssignmentHelper.FAVORED_NODES_NUM
    ) {
      return null;
    }
    for (Position p : Position.values()) {
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java`
#### Snippet
```java
  public synchronized List<ServerName> getFavoredNodesWithDNPort(RegionInfo regionInfo) {
    if (getFavoredNodes(regionInfo) == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
    if (numServers == 0) {
      LOG.warn("Wanted to retain assignment but no servers to assign to");
      return null;
    }
    if (numServers == 1) { // Only one server, nothing fancy we can do here
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      new BalancerClusterState(loadOfOneTable, null, this.regionFinder, this.rackManager);
    if (!needsBalance(c) && !this.overallNeedsBalance()) {
      return null;
    }
    ClusterLoadState cs = new ClusterLoadState(loadOfOneTable);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
    List<ServerName> serversWithoutStartCodes) {
    if (serversWithoutStartCodes == null) {
      return null;
    } else {
      List<ServerName> result = Lists.newArrayList();
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java`
#### Snippet
```java
    RegionLocations locations = CatalogFamilyFormat.getRegionLocations(r);
    return locations == null
      ? null
      : locations.getRegionLocation(
        parsedInfo == null ? RegionInfo.DEFAULT_REPLICA_ID : parsedInfo.getReplicaId());
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java`
#### Snippet
```java
    throws IOException {
    HRegionLocation location = getRegionLocation(connection, regionName);
    return location == null ? null : new Pair<>(location.getRegion(), location.getServerName());
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
      @Override
      public Cell getKey() {
        if (atEnd) return null;
        return delegate.getKey();
      }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
      @Override
      public Cell getNextIndexedKey() {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
      @Override
      public String getValueString() {
        if (atEnd) return null;

        return delegate.getValueString();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
      @Override
      public String getKeyString() {
        if (atEnd) return null;

        return delegate.getKeyString();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
      @Override
      public Cell getCell() {
        if (atEnd) return null;

        return delegate.getCell();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
      @Override
      public ByteBuffer getValue() {
        if (atEnd) return null;

        return delegate.getValue();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
            if ((args == null || args.length == 0) && "close".equals(method.getName())) {
              RPC.stopProxy(cp);
              return null;
            } else {
              Object res = method.invoke(cp, args);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
    }

    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
      return ((FileLinkInputStream) stream.getWrappedStream()).getUnderlyingInputStream();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java`
#### Snippet
```java
  @Override
  public Writable getDataWriter() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileIndexBlockEncoderImpl.java`
#### Snippet
```java
  @Override
  public EncodedSeeker createSeeker() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
    public Cell midkey(CachingBlockReader cachingBlockReader) throws IOException {
      // Not needed here
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      int rootLevelIndex = rootBlockContainingKey(key);
      if (rootLevelIndex < 0 || rootLevelIndex >= blockOffsets.length) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      int numEntries = nonRootIndex.getInt(0);
      if (i < 0 || i >= numEntries) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      throws IOException {
      // this would not be needed
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
        cacheBlocks, pread, isCompaction, expectedDataBlockEncoding, cachingBlockReader);
      if (blockWithScanInfo == null) {
        return null;
      } else {
        return blockWithScanInfo.getHFileBlock();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java`
#### Snippet
```java
      }
      LOG.warn("No Comparator class for " + comparatorClassName + ". Returning Null.");
      return null;
    } catch (Exception e) {
      throw new IOException("Comparator class " + comparatorClassName + " is not instantiable", e);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    public Cell getCell() {
      if (this.curBlock == null) {
        return null;
      }
      return seeker.getCell();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
  public HFileBlock getMetaBlock(String metaBlockName, boolean cacheBlock) throws IOException {
    if (trailer.getMetaIndexCount() == 0) {
      return null; // there are no meta blocks
    }
    if (metaBlockIndexReader == null) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    int block = metaBlockIndexReader.rootBlockContainingKey(mbname, 0, mbname.length);
    if (block == -1) {
      return null;
    }
    long blockSize = metaBlockIndexReader.getRootBlockDataSize(block);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    public Cell getCell() {
      if (!isSeeked()) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
      long lastDataBlockOffset = reader.getTrailer().getLastDataBlockOffset();
      if (curBlock == null) {
        return null;
      }
      HFileBlock block = this.curBlock;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
        if (block.getOffset() >= lastDataBlockOffset) {
          releaseIfNotCurBlock(block);
          return null;
        }
        if (block.getOffset() < 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
            returnAndEvictBlock(cache, cacheKey, cachedBlock);
          }
          return null;
        }
        return cachedBlock;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
          Class.forName("org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache"));
      } catch (ClassNotFoundException e) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
      LOG.warn("Error creating external block cache", e);
    }
    return null;

  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
    String bucketCacheIOEngineName = c.get(BUCKET_CACHE_IOENGINE_KEY, null);
    if (bucketCacheIOEngineName == null || bucketCacheIOEngineName.length() <= 0) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
    final long cacheSize = MemorySizeUtil.getOnHeapCacheSize(c);
    if (cacheSize < 0) {
      return null;
    }
    String policy = c.get(BLOCKCACHE_POLICY_KEY, BLOCKCACHE_POLICY_DEFAULT);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.java`
#### Snippet
```java
    FirstLevelBlockCache l1Cache = createFirstLevelCache(conf);
    if (l1Cache == null) {
      return null;
    }
    boolean useExternal = conf.getBoolean(EXTERNAL_BLOCKCACHE_KEY, EXTERNAL_BLOCKCACHE_DEFAULT);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
      int numEntries = nonRootIndex.getInt(0);
      if (i < 0 || i >= numEntries) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
      int rootLevelIndex = rootBlockContainingKey(key);
      if (rootLevelIndex < 0 || rootLevelIndex >= blockOffsets.length) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
      } else {
        // left == right
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
      } else {
        // left == right
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
    if (value == null) {
      if (repeat) {
        return null;
      }
      if (updateCacheMetrics) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
  @Override
  public BlockCache[] getBlockCaches() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
      return new BlockCache[] { this, this.victimHandler };
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
        return result;
      }
      return null;
    }
    if (updateCacheMetrics) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
      return new BlockCache[] { this, this.victimHandler };
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
        return result;
      }
      return null;
    }
    if (updateCacheMetrics) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
        // Verify checksum of the data before using it for building HFileBlock.
        if (verifyChecksum && !validateChecksum(offset, curBlock, hdrSize)) {
          return null;
        }
        // remove checksum from buffer now that it's verified
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
        public HFileBlock nextBlock() throws IOException {
          if (offset >= endOffset) {
            return null;
          }
          HFileBlock b = readBlockData(offset, length, false, false, true);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
    private ByteBuff getCachedHeader(final long offset) {
      PrefetchedHeader ph = this.prefetchedHeader.get();
      return ph != null && ph.offset == offset ? ph.buf : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
    // Is the rack valid? Do we recognize it?
    if (rack == null || getServersFromRack(rack) == null || getServersFromRack(rack).isEmpty()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
      // Do we have any servers left to choose from?
      if (serversToChooseFrom.isEmpty()) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
      return ServerName.valueOf(randomServer.getAddress(), randomServer.getStartcode());
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
    throws IOException {
    if (CollectionUtils.isEmpty(favoredNodeList)) {
      return null;
    }
    long time = EnvironmentEdgeManager.currentTime();
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
      // Single region server case: cannot not place the favored nodes
      // on any server;
      return null;
    } else {
      // Randomly select two region servers from the server list and make sure
```

### ReturnNull
Return of `null`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
    boolean updateCacheMetrics) {
    if (!cacheEnabled) {
      return null;
    }
    RAMQueueEntry re = ramCache.get(key);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      cacheStats.miss(caching, key.isPrimary(), key.getBlockType());
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
  @Override
  public BlockCache[] getBlockCaches() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      previousEntry.withWriteLock(offsetLock, () -> {
        blockEvicted(key, previousEntry, false, false);
        return null;
      });
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
            blockEvicted(key, bucketEntry, false, false);
          }
          return null;
        });
      }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
        return entry;
      });
      return absent.get() ? null : re;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      // This cacheable thing can't be serialized
      if (len == 0) {
        return null;
      }
      long offset = alloc.allocateBlock(len);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
          public BlockType getBlockType() {
            // Not held by BucketEntry. Could add it if wanted on BucketEntry creation.
            return null;
          }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
      if (b != null) return b;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
    for (int i = 0; i < bucketSizes.length; ++i)
      if (blockSize <= bucketSizes[i]) return bucketSizeInfos[i];
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
      if (bucketList.size() == 1) {
        // So we never get complete starvation of a bucket for a size
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
    // If simple auth, return
    if (saslServer == null) {
      return null;
    }
    // check if rpc encryption with Crypto AES
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
      && this.rpcServer.conf.getBoolean("hbase.rpc.crypto.encryption.aes.enabled", false);
    if (!isCryptoAesEncryption) {
      return null;
    }
    if (!connectionHeader.hasRpcCryptoCipherTransformation()) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
    }
    if (!connectionHeader.hasRpcCryptoCipherTransformation()) {
      return null;
    }
    String transformation = connectionHeader.getRpcCryptoCipherTransformation();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
    String transformation = connectionHeader.getRpcCryptoCipherTransformation();
    if (transformation == null || transformation.length() == 0) {
      return null;
    }
    // Negotiates AES based on complete saslServer.
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
      return connectionHeader.getVersionInfo();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java

    if (!head.hasUserInfo()) {
      return null;
    }
    UserInformation userInfoProto = head.getUserInfo();
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          GetRSGroupInfoRequest.newBuilder().setRSGroupName(groupName).build(),
          (s, c, req, done) -> s.getRSGroupInfo(c, req, done),
          resp -> resp.hasRSGroupInfo() ? ProtobufUtil.toGroupInfo(resp.getRSGroupInfo()) : null))
      .call();
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
            .setTableName(ProtobufUtil.toProtoTableName(table)).build(),
          (s, c, req, done) -> s.getRSGroupInfoOfTable(c, req, done),
          resp -> resp.hasRSGroupInfo() ? ProtobufUtil.toGroupInfo(resp.getRSGroupInfo()) : null))
      .call();
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        controller, stub, ExecProcedureRequest.newBuilder().setProcedure(proDesc).build(),
        (s, c, req, done) -> s.execProcedureWithRet(c, req, done),
        resp -> resp.hasReturnData() ? resp.getReturnData().toByteArray() : null))
      .call();
  }
```

### ReturnNull
Return of `null`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
            .build(),
          (s, c, req, done) -> s.getRSGroupInfoOfServer(c, req, done),
          resp -> resp.hasRSGroupInfo() ? ProtobufUtil.toGroupInfo(resp.getRSGroupInfo()) : null))
      .call();
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  public synchronized InetSocketAddress getListenerAddress() {
    if (listener == null) {
      return null;
    }
    return listener.getAddress();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java`
#### Snippet
```java
      return call.getMethod().getName();
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFile.java`
#### Snippet
```java
        }
      }
      return null;
    } finally {
      if (scanner != null && !succ) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java`
#### Snippet
```java
   */
  default byte[] getExtendedAttribute(String attributeKey) {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
    final String serviceName) {
    BlockingServiceAndInterface bsasi = getServiceAndInterface(services, serviceName);
    return bsasi == null ? null : bsasi.getBlockingService();
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java

  protected AuthenticationTokenSecretManager createSecretManager() {
    if (!isSecurityEnabled) return null;
    if (server == null) return null;
    Configuration conf = server.getConfiguration();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
  protected AuthenticationTokenSecretManager createSecretManager() {
    if (!isSecurityEnabled) return null;
    if (server == null) return null;
    Configuration conf = server.getConfiguration();
    long keyUpdateInterval = conf.getLong("hbase.auth.key.update.interval", 24 * 60 * 60 * 1000);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
      return call.getRemoteAddress();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
    final String serviceName) {
    BlockingServiceAndInterface bsasi = getServiceAndInterface(services, serviceName);
    return bsasi == null ? null : bsasi.getServiceInterface();
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredEditsOutputSink.java`
#### Snippet
```java
      isSuccessful &= writeRemainingEntryBuffers();
    }
    return isSuccessful ? splits : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    // If the path contains oldWALs keyword then exit early.
    if (path.toString().contains(HConstants.HREGION_OLDLOGDIR_NAME)) {
      return null;
    }
    Path walRootDir = CommonFSUtils.getWALRootDir(conf);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    }
    LOG.error("Couldn't locate log: " + path);
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    throws IOException {
    if (path == null || path.length() <= HConstants.HREGION_LOGDIR_NAME.length()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    } catch (IllegalArgumentException e) {
      LOG.info("Call to makeQualified failed on " + path + " " + e.getMessage());
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java

    if (!fullPath.startsWith(startPath)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    if (serverNameAndFile.indexOf('/') < "a,0,0".length()) {
      // Either it's a file (not a directory) or it's not a ServerName format
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
      updateStatusWithMsg(errorMsg);
      thrown.add(ioe);
      return null;
    }
    final String msg = "Closed recovered edits writer path=" + editsWriter.path + " (wrote "
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
        throw new IOException("Failed deleting empty  " + editsWriter.path);
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
      updateStatusWithMsg(errorMsg);
      thrown.add(ioe);
      return null;
    }
    return dst;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java`
#### Snippet
```java
    ret = createRecoveredEditsWriter(tableName, region, seqId);
    if (ret == null) {
      return null;
    }
    LOG.trace("Created {}", ret.path);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java`
#### Snippet
```java
      isSuccessful &= closeWriters();
    }
    return isSuccessful ? splits : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      // truncated files are expected if a RS crashes (see HBASE-2643)
      LOG.info("EOF from {}; continuing.", path);
      return null;
    } catch (IOException e) {
      // If the IOE resulted from bad file format,
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      ) {
        LOG.warn("Parse exception from {}; continuing", path, e);
        return null;
      }
      if (!skipErrors) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
        }
        // EOFException being ignored
        return null;
      }
    } catch (IOException e) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
        // A wal file may not exist anymore. Nothing can be recovered so move on
        LOG.warn("File {} does not exist anymore", path, e);
        return null;
      }
      if (!skipErrors || e instanceof InterruptedIOException) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java
   */
  public static CompactionDescriptor getCompaction(Cell kv) throws IOException {
    return isCompactionMarker(kv) ? CompactionDescriptor.parseFrom(CellUtil.cloneValue(kv)) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java
    return CellUtil.matchingColumnFamilyAndQualifierPrefix(cell, METAFAMILY, REGION_EVENT_PREFIX)
      ? RegionEventDescriptor.parseFrom(CellUtil.cloneValue(cell))
      : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java
    return CellUtil.matchingColumn(cell, METAFAMILY, BULK_LOAD)
      ? WALProtos.BulkLoadDescriptor.parseFrom(CellUtil.cloneValue(cell))
      : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java
    return CellUtil.matchingColumn(cell, METAFAMILY, FLUSH)
      ? FlushDescriptor.parseFrom(CellUtil.cloneValue(cell))
      : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
    }
    if (biggestBufferKey == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedEntryBuffers.java`
#### Snippet
```java
  synchronized RegionEntryBuffer getChunkToWrite() {
    if (totalBuffered < maxHeapUsage) {
      return null;
    }
    return super.getChunkToWrite();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java`
#### Snippet
```java
      isSuccessful &= writeRemainingEntryBuffers();
    }
    return isSuccessful ? splits : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConnectionCache.java`
#### Snippet
```java
      LOG.error("Error getting connection: ", e);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java`
#### Snippet
```java
        }
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java`
#### Snippet
```java
    @Override
    public Long startCacheFlush(final byte[] encodedRegionName, Set<byte[]> flushedFamilyNames) {
      if (closed.get()) return null;
      return HConstants.NO_SEQNUM;
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java`
#### Snippet
```java
    // namespace is the 4th parent of file
    for (int i = 0; i < 5; i++) {
      if (p == null) return null;
      if (i == 3) tbl = p.getName();
      p = p.getParent();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java`
#### Snippet
```java
      p = p.getParent();
    }
    if (p == null) return null;
    return TableName.valueOf(p.getName(), tbl);
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
      return filePath;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConfigurationUtil.java`
#### Snippet
```java

    if (kvps == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
    }

    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java

    if (masters == null || masters.isEmpty()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
    final Configuration conf, final Path rootDir, final TableDescriptor tableDescriptor,
    final RegionInfo[] newRegions, final RegionFillTask task) throws IOException {
    if (newRegions == null) return null;
    int regionNumber = newRegions.length;
    CompletionService<RegionInfo> completionService = new ExecutorCompletionService<>(exec);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
    final TableDescriptor tableDescriptor, final RegionInfo[] newRegions, final RegionFillTask task)
    throws IOException {
    if (newRegions == null) return null;
    int regionNumber = newRegions.length;
    ThreadPoolExecutor exec = getRegionOpenAndInitThreadPool(conf,
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java`
#### Snippet
```java
      }
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MunkresAssignment.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
      // remove endpoints, which are included in the splits list

      return splits == null ? null : Arrays.copyOfRange(splits, 1, splits.length - 1);
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
        return e.getRegionInfo().getTable();
      }
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
        return e.getRegionInfo().getRegionName();
      }
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    } else {
      LOG.error("Entry " + this + " has no meta or hdfs region start key.");
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  public Path getHdfsRegionDir() {
    if (hdfsEntry == null) {
      return null;
    }
    return hdfsEntry.regionDir;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    } else {
      LOG.error("Entry " + this + " has no meta or hdfs region start key.");
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  public RegionInfo getHdfsHRI() {
    if (hdfsEntry == null) {
      return null;
    }
    return hdfsEntry.hri;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ZKDataMigrator.java`
#### Snippet
```java
      ZNodePaths.joinZNode(zkw.getZNodePaths().tableZNode, tableName.getNameAsString());
    byte[] data = ZKUtil.getData(zkw, znode);
    if (data == null || data.length <= 0) return null;
    try {
      ProtobufUtil.expectPBMagicPrefix(data);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java`
#### Snippet
```java
    throws IOException {
    if (!admin.isTableEnabled(region.getTable())) {
      return null;
    }
    HRegionLocation loc = conn.getRegionLocator(region.getTable())
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java`
#### Snippet
```java
      return loc.getServerName();
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java`
#### Snippet
```java

    public E peek() {
      return (head != tail) ? objects[head] : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java`
#### Snippet
```java
      LOG.trace("Bloom filters are disabled by configuration for " + writer.getPath()
        + (conf == null ? " (configuration is null)" : ""));
      return null;
    } else if (bloomType == BloomType.NONE) {
      LOG.trace("Bloom filter is turned off for the column family");
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java`
#### Snippet
```java
    } else if (bloomType == BloomType.NONE) {
      LOG.trace("Bloom filter is turned off for the column family");
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java`
#### Snippet
```java
      LOG.info("Delete Bloom filters are disabled by configuration for " + writer.getPath()
        + (conf == null ? " (configuration is null)" : ""));
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
        "None/Multiple table descriptors found for table '" + tableName + "' regions: " + htds);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      } catch (RemoteException e) {
        if (AlreadyBeingCreatedException.class.getName().equals(e.getClassName())) {
          return null;
        } else {
          throw e;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    if (!fs.exists(regionDir)) {
      LOG.warn("No previous " + regionDir + " exists.  Continuing.");
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/ClusterCompactionQueues.java`
#### Snippet
```java
        return compactionQueues.get(serverName).remove(0);
      }
      return null;
    } finally {
      lock.writeLock().unlock();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerRegionReplicaUtil.java`
#### Snippet
```java
  public static RegionInfo getRegionInfoForFs(RegionInfo regionInfo) {
    if (regionInfo == null) {
      return null;
    }
    return RegionReplicaUtil.getRegionInfoForDefaultReplica(regionInfo);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/ClientSideRegionScanner.java`
#### Snippet
```java
    if (values.isEmpty()) {
      // we are done
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java`
#### Snippet
```java
        currentRegion++;
        if (currentRegion >= regions.size()) {
          return null;
        }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
    @Override
    public Path apply(File file) {
      return file == null ? null : file.getPath();
    }
  };
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java`
#### Snippet
```java
  public synchronized Date getTimeOfDeath(final ServerName deadServerName) {
    Long time = deadServers.get(deadServerName);
    return time == null ? null : new Date(time);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStateStore.java`
#### Snippet
```java
    byte[] zkData = ZKUtil.getDataNoWatch(watcher, zkPath, null);
    if (zkData == null || zkData.length == 0) {
      return null;
    }
    update(zkData);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java`
#### Snippet
```java
  /** Returns return null if current is zk-based WAL splitting */
  default SplitWALManager getSplitWALManager() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionServerTracker.java`
#### Snippet
```java
      // need to add it to the region server set.
      LOG.warn("Server node {} does not exist, already dead?", serverName);
      return null;
    }
    if (data.length == 0 || !ProtobufUtil.isPBMagicPrefix(data)) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionServerTracker.java`
#### Snippet
```java
      LOG.warn("Invalid data for region server node {} on zookeeper, data length = {}", serverName,
        data.length);
      return null;
    }
    RegionServerInfo.Builder builder = RegionServerInfo.newBuilder();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
      // TODO what if this is due to a bad HFile?
      LOG.info("hfile " + hfilePath + " has no entries, skipping");
      return null;
    }
    if (Bytes.compareTo(first.get(), last.get()) > 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
    // group regions.
    regionGroups.put(ByteBuffer.wrap(startEndKeys.get(firstKeyRegionIdx).getFirst()), item);
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
  public static List<ServerName> getFavoredNodeList(String favoredNodesStr) {
    String[] favoredNodesArray = StringUtils.split(favoredNodesStr, ",");
    if (favoredNodesArray == null) return null;

    List<ServerName> serverList = new ArrayList<>();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
  private String getClusterId() {
    if (!isClusterIdSet.get()) {
      return null;
    }
    // It is ok to read without a lock since clusterId is immutable once set.
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
  public String getFromCacheOrFetch() {
    if (server.isStopping() || server.isStopped()) {
      return null;
    }
    String id = getClusterId();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
          + "region server name; leaving in place. If you see later errors about missing "
          + "write ahead logs they may be saved in this location.", s.getPath());
        return null;
      }
      return serverName;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/RegionReplicaInfo.java`
#### Snippet
```java

  public Integer getReplicaId() {
    return regionInfo != null ? regionInfo.getReplicaId() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/RegionReplicaInfo.java`
#### Snippet
```java

  public byte[] getEndKey() {
    return regionInfo != null ? regionInfo.getEndKey() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/RegionReplicaInfo.java`
#### Snippet
```java

  public byte[] getStartKey() {
    return regionInfo != null ? regionInfo.getStartKey() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/RegionReplicaInfo.java`
#### Snippet
```java

  public byte[] getRegionName() {
    return regionInfo != null ? regionInfo.getRegionName() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MasterStatusServlet.java`
#### Snippet
```java
    RegionStateNode rsn = master.getAssignmentManager().getRegionStates()
      .getRegionStateNode(RegionInfoBuilder.FIRST_META_REGIONINFO);
    return rsn.isInState(RegionState.State.OPEN) ? rsn.getRegionLocation() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MasterStatusServlet.java`
#### Snippet
```java
      return FSUtils.getTableFragmentation(master);
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
    if (oldtask == null) {
      batch.installed++;
      return null;
    }
    // new task was not used.
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
          // batch.done. There is no need for the batch creator to wait for
          // this task to complete.
          return (null);
        }
        if (oldtask.status == IN_PROGRESS) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
          batch.installed++;
          LOG.debug("Previously orphan task " + path + " is now being waited upon");
          return null;
        }
        while (oldtask.status == FAILURE) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
        if (t == null) {
          batch.installed++;
          return null;
        }
        LOG.error(HBaseMarkers.FATAL, "Logic error. Deleted task still present in tasks map");
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
  private static String urlDecode(final String val) {
    if (StringUtils.isEmpty(val)) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
      return URLDecoder.decode(val, StandardCharsets.UTF_8.toString());
    } catch (UnsupportedEncodingException e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
  private static Integer tryParseInt(final String val) {
    if (StringUtils.isEmpty(val)) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
      return Integer.parseInt(val);
    } catch (NumberFormatException e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
  private static String urlEncode(final String val) {
    if (StringUtils.isEmpty(val)) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
      return URLEncoder.encode(val, StandardCharsets.UTF_8.toString());
    } catch (UnsupportedEncodingException e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);
    if (requestValue == null) {
      return null;
    }
    return TableName.valueOf(requestValue);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);
    if (requestValue == null) {
      return null;
    }
    return Bytes.toBytesBinary(requestValue);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
  static String buildStartParamFrom(final byte[] lastRow) {
    if (lastRow == null) {
      return null;
    }
    return urlEncode(Bytes.toStringBinary(lastRow));
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
  private Filter buildScanFilter() {
    if (scanTable == null && scanRegionState == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    final String param) {
    if (request == null) {
      return null;
    }
    final String requestValueStrEnc = request.getParameter(param);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    final String requestValueStrEnc = request.getParameter(param);
    if (StringUtils.isBlank(requestValueStrEnc)) {
      return null;
    }
    return urlDecode(requestValueStrEnc);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
  private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz, final String value) {
    if (clazz == null || value == null) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
      return Enum.valueOf(clazz, value);
    } catch (IllegalArgumentException e) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);
    if (StringUtils.isBlank(requestValueStr)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    if (requestValue == null) {
      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));
      return null;
    }
    if (requestValue <= 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);
    if (requestValueStr == null) {
      return null;
    }
    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/MetaBrowser.java`
#### Snippet
```java
    if (requestValue == null) {
      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));
      return null;
    }
    return requestValue;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/RegionScannerAsResultScanner.java`
#### Snippet
```java
  @Override
  public ScanMetrics getScanMetrics() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/RegionScannerAsResultScanner.java`
#### Snippet
```java
  public Result next() throws IOException {
    if (!moreRows) {
      return null;
    }
    for (;;) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/RegionScannerAsResultScanner.java`
#### Snippet
```java
      if (cells.isEmpty()) {
        if (!moreRows) {
          return null;
        } else {
          continue;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKeyImpl.java`
#### Snippet
```java
  @Override
  public byte[] getExtendedAttribute(String attributeKey) {
    return extendedAttributes != null ? extendedAttributes.get(attributeKey) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
        break;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/ReportMakingVisitor.java`
#### Snippet
```java
          + "row={} {}; See if RegionInfo is referenced in another hbase:meta row? Delete?",
        Bytes.toStringBinary(metaTableRow.getRow()), ri.getRegionNameAsString());
      return null;
    }
    // Skip split parent region
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
    // Local master locks don't store any state, so on recovery, simply finish this procedure
    // immediately.
    if (recoveredMasterLock) return null;
    if (lockAcquireLatch != null) {
      lockAcquireLatch.countDown();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
      locked.set(false);
      LOG.debug((unlock.get() ? "UNLOCKED " : "TIMED OUT ") + toString());
      return null;
    }
    synchronized (event) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MasterClusterInfoProvider.java`
#### Snippet
```java
  public TableDescriptor getTableDescriptor(TableName tableName) throws IOException {
    TableDescriptors tds = services.getTableDescriptors();
    return tds != null ? tds.get(tableName) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MasterClusterInfoProvider.java`
#### Snippet
```java
  public ServerMetrics getLoad(ServerName serverName) {
    ServerManager sm = services.getServerManager();
    return sm != null ? sm.getLoad(serverName) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
      LOG.warn("Can NOT create CleanerDelegate={}", className, e);
      // skipping if can't instantiate
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MaintenanceLoadBalancer.java`
#### Snippet
```java
    // should only have 1 region server in maintenance mode
    assert servers.size() == 1;
    return regionInfo.getTable().isSystemTable() ? servers.get(0) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
    if (dispatched) {
      if (succ) {
        return null;
      }
      dispatched = false;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.java`
#### Snippet
```java
      LOG.warn("Can not send remote operation {} to {}, this operation will "
        + "be retried to send to another server", this.getProcId(), targetServer);
      return null;
    }
    dispatched = true;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java
      return group;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java
      return User.create(UserGroupInformation.createRemoteUser(effectiveUser));
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    throws IOException {
    if (!CommonFSUtils.isHDFS(c)) {
      return null;
    }
    // getHedgedReadMetrics is package private. Get the DFSClient instance that is internal
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      LOG.warn(
        "Failed find method " + name + " in dfsclient; no hedged read metrics: " + e.getMessage());
      return null;
    } catch (SecurityException e) {
      LOG.warn(
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      LOG.warn(
        "Failed find method " + name + " in dfsclient; no hedged read metrics: " + e.getMessage());
      return null;
    }
    m.setAccessible(true);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      LOG.warn("Failed invoking method " + name + " on dfsclient; no hedged read metrics: "
        + e.getMessage());
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
  public static List<FileStatus> filterFileStatuses(Iterator<FileStatus> input,
    FileStatusFilter filter) {
    if (input == null) return null;
    ArrayList<FileStatus> results = new ArrayList<>();
    while (input.hasNext()) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      status = fs.listStatus(versionFile);
    } catch (FileNotFoundException fnfe) {
      return null;
    }
    if (ArrayUtils.getLength(status) == 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    }
    if (ArrayUtils.getLength(status) == 0) {
      return null;
    }
    String version = null;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    } catch (FileNotFoundException fnfe) {
      LOG.trace("{} does not exist", dir);
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java

    if (ArrayUtils.getLength(status) == 0) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      List<FileStatus> status2 = filterFileStatuses(status, filter);
      if (status2 == null || status2.isEmpty()) {
        return null;
      } else {
        return status2;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
   */
  public static List<FileStatus> filterFileStatuses(FileStatus[] input, FileStatusFilter filter) {
    if (input == null) return null;
    return filterFileStatuses(Iterators.forArray(input), filter);
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    Procedure<?> procedure = procedureExecutor.getProcedure(procId);
    if (procedure == null) {
      return null;
    }
    assert procedure instanceof RemoteProcedure;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      return map;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      return NamedQueueRecorder.getInstance(conf);
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      return unknownServerNames;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    List<ReplicationPeerDescription> peerList = this.getReplicationPeerManager().listPeers(null);
    if (peerList == null) {
      return null;
    }
    HashMap<String, List<Pair<ServerName, ReplicationLoadSource>>> replicationLoadSourceMap =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java`
#### Snippet
```java
        LOG.error("{} is not of type MasterCoprocessor. Check the configuration of {}",
          implClass.getName(), CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY);
        return null;
      }
    } catch (NoSuchMethodException | InvocationTargetException e) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.java`
#### Snippet
```java
          .getProcedure(SnapshotProcedure.class, getParentProcId());
        if (parent != null && parent.isSnapshotCorrupted()) {
          return null;
        }
      }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java`
#### Snippet
```java
  RegionInfo getFirstRegionInfo() {
    if (regions == null || regions.isEmpty()) {
      return null;
    }
    return regions.get(0);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    final Map<TableName, SnapshotSentinel> sentinels, final SnapshotDescription snapshot) {
    if (!snapshot.hasTable()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    SnapshotSentinel h = sentinels.get(snapshotTable);
    if (h == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    if (!h.getSnapshot().getName().equals(snapshot.getName())) {
      // specified snapshot is to the one currently running
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
    HFileDeleteTask task = new HFileDeleteTask(file, cleanerThreadTimeoutMsec);
    boolean enqueued = dispatch(task);
    return enqueued ? task : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/FairQueue.java`
#### Snippet
```java
  public Queue<T> poll() {
    if (queueHead == null) {
      return null;
    }
    Queue<T> q = queueHead;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/FairQueue.java`
#### Snippet
```java
      q = AvlIterableList.readNext(q);
    } while (q != queueHead);
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
        break;
    }
    return queue != null ? createLockedResource(resourceType, resourceName, queue) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotRegionProcedure.java`
#### Snippet
```java
    if (dispatched) {
      if (succ) {
        return null;
      }
      dispatched = false;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.java`
#### Snippet
```java
  @Override
  protected Procedure[] execute(final MasterProcedureEnv env) {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.java`
#### Snippet
```java
  public TableName getTableName() {
    RegionInfo hri = getRegionInfo();
    return hri != null ? hri.getTable() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase.java`
#### Snippet
```java
              + "the parent procedure will take care of this.", this, region, targetServer, e);
            unattach(env);
            return null;
          }
          event.suspend();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase.java`
#### Snippet
```java
          env.getAssignmentManager().persistToMeta(regionNode);
          unattach(env);
          return null;
        case REGION_REMOTE_PROCEDURE_DISPATCH_FAIL:
          // the remote call is failed so we do not need to change the region state, just return.
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase.java`
#### Snippet
```java
          // the remote call is failed so we do not need to change the region state, just return.
          unattach(env);
          return null;
        case REGION_REMOTE_PROCEDURE_SERVER_CRASH:
          env.getAssignmentManager().regionClosedAbnormally(regionNode);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase.java`
#### Snippet
```java
          env.getAssignmentManager().regionClosedAbnormally(regionNode);
          unattach(env);
          return null;
        default:
          throw new IllegalStateException("Unknown state: " + state);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
      if (!namespaceLock.trySharedLock(procedure)) {
        waitProcedure(namespaceLock, procedure);
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
        namespaceLock.releaseSharedLock();
        waitProcedure(tableLock, procedure);
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
    Queue<T> rq = fairq.poll();
    if (rq == null || !rq.isAvailable()) {
      return null;
    }
    // loop until we find out a procedure which is ready to run, or if we have checked all the
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
    // no procedure is ready for execution, remove from run queue
    removeFromRunQueue(fairq, rq, () -> "no procedure can be executed");
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
            .isInTransition()
        ) {
          return null;
        }
      }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
            if (ignoreIfInTransition) {
              if (regionNode.isInTransition()) {
                return null;
              }
            } else {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    final RegionStateNode node = encodedRegionsMap.get(encodedRegionName);
    if (node == null) {
      return null;
    }
    return node.toRegionState();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    // merged or split, no need to check it
    if (node == null) {
      return null;
    }
    node.lock();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
          if (node.getOpenSeqNum() > oldLoc.getSeqNum()) {
            // normal case, the region has been reopened
            return null;
          } else {
            // the open seq num does not change, need to reopen again
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
          // the state has been changed so we can make sure that the region has been reopened(not
          // finished maybe, but not a problem).
          return null;
        }
      } else {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
          // not in OPEN or OPENING state, then we can make sure that the region has been
          // reopened(not finished maybe, but not a problem)
          return null;
        } else {
          if (!node.getRegionLocation().equals(oldLoc.getServerName())) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
          if (!node.getRegionLocation().equals(oldLoc.getServerName())) {
            // the region has been moved, so we can make sure that the region has been reopened.
            return null;
          }
          // normal case, we are still in OPENING state, or the reopen has been opened and the state
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    RegionStateNode node = regionInTransition.get(hri);
    if (node == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    node.lock();
    try {
      return node.isInTransition() ? node.toRegionState() : null;
    } finally {
      node.unlock();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    try {
      if (!include(node, false)) {
        return null;
      }
      if (node.isInState(State.OPEN)) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
        return new HRegionLocation(node.getRegionInfo(), node.getRegionLocation(), -1);
      } else {
        return null;
      }
    } finally {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  public RegionState getRegionState(final RegionInfo regionInfo) {
    RegionStateNode regionStateNode = getRegionStateNode(regionInfo);
    return regionStateNode == null ? null : regionStateNode.toRegionState();
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java`
#### Snippet
```java
  RegionInfo getFirstRegionInfo() {
    if (newRegions == null || newRegions.isEmpty()) {
      return null;
    }
    return newRegions.get(0);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java`
#### Snippet
```java
      r.getColumnLatestCell(HConstants.CATALOG_FAMILY, getStateColumn(regionInfo.getReplicaId()));
    if (cell == null || cell.getValueLength() == 0) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java`
#### Snippet
```java
          + "Consider using HBCK2 setRegionState ENCODED_REGION_NAME STATE",
        state, regionInfo.getEncodedName());
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ModifyPeerProcedure.java`
#### Snippet
```java

  protected ReplicationPeerConfig getOldPeerConfig() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ClaimReplicationQueuesProcedure.java`
#### Snippet
```java
        storage.removeReplicatorIfQueueIsEmpty(crashedServer);
        // we are done
        return null;
      }
      LOG.debug("There are {} replication queues need to be claimed for {}", queues.size(),
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/NamespaceQuotaSnapshotStore.java`
#### Snippet
```java
      return quotas.getSpace();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
      return map;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/GlobalQuotaSettingsImpl.java`
#### Snippet
```java

    if (throttleBuilder == null && removeSpaceBuilder && bypassGlobals == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/NoOpRegionSizeStore.java`
#### Snippet
```java
  @Override
  public RegionSize remove(RegionInfo regionInfo) {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/NoOpRegionSizeStore.java`
#### Snippet
```java
  @Override
  public RegionSize getRegionSize(RegionInfo regionInfo) {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/NoOpRegionSizeStore.java`
#### Snippet
```java
  @Override
  public Iterator<Entry<RegionInfo, RegionSize>> iterator() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TableQuotaSnapshotStore.java`
#### Snippet
```java
      return quotas.getSpace();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java`
#### Snippet
```java
        return ProtobufUtil.toGroupInfo(resp.getRSGroupInfo());
      }
      return null;
    } catch (ServiceException e) {
      throw ProtobufUtil.handleRemoteException(e);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
    } catch (InterruptedException e) {
      Thread.currentThread().interrupt();
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java`
#### Snippet
```java
        return ProtobufUtil.toGroupInfo(resp.getRSGroupInfo());
      }
      return null;
    } catch (ServiceException e) {
      throw ProtobufUtil.handleRemoteException(e);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java`
#### Snippet
```java
        return ProtobufUtil.toGroupInfo(resp.getRSGroupInfo());
      }
      return null;
    } catch (ServiceException e) {
      throw ProtobufUtil.handleRemoteException(e);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/DisabledRSGroupInfoManager.java`
#### Snippet
```java
  @Override
  public RSGroupInfo getRSGroupForTable(TableName tableName) throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/DisabledRSGroupInfoManager.java`
#### Snippet
```java
      return new RSGroupInfo(RSGroupInfo.DEFAULT_GROUP, getOnlineServers());
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/DisabledRSGroupInfoManager.java`
#### Snippet
```java
      return new RSGroupInfo(RSGroupInfo.DEFAULT_GROUP, onlineServers);
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
  private Map<byte[], ? extends Collection<byte[]>> makeFamilyMap(byte[] family, byte[] qualifier) {
    if (family == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    byte[] family = CellUtil.cloneFamily(kv);
    if (!Bytes.equals(family, ACL_LIST_FAMILY)) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
      // Filter the permissions cell record if client query
      if (filterPerms && !validateFilterUser(username, filterUser, filterUserGroups)) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
      // Filter the permissions cell record if client query
      if (filterPerms && !validateFilterUser(username, filterUser, filterUserGroups)) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
      // Validate the filterUser when specified
      if (filterUser != null && !validateFilterUser(username, filterUser, filterUserGroups)) {
        return null;
      }
      if (!validateCFAndCQ(permFamily, cf, permQualifier, cq)) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
      }
      if (!validateCFAndCQ(permFamily, cf, permQualifier, cq)) {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    // Save an object allocation where we can
    if (cell.getTagsLength() == 0) {
      return null;
    }
    List<Permission> results = Lists.newArrayList();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
      }
    }
    return null; // No task launched so there will be nothing to cancel later
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
        .isNotFamilyOrQualifierPermission((TablePermission) userPermission.getPermission()))
      .collect(Collectors.toList());
    return permissions.size() > 0 ? permissions.get(0) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
    List<UserPermission> permissions = PermissionStorage.getUserPermissions(conf,
      PermissionStorage.ACL_GLOBAL_NAME, null, null, userName, true);
    return permissions.size() > 0 ? permissions.get(0) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
    List<UserPermission> permissions =
      PermissionStorage.getUserNamespacePermissions(conf, namespace, userName, true);
    return permissions.size() > 0 ? permissions.get(0) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    final RegionStateNode regionState =
      regionStates.getRegionStateNodeFromEncodedRegionName(encodedRegionName);
    return regionState != null ? regionState.getRegionInfo() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    try {
      if (regionNode.isInState(State.OFFLINE, State.CLOSED, State.SPLIT)) {
        return null;
      }
      // in general, a split parent should be in CLOSED or SPLIT state, but anyway, let's check it
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
      if (regionNode.getRegionInfo().isSplit()) {
        LOG.warn("{} is a split parent but not in CLOSED or SPLIT state", regionNode);
        return null;
      }
      // As in DisableTableProcedure or ModifyTableProcedure, we will hold the xlock for table, so
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  public RegionInfo getRegionInfo(final byte[] regionName) {
    final RegionStateNode regionState = regionStates.getRegionStateNodeFromName(regionName);
    return regionState != null ? regionState.getRegionInfo() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java

      if (!isRunning()) {
        return null;
      }
      assignQueueFullCond.await(assignDispatchWaitMillis, TimeUnit.MILLISECONDS);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
          try {
            if (rsn.getProcedure() != null) {
              return null;
            } else {
              return rsn.setProcedure(TransitRegionStateProcedure.assign(getProcedureEnvironment(),
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
      LOG.debug("Skip region plan {}, source server not match, current region location is {}",
        regionPlan, current == null ? "(null)" : current);
      return null;
    }
    return moveAsync(regionPlan);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefinedSetFilterScanLabelGenerator.java`
#### Snippet
```java
      return dropLabelsNotInUserAuths(labels, new ArrayList<>(auths), userName);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/GssSaslServerAuthenticationProvider.java`
#### Snippet
```java
            Sasl.createSaslServer(getSaslAuthMethod().getSaslMechanism(), names[0], names[1],
              saslProps, new SaslGssCallbackHandler()),
            () -> null);
        }
      });
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/SimpleScanLabelGenerator.java`
#### Snippet
```java
      return authorizations.getLabels();
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
        return Operator.NOT;
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    String getRSGroup(String namespace, String tablename) {
      if (script == null || script.isEmpty()) {
        return null;
      }
      Shell.ShellCommandExecutor rsgroupMappingScript =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.java`
#### Snippet
```java
    });

    if (manifestFiles == null || manifestFiles.length == 0) return null;

    final ExecutorCompletionService<SnapshotRegionManifest> completionService =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java`
#### Snippet
```java
    if (regions == null) {
      LOG.debug("No regions under directory:" + snapshotDir);
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
  private List<StoreFileInfo> getStoreFiles(Path storeDir) throws IOException {
    FileStatus[] stats = CommonFSUtils.listStatus(rootFs, storeDir);
    if (stats == null) return null;

    ArrayList<StoreFileInfo> storeFiles = new ArrayList<>(stats.length);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
   */
  public Map<String, SnapshotRegionManifest> getRegionManifestsMap() {
    if (regionManifests == null || regionManifests.isEmpty()) return null;

    HashMap<String, SnapshotRegionManifest> regionsMap = new HashMap<>(regionManifests.size());
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java
      return SnapshotDataManifest.parseFrom(cin);
    } catch (FileNotFoundException e) {
      return null;
    } catch (InvalidProtocolBufferException e) {
      throw new CorruptedSnapshotException("unable to parse data manifest " + e.getMessage(), e);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
      return Bytes.toBytes(visibilityString.toString());
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
      return createModifiedVisExpression(tags);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java`
#### Snippet
```java
    } catch (IOException e) {
      LOG.error("Error while fetching namespace descriptor for namespace : " + namespaceAsString);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceAuditor.java`
#### Snippet
```java
      return stateManager.getState(namespace);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    final Map<String, SnapshotRegionManifest> regionManifests, final List<RegionInfo> regions)
    throws IOException {
    if (regions == null || regions.isEmpty()) return null;

    final Map<String, RegionInfo> snapshotRegions = new HashMap<>(regions.size());
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
      CommonFSUtils.listStatus(fs, tableDir, new FSUtils.RegionDirFilter(fs));
    if (regionDirs == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    if (regionManifests == null) {
      LOG.warn("Nothing to restore. Snapshot " + snapshotDesc + " looks empty");
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManager.java`
#### Snippet
```java
   */
  public byte[] execProcedureWithRet(ProcedureDescription desc) throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
        if (isMissing()) return "NOT FOUND";
        if (inArchive()) return "archive";
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java`
#### Snippet
```java
    if (!this.submitProcedure(proc)) {
      LOG.error("Failed to submit procedure '" + procName + "'");
      return null;
    }
    return proc;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java`
#### Snippet
```java
    String value = builder.getValue(key);

    return value == null ? null : new Pair<>(key, value);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java`
#### Snippet
```java
    String value = desc.getValue(key);

    return value == null ? null : new Pair<>(key, value);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerQueueService.java`
#### Snippet
```java
  @Override
  public NamedQueueGetResponse getNamedQueueRecords(NamedQueueGetRequest request) {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/BalancerDecisionQueueService.java`
#### Snippet
```java
  public NamedQueueGetResponse getNamedQueueRecords(NamedQueueGetRequest request) {
    if (!isBalancerDecisionRecording) {
      return null;
    }
    List<RecentLogs.BalancerDecision> balancerDecisions =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/BalancerRejectionQueueService.java`
#### Snippet
```java
  public NamedQueueGetResponse getNamedQueueRecords(NamedQueueGetRequest request) {
    if (!isBalancerRejectionRecording) {
      return null;
    }
    List<RecentLogs.BalancerRejection> balancerRejections =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.java`
#### Snippet
```java
      LOG.error("slowLog and largeLog both are false. Ignoring the event. rpcCallDetails: {}",
        rpcCallDetails);
      return null;
    }
    if (isSlowLog && isLargeLog) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.java`
#### Snippet
```java
  public NamedQueueGetResponse getNamedQueueRecords(NamedQueueGetRequest request) {
    if (!isOnlineLogProviderEnabled) {
      return null;
    }
    final AdminProtos.SlowLogResponseRequest slowLogResponseRequest =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
      final String tableRowKey = Bytes.toString(op.getRow());
      if (StringUtils.isEmpty(tableRowKey)) {
        return null;
      }
      final String[] splits = tableRowKey.split(",");
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
      }
      final String[] splits = tableRowKey.split(",");
      return splits.length > 0 ? splits[0] : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
      final String tableRowKey = Bytes.toString(op.getRow());
      if (StringUtils.isEmpty(tableRowKey)) {
        return null;
      }
      final String[] splits = tableRowKey.split(",");
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
      }
      final String[] splits = tableRowKey.split(",");
      return splits.length > 2 ? splits[2] : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ClusterMarkingEntryFilter.java`
#### Snippet
```java
      }
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BaseReplicationEndpoint.java`
#### Snippet
```java
      }
    }
    return filters.isEmpty() ? null : new ChainWALEntryFilter(filters);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
    for (WALEntryFilter filter : filters) {
      if (entry == null) {
        return null;
      }
      entry = filter.filter(entry);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEntryFilter.java`
#### Snippet
```java
    entry = filterEntry(entry);
    if (entry == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ScopeWALEntryFilter.java`
#### Snippet
```java
    NavigableMap<byte[], Integer> scopes = entry.getKey().getReplicationScopes();
    if (scopes == null || scopes.isEmpty()) {
      return null;
    }
    byte[] family = CellUtil.cloneFamily(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ScopeWALEntryFilter.java`
#### Snippet
```java
      });
    }
    return hasGlobalScope(scopes, family) ? cell : null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/NamespaceTableCfWALEntryFilter.java`
#### Snippet
```java
      return entry;
    } else {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/NamespaceTableCfWALEntryFilter.java`
#### Snippet
```java
      return bulkLoadFilter.filterCell(cell, fam -> !peerConfig.needToReplicate(tableName, fam));
    } else {
      return peerConfig.needToReplicate(tableName, CellUtil.cloneFamily(cell)) ? cell : null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/SystemTableWALEntryFilter.java`
#### Snippet
```java
  @Override
  public Entry filter(Entry entry) {
    return entry.getKey().getTableName().isSystemTable() ? null : entry;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/VerifyWALEntriesReplicationEndpoint.java`
#### Snippet
```java
  @Override
  public WALEntryFilter getWALEntryfilter() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java`
#### Snippet
```java
      return cell;
    } else if (copiedStoresList.isEmpty()) {
      return null;
    }
    BulkLoadDescriptor.Builder newDesc = BulkLoadDescriptor.newBuilder()
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEmptyEntryFilter.java`
#### Snippet
```java
    entry = super.filter(entry);
    if (filterEmptyEntry && entry != null && entry.getEdit().isEmpty()) {
      return null;
    }
    return entry;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationSinkServiceImpl.java`
#### Snippet
```java
  public ReplicationLoad refreshAndGetReplicationLoad() {
    if (replicationLoad == null) {
      return null;
    }
    // always build for latest data
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java`
#### Snippet
```java
      if (length <= 0) {
        LOG.warn("File is empty. Could not open {} for reading because {}", path, e);
        return null;
      }
      throw e;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java`
#### Snippet
```java
  private boolean filter(Entry entry) {
    WALEdit edit = entry.getEdit();
    WALUtil.filterCells(edit, c -> CellUtil.matchingFamily(c, WALEdit.METAFAMILY) ? null : c);
    return !edit.isEmpty();
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java`
#### Snippet
```java
   */
  public Entry peek() throws IOException {
    return hasNext() ? currentEntry : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
        return WALEntryBatch.endOfFile(currentPath);
      } else {
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java`
#### Snippet
```java
   */
  public String sinkToString() {
    if (this.replicationLoadSink == null) return null;

    StringBuilder sb = new StringBuilder();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
            } else {
              missingCFs.add(cf);
              return null;
            }
          });
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
    @Override
    public FileSystem getFileSystem() {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
    @Override
    public AsyncClusterConnection getAsyncClusterConnection() {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
    @Override
    public Connection getConnection() {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
    @Override
    public ChoreService getChoreService() {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
    @Override
    public Connection createConnection(Configuration conf) throws IOException {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
    @Override
    public CoordinatedStateManager getCoordinatedStateManager() {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
    // ignore the edits.
    if (!replicationSinkTrackerEnabled) {
      return null;
    }
    WALProtos.ReplicationMarkerDescriptor descriptor =
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java`
#### Snippet
```java
  public ReplicationLoad refreshAndGetReplicationLoad() {
    if (this.replicationLoad == null) {
      return null;
    }
    // always build for latest data
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      if (impl == null) {
        LOG.error("Cannot load coprocessor " + implClass.getSimpleName());
        return null;
      }
    } catch (InstantiationException | IllegalAccessException e) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    boolean bypass = execOperation(observerOperation);
    R result = observerOperation.getResult();
    return bypass == observerOperation.isBypassable() ? result : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      // If already loaded will just continue
      LOG.warn("Attempted duplicate loading of {}; skipped", className);
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
    final Reference reference, final FileStatus status) throws IOException {
    if (status == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.java`
#### Snippet
```java
    MetricsTableValues metricsTable = metricsTableMap.get(TableName.valueOf(table));
    if (metricsTable == null) {
      return null;
    } else {
      return metricsTable.perStoreMemstoreOnlyReadCount;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.java`
#### Snippet
```java
    MetricsTableValues metricsTable = metricsTableMap.get(TableName.valueOf(table));
    if (metricsTable == null) {
      return null;
    } else {
      return metricsTable.perStoreMixedReadCount;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NonLazyKeyValueScanner.java`
#### Snippet
```java
  public Path getFilePath() {
    // Not a file by default.
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NonLazyKeyValueScanner.java`
#### Snippet
```java
  @Override
  public Cell getNextIndexedKey() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScanner.java`
#### Snippet
```java
   */
  default String getOperationId() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreMergerSegmentsIterator.java`
#### Snippet
```java
      throw new IllegalStateException(ie);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  @Override
  public Cell getNextIndexedKey() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  public Cell peek() { // sanity check, the current should be always valid
    if (closed) {
      return null;
    }
    if (current != null && current.getSequenceId() > readPoint) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  private Cell getHighest(Cell first, Cell second) {
    if (first == null && second == null) {
      return null;
    }
    if (first != null && second != null) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  @Override
  public Path getFilePath() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  public Cell next() throws IOException {
    if (closed) {
      return null;
    }
    Cell oldCurrent = current;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
  public Cell next() throws IOException {
    if (this.current == null) {
      return null;
    }
    Cell kvReturn = this.current.next();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
      return regionInfo.getTable();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    }

    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    }

    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
  private Map<byte[], ? extends Collection<byte[]>> makeFamilyMap(byte[] family, byte[] qualifier) {
    if (family == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
      return getTableName(region);
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  public Region getRegion() {
    return regionEnv != null ? regionEnv.getRegion() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  default Result preIncrementAfterRowLock(ObserverContext<RegionCoprocessorEnvironment> c,
    Increment increment) throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  default Result preAppend(ObserverContext<RegionCoprocessorEnvironment> c, Append append)
    throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  default Result preIncrement(ObserverContext<RegionCoprocessorEnvironment> c, Increment increment)
    throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  default Result preAppendAfterRowLock(ObserverContext<RegionCoprocessorEnvironment> c,
    Append append) throws IOException {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
      + " or it is corrupt");
    if (readEmptyValueOnMobCellMiss) {
      return null;
    } else if (
      (ioe instanceof FileNotFoundException) || (ioe.getCause() instanceof FileNotFoundException)
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  public Mutation[] getOperationsFromCoprocessors(int index) {
    return operationsFromCoprocessors == null
      ? null
      : operationsFromCoprocessors[getAbsoluteIndex(index)];
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
        LOG.trace("No StoreFiles for: " + familyDir);
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
        LOG.trace("No StoreFiles for: " + familyDir);
      }
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
    FileStatus[] fds =
      CommonFSUtils.listStatus(fs, getRegionDir(), new FSUtils.FamilyDirFilter(fs));
    if (fds == null) return null;

    ArrayList<String> families = new ArrayList<>(fds.length);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
          // If lastKey is null means storefile is empty.
          if (!lastKey.isPresent()) {
            return null;
          }
          if (f.getComparator().compare(splitKey, lastKey.get()) > 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
          }
          if (f.getComparator().compare(splitKey, lastKey.get()) > 0) {
            return null;
          }
          if (firstKey.isPresent() && f.getComparator().compare(splitKey, firstKey.get()) <= 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
          // If firstKey is null means storefile is empty.
          if (!firstKey.isPresent()) {
            return null;
          }
          if (f.getComparator().compare(splitKey, firstKey.get()) < 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
          }
          if (f.getComparator().compare(splitKey, firstKey.get()) < 0) {
            return null;
          }
          if (lastKey.isPresent() && f.getComparator().compare(splitKey, lastKey.get()) >= 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserAggregateFactory.java`
#### Snippet
```java
        @Override
        public MetricsUserAggregateSource getSource() {
          return null;
        }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentFactory.java`
#### Snippet
```java
    List<MemStoreLAB> mslabs = new ArrayList<>();
    if (!conf.getBoolean(MemStoreLAB.USEMSLAB_KEY, MemStoreLAB.USEMSLAB_DEFAULT)) {
      return null;
    }
    for (ImmutableSegment segment : segments) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
  public Cell peek() {
    if (this.current == null) {
      return null;
    }
    return this.current.peek();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
  public Cell next() throws IOException {
    if (this.current == null) {
      return null;
    }
    Cell kvReturn = this.current.next();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
  public Cell getNextIndexedKey() {
    // here we return the next index key from the top scanner
    return current == null ? null : current.getNextIndexedKey();
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
    KeyValueScanner kvScanner = heap.poll();
    if (kvScanner == null) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
      }
      if (filesToReopen.isEmpty()) {
        return null;
      }
      return getScanners(filesToReopen, cacheBlocks, false, false, matcher, startRow,
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    // StoreFileWriterCreationTracker.
    this.storeFileWriterCreationTrackerFactory = storeEngine.requireWritingToTmpDirFirst()
      ? () -> null
      : () -> new StoreFileWriterCreationTracker();
    refreshStoreSizeAndTotalBytes();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java`
#### Snippet
```java
        LOG.error("{} is not of type RegionServerCoprocessor. Check the configuration of {}",
          implClass.getName(), CoprocessorHost.REGIONSERVER_COPROCESSOR_CONF_KEY);
        return null;
      }
    } catch (NoSuchMethodException | InvocationTargetException e) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
            LOG.error("Failed to complete bulk load", e);
          }
          return null;
        }
      });
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
      case JUMBO_CHUNK: // a jumbo chunk doesn't have a fixed size
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
    // don't cause fragmentation as badly.
    if (size > maxAlloc) {
      return null;
    }
    Chunk c = null;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
    // Callers should satisfy large allocations from JVM heap so limit fragmentation.
    if (size > maxAlloc) {
      return null;
    }
    Chunk c = null;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
    RegionScanner scanner = getScanner(scannerId);
    if (scanner == null) {
      return null;
    }
    StringBuilder builder = new StringBuilder();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
  RegionScanner getScanner(long scannerId) {
    RegionScannerHolder rsh = getRegionScannerHolder(scannerId);
    return rsh == null ? null : rsh.s;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
    try {
      if (!request.hasRegion()) {
        return null;
      }
      Region region = getRegion(request.getRegion());
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      return builder.toString();
    } catch (IOException ignored) {
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellArrayMap.java`
#### Snippet
```java
  @Override
  protected Cell getCell(int i) {
    if ((i < minCellIdx) || (i >= maxCellIdx)) return null;
    return block[i];
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java`
#### Snippet
```java
    List<? extends Segment> localCopy = getSegments();
    if (localCopy.isEmpty()) {
      return null;
    }
    return localCopy.get(localCopy.size() - 1);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell get(Object o) {
    int index = find((Cell) o);
    return (index >= 0) ? getCell(index) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell ceilingKey(Cell k) {
    if (isEmpty()) {
      return null;
    }
    int index = find(k);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    int index = find(k);
    index = (index >= 0) ? index : -(index) + 1;
    return (index < minCellIdx || index >= maxCellIdx) ? null : getCell(index);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    Cell cell = lastKey();
    if (cell == null) {
      return null;
    }
    return new CellFlatMapEntry(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell lastKey() {
    if (isEmpty()) {
      return null;
    }
    return descending ? getCell(minCellIdx) : getCell(maxCellIdx - 1);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    Cell cell = lowerKey(k);
    if (cell == null) {
      return null;
    }
    return new CellFlatMapEntry(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell lowerKey(Cell k) {
    if (isEmpty()) {
      return null;
    }
    int index = find(k);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    // If index>=0 there's a key exactly equal
    index = (index >= 0) ? index - 1 : -(index);
    return (index < minCellIdx || index >= maxCellIdx) ? null : getCell(index);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell firstKey() {
    if (isEmpty()) {
      return null;
    }
    return descending ? getCell(maxCellIdx - 1) : getCell(minCellIdx);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    Cell cell = higherKey(k);
    if (cell == null) {
      return null;
    }
    return new CellFlatMapEntry(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell higherKey(Cell k) {
    if (isEmpty()) {
      return null;
    }
    int index = find(k);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    int index = find(k);
    index = (index >= 0) ? index + 1 : -(index) + 1;
    return (index < minCellIdx || index >= maxCellIdx) ? null : getCell(index);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    Cell cell = ceilingKey(k);
    if (cell == null) {
      return null;
    }
    return new CellFlatMapEntry(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    Cell cell = floorKey(k);
    if (cell == null) {
      return null;
    }
    return new CellFlatMapEntry(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    Cell cell = firstKey();
    if (cell == null) {
      return null;
    }
    return new CellFlatMapEntry(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  public Cell floorKey(Cell k) {
    if (isEmpty()) {
      return null;
    }
    int index = find(k);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    int index = find(k);
    index = (index >= 0) ? index : -(index);
    return (index < minCellIdx || index >= maxCellIdx) ? null : getCell(index);
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.java`
#### Snippet
```java
      LOG.error(implClass.getName() + " is not of type WALCoprocessor. Check the "
        + "configuration " + CoprocessorHost.WAL_COPROCESSOR_CONF_KEY);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
  private Chunk createChunkForPool(ChunkType chunkType, int chunkSize) {
    if (chunkSize != dataChunksPool.getChunkSize() && chunkSize != indexChunksPool.getChunkSize()) {
      return null;
    }
    return createChunk(true, chunkType, chunkSize);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
    if (poolSizePercentage <= 0) {
      LOG.info("{} poolSizePercentage is less than 0. So not using pool", label);
      return null;
    }
    if (chunkPoolDisabled) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
    }
    if (chunkPoolDisabled) {
      return null;
    }
    if (poolSizePercentage > 1.0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ReaderBase.java`
#### Snippet
```java
      emptyCompressionContext = false;
    }
    return hasEntry ? e : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
      return listener;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
            // give up with no connection.
            LOG.debug("No master found and cluster is stopped; bailing out");
            return null;
          }
          if (EnvironmentEdgeManager.currentTime() > (previousLogTime + 1000)) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    RegionServerStatusService.BlockingInterface rss = rssStub;
    if (masterServerName == null || rss == null) {
      return null;
    }
    RegionServerStartupResponse result = null;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  public RegionLoad createRegionLoad(final String encodedRegionName) throws IOException {
    HRegion r = onlineRegions.get(encodedRegionName);
    return r != null ? createRegionLoad(r, null, null) : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java`
#### Snippet
```java
    try {
      if (doneTxid == NOT_DONE) {
        return null;
      }
      return this.throwable;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
  private <T extends Map<?, Long>> Map<byte[], Long> flattenToLowestSequenceId(Map<byte[], T> src) {
    if (src == null || src.isEmpty()) {
      return null;
    }
    Map<byte[], Long> tgt = new HashMap<>();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
    if ((ioEx != null) && (ioEx.getMessage() != null)) {
      if (ioEx.getMessage().contains("EOF")) return ioEx;
      return null;
    }
    return null;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
      return null;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
  OutputStream getOutputStream() {
    FSDataOutputStream fsdos = this.hdfs_out;
    return fsdos != null ? fsdos.getWrappedStream() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
      return heap.peek() == null ? NextState.NO_MORE_VALUES : NextState.MORE_VALUES;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
  @Override
  public Cell peek() {
    return heap != null ? heap.peek() : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    boolean defaultResult = false;
    if (coprocEnvironments.isEmpty()) {
      return null;
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Boolean>(
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    CheckAndMutateResult defaultResult = new CheckAndMutateResult(false, null);
    if (coprocEnvironments.isEmpty()) {
      return null;
    }
    return execOperationWithResult(
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    @Override
    public Connection createConnection(Configuration conf) throws IOException {
      return services != null ? this.services.createConnection(conf) : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    CheckAndMutateResult defaultResult = new CheckAndMutateResult(false, null);
    if (coprocEnvironments.isEmpty()) {
      return null;
    }
    return execOperationWithResult(
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
        LOG.error("{} is not of type RegionCoprocessor. Check the configuration of {}",
          implClass.getName(), CoprocessorHost.REGION_COPROCESSOR_CONF_KEY);
        return null;
      }
    } catch (NoSuchMethodException | InvocationTargetException e) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    @Override
    public ServerName getServerName() {
      return services != null ? services.getServerName() : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    public Connection getConnection() {
      // Mocks may have services as null at test time.
      return services != null ? new SharedConnection(services.getConnection()) : null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    final Reference r) throws IOException {
    if (coprocEnvironments.isEmpty()) {
      return null;
    }
    return execOperationWithResult(
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    boolean defaultResult = false;
    if (coprocEnvironments.isEmpty()) {
      return null;
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Boolean>(
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java`
#### Snippet
```java
      return result;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/CompactionScanQueryMatcher.java`
#### Snippet
```java
  public Filter getFilter() {
    // no filter when compaction
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/CompactionScanQueryMatcher.java`
#### Snippet
```java
  public Cell getNextKeyHint(Cell cell) throws IOException {
    // no filter, so no key hint.
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanWildcardColumnTracker.java`
#### Snippet
```java
  @Override
  public ColumnCount getColumnHint() {
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
  public Cell getNextKeyHint(Cell cell) throws IOException {
    if (filter == null) {
      return null;
    } else {
      return filter.getNextCellHint(cell);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/DropDeletesCompactionScanQueryMatcher.java`
#### Snippet
```java
        return columns.getNextRowOrNextColumn(cell);
      } else {
        return null;
      }
    } else {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
      // Return if nothing to flush.
      if (!force && this.writer != null && this.numEntries.get() <= 0) {
        return null;
      }
      Map<byte[], List<byte[]>> regionsToFlush = null;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher.java`
#### Snippet
```java
      return MatchCode.SKIP;
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher.java`
#### Snippet
```java
  protected final MatchCode checkDeleted(DeleteTracker deletes, Cell cell) {
    if (deletes.isEmpty() && !(deletes instanceof NewVersionBehaviorTracker)) {
      return null;
    }
    // MvccSensitiveTracker always need check all cells to save some infos.
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher.java`
#### Snippet
```java
        return MatchCode.SKIP;
      case NOT_DELETED:
        return null;
      default:
        throw new RuntimeException("Unexpected delete result: " + deleteResult);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFile.java`
#### Snippet
```java
      initializeTrackFiles(System.currentTimeMillis());
      nextTrackFile = 0;
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java`
#### Snippet
```java
    toStackTraceElementMessages(StackTraceElement[] trace) {
    // if there is no stack trace, ignore it and just return the message
    if (trace == null) return null;
    // build the stack trace for the message
    List<StackTraceElementMessage> pbTrace = new ArrayList<>(trace.length);
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java`
#### Snippet
```java

      if (tables == null) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
    long cfTtl = this.storeConfigInfo.getStoreFileTtl();
    if (cfTtl == Long.MAX_VALUE) {
      return null; // minversion might be set, cannot delete old files
    }
    long timestampCutoff = EnvironmentEdgeManager.currentTime() - cfTtl;
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
      bestLength = length;
    }
    if (bestLength == 0) return null;
    if (bestLength == 1) {
      // This is currently inefficient. If only one stripe expired, we will rewrite some
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
      // cannot simply be updated in an old file. When we either determine stripe dynamically
      // or move metadata to manifest, we can just drop the "expired stripes".
      if (bestStart == (stripes.size() - 1)) return null;
      ++bestLength;
    }
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
    if (!filesCompacting.isEmpty()) {
      LOG.debug("Not selecting compaction: " + filesCompacting.size() + " files compacting");
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
    if (stripeCount == 0) {
      if (!shouldCompactL0) {
        return null; // nothing to do.
      }
      return selectL0OnlyCompaction(si);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
    if (bqSelection == null) {
      LOG.debug("No good compaction is possible in any stripe");
      return null;
    }
    List<HStoreFile> filesToCompact = new ArrayList<>(bqSelection);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
        // We want to avoid the scenario where we compact a stripe w/L0 and then split it.
        // So, if we might split, don't compact the stripe with L0.
        return null;
      }
      Pair<Long, Integer> kvsAndCount = estimateTargetKvs(filesToCompact, config.getSplitCount());
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
    ArrayList<BackupInfo> sessions = systemTable.getBackupInfos(BackupState.RUNNING);
    if (sessions.size() == 0) {
      return null;
    }
    return sessions.get(0).getBackupId();
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

        if (value == null) {
          return null;
        }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

        if (tables == null) {
          return null;
        }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

      if (value == null) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
        if (recentSessions.isEmpty()) {
          LOG.warn("No ongoing sessions found.");
          return null;
        }
        // else show status for ongoing session
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java

      if (list == null) {
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollBackupSubprocedure.java`
#### Snippet
```java
  public byte[] insideBarrier() throws ForeignException {
    rolllog();
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
        } else {
          LOG.error("Skip log file (can't parse): " + p);
          return null;
        }
      }
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    } catch (Exception e) {
      LOG.error("Skip log file (can't parse): " + p, e);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java

    if (status == null || status.length < 1) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    getRSLogTimestampMins(Map<TableName, Map<String, Long>> rsLogTimestampMap) {
    if (rsLogTimestampMap == null || rsLogTimestampMap.isEmpty()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
  public static TableName[] parseTableNames(String tables) {
    if (tables == null) {
      return null;
    }
    return Splitter.on(BackupRestoreConstants.TABLENAME_DELIMITER_IN_COMMAND).splitToStream(tables)
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    // Skip master wals
    if (p.getName().endsWith(MasterRegionFactory.ARCHIVED_WAL_SUFFIX)) {
      return null;
    }
    try {
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    } catch (Exception e) {
      LOG.warn("Skip log file (can't parse): {}", p);
      return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
      return FSTableDescriptors.getTableDescriptorFromFs(fileSys, new Path(target));
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res = table.get(get);
      if (res.isEmpty()) {
        return null;
      }
      return resultToBackupInfo(res);
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res = table.get(get);
      if (res.isEmpty()) {
        return null;
      }
      Cell cell = res.listCells().get(0);
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      byte[] val = CellUtil.cloneValue(cell);
      if (val.length == 0) {
        return null;
      }
      return Splitter.on(',').splitToStream(new String(val, StandardCharsets.UTF_8))
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res = table.get(get);
      if (res.isEmpty()) {
        return null;
      }
      res.advance();
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res = table.get(get);
      if (res.isEmpty()) {
        return null;
      }
      Cell cell = res.listCells().get(0);
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      byte[] val = CellUtil.cloneValue(cell);
      if (val.length == 0) {
        return null;
      }
      return new String(val, StandardCharsets.UTF_8);
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      Result res = table.get(get);
      if (res.isEmpty()) {
        return null;
      }
      Cell cell = res.listCells().get(0);
```

### ReturnNull
Return of `null`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      byte[] val = CellUtil.cloneValue(cell);
      if (val.length == 0) {
        return null;
      }
      return Splitter.on(',').splitToStream(new String(val, StandardCharsets.UTF_8))
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java`
#### Snippet
```java

    if (famAndQf.length != 2) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java`
#### Snippet
```java
          System.exit(-1);
        }
        return null;
      }
    });
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
      LOG.error("Error fetching cluster ID: ", e);
    }
    return null;
  }
}
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java`
#### Snippet
```java
      } else {
        // scan finished
        return null;
      }
    }
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return TDurability.FSYNC_WAL;
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  public static byte[][] splitKeyFromThrift(List<ByteBuffer> in) {
    if (in == null || in.size() == 0) {
      return null;
    }
    byte[][] out = new byte[in.size()][];
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return CompareOperator.NO_OP;
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  public static Result resultFromThrift(TResult in) {
    if (in == null) {
      return null;
    }
    if (!in.isSetColumnValues() || in.getColumnValues().isEmpty()) {
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return ReadType.PREAD;
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  public static List<ByteBuffer> splitKeyFromHBase(byte[][] in) {
    if (in == null || in.length == 0) {
      return null;
    }
    List<ByteBuffer> out = new ArrayList<>(in.length);
```

### ReturnNull
Return of `null`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return TCompareOperator.NO_OP;
      default:
        return null;
    }
  }
```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCommonTestingUtility.java`
#### Snippet
```java
    if (this.dataTestDir != null) {
      LOG.warn("Data test dir already setup in " + dataTestDir.getAbsolutePath());
      return null;
    }
    Path testPath = getRandomDir();
```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
    int index = getServerWith(regionName);
    if (index < 0) {
      return null;
    }
    return getRegionServer(index).getServerName();
```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
  public ClusterMetrics getClusterMetrics() throws IOException {
    HMaster master = getMaster();
    return master == null ? null : master.getClusterMetrics();
  }

```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
        public Object run() {
          abortRegionServer(reason, cause);
          return null;
        }
      });
```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
          public Object run() {
            runRegionServer();
            return null;
          }
        });
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    @Override
    public Mutation[] getMutationsForCoprocs() {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  public ClientProtos.RegionLoadStats getLoadStatistics() {
    if (!regionStatsEnabled) {
      return null;
    }
    ClientProtos.RegionLoadStats.Builder stats = ClientProtos.RegionLoadStats.newBuilder();
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        LOG.warn("There was a recoverable bulk load failure likely due to a split. These (family,"
          + " HFile) pairs were not loaded: {}, in region {}", list.toString(), this);
        return null;
      }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      closeBulkRegionOperation();
    }
    return isSuccessful ? storeFiles : null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            + " because its sequence id is smaller than this regions lastReplayedOpenRegionSeqId "
            + " of " + lastReplayedOpenRegionSeqId);
          return null;
        }
        if (numMutationsWithoutWAL.sum() > 0) {
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

    if (results.isEmpty()) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    if (isClosed()) {
      LOG.warn("Region " + this + " already closed");
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        status.abort("Already got closed by another process");
        // SplitTransaction handles the null
        return null;
      }
      LOG.debug("Updates disabled for region " + this);
```

### ReturnNull
Return of `null`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          return new RowLockImpl(this, l);
        } else {
          return null;
        }
      }
```

### ReturnNull
Return of `null`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java
    if (!ExportUtils.isValidArguements(args)) {
      ExportUtils.usage("Wrong number of arguments: " + ArrayUtils.getLength(otherArgs));
      return null;
    }
    Triple<TableName, Scan, Path> arguments =
```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    List<RegionInfo> regions = getRegions(tableName);
    if (regions == null || regions.isEmpty()) {
      return null;
    }
    LOG.debug("Found " + regions.size() + " regions for table " + tableName);
```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      retrier.sleepUntilNextRetry();
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    Path testPath = super.setupDataTestDir();
    if (null == testPath) {
      return null;
    }

```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      attempts++;
    } while (maxAttempts == -1 || attempts < maxAttempts);
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      scanner.next(cells);
      if (r.getRegionInfo().isMetaRegion() && !isTargetTable(row, cells.get(0))) {
        return null;
      }
      return Result.create(cells);
```

### ReturnNull
Return of `null`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
            return ci.getCellValueFromProto(q);
          }
          return null;
        }
      }, aMaxCallBack);
```

### ReturnNull
Return of `null`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
      }
    }
    return null;
  }

```

### ReturnNull
Return of `null`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
            return ci.getCellValueFromProto(q);
          }
          return null;
        }
      }, minCallBack);
```

### ReturnNull
Return of `null`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
          }
          if (response.getFirstPartCount() == 0) {
            return null;
          }
          ByteString b = response.getFirstPart(0);
```

## RuleId[id=AssignmentToLambdaParameter]
### AssignmentToLambdaParameter
Assignment to lambda parameter `error`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
        RegionLocateType.CURRENT, locateTimeoutNs).whenComplete((loc, error) -> {
          if (error != null) {
            error = unwrapCompletionException(translateException(error));
            if (error instanceof DoNotRetryIOException) {
              failOne(action, tries, error, EnvironmentEdgeManager.currentTime(), "");
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `e`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractRpcBasedConnectionRegistry.java`
#### Snippet
```java
        }
        if (e == null && !isValidResp.test(r)) {
          e = badResponse(debug);
        }
        if (e != null) {
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `proc`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
        }
        stack.push(proc);
        proc = procedures.get(proc.getParentProcId());
      }
    });
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `count`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      perServerFailuresCount.compute(serverName, (server, count) -> {
        if (count == null) {
          count = new LongAdder();
        }
        count.increment();
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `count`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      perTableFailuresCount.compute(region.getTable().getNameAsString(), (tableName, count) -> {
        if (count == null) {
          count = new LongAdder();
        }
        count.increment();
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `value`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
        value.decrement();
      } else {
        value = null;
      }
      return value;
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `value`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    ugiReferenceCounter.compute(ugi, (key, value) -> {
      if (value == null) {
        value = new MutableInt(1);
      } else {
        value.increment();
```

### AssignmentToLambdaParameter
Assignment to lambda parameter `connection`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
          try {
            User user = UserProvider.instantiate(conf).getCurrent();
            connection = getAsyncConnection(user);
          } catch (IOException ioe) {
            throw new UncheckedIOException("Failed to create connection", ioe);
```

## RuleId[id=UnnecessaryLocalVariable]
### UnnecessaryLocalVariable
Local variable `buffer` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    byte[] bytes = new byte[keyLength(cell)];
    appendKeyTo(cell, bytes, 0);
    ByteBuffer buffer = ByteBuffer.wrap(bytes);
    return buffer;
  }
```

### UnnecessaryLocalVariable
Local variable `blockType` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java`
#### Snippet
```java
    byte[] magicBuf = new byte[Math.min(buf.limit() - buf.position(), MAGIC_LENGTH)];
    buf.get(magicBuf);
    BlockType blockType = parse(magicBuf, 0, magicBuf.length);
    // If we got here, we have read exactly MAGIC_LENGTH bytes.
    return blockType;
```

### UnnecessaryLocalVariable
Local variable `provider` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java`
#### Snippet
```java
      conf.get(HConstants.CRYPTO_CIPHERPROVIDER_CONF_KEY, DefaultCipherProvider.class.getName());
    try {
      CipherProvider provider = (CipherProvider) ReflectionUtils.newInstance(
        getClassLoaderForClass(CipherProvider.class).loadClass(providerClassName), conf);
      return provider;
```

### UnnecessaryLocalVariable
Local variable `buffer` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java

  public static ByteBuffer getValueBufferShallowCopy(Cell cell) {
    ByteBuffer buffer =
      ByteBuffer.wrap(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());
    return buffer;
```

### UnnecessaryLocalVariable
Local variable `bis2` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      }
      CompressionInputStream cis = codec.createInputStream(downStream, decompressor);
      BufferedInputStream bis2 = new BufferedInputStream(cis, DATA_IBUF_SIZE);
      return bis2;

```

### UnnecessaryLocalVariable
Local variable `bos2` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
      }
      CompressionOutputStream cos = createPlainCompressionStream(bos1, compressor);
      BufferedOutputStream bos2 =
        new BufferedOutputStream(new FinishOnFlushCompressionStream(cos), DATA_OBUF_SIZE);
      return bos2;
```

### UnnecessaryLocalVariable
Local variable `i_m` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java`
#### Snippet
```java
    int len_m = len_4 << 2;
    int left = length - len_m;
    int i_m = len_m;

    if (left != 0) {
```

### UnnecessaryLocalVariable
Local variable `val` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/RawDouble.java`
#### Snippet
```java
   */
  public double decodeDouble(byte[] buff, int offset) {
    double val = Bytes.toDouble(buff, offset);
    return val;
  }
```

### UnnecessaryLocalVariable
Local variable `userForTesting` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
   */
  public static User createUserForTesting(Configuration conf, String name, String[] groups) {
    User userForTesting = SecureHadoopUser.createUserForTesting(conf, name, groups);
    return userForTesting;
  }
```

### UnnecessaryLocalVariable
Local variable `vlength` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
  @Override
  public int getValueLength() {
    int vlength = Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
    return vlength;
  }
```

### UnnecessaryLocalVariable
Local variable `diff` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    protected int compareFamilies(final byte[] left, final int loffset, final int lfamilylength,
      final byte[] right, final int roffset, final int rfamilylength) {
      int diff = Bytes.compareTo(left, loffset, lfamilylength, right, roffset, rfamilylength);
      return diff;
    }
```

### UnnecessaryLocalVariable
Local variable `compare` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    @Override
    public int compare(final Cell left, final Cell right) {
      int compare = CellComparatorImpl.COMPARATOR.compare(left, right);
      return compare;
    }
```

### UnnecessaryLocalVariable
Local variable `voffset` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
  @Override
  public int getValueOffset() {
    int voffset = getKeyOffset() + getKeyLength();
    return voffset;
  }
```

### UnnecessaryLocalVariable
Local variable `diff` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    protected int compareColumns(final byte[] left, final int loffset, final int lquallength,
      final byte[] right, final int roffset, final int rquallength) {
      int diff = Bytes.compareTo(left, loffset, lquallength, right, roffset, rquallength);
      return diff;
    }
```

### UnnecessaryLocalVariable
Local variable `modifyingScanner` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ValueRewritingObserver.java`
#### Snippet
```java
    final InternalScanner scanner, ScanType scanType, CompactionLifeCycleTracker tracker,
    CompactionRequest request) {
    InternalScanner modifyingScanner = new InternalScanner() {
      @Override
      public boolean next(List<Cell> result, ScannerContext scannerContext) throws IOException {
```

### UnnecessaryLocalVariable
Local variable `cellBlock` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CellBlockBuilder.java`
#### Snippet
```java
  private ByteBuffer decompress(CompressionCodec compressor, byte[] compressedCellBlock)
    throws IOException {
    ByteBuffer cellBlock = decompress(compressor, new ByteArrayInputStream(compressedCellBlock),
      compressedCellBlock.length * this.cellBlockDecompressionMultiplier);
    return cellBlock;
```

### UnnecessaryLocalVariable
Local variable `kvWithTag` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
   */
  KeyValue createPutKeyValue(byte[] family, byte[] qualifier, long ts, byte[] value, Tag[] tags) {
    KeyValue kvWithTag = new KeyValue(this.row, family, qualifier, ts, value, tags);
    return kvWithTag;
  }
```

### UnnecessaryLocalVariable
Local variable `result` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
    }

    int result = (int) tempResult;
    return result;
  }
```

### UnnecessaryLocalVariable
Local variable `orFilter` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
        Filter filter = filterStack.pop();
        listOfFilters.add(0, filter);
        Filter orFilter = new FilterList(FilterList.Operator.MUST_PASS_ONE, listOfFilters);
        return orFilter;
      } catch (EmptyStackException e) {
```

### UnnecessaryLocalVariable
Local variable `andFilter` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
        Filter filter = filterStack.pop();
        listOfFilters.add(0, filter);
        Filter andFilter = new FilterList(FilterList.Operator.MUST_PASS_ALL, listOfFilters);
        return andFilter;
      } catch (EmptyStackException e) {
```

### UnnecessaryLocalVariable
Local variable `newModified` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
      int delta = (int) (newEnd - getEnd()) >> ADDRESS_BITS_PER_WORD;
      int newSize = modified.length + delta;
      long[] newModified = Arrays.copyOf(modified, newSize);
      modified = newModified;
      long[] newDeleted = Arrays.copyOf(deleted, newSize);
```

### UnnecessaryLocalVariable
Local variable `trailer` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALFormat.java`
#### Snippet
```java
    }

    ProcedureWALTrailer trailer =
      ProcedureWALTrailer.newBuilder().setVersion(version).setTrackerPos(stream.getPos()).build();
    return trailer;
```

### UnnecessaryLocalVariable
Local variable `onlineLogRecord` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  private static LogEntry getSlowLogRecord(final TooSlowLog.SlowLogPayload slowLogPayload) {
    OnlineLogRecord onlineLogRecord =
      new OnlineLogRecord.OnlineLogRecordBuilder().setCallDetails(slowLogPayload.getCallDetails())
        .setClientAddress(slowLogPayload.getClientAddress())
```

### UnnecessaryLocalVariable
Local variable `kv` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
      @Override
      public void write(ImmutableBytesWritable row, V cell) throws IOException {
        Cell kv = cell;
        // null input == user explicitly wants to flush
        if (row == null && kv == null) {
```

### UnnecessaryLocalVariable
Local variable `racks` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/RackManager.java`
#### Snippet
```java
      serversAsString.add(server.getHostname());
    }
    List<String> racks = switchMapping.resolve(serversAsString);
    return racks;
  }
```

### UnnecessaryLocalVariable
Local variable `mPools` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java

  public static List<MemoryPoolMXBean> getMemoryPools() {
    List<MemoryPoolMXBean> mPools = ManagementFactory.getMemoryPoolMXBeans();
    return mPools;
  }
```

### UnnecessaryLocalVariable
Local variable `gcBeans` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java

  public static List<GarbageCollectorMXBean> getGcCollectorBeans() {
    List<GarbageCollectorMXBean> gcBeans = ManagementFactory.getGarbageCollectorMXBeans();
    return gcBeans;
  }
```

### UnnecessaryLocalVariable
Local variable `blocksDistribution` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java
      TableDescriptor tableDescriptor = getDescriptor(region.getTable());
      if (tableDescriptor != null) {
        HDFSBlocksDistribution blocksDistribution =
          provider.computeHDFSBlocksDistribution(getConf(), tableDescriptor, region);
        return blocksDistribution;
```

### UnnecessaryLocalVariable
Local variable `file_content` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ZNodeClearer.java`
#### Snippet
```java
    try {
      br = new BufferedReader(znodeFile);
      String file_content = br.readLine();
      return file_content;
    } finally {
```

### UnnecessaryLocalVariable
Local variable `clearSlowLogResponses` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
        queueRecorder -> queueRecorder.clearNamedQueue(NamedQueuePayload.NamedQueueEvent.SLOW_LOG))
      .orElse(false);
    ClearSlowLogResponses clearSlowLogResponses =
      ClearSlowLogResponses.newBuilder().setIsCleaned(slowLogsCleaned).build();
    return clearSlowLogResponses;
```

### UnnecessaryLocalVariable
Local variable `s` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java`
#### Snippet
```java
  public static String createHFileLinkName(final TableName tableName, final String regionName,
    final String hfileName) {
    String s = String.format("%s=%s-%s",
      tableName.getNameAsString().replace(TableName.NAMESPACE_DELIM, '='), regionName, hfileName);
    return s;
```

### UnnecessaryLocalVariable
Local variable `l1CachePercent` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/util/MemorySizeUtil.java`
#### Snippet
```java
  public static float getBlockCacheHeapPercent(final Configuration conf) {
    // L1 block cache is always on heap
    float l1CachePercent = conf.getFloat(HConstants.HFILE_BLOCK_CACHE_SIZE_KEY,
      HConstants.HFILE_BLOCK_CACHE_SIZE_DEFAULT);
    return l1CachePercent;
```

### UnnecessaryLocalVariable
Local variable `context` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java
      builder.withEncryptionContext(cryptoContext);
    }
    HFileContext context = builder.build();
    return context;
  }
```

### UnnecessaryLocalVariable
Local variable `buffer` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.java`
#### Snippet
```java
      @Override
      public ByteBuffer allocate(long size) throws IOException {
        ByteBuffer buffer = fileChannel.map(java.nio.channels.FileChannel.MapMode.READ_WRITE,
          pos.getAndIncrement() * size, size);
        return buffer;
```

### UnnecessaryLocalVariable
Local variable `future` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public CompletableFuture<Boolean> exceedThrottleQuotaSwitch(boolean enable) {
    CompletableFuture<Boolean> future = this.<Boolean> newMasterCaller()
      .action((controller, stub) -> this.<SwitchExceedThrottleQuotaRequest,
        SwitchExceedThrottleQuotaResponse, Boolean> call(controller, stub,
```

### UnnecessaryLocalVariable
Local variable `future` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public CompletableFuture<Boolean> isRpcThrottleEnabled() {
    CompletableFuture<Boolean> future = this.<Boolean> newMasterCaller()
      .action((controller, stub) -> this.<IsRpcThrottleEnabledRequest, IsRpcThrottleEnabledResponse,
        Boolean> call(controller, stub, IsRpcThrottleEnabledRequest.newBuilder().build(),
```

### UnnecessaryLocalVariable
Local variable `future` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public CompletableFuture<Boolean> switchRpcThrottle(boolean enable) {
    CompletableFuture<Boolean> future = this.<Boolean> newMasterCaller()
      .action((controller, stub) -> this.<SwitchRpcThrottleRequest, SwitchRpcThrottleResponse,
        Boolean> call(controller, stub,
```

### UnnecessaryLocalVariable
Local variable `w` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java
      .withCreateTime(EnvironmentEdgeManager.currentTime()).build();

    StoreFileWriter w = new StoreFileWriter.Builder(conf, writerCacheConf, fs).withFilePath(path)
      .withBloomType(bloomType).withMaxKeyCount(maxKeyCount).withFileContext(hFileContext)
      .withWriterCreationTracker(writerCreationTracker).build();
```

### UnnecessaryLocalVariable
Local variable `readWriteLock` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool.java`
#### Snippet
```java
  public ReentrantReadWriteLock getLock(T id) {
    lockPool.purge();
    ReentrantReadWriteLock readWriteLock = lockPool.get(id);
    return readWriteLock;
  }
```

### UnnecessaryLocalVariable
Local variable `descriptor` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
      }
    }
    TableDescriptor descriptor = this.cache.remove(tablename);
    return descriptor;
  }
```

### UnnecessaryLocalVariable
Local variable `regionOpenAndInitThreadPool` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
    int maxThreads =
      Math.min(regionNumber, conf.getInt("hbase.hregion.open.and.init.threads.max", 16));
    ThreadPoolExecutor regionOpenAndInitThreadPool = Threads.getBoundedCachedThreadPool(maxThreads,
      30L, TimeUnit.SECONDS, new ThreadFactoryBuilder().setNameFormat(threadNamePrefix + "-pool-%d")
        .setDaemon(true).setUncaughtExceptionHandler(Threads.LOGGING_EXCEPTION_HANDLER).build());
```

### UnnecessaryLocalVariable
Local variable `corruptHfile` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
    Path corruptRegionDir = new Path(corruptTableDir, regionDir.getName());
    Path corruptFamilyDir = new Path(corruptRegionDir, cfDir.getName());
    Path corruptHfile = new Path(corruptFamilyDir, hFile.getName());
    return corruptHfile;
  }
```

### UnnecessaryLocalVariable
Local variable `response` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
        server.getMasterCoprocessorHost().postGetUserPermissions(userName, namespace, table, cf,
          cq);
        AccessControlProtos.GetUserPermissionsResponse response =
          ShadedAccessControlUtil.buildGetUserPermissionsResponse(perms);
        return response;
```

### UnnecessaryLocalVariable
Local variable `newValue` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
  boolean switchBalancer(final boolean b, BalanceSwitchMode mode) throws IOException {
    boolean oldValue = server.loadBalancerStateStore.get();
    boolean newValue = b;
    try {
      if (server.cpHost != null) {
```

### UnnecessaryLocalVariable
Local variable `parameters` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureDescriber.java`
#### Snippet
```java
    try {
      ProcedureProtos.Procedure protoProc = ProcedureUtil.convertToProtoProcedure(proc);
      List<Object> parameters = protoProc.getStateMessageList().stream().map((any) -> {
        try {
          return ProtobufMessageConverter.toJavaObject(any);
```

### UnnecessaryLocalVariable
Local variable `maxBalancingTime` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private int getMaxBalancingTime() {
    // if max balancing time isn't set, defaulting it to period time
    int maxBalancingTime =
      getConfiguration().getInt(HConstants.HBASE_BALANCER_MAX_BALANCING, getConfiguration()
        .getInt(HConstants.HBASE_BALANCER_PERIOD, HConstants.DEFAULT_HBASE_BALANCER_PERIOD));
```

### UnnecessaryLocalVariable
Local variable `unknownServerNames` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      final Set<ServerName> serverNames = getAssignmentManager().getRegionStates().getRegionStates()
        .stream().map(RegionState::getServerName).collect(Collectors.toSet());
      final List<ServerName> unknownServerNames = serverNames.stream()
        .filter(sn -> sn != null && serverManager.isServerUnknown(sn)).collect(Collectors.toList());
      return unknownServerNames;
```

### UnnecessaryLocalVariable
Local variable `procId` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    // This special create table is called locally to master. Therefore, no RPC means no need
    // to use nonce to detect duplicated RPC call.
    long procId = this.procedureExecutor.submitProcedure(
      new CreateTableProcedure(procedureExecutor.getEnvironment(), tableDescriptor, newRegions));

```

### UnnecessaryLocalVariable
Local variable `notifier` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  private SpaceQuotaSnapshotNotifier createQuotaSnapshotNotifier() {
    SpaceQuotaSnapshotNotifier notifier =
      SpaceQuotaSnapshotNotifierFactory.getInstance().create(getConfiguration());
    return notifier;
```

### UnnecessaryLocalVariable
Local variable `clone` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/LeafExpressionNode.java`
#### Snippet
```java
  @Override
  public LeafExpressionNode deepClone() {
    LeafExpressionNode clone = new LeafExpressionNode(this.identifier);
    return clone;
  }
```

### UnnecessaryLocalVariable
Local variable `visibilityLabelFilter` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
    VisibilityLabelService vls =
      VisibilityLabelServiceManager.getInstance().getVisibilityLabelService();
    Filter visibilityLabelFilter =
      new VisibilityLabelFilter(vls.getVisibilityExpEvaluator(authorizations), cfVsMaxVersions);
    return visibilityLabelFilter;
```

### UnnecessaryLocalVariable
Local variable `snapshotRegionFs` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java`
#### Snippet
```java
    @Override
    public HRegionFileSystem regionOpen(final RegionInfo regionInfo) throws IOException {
      HRegionFileSystem snapshotRegionFs =
        HRegionFileSystem.createRegionOnFileSystem(conf, workingDirFs, snapshotDir, regionInfo);
      return snapshotRegionFs;
```

### UnnecessaryLocalVariable
Local variable `familyDir` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java`
#### Snippet
```java
    @Override
    public Path familyOpen(final HRegionFileSystem snapshotRegionFs, final byte[] familyName) {
      Path familyDir = snapshotRegionFs.getStoreDir(Bytes.toString(familyName));
      return familyDir;
    }
```

### UnnecessaryLocalVariable
Local variable `age` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceLogQueue.java`
#### Snippet
```java
      timestamp = now;
    }
    long age = now - timestamp;
    return age;
  }
```

### UnnecessaryLocalVariable
Local variable `result` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
    }

    Pair<Integer, Integer> result = new Pair<>(distinctRowKeys, totalHFileEntries);
    return result;
  }
```

### UnnecessaryLocalVariable
Local variable `cpInstance` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      // switch temporarily to the thread classloader for custom CP
      currentThread.setContextClassLoader(cl);
      E cpInstance = checkAndLoadInstance(implClass, priority, conf);
      return cpInstance;
    } finally {
```

### UnnecessaryLocalVariable
Local variable `p` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
    this.noReadahead =
      this.conf.getBoolean(STORE_FILE_READER_NO_READAHEAD, DEFAULT_STORE_FILE_READER_NO_READAHEAD);
    Path p = initialPath;
    if (HFileLink.isHFileLink(p)) {
      // HFileLink
```

### UnnecessaryLocalVariable
Local variable `clazz` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushPolicyFactory.java`
#### Snippet
```java
    }
    try {
      Class<? extends FlushPolicy> clazz = Class.forName(className).asSubclass(FlushPolicy.class);
      return clazz;
    } catch (Exception e) {
```

### UnnecessaryLocalVariable
Local variable `clazz` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java`
#### Snippet
```java

    try {
      Class<? extends RegionSplitPolicy> clazz =
        Class.forName(className).asSubclass(RegionSplitPolicy.class);
      return clazz;
```

### UnnecessaryLocalVariable
Local variable `bulkToken` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    region.getCoprocessorHost().prePrepareBulkLoad(user);

    String bulkToken =
      createStagingDir(baseStagingDir, user, region.getTableDescriptor().getTableName()).toString();

```

### UnnecessaryLocalVariable
Local variable `hRegion` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
            CompactionDescriptor compactionDesc = WALEdit.getCompaction(metaCell);
            boolean isDefaultReplica = RegionReplicaUtil.isDefaultReplica(region.getRegionInfo());
            HRegion hRegion = region;
            if (compactionDesc != null) {
              // replay the compaction. Remove the files from stores only if we are the primary
```

### UnnecessaryLocalVariable
Local variable `shipper` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
    }
    RegionScannerImpl coreScanner = region.getScanner(scan);
    Shipper shipper = coreScanner;
    RegionScanner scanner = coreScanner;
    try {
```

### UnnecessaryLocalVariable
Local variable `state` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
      }
      // Create new state and update parent.
      State state = createNewState(false);
      StripeStoreFileManager.this.state = state;
      updateMetadataMaps();
```

### UnnecessaryLocalVariable
Local variable `state` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
      this.compactedFiles = compactedFiles;
      // Create new state and update parent.
      State state = createNewState(true);
      StripeStoreFileManager.this.state = state;
      updateMetadataMaps();
```

### UnnecessaryLocalVariable
Local variable `numOfCellsInChunk` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.java`
#### Snippet
```java
  private int calcNumOfCellsInChunk(int chunkSize) {
    int chunkSpace = chunkSize - ChunkCreator.SIZEOF_CHUNK_HEADER;
    int numOfCellsInChunk = chunkSpace / ClassSize.CELL_CHUNK_MAP_ENTRY;
    return numOfCellsInChunk;
  }
```

### UnnecessaryLocalVariable
Local variable `payload` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEventTrackerListener.java`
#### Snippet
```java
  private WALEventTrackerPayload getPayload(String path, String state, long walLength) {
    long timestamp = EnvironmentEdgeManager.currentTime();
    WALEventTrackerPayload payload =
      new WALEventTrackerPayload(serverName, path, timestamp, state, walLength);
    return payload;
```

### UnnecessaryLocalVariable
Local variable `listener` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  private WALActionsListener getWALEventTrackerListener(Configuration conf) {
    if (conf.getBoolean(WAL_EVENT_TRACKER_ENABLED_KEY, WAL_EVENT_TRACKER_ENABLED_DEFAULT)) {
      WALEventTrackerListener listener =
        new WALEventTrackerListener(conf, getNamedQueueRecorder(), getServerName());
      return listener;
```

### UnnecessaryLocalVariable
Local variable `defaultResult` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    final ScanType scanType, final CompactionLifeCycleTracker tracker,
    final CompactionRequest request, final User user) throws IOException {
    InternalScanner defaultResult = scanner;
    if (coprocEnvironments.isEmpty()) {
      return defaultResult;
```

### UnnecessaryLocalVariable
Local variable `result` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java`
#### Snippet
```java
    // Nothing to compact
    Collection<HStoreFile> toCompact = getExpiredStores(candidateFiles, filesCompacting);
    CompactionRequestImpl result = new CompactionRequestImpl(toCompact);
    return result;
  }
```

### UnnecessaryLocalVariable
Local variable `result` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java`
#### Snippet
```java
      Map<Long, String> boundaryPolicyMap =
        getBoundariesStoragePolicyForMinor(singleOutput, window, now);
      DateTieredCompactionRequest result =
        new DateTieredCompactionRequest(storeFileSelection, boundaries, boundaryPolicyMap);
      return result;
```

### UnnecessaryLocalVariable
Local variable `pathList` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactor.java`
#### Snippet
```java
  protected List<Path> commitWriter(DateTieredMultiFileWriter writer, FileDetails fd,
    CompactionRequestImpl request) throws IOException {
    List<Path> pathList =
      writer.commitWriters(fd.maxSeqId, request.isAllFiles(), request.getFiles());
    return pathList;
```

### UnnecessaryLocalVariable
Local variable `localLogsToArchive` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java

    if (logsToArchive != null) {
      final List<Pair<Path, Long>> localLogsToArchive = logsToArchive;
      // make it async
      for (Pair<Path, Long> log : localLogsToArchive) {
```

### UnnecessaryLocalVariable
Local variable `newPath` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
  private Path getNewPath() throws IOException {
    this.filenum.set(Math.max(getFilenum() + 1, EnvironmentEdgeManager.currentTime()));
    Path newPath = getCurrentFileName();
    return newPath;
  }
```

### UnnecessaryLocalVariable
Local variable `hFileContext` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTrackerBase.java`
#### Snippet
```java
    }
    ColumnFamilyDescriptor family = ctx.getFamily();
    HFileContext hFileContext = new HFileContextBuilder().withIncludesMvcc(includeMVCCReadpoint)
      .withIncludesTags(includesTag).withCompression(compression)
      .withCompressTags(family.isCompressTags()).withChecksumType(StoreUtils.getChecksumType(conf))
```

### UnnecessaryLocalVariable
Local variable `e` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java`
#### Snippet
```java
    StackTraceElement[] trace = ForeignException.toStackTrace(gem.getTraceList());
    ProxyThrowable dfe = new ProxyThrowable(gem.getMessage(), trace);
    ForeignException e = new ForeignException(eem.getSource(), dfe);
    return e;
  }
```

### UnnecessaryLocalVariable
Local variable `manifest` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/HBackupFileSystem.java`
#### Snippet
```java
  public static BackupManifest getManifest(Configuration conf, Path backupRootPath, String backupId)
    throws IOException {
    BackupManifest manifest =
      new BackupManifest(conf, getManifestPath(conf, backupRootPath, backupId));
    return manifest;
```

### UnnecessaryLocalVariable
Local variable `deletables` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupHFileCleaner.java`
#### Snippet
```java
      return Collections.emptyList();
    }
    Iterable<FileStatus> deletables = Iterables.filter(files, file -> {
      // If the file is recent, be conservative and wait for one more scan of backup:system table
      if (file.getModificationTime() > secondPrevReadFromBackupTbl) {
```

### UnnecessaryLocalVariable
Local variable `result` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
    }
    Job job = createSubmittableJob(args);
    int result = job.waitForCompletion(true) ? 0 : 1;
    return result;
  }
```

### UnnecessaryLocalVariable
Local variable `info` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
        // Load BackupManifest
        BackupManifest manifest = new BackupManifest(fs, lfs.getPath().getParent());
        BackupInfo info = manifest.toBackupInfo();
        return info;
      }
```

### UnnecessaryLocalVariable
Local variable `request` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    boolean check, TableName[] fromTables, TableName[] toTables, boolean isOverwrite) {
    RestoreRequest.Builder builder = new RestoreRequest.Builder();
    RestoreRequest request =
      builder.withBackupRootDir(backupRootDir).withBackupId(backupId).withCheck(check)
        .withFromTables(fromTables).withToTables(toTables).withOvewrite(isOverwrite).build();
```

### UnnecessaryLocalVariable
Local variable `hfofDir` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
    for (Path regionDir : regionDirList) {
      LOG.debug("Parsing region dir: " + regionDir);
      Path hfofDir = regionDir;

      if (!fs.exists(hfofDir)) {
```

### UnnecessaryLocalVariable
Local variable `ioe` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      LOG.warn("Error to get row lock for {}, in region {}, cause: {}", Bytes.toStringBinary(row),
        getRegionInfo().getRegionNameAsString(), error);
      IOException ioe = new IOException(error);
      throw ioe;
    } finally {
```

### UnnecessaryLocalVariable
Local variable `region` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      tableDir);
    HRegionFileSystem.createRegionOnFileSystem(conf, fs, tableDir, regionInfo);
    HRegion region = HRegion.newHRegion(tableDir, null, fs, conf, regionInfo, tableDesc, null);
    return region;
  }
```

### UnnecessaryLocalVariable
Local variable `user` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    // code is taken from the AppendTestUtil over in hdfs.
    String username = User.getCurrent().getName() + differentiatingSuffix;
    User user = User.createUserForTesting(c, username, new String[] { "supergroup" });
    return user;
  }
```

### UnnecessaryLocalVariable
Local variable `p` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        l.add(sumVal);
        l.add(sumSqVal);
        Pair<List<S>, Long> p = new Pair<>(l, rowCountVal);
        return p;
      }
```

### UnnecessaryLocalVariable
Local variable `p` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
        l.add(sumVal);
        l.add(sumWeights);
        Pair<NavigableMap<byte[], List<S>>, List<S>> p = new Pair<>(map, l);
        return p;
      }
```

### UnnecessaryLocalVariable
Local variable `s` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
          ByteString b = response.getFirstPart(0);
          T t = getParsedGenericInstance(ci.getClass(), 4, b);
          S s = ci.getPromotedValueFromProto(t);
          return s;
        }
```

## RuleId[id=BusyWait]
### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/IdLock.java`
#### Snippet
```java
        }
      }
      Thread.sleep(100);
    }
  }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/KeyPressGenerator.java`
#### Snippet
```java
          }
        } else {
          Thread.sleep(20);
        }
      } catch (InterruptedException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java`
#### Snippet
```java
        break;
      }
      Thread.sleep(HConstants.SOCKET_RETRY_WAIT_MS);
    }
    return sn;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      }
      try {
        Thread.sleep(TIMEOUT);
      } catch (InterruptedException e) {
        throw (InterruptedIOException) new InterruptedIOException().initCause(e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      }
      try {
        Thread.sleep(TIMEOUT);
      } catch (InterruptedException e) {
        throw (InterruptedIOException) new InterruptedIOException().initCause(e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/util/RecoverLeaseFSUtils.java`
#### Snippet
```java
                < subsequentPauseBase * nbAttempt
          ) {
            Thread.sleep(conf.getInt("hbase.lease.recovery.pause", 1000));
            if (findIsFileClosedMeth) {
              try {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
        overwrite = true;
        try {
          Thread.sleep(ConnectionUtils.getPauseTime(100, retry));
        } catch (InterruptedException ie) {
          throw new InterruptedIOException();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java
    while (!executor.isTerminated()) {
      // wait till all the threads are done
      Thread.sleep(1000);
    }
  }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java
      LOG.info("Running Chaos Agent on : " + agentName);
      while (!this.isConnected()) {
        Thread.sleep(100);
      }
      this.getTasks();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java
      this.getTasks();
      while (!stopChaosAgent.get()) {
        Thread.sleep(500);
      }
    } catch (InterruptedException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
        // try the next port number
        listener.setPort(++port);
        Thread.sleep(100);
      }
    }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
          freeSpace("Full!");
        } else {
          Thread.sleep(50);
        }
      } catch (IOException ioex) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
            connectionManager.closeIdle(true);
            try {
              Thread.sleep(60000);
            } catch (InterruptedException ex) {
              LOG.debug("Interrupted while sleeping");
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java`
#### Snippet
```java
          SimpleRpcServer.LOG.warn(getName() + ": OutOfMemoryError in server select", e);
          try {
            Thread.sleep(60000);
          } catch (InterruptedException ex) {
            SimpleRpcServer.LOG.debug("Interrupted while sleeping");
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java`
#### Snippet
```java
        htd.getTableName(), hcd.getNameAsString(), toCompact.size(), regions.size(),
        totalCompacted);
      Thread.sleep(10000);
    }
    LOG.info("Finished major MOB compacting table={}. cf={}", htd.getTableName(),
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java`
#### Snippet
```java
    while (admin.getCompactionStateForRegion(region.getRegionName()) == CompactionState.NONE) {
      // Is 1 second too aggressive?
      Thread.sleep(1000);
      if (EnvironmentEdgeManager.currentTime() - startTime > waitTime) {
        LOG.warn(
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
  public void waitUntilWalRollFinished() throws InterruptedException {
    while (!walRollFinished()) {
      Thread.sleep(100);
    }
  }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
                LOG.warn("Retry to roll log, nAttempts={}, waiting time={}ms, sleeping 1s to retry,"
                  + " last exception", nAttempts, waitingTime, ioe);
                sleep(1000);
              } else {
                LOG.error("Roll wal failed and waiting timeout, will not retry", ioe);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java`
#### Snippet
```java
              } else {
                try {
                  Thread.sleep(nbAttempt < 3 ? 500 : 1000);
                  continue; // retry
                } catch (InterruptedException ie) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          while (!monitor.isDone()) {
            // wait for 1 sec
            Thread.sleep(1000);
            // exit if any error occurs
            if (failOnError && monitor.hasError()) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        }

        Thread.sleep(interval);
      } while (interval > 0);
    } // try-with-resources close
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java`
#### Snippet
```java
          break;
        }
        Thread.sleep(1000);
      }
      if (forceStop) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java

      try {
        Thread.sleep(100);
      } catch (InterruptedException e) {
        throw (InterruptedIOException) new InterruptedIOException().initCause(e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
                    local_finished = splitScan(outstanding, connection, tableName, splitAlgo);
                    if (local_finished.isEmpty()) {
                      Thread.sleep(30 * 1000);
                    } else {
                      finished.addAll(local_finished);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
                  splitScan(outstanding, connection, tableName, splitAlgo);
                if (finished.isEmpty()) {
                  Thread.sleep(30 * 1000);
                } else {
                  outstanding.removeAll(finished);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/IdReadWriteLock.java`
#### Snippet
```java
        }
      }
      Thread.sleep(50);
    }
  }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java`
#### Snippet
```java
          break;
        }
        Thread.sleep(1000);
      }
    }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmPauseMonitor.java`
#### Snippet
```java
        sw.reset().start();
        try {
          Thread.sleep(SLEEP_INTERVAL_MS);
        } catch (InterruptedException ie) {
          return;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java`
#### Snippet
```java
        LOG.warn("Exception when waiting for region to become assigned," + " retrying", e);
      }
      Thread.sleep(1000);
    }
    throw new IOException("Region " + region + " failed to move out of "
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
        LOG.warn("Could not get list of region servers", e);
      }
      Thread.sleep(500);
    }
    if (server == null) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
        if (!skipWait) {
          while (isCompacting(request)) {
            Thread.sleep(sleepForMs);
            LOG.debug("Waiting for compaction to complete for region: "
              + request.getRegion().getEncodedName());
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
      while (clusterCompactionQueues.atCapacity()) {
        LOG.debug("Waiting for servers to complete Compactions");
        Thread.sleep(sleepForMs);
      }
      Optional<ServerName> serverToProcess =
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
      } else {
        // haven't assigned anything so we sleep.
        Thread.sleep(sleepForMs);
      }
    }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java
            sleepTime = testingSleepTime;
          }
          Thread.sleep(sleepTime);
        } catch (InterruptedException e) {
          // Since there won't be any more heartbeats, assume lock will be lost.
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterInitializationMonitor.java`
#### Snippet
```java
    try {
      while (!master.isStopped() && master.isActiveMaster()) {
        Thread.sleep(timeout);
        if (master.isInitialized()) {
          LOG.debug("Initialization completed within allotted tolerance. Monitor exiting.");
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
      // We sleep for some time
      final long sleepTime = 50;
      Thread.sleep(sleepTime);
      now = EnvironmentEdgeManager.currentTime();
      slept = now - startTime;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
          ioe);
      }
      Thread.sleep(1000);
    }
    throw new IOException("Region " + region + " failed to close within" + " timeout " + timeout);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
            cause);
          try {
            Thread.sleep(wait);
          } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      LOG.info("Waiting for dfs to exit safe mode...");
      try {
        Thread.sleep(wait);
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
          LOG.warn("Unable to check cluster ID file in {}, retrying in {}ms", rootdir, wait, ioe);
          try {
            Thread.sleep(wait);
          } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
          try {
            if (wait > 0) {
              Thread.sleep(wait);
            }
          } catch (InterruptedException ie) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    ) {
      try {
        Thread.sleep(1000);
      } catch (InterruptedException e) {
        throw new IOException("Wait interrupted", e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    ) {
      try {
        Thread.sleep(100);
      } catch (InterruptedException ie) {
        interrupted = true;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      try {
        // sleep if the number of regions in transition exceeds the limit
        Thread.sleep(100);
      } catch (InterruptedException ie) {
        interrupted = true;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java
      }
      try {
        Thread.sleep(waitingTimeForEvents);
      } catch (InterruptedException e) {
        LOG.warn("Interrupted while sleeping, waiting on " + purpose);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java
          throw new NotAllMetaRegionsOnlineException();
        }
        Thread.sleep(HConstants.SOCKET_RETRY_WAIT_MS);
      }
    } catch (InterruptedException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java`
#### Snippet
```java
        // wait for the thread to shutdown completely.
        while (!threadPool.isTerminated()) {
          Thread.sleep(50);
        }
        throw new IOException(
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ModifyPeerProcedure.java`
#### Snippet
```java
          return false;
        }
        Thread.sleep(SLEEP_INTERVAL_MS);
      } catch (TableNotFoundException e) {
        return false;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/AbstractPeerProcedure.java`
#### Snippet
```java
          return true;
        }
        Thread.sleep(SLEEP_INTERVAL_MS);
      } catch (TableNotFoundException e) {
        return false;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      do {
        try {
          Thread.sleep(100);
        } catch (InterruptedException e) {
          LOG.warn("Interrupted while waiting for Quota Manager to be initialized.");
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java

        try {
          Thread.sleep(5000);
        } catch (InterruptedException ie) {
          if (LOG.isDebugEnabled()) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      ) {
        try {
          Thread.sleep(100);
        } catch (InterruptedException e) {
          throw new IOException("Wait interrupted ", e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java`
#### Snippet
```java
      while (running) {
        try {
          Thread.sleep(monitorInterval);
          if (tasks.isFull()) {
            purgeExpiredTasks();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java`
#### Snippet
```java
        } else {
          // Wait both shipper and reader threads to stop
          Thread.sleep(this.sleepForRetries);
        }
      } catch (InterruptedException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java`
#### Snippet
```java
    while (!canPush(entry, row)) {
      LOG.debug("Can not push {}, wait", entry);
      Thread.sleep(waitTimeMs);
    }
  }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
      claimReplicationQueues(zkw, manager);
      while (manager.activeFailoverTaskCount() > 0) {
        Thread.sleep(SLEEP_TIME);
      }
      while (manager.getOldSources().size() > 0) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.java`
#### Snippet
```java
      }
      while (manager.getOldSources().size() > 0) {
        Thread.sleep(SLEEP_TIME);
      }
      manager.join();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
      LOG.debug("Retry listChildren of znode " + watcher.getZNodePaths().splitLogZNode
        + " after sleep for " + sleepTime + "ms!");
      Thread.sleep(sleepTime);
    }
    return childrenPaths;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
                + this.tasksInProgress.get() + " tasks in progress and can't take more.");
            }
            Thread.sleep(100);
          }
        }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
      if (!taskGrabbed && !shouldStop) {
        // do not grab any tasks, sleep a little bit to reduce zk request.
        Thread.sleep(1000);
      }
      SplitLogCounters.tot_wkr_task_grabing.increment();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LeaseManager.java`
#### Snippet
```java
        toWait = Math.max(MIN_WAIT_TIME, toWait);

        Thread.sleep(toWait);
      } catch (InterruptedException | ConcurrentModificationException e) {
        continue;
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
      if (lastException != null && i < (flushRetriesNumber - 1)) {
        try {
          Thread.sleep(pauseTime);
        } catch (InterruptedException e) {
          IOException iie = new InterruptedIOException();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
                // sleep a little bit to avoid spinning, and then pretend that
                // we flushed one, so anyone blocked will check again
                Thread.sleep(1000);
                wakeUpIfBlocking();
              }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DualAsyncFSWAL.java`
#### Snippet
```java
        LOG.warn("create remote writer {} failed, retry = {}", remoteWAL, retry, e);
        try {
          Thread.sleep(ConnectionUtils.getPauseTime(100, retry));
        } catch (InterruptedException ie) {
          // restore the interrupt state
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
          // sleep, then repeat
          try {
            Thread.sleep(1000);
          } catch (InterruptedException e1) {
            // Restore the interrupted status
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollBackupSubprocedure.java`
#### Snippet
```java
      long start = EnvironmentEdgeManager.currentTime();
      while (!((HRegionServer) rss).getWalRoller().walRollFinished()) {
        Thread.sleep(20);
      }
      LOG.debug("log roll took " + (EnvironmentEdgeManager.currentTime() - start));
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
      int maxAttempts = 600;
      while (!admin.isTableAvailable(desc.getTableName())) {
        Thread.sleep(100);
        attempt++;
        if (attempt++ > maxAttempts) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
      while (!admin.isTableAvailable(targetTableName)) {
        try {
          Thread.sleep(100);
        } catch (InterruptedException ie) {
          Thread.currentThread().interrupt();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
            lastProgress = newProgress;
          }
          Thread.sleep(progressReportFreq);
        }
        // update the progress data after copy job complete
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    while (!admin.tableExists(tableName) || !admin.isTableAvailable(tableName)) {
      try {
        Thread.sleep(100);
      } catch (InterruptedException e) {
        throw (IOException) new InterruptedIOException().initCause(e);
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
        break;
      }
      Thread.sleep(10);
    }
  }
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      newZK.getState() != States.CONNECTED && EnvironmentEdgeManager.currentTime() - start < 1000
    ) {
      Thread.sleep(1);
    }
    newZK.close();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      while (!stopped) {
        try {
          Thread.sleep(30000);
        } catch (InterruptedException e) {
          Thread.currentThread().interrupt();
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    try {
      while (!condition.getAsBoolean()) {
        Thread.sleep(sleepMillis);
      }
    } catch (RuntimeException e) {
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      long now = EnvironmentEdgeManager.currentTime();
      if (now > timeoutTime) break;
      Thread.sleep(10);
    }
    throw new AssertionError(
```

### BusyWait
Call to `Thread.sleep()` in a loop, probably busy-waiting
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      long now = EnvironmentEdgeManager.currentTime();
      if (now > timeoutTime) break;
      Thread.sleep(10);
    }
    throw new AssertionError(
```

## RuleId[id=UseCompareMethod]
### UseCompareMethod
Expression can be replaced with 'Long.compare'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
  private static int unsignedCmp(long x1, long x2) {
    int cmp;
    if ((cmp = (x1 < x2 ? -1 : (x1 == x2 ? 0 : 1))) == 0) return 0;
    // invert the result when either value is negative
    if ((x1 < 0) != (x2 < 0)) return -cmp;
```

### UseCompareMethod
Expression can be replaced with 'Long.compare'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExponentialCompactionWindowFactory.java`
#### Snippet
```java
      }
      long pos = timestamp / windowMillis;
      return divPosition == pos ? 0 : divPosition < pos ? -1 : 1;
    }

```

## RuleId[id=RedundantFileCreation]
### RedundantFileCreation
`new File` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyStoreKeyProvider.java`
#### Snippet
```java
    if (name.equalsIgnoreCase(KeyProvider.PASSWORDFILE)) {
      Properties p = new Properties();
      InputStream in = new BufferedInputStream(new FileInputStream(new File(value)));
      try {
        p.load(in);
```

### RedundantFileCreation
`new File` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/KeyStoreKeyProvider.java`
#### Snippet
```java
      throw new RuntimeException("KeyProvider parameters should specify a path");
    }
    InputStream is = new FileInputStream(new File(path));
    try {
      store.load(is, password);
```

## RuleId[id=PointlessBooleanExpression]
### PointlessBooleanExpression
`validFileEncountered == false` can be simplified to '!validFileEncountered'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java`
#### Snippet
```java
        }
      }
      if (validFileEncountered == false) {
        // all items returned by globStatus() are directories
        throw new FileNotFoundException("No file found matching " + pathPattern1.toString());
```

### PointlessBooleanExpression
`Boolean.valueOf(value) == false` can be simplified to '!Boolean.valueOf(value)'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
        String value = Bytes.toString(entry.getValue().get());
        if (key.equalsIgnoreCase(IS_META)) {
          if (Boolean.valueOf(value) == false) {
            continue;
          }
```

### PointlessBooleanExpression
`readonly == true` can be simplified to 'readonly'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
   */
  private void checkReadonly() {
    if (readonly == true) {
      throw new UnsupportedOperationException("Attempting to modify readonly EMPTY_RESULT!");
    }
```

### PointlessBooleanExpression
`ranges.get(index).isSearchRowInclusive() == false` can be simplified to '!ranges.get(index).isSearchRowInclusive()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
      }
      // the row key equals one of the start keys, and the the range exclude the start key
      if (ranges.get(index).isSearchRowInclusive() == false) {
        exclusive = true;
      }
```

### PointlessBooleanExpression
`stopRowInclusive == true` can be simplified to 'stopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
        || Bytes.equals(stopRow, HConstants.EMPTY_BYTE_ARRAY)
        || Bytes.compareTo(startRow, stopRow) < 0
        || (Bytes.compareTo(startRow, stopRow) == 0 && stopRowInclusive == true);
    }

```

### PointlessBooleanExpression
`lastStopRowInclusive == false` can be simplified to '!lastStopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
        (Bytes.compareTo(lastStopRow, range.startRow) > 0)
          || (Bytes.compareTo(lastStopRow, range.startRow) == 0
            && !(lastStopRowInclusive == false && range.isStartRowInclusive() == false))
      ) {
        if (Bytes.equals(range.stopRow, HConstants.EMPTY_BYTE_ARRAY)) {
```

### PointlessBooleanExpression
`range.isStartRowInclusive() == false` can be simplified to '!range.isStartRowInclusive()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
        (Bytes.compareTo(lastStopRow, range.startRow) > 0)
          || (Bytes.compareTo(lastStopRow, range.startRow) == 0
            && !(lastStopRowInclusive == false && range.isStartRowInclusive() == false))
      ) {
        if (Bytes.equals(range.stopRow, HConstants.EMPTY_BYTE_ARRAY)) {
```

### PointlessBooleanExpression
`lastStopRowInclusive == true` can be simplified to 'lastStopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
        if (Bytes.compareTo(lastStopRow, range.stopRow) >= 0) {
          if ((Bytes.compareTo(lastStopRow, range.stopRow) == 0)) {
            if (lastStopRowInclusive == true || range.stopRowInclusive == true) {
              lastStopRowInclusive = true;
            }
```

### PointlessBooleanExpression
`range.stopRowInclusive == true` can be simplified to 'range.stopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
        if (Bytes.compareTo(lastStopRow, range.stopRow) >= 0) {
          if ((Bytes.compareTo(lastStopRow, range.stopRow) == 0)) {
            if (lastStopRowInclusive == true || range.stopRowInclusive == true) {
              lastStopRowInclusive = true;
            }
```

### PointlessBooleanExpression
`lastStopRowInclusive == true` can be simplified to 'lastStopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
            (Bytes.compareTo(lastStopRow, range.startRow) > 0)
              || (Bytes.compareTo(lastStopRow, range.startRow) == 0
                && (lastStopRowInclusive == true || range.startRowInclusive == true))
          ) {
            if (Bytes.equals(range.stopRow, HConstants.EMPTY_BYTE_ARRAY)) {
```

### PointlessBooleanExpression
`range.startRowInclusive == true` can be simplified to 'range.startRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
            (Bytes.compareTo(lastStopRow, range.startRow) > 0)
              || (Bytes.compareTo(lastStopRow, range.startRow) == 0
                && (lastStopRowInclusive == true || range.startRowInclusive == true))
          ) {
            if (Bytes.equals(range.stopRow, HConstants.EMPTY_BYTE_ARRAY)) {
```

### PointlessBooleanExpression
`lastStopRowInclusive == true` can be simplified to 'lastStopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
            // if this first range contain second range, ignore the second range
            if (Bytes.compareTo(lastStopRow, range.stopRow) >= 0) {
              if (lastStopRowInclusive == true || range.stopRowInclusive == true) {
                lastStopRowInclusive = true;
              }
```

### PointlessBooleanExpression
`range.stopRowInclusive == true` can be simplified to 'range.stopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
            // if this first range contain second range, ignore the second range
            if (Bytes.compareTo(lastStopRow, range.stopRow) >= 0) {
              if (lastStopRowInclusive == true || range.stopRowInclusive == true) {
                lastStopRowInclusive = true;
              }
```

### PointlessBooleanExpression
`lastStopRowInclusive == false` can be simplified to '!lastStopRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
              (Bytes.compareTo(lastStopRow, range.startRow) < 0)
                || (Bytes.compareTo(lastStopRow, range.startRow) == 0
                  && lastStopRowInclusive == false && range.startRowInclusive == false)
            ) {
              newRanges.add(new RowRange(lastStartRow, lastStartRowInclusive, lastStopRow,
```

### PointlessBooleanExpression
`range.startRowInclusive == false` can be simplified to '!range.startRowInclusive'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
              (Bytes.compareTo(lastStopRow, range.startRow) < 0)
                || (Bytes.compareTo(lastStopRow, range.startRow) == 0
                  && lastStopRowInclusive == false && range.startRowInclusive == false)
            ) {
              newRanges.add(new RowRange(lastStartRow, lastStartRowInclusive, lastStopRow,
```

### PointlessBooleanExpression
`hasFilters |= true` can be simplified to 'hasFilters=true'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
  public QuotaFilter addTypeFilter(final QuotaType type) {
    this.types.add(type);
    hasFilters |= true;
    return this;
  }
```

### PointlessBooleanExpression
`result == false` can be simplified to '!result'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
  public Cell getNextCellHint(Cell currentCell) {
    boolean result = tracker.updateTracker(currentCell);
    if (result == false) {
      done = true;
      return null;
```

### PointlessBooleanExpression
`cellScanner.advance() == false` can be simplified to '!cellScanner.advance()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ResponseConverter.java`
#### Snippet
```java
        for (int j = 0; j < noOfCells; j++) {
          try {
            if (cellScanner.advance() == false) {
              // We are not able to retrieve the exact number of cells which ResultCellMeta says us.
              // We have to scan for the same results again. Throwing DNRIOE as a client retry on
```

### PointlessBooleanExpression
`this.enforceLocality == true` can be simplified to 'this.enforceLocality'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
    // Get the region locality map
    Map<String, Map<String, Float>> regionLocalityMap = null;
    if (this.enforceLocality == true) {
      regionLocalityMap = FSUtils.getRegionDegreeLocalityMappingFromFS(conf);
    }
```

### PointlessBooleanExpression
`fs.delete(familyDir, true) == false` can be simplified to '!fs.delete(familyDir, true)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java`
#### Snippet
```java
    Path familyDir =
      new Path(tableDir, new Path(region.getEncodedName(), Bytes.toString(familyName)));
    if (fs.delete(familyDir, true) == false) {
      if (fs.exists(familyDir)) {
        throw new IOException(
```

### PointlessBooleanExpression
`getTableNamespaceManager(env).doesNamespaceExist(namespaceName) == false` can be simplified to '!getTableNamespaceManager(env).doesNamespaceExist(namespaceName)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java`
#### Snippet
```java
   */
  private boolean prepareDelete(final MasterProcedureEnv env) throws IOException {
    if (getTableNamespaceManager(env).doesNamespaceExist(namespaceName) == false) {
      setFailure("master-delete-namespace", new NamespaceNotFoundException(namespaceName));
      return false;
```

### PointlessBooleanExpression
`resubmitTask(path, findOrCreateOrphanTask(path), directive) == false` can be simplified to '!resubmitTask(path, findOrCreateOrphanTask(path), directive)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java

  private void resubmitOrFail(String path, ResubmitDirective directive) {
    if (resubmitTask(path, findOrCreateOrphanTask(path), directive) == false) {
      setDone(path, FAILURE);
    }
```

### PointlessBooleanExpression
`ZKUtil.setData(this.watcher, path, slt.toByteArray(), version) == false` can be simplified to '!ZKUtil.setData(this.watcher, path, slt.toByteArray(), version)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      // blocking zk call but this is done from the timeout thread
      SplitLogTask slt = new SplitLogTask.Unassigned(this.details.getServerName());
      if (ZKUtil.setData(this.watcher, path, slt.toByteArray(), version) == false) {
        LOG.debug("Failed to resubmit task " + path + " version changed");
        return false;
```

### PointlessBooleanExpression
`fromPool == true` can be simplified to 'fromPool'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/OffheapChunk.java`
#### Snippet
```java
  OffheapChunk(int size, int id, ChunkType chunkType, boolean fromPool) {
    super(size, id, chunkType, fromPool);
    assert fromPool == true;
  }

```

### PointlessBooleanExpression
`CommonFSUtils.delete(fs, file.getPath(), true) == false` can be simplified to '!CommonFSUtils.delete(fs, file.getPath(), true)'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
      if (file.getPath().getName().startsWith("exportSnapshot-")) {
        LOG.debug("Delete log files of exporting snapshot: " + file.getPath().getName());
        if (CommonFSUtils.delete(fs, file.getPath(), true) == false) {
          LOG.warn("Can not delete " + file.getPath());
        }
```

### PointlessBooleanExpression
`image1TableList.contains(image2TableList.get(i)) == false` can be simplified to '!image1TableList.contains(image2TableList.get(i))'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java`
#### Snippet
```java

    for (int i = 0; i < image2TableList.size(); i++) {
      if (image1TableList.contains(image2TableList.get(i)) == false) {
        return false;
      }
```

## RuleId[id=UseBulkOperation]
### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java`
#### Snippet
```java
    List<String> argsList = new ArrayList<>(args.length);
    for (String arg : args) {
      argsList.add(arg);
    }
    // For backward compatibility of args which can't be parsed as Option. See javadoc for
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      stale = stale || r.isStale();
      for (Cell c : r.rawCells()) {
        cells.add(c);
      }
    }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java`
#### Snippet
```java
        FileSystem fs = status.getPath().getFileSystem(job.getConfiguration());
        for (FileStatus match : fs.listStatus(status.getPath(), HIDDEN_FILE_FILTER)) {
          result.add(match);
        }
      } else {
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      this.multiTableSupport = conf.getBoolean(MULTI_TABLES_SUPPORT, false);
      for (String table : tables) {
        tableSet.add(table);
      }
    }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
      }
      for (String v : entry.getValue()) {
        strings.add(v);
      }
    }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaRegionLocationCache.java`
#### Snippet
```java
    // Explicitly iterate instead of new ArrayList<>(snapshot.values()) because the underlying
    // ArrayValueCollection does not implement toArray().
    snapshot.values().forEach(location -> result.add(location));
    return result;
  }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.java`
#### Snippet
```java
  public CoprocessorMethod withParameters(String... parameters) {
    for (String parameter : parameters) {
      this.parameters.add(parameter);
    }

```

### UseBulkOperation
Iteration can be replaced with bulk 'Map.putAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    if (serverManager != null) {
      final Map<ServerName, ServerMetrics> map = new HashMap<>();
      serverManager.getOnlineServers().entrySet().forEach(e -> map.put(e.getKey(), e.getValue()));
      return map;
    }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
      for (StoreFileReference ref : unreferencedStoreFileNames) {
        for (String fileNames : ref.getFamilyToFilesMapping().values()) {
          snapshotReferencedFiles.add(fileNames);
        }
      }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
      for (Result res : scanner) {
        for (byte[] q : res.getFamilyMap(ACL_LIST_FAMILY).navigableKeySet()) {
          qualifierSet.add(q);
        }
      }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Map.putAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    Map<ServerName, Set<byte[]>> rsReportsSnapshot = new HashMap<>();
    synchronized (rsReports) {
      rsReports.entrySet().forEach(e -> rsReportsSnapshot.put(e.getKey(), e.getValue()));
    }
    return rsReportsSnapshot;
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerQueueService.java`
#### Snippet
```java
    Iterator<WALEventTrackerPayload> iterator = queue.iterator();
    while (iterator.hasNext()) {
      retQueue.add(iterator.next());
    }
    queue.clear();
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
    if (current != null) allScanners.add(current);
    for (KeyValueScanner scanner : heap.getHeap())
      allScanners.add(scanner);
    return allScanners;
  }
```

### UseBulkOperation
Iteration can be replaced with bulk 'Collection.addAll()' call
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
      List<String> result = new ArrayList<>(namespaces.length);
      for (String ns : namespaces) {
        result.add(ns);
      }
      return result;
```

## RuleId[id=AssignmentToCatchBlockParameter]
### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
          LOG.debug("Caught a ServiceException with null cause", e);
        } else {
          e = e.getCause();
        }
      }
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      corrupt = true;
    } catch (IOException e) {
      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
      throw e;
    } finally {
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java
          response = stub.lockHeartbeat(null, lockHeartbeatRequest);
        } catch (Exception e) {
          e = ProtobufUtil.handleRemoteException(e);
          locked.set(false);
          LOG.error("Heartbeat failed, releasing " + EntityLock.this, e);
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
        }
      } catch (IOException e) {
        e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
        LOG.warn("Error while deleting: " + filePath, e);
      }
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
        sendRequest(getServerName(), request.build());
      } catch (IOException e) {
        e = unwrapException(e);
        // TODO: In the future some operation may want to bail out early.
        // TODO: How many times should we retry (use numberOfAttemptsSoFar)
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
              compactSplitThread.requestSystemCompaction(region, Thread.currentThread().getName());
            } catch (IOException e) {
              e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
              LOG.error("Cache flush failed for region "
                + Bytes.toStringBinary(region.getRegionInfo().getRegionName()), e);
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `ex`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
      return false;
    } catch (IOException ex) {
      ex = ex instanceof RemoteException ? ((RemoteException) ex).unwrapRemoteException() : ex;
      LOG.error("Cache flush failed" + (region != null
        ? (" for region " + Bytes.toStringBinary(region.getRegionInfo().getRegionName()))
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BrokenStoreFileCleaner.java`
#### Snippet
```java
      }
    } catch (IOException e) {
      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
      LOG.warn("Error while deleting: " + filePath, e);
    }
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        } catch (IOException e) {
          if (EnvironmentEdgeManager.currentTime() > (previousLogTime + 1000)) {
            e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
            if (e instanceof ServerNotRunningYetException) {
              LOG.info("Master isn't available yet, retrying");
```

### AssignmentToCatchBlockParameter
Assignment to 'catch' block parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        }
      } catch (Throwable e) {
        e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
        LOG.error("Shutdown / close of WAL failed: " + e);
        LOG.debug("Shutdown / close exception details:", e);
```

## RuleId[id=ConditionalBreakInInfiniteLoop]
### ConditionalBreakInInfiniteLoop
Conditional break inside loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
      if (env.getInstance() instanceof RegionObserver) {
        Class<?> clazz = env.getInstance().getClass();
        for (;;) {
          if (clazz == Object.class) {
            // we dont need to look postScannerFilterRow into Object class
```

### ConditionalBreakInInfiniteLoop
Conditional break inside loop
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java`
#### Snippet
```java
        // we check stopped_ first to make sure we're not supposed to be shutting
        // down. this is necessary for graceful shutdown.
        while (true) {
          if (stopped) {
            break;
```

## RuleId[id=StaticCallOnSubclass]
### StaticCallOnSubclass
Static method `getBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
      } else if (archive.getFileSystem(conf).exists(new Path(archive, file))) {

        Path backRefDir = HFileLink.getBackReferencesDir(archive, file);
        try {
          FileStatus[] backRefs = CommonFSUtils.listStatus(archive.getFileSystem(conf), backRefDir);
```

### StaticCallOnSubclass
Static method `setOutputPath()` declared in class 'org.apache.hadoop.mapreduce.lib.output.FileOutputFormat' but referenced via subclass 'org.apache.hadoop.mapreduce.lib.output.TextOutputFormat'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
      job.setReducerClass(MobRefReducer.class);
      job.setOutputFormatClass(TextOutputFormat.class);
      TextOutputFormat.setOutputPath(job, new Path(output));

      job.setJobName(getClass().getSimpleName() + "-" + tn + "-" + familyName);
```

### StaticCallOnSubclass
Static method `setOutputPath()` declared in class 'org.apache.hadoop.mapreduce.lib.output.FileOutputFormat' but referenced via subclass 'org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      bulkloadDir = generateUniqTempDir(false);
      LOG.info("HFiles will be stored at " + this.bulkloadDir);
      HFileOutputFormat2.setOutputPath(job, bulkloadDir);
      try (Connection conn = ConnectionFactory.createConnection(getConf());
        Admin admin = conn.getAdmin()) {
```

### StaticCallOnSubclass
Static method `configureIncrementalLoad()` declared in class 'org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2' but referenced via subclass 'org.apache.hadoop.hbase.mapreduce.MultiTableHFileOutputFormat'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java`
#### Snippet
```java
  public static void configureIncrementalLoad(Job job, List<TableInfo> multiTableDescriptors)
    throws IOException {
    MultiTableHFileOutputFormat.configureIncrementalLoad(job, multiTableDescriptors,
      MultiTableHFileOutputFormat.class);
  }
```

### StaticCallOnSubclass
Static method `addInputPath()` declared in class 'org.apache.hadoop.mapreduce.lib.input.FileInputFormat' but referenced via subclass 'org.apache.hadoop.hbase.regionserver.CompactionTool.CompactionInputFormat'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
      List<Path> storeDirs =
        CompactionInputFormat.createInputFile(fs, stagingFs, inputPath, toCompactDirs);
      CompactionInputFormat.addInputPath(job, inputPath);

      // Initialize credential for secure cluster
```

### StaticCallOnSubclass
Static method `systemDefault()` declared in class 'java.time.ZoneId' but referenced via subclass 'java.time.ZoneOffset'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/BaseTimeToLiveFileCleaner.java`
#### Snippet
```java

  private static final DateTimeFormatter FORMATTER =
    DateTimeFormatter.ISO_DATE_TIME.withZone(ZoneOffset.systemDefault());

  // Configured time a log can be kept after it was closed
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java`
#### Snippet
```java
      // The back ref can be deleted only if the referenced file doesn't exists.
      Path parentDir = filePath.getParent();
      if (HFileLink.isBackReferencesDir(parentDir)) {
        Path hfilePath = null;
        try {
```

### StaticCallOnSubclass
Static method `getBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java`
#### Snippet
```java

      // HFile is deletable only if has no links
      Path backRefDir = HFileLink.getBackReferencesDir(parentDir, filePath.getName());
      try {
        FileStatus[] fileStatuses = CommonFSUtils.listStatus(fs, backRefDir);
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/TimeToLiveHFileCleaner.java`
#### Snippet
```java
  @Override
  protected boolean valiateFilename(Path file) {
    return HFileLink.isBackReferencesDir(file) || HFileLink.isBackReferencesDir(file.getParent())
      || StoreFileInfo.validateStoreFileName(file.getName());
  }
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/TimeToLiveHFileCleaner.java`
#### Snippet
```java
  @Override
  protected boolean valiateFilename(Path file) {
    return HFileLink.isBackReferencesDir(file) || HFileLink.isBackReferencesDir(file.getParent())
      || StoreFileInfo.validateStoreFileName(file.getName());
  }
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  @Override
  protected boolean validate(Path file) {
    return HFileLink.isBackReferencesDir(file) || HFileLink.isBackReferencesDir(file.getParent())
      || StoreFileInfo.validateStoreFileName(file.getName())
      || file.getName().endsWith(MasterRegionFactory.ARCHIVED_HFILE_SUFFIX);
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  @Override
  protected boolean validate(Path file) {
    return HFileLink.isBackReferencesDir(file) || HFileLink.isBackReferencesDir(file.getParent())
      || StoreFileInfo.validateStoreFileName(file.getName())
      || file.getName().endsWith(MasterRegionFactory.ARCHIVED_HFILE_SUFFIX);
```

### StaticCallOnSubclass
Static method `getNamespaceQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotasObserver.java`
#### Snippet
```java
    }
    final Connection conn = ctx.getEnvironment().getConnection();
    Quotas quotas = QuotaUtil.getNamespaceQuota(conn, namespace);
    if (quotas != null) {
      if (quotas.hasSpace()) {
```

### StaticCallOnSubclass
Static method `getTableQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotasObserver.java`
#### Snippet
```java
    }
    final Connection conn = ctx.getEnvironment().getConnection();
    Quotas tableQuotas = QuotaUtil.getTableQuota(conn, tableName);
    Quotas namespaceQuotas = QuotaUtil.getNamespaceQuota(conn, tableName.getNamespaceAsString());
    if (tableQuotas != null || namespaceQuotas != null) {
```

### StaticCallOnSubclass
Static method `getNamespaceQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotasObserver.java`
#### Snippet
```java
    final Connection conn = ctx.getEnvironment().getConnection();
    Quotas tableQuotas = QuotaUtil.getTableQuota(conn, tableName);
    Quotas namespaceQuotas = QuotaUtil.getNamespaceQuota(conn, tableName.getNamespaceAsString());
    if (tableQuotas != null || namespaceQuotas != null) {
      // Remove regions of table from space quota map.
```

### StaticCallOnSubclass
Static method `getRegionServerQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      public GlobalQuotaSettingsImpl fetch() throws IOException {
        return new GlobalQuotaSettingsImpl(null, null, null, regionServer,
          QuotaUtil.getRegionServerQuota(masterServices.getConnection(), regionServer));
      }

```

### StaticCallOnSubclass
Static method `getTableQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      public GlobalQuotaSettingsImpl fetch() throws IOException {
        return new GlobalQuotaSettingsImpl(null, table, null, null,
          QuotaUtil.getTableQuota(masterServices.getConnection(), table));
      }

```

### StaticCallOnSubclass
Static method `getUserQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      public GlobalQuotaSettingsImpl fetch() throws IOException {
        return new GlobalQuotaSettingsImpl(userName, table, null, null,
          QuotaUtil.getUserQuota(masterServices.getConnection(), userName, table));
      }

```

### StaticCallOnSubclass
Static method `getUserQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      public GlobalQuotaSettingsImpl fetch() throws IOException {
        return new GlobalQuotaSettingsImpl(req.getUserName(), null, null, null,
          QuotaUtil.getUserQuota(masterServices.getConnection(), userName));
      }

```

### StaticCallOnSubclass
Static method `getNamespaceQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      public GlobalQuotaSettingsImpl fetch() throws IOException {
        return new GlobalQuotaSettingsImpl(null, null, namespace, null,
          QuotaUtil.getNamespaceQuota(masterServices.getConnection(), namespace));
      }

```

### StaticCallOnSubclass
Static method `getUserQuota()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
      public GlobalQuotaSettingsImpl fetch() throws IOException {
        return new GlobalQuotaSettingsImpl(userName, null, namespace, null,
          QuotaUtil.getUserQuota(masterServices.getConnection(), userName, namespace));
      }

```

### StaticCallOnSubclass
Static method `makeGetForRegionServerQuotas()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
          @Override
          public Get makeGet(final Map.Entry<String, QuotaState> entry) {
            return QuotaUtil.makeGetForRegionServerQuotas(entry.getKey());
          }

```

### StaticCallOnSubclass
Static method `makeGetForNamespaceQuotas()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
        @Override
        public Get makeGet(final Map.Entry<String, QuotaState> entry) {
          return QuotaUtil.makeGetForNamespaceQuotas(entry.getKey());
        }

```

### StaticCallOnSubclass
Static method `makeGetForTableQuotas()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
        @Override
        public Get makeGet(final Map.Entry<TableName, QuotaState> entry) {
          return QuotaUtil.makeGetForTableQuotas(entry.getKey());
        }

```

### StaticCallOnSubclass
Static method `makeGetForUserQuotas()` declared in class 'org.apache.hadoop.hbase.quotas.QuotaTableUtil' but referenced via subclass 'org.apache.hadoop.hbase.quotas.QuotaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
        @Override
        public Get makeGet(final Map.Entry<String, UserQuotaState> entry) {
          return QuotaUtil.makeGetForUserQuotas(entry.getKey(), tables, namespaces);
        }

```

### StaticCallOnSubclass
Static method `getBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
      if ((c != null) && (c == 1)) {
        Path parentDir = filePath.getParent();
        Path backRefDir = HFileLink.getBackReferencesDir(parentDir, filePath.getName());
        try {
          if (CommonFSUtils.listStatus(fs, backRefDir) == null) {
```

### StaticCallOnSubclass
Static method `getCellLength()` declared in class 'org.apache.hadoop.hbase.regionserver.Segment' but referenced via subclass 'org.apache.hadoop.hbase.regionserver.MutableSegment'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java
  protected boolean checkAndAddToActiveSize(MutableSegment currActive, Cell cellToAdd,
    MemStoreSizing memstoreSizing) {
    long cellSize = MutableSegment.getCellLength(cellToAdd);
    boolean successAdd = false;
    while (true) {
```

### StaticCallOnSubclass
Static method `isDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      HRegion region = server.getRegionByEncodedName(regionName.toStringUtf8());
      RegionCoprocessorHost coprocessorHost =
        ServerRegionReplicaUtil.isDefaultReplica(region.getRegionInfo())
          ? region.getCoprocessorHost()
          : null; // do not invoke coprocessors if this is a secondary region replica
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BrokenStoreFileCleaner.java`
#### Snippet
```java

  boolean validate(Path file) {
    if (HFileLink.isBackReferencesDir(file) || HFileLink.isBackReferencesDir(file.getParent())) {
      return true;
    }
```

### StaticCallOnSubclass
Static method `isBackReferencesDir()` declared in class 'org.apache.hadoop.hbase.io.FileLink' but referenced via subclass 'org.apache.hadoop.hbase.io.HFileLink'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BrokenStoreFileCleaner.java`
#### Snippet
```java

  boolean validate(Path file) {
    if (HFileLink.isBackReferencesDir(file) || HFileLink.isBackReferencesDir(file.getParent())) {
      return true;
    }
```

### StaticCallOnSubclass
Static method `isDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
   */
  private void triggerFlushInPrimaryRegion(final HRegion region) {
    if (ServerRegionReplicaUtil.isDefaultReplica(region.getRegionInfo())) {
      return;
    }
```

### StaticCallOnSubclass
Static method `getRegionInfoForDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
    if (LOG.isDebugEnabled()) {
      LOG.debug("RPC'ing to primary "
        + ServerRegionReplicaUtil.getRegionInfoForDefaultReplica(region.getRegionInfo())
          .getRegionNameAsString()
        + " from " + region.getRegionInfo().getRegionNameAsString() + " to trigger FLUSH");
```

### StaticCallOnSubclass
Static method `getRegionInfoForDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
      try {
        response = FutureUtils.get(connection.flush(ServerRegionReplicaUtil
          .getRegionInfoForDefaultReplica(region.getRegionInfo()).getRegionName(), true));
      } catch (IOException e) {
        if (
```

### StaticCallOnSubclass
Static method `getRegionInfoForDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
        // frequent...
        LOG.debug("Failed to trigger a flush of primary region replica {} of region {}, retry={}",
          ServerRegionReplicaUtil.getRegionInfoForDefaultReplica(region.getRegionInfo())
            .getRegionNameAsString(),
          region.getRegionInfo().getRegionNameAsString(), counter.getAttemptTimes(), e);
```

### StaticCallOnSubclass
Static method `getRegionInfoForDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
        if (LOG.isDebugEnabled()) {
          LOG.debug("Triggered flush of primary region replica "
            + ServerRegionReplicaUtil.getRegionInfoForDefaultReplica(region.getRegionInfo())
              .getRegionNameAsString()
            + " for " + region.getRegionInfo().getEncodedName()
```

### StaticCallOnSubclass
Static method `getRegionInfoForDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
            if (LOG.isDebugEnabled()) {
              LOG.debug("Triggered empty flush marker (memstore empty) on primary region replica "
                + ServerRegionReplicaUtil.getRegionInfoForDefaultReplica(region.getRegionInfo())
                  .getRegionNameAsString()
                + " for " + region.getRegionInfo().getEncodedName()
```

### StaticCallOnSubclass
Static method `getRegionInfoForDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
                "Cannot cause primary replica {} to flush or drop a wal marker "
                  + "for region replica {}, retry={}",
                ServerRegionReplicaUtil.getRegionInfoForDefaultReplica(region.getRegionInfo())
                  .getRegionNameAsString(),
                region.getRegionInfo().getRegionNameAsString(), counter.getAttemptTimes());
```

### StaticCallOnSubclass
Static method `isDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    startRegionOperation(Operation.REPLAY_EVENT);
    try {
      if (ServerRegionReplicaUtil.isDefaultReplica(this.getRegionInfo())) {
        return; // if primary nothing to do
      }
```

### StaticCallOnSubclass
Static method `isDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    checkTargetRegion(flush.getEncodedRegionName().toByteArray(), "Flush marker from WAL ", flush);

    if (ServerRegionReplicaUtil.isDefaultReplica(this.getRegionInfo())) {
      return; // if primary nothing to do
    }
```

### StaticCallOnSubclass
Static method `isDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      justification = "Notify is about post replay. Intentional")
  protected boolean refreshStoreFiles(boolean force) throws IOException {
    if (!force && ServerRegionReplicaUtil.isDefaultReplica(this.getRegionInfo())) {
      return false; // if primary nothing to do
    }
```

### StaticCallOnSubclass
Static method `isDefaultReplica()` declared in class 'org.apache.hadoop.hbase.client.RegionReplicaUtil' but referenced via subclass 'org.apache.hadoop.hbase.util.ServerRegionReplicaUtil'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      "BulkLoad marker from WAL ", bulkLoadEvent);

    if (ServerRegionReplicaUtil.isDefaultReplica(this.getRegionInfo())) {
      return; // if primary nothing to do
    }
```

## RuleId[id=UnnecessaryStringEscape]
### UnnecessaryStringEscape
`\'` is unnecessarily escaped
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java`
#### Snippet
```java
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("{NAME => \'");
    sb.append(namespaceName);
    sb.append("\'");
```

### UnnecessaryStringEscape
`\'` is unnecessarily escaped
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java`
#### Snippet
```java
    sb.append("{NAME => \'");
    sb.append(namespaceName);
    sb.append("\'");
    if (properties != null) {
      for (Map.Entry<String, String> entry : properties.entrySet()) {
```

### UnnecessaryStringEscape
`\'` is unnecessarily escaped
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java`
#### Snippet
```java
        sb.append(" => '");
        sb.append(entry.getValue());
        sb.append("\'");
      }
    }
```

### UnnecessaryStringEscape
`\'` is unnecessarily escaped
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    + "  set        backup set management\n" + "  repair     repair backup system table\n"
    + "  merge      merge backup images\n"
    + "Run \'hbase backup COMMAND -h\' to see help message for each command\n";

  public static final String CREATE_CMD_USAGE =
```

### UnnecessaryStringEscape
`\'` is unnecessarily escaped
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    + "  set        backup set management\n" + "  repair     repair backup system table\n"
    + "  merge      merge backup images\n"
    + "Run \'hbase backup COMMAND -h\' to see help message for each command\n";

  public static final String CREATE_CMD_USAGE =
```

## RuleId[id=ManualArrayCopy]
### ManualArrayCopy
Manual array copy
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
      byte[] aPaddedAdditional = new byte[aPadded.length + 1];
      byte[] bPaddedAdditional = new byte[bPadded.length + 1];
      for (int i = 0; i < aPadded.length; i++) {
        aPaddedAdditional[i] = aPadded[i];
      }
```

### ManualArrayCopy
Manual array copy
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
        aPaddedAdditional[i] = aPadded[i];
      }
      for (int j = 0; j < bPadded.length; j++) {
        bPaddedAdditional[j] = bPadded[j];
      }
```

## RuleId[id=ObsoleteCollection]
### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
   * @return the evaluated filter
   */
  public static Filter popArguments(Stack<ByteBuffer> operatorStack, Stack<Filter> filterStack) {
    ByteBuffer argumentOnTopOfStack = operatorStack.peek();

```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
   * @return the evaluated filter
   */
  public static Filter popArguments(Stack<ByteBuffer> operatorStack, Stack<Filter> filterStack) {
    ByteBuffer argumentOnTopOfStack = operatorStack.peek();

```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
  public Filter parseFilterString(byte[] filterStringAsByteArray) throws CharacterCodingException {
    // stack for the operators and parenthesis
    Stack<ByteBuffer> operatorStack = new Stack<>();
    // stack for the filter objects
    Stack<Filter> filterStack = new Stack<>();
```

### ObsoleteCollection
Obsolete collection type `Stack<>` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
  public Filter parseFilterString(byte[] filterStringAsByteArray) throws CharacterCodingException {
    // stack for the operators and parenthesis
    Stack<ByteBuffer> operatorStack = new Stack<>();
    // stack for the filter objects
    Stack<Filter> filterStack = new Stack<>();
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
    Stack<ByteBuffer> operatorStack = new Stack<>();
    // stack for the filter objects
    Stack<Filter> filterStack = new Stack<>();

    Filter filter = null;
```

### ObsoleteCollection
Obsolete collection type `Stack<>` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
    Stack<ByteBuffer> operatorStack = new Stack<>();
    // stack for the filter objects
    Stack<Filter> filterStack = new Stack<>();

    Filter filter = null;
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
   * @param operator      the operator found while parsing the filterString
   */
  public void reduce(Stack<ByteBuffer> operatorStack, Stack<Filter> filterStack,
    ByteBuffer operator) {
    while (
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
   * @param operator      the operator found while parsing the filterString
   */
  public void reduce(Stack<ByteBuffer> operatorStack, Stack<Filter> filterStack,
    ByteBuffer operator) {
    while (
```

### ObsoleteCollection
Obsolete collection type `Hashtable` used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
  }

  public static ObjectName buildObjectName(String domain, Hashtable<String, String> keyValueTable)
    throws MalformedObjectNameException {
    return new ObjectName(domain, keyValueTable);
```

### ObsoleteCollection
Obsolete collection type `Hashtable` used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
   */
  @SuppressWarnings("JdkObsolete") // javax requires hashtable param for ObjectName constructor
  public static Hashtable<String, String> buldKeyValueTable(String[] keys, String[] values) {
    if (keys.length != values.length) {
      LOG.error("keys and values arrays must be same size");
```

### ObsoleteCollection
Obsolete collection type `Hashtable` used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
      return null;
    }
    Hashtable<String, String> table = new Hashtable<>();
    for (int i = 0; i < keys.length; i++) {
      table.put(keys[i], values[i]);
```

### ObsoleteCollection
Obsolete collection type `Hashtable<>` used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
      return null;
    }
    Hashtable<String, String> table = new Hashtable<>();
    for (int i = 0; i < keys.length; i++) {
      table.put(keys[i], values[i]);
```

### ObsoleteCollection
Obsolete collection type `Vector` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    @Override
    public synchronized Void call() throws InterruptedException, ExecutionException {
      final Vector<Exception> exceptions = new Vector<>();

      try {
```

### ObsoleteCollection
Obsolete collection type `Vector<>` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    @Override
    public synchronized Void call() throws InterruptedException, ExecutionException {
      final Vector<Exception> exceptions = new Vector<>();

      try {
```

### ObsoleteCollection
Obsolete collection type `Vector` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    // should be regions.
    final FamilyDirFilter familyFilter = new FamilyDirFilter(fs);
    final Vector<Exception> exceptions = new Vector<>();

    try {
```

### ObsoleteCollection
Obsolete collection type `Vector<>` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    // should be regions.
    final FamilyDirFilter familyFilter = new FamilyDirFilter(fs);
    final Vector<Exception> exceptions = new Vector<>();

    try {
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  }

  private void processOpenParan(Stack<ExpressionNode> expStack, String expS, int index)
    throws ParseException {
    if (!expStack.isEmpty()) {
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  }

  private void processCloseParan(Stack<ExpressionNode> expStack, String expS, int index)
    throws ParseException {
    if (expStack.size() < 2) {
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  public ExpressionNode parse(String expS) throws ParseException {
    expS = expS.trim();
    Stack<ExpressionNode> expStack = new Stack<>();
    int index = 0;
    byte[] exp = Bytes.toBytes(expS);
```

### ObsoleteCollection
Obsolete collection type `Stack<>` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  public ExpressionNode parse(String expS) throws ParseException {
    expS = expS.trim();
    Stack<ExpressionNode> expStack = new Stack<>();
    int index = 0;
    byte[] exp = Bytes.toBytes(expS);
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  }

  private void processANDorOROp(Operator op, Stack<ExpressionNode> expStack, String expS, int index)
    throws ParseException {
    if (expStack.isEmpty()) {
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  }

  private void processNOTOp(Stack<ExpressionNode> expStack, String expS, int index)
    throws ParseException {
    // When ! comes, the stack can be empty or top ( or top can be some exp like
```

### ObsoleteCollection
Obsolete collection type `Stack` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
  }

  private void processLabelExpNode(LeafExpressionNode node, Stack<ExpressionNode> expStack,
    String expS, int index) throws ParseException {
    if (expStack.isEmpty()) {
```

### ObsoleteCollection
Obsolete collection type `Hashtable` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.java`
#### Snippet
```java
public class MasterProcedureManagerHost extends ProcedureManagerHost<MasterProcedureManager> {

  private Hashtable<String, MasterProcedureManager> procedureMgrMap = new Hashtable<>();

  @Override
```

### ObsoleteCollection
Obsolete collection type `Hashtable<>` used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.java`
#### Snippet
```java
public class MasterProcedureManagerHost extends ProcedureManagerHost<MasterProcedureManager> {

  private Hashtable<String, MasterProcedureManager> procedureMgrMap = new Hashtable<>();

  @Override
```

## RuleId[id=RefusedBequest]
### RefusedBequest
Method `clone()` does not call 'super.clone()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
      justification = "The clone below is complete")
  @Override
  public BufferedMutatorParams clone() {
    BufferedMutatorParams clone = new BufferedMutatorParams(this.tableName);
    clone.writeBufferSize = this.writeBufferSize;
```

## RuleId[id=LongLiteralsEndingWithLowercaseL]
### LongLiteralsEndingWithLowercaseL
'long' literal `0xffl` ends with lowercase 'l'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java
    long tmpLength = 0;
    for (int i = 0; i < fitInBytes; ++i) {
      tmpLength |= (in.get() & 0xffl) << (8l * i);
    }
    return tmpLength;
```

### LongLiteralsEndingWithLowercaseL
'long' literal `8l` ends with lowercase 'l'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java
    long tmpLength = 0;
    for (int i = 0; i < fitInBytes; ++i) {
      tmpLength |= (in.get() & 0xffl) << (8l * i);
    }
    return tmpLength;
```

### LongLiteralsEndingWithLowercaseL
'long' literal `1l` ends with lowercase 'l'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java`
#### Snippet
```java
  @Override
  public Long increment(Long o) {
    return o == null ? null : (o + 1l);
  }

```

## RuleId[id=RedundantArrayCreation]
### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/FileChangeWatcher.java`
#### Snippet
```java

    dirPath.register(watchService,
      new WatchEvent.Kind<?>[] { StandardWatchEventKinds.ENTRY_CREATE,
        StandardWatchEventKinds.ENTRY_DELETE, StandardWatchEventKinds.ENTRY_MODIFY,
        StandardWatchEventKinds.OVERFLOW });
    state = State.NEW;
    this.watcherThread = new WatcherThread(watchService, callback);
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
  /** Directories that are not HBase table directories */
  public static final List<String> HBASE_NON_TABLE_DIRS = Collections.unmodifiableList(
    Arrays.asList(new String[] { HBCK_SIDELINEDIR_NAME, HBASE_TEMP_DIRECTORY, MIGRATION_NAME }));

  /**
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/util/RecoverLeaseFSUtils.java`
#### Snippet
```java
              try {
                isFileClosedMeth =
                  dfs.getClass().getMethod("isFileClosed", new Class[] { Path.class });
              } catch (NoSuchMethodException nsme) {
                LOG.debug("isFileClosed not available");
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
    Method m = null;
    try {
      m = context.getClass().getMethod("getCounter", new Class[] { String.class, String.class });
    } catch (SecurityException e) {
      throw new IOException("Failed test for getCounter", e);
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java`
#### Snippet
```java
      String sourceScheme = sourceClusterPath.toUri().getScheme();
      String disableCacheName =
        String.format("fs.%s.impl.disable.cache", new Object[] { sourceScheme });
      sourceClusterConf.setBoolean(disableCacheName, true);

```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
          conf.getClass(MEMSTORE_CLASS_NAME, DefaultMemStore.class, MemStore.class);
        ms = ReflectionUtils.newInstance(memStoreClass,
          new Object[] { conf, getComparator(), this.getHRegion().getRegionServicesForStores() });
        break;
      default:
```

### RedundantArrayCreation
Redundant array creation for calling varargs method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
          conf.getClass(MEMSTORE_CLASS_NAME, CompactingMemStore.class, CompactingMemStore.class);
        ms =
          ReflectionUtils.newInstance(compactingMemStoreClass, new Object[] { conf, getComparator(),
            this, this.getHRegion().getRegionServicesForStores(), inMemoryCompaction });
    }
    return ms;
```

## RuleId[id=DuplicateBranchesInSwitch]
### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        return src.getPosition() - start;
      case ZERO:
        return 1;
      case POS_SMALL: /* Small positive number: 0x16, ~-E, M */
        skipVaruint64(src, DESCENDING != ord);
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        return src.getPosition() - start;
      case POS_INF:
        return 1;
      case NAN:
        return 1;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        return 1;
      case NAN:
        return 1;
      case FIXED_INT8:
        src.setPosition(src.getPosition() + 1);
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        return src.getPosition() - start;
      case FIXED_FLOAT32:
        src.setPosition(src.getPosition() + 4);
        return src.getPosition() - start;
      case FIXED_FLOAT64:
        src.setPosition(src.getPosition() + 8);
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        return src.getPosition() - start;
      case FIXED_FLOAT64:
        src.setPosition(src.getPosition() + 8);
        return src.getPosition() - start;
      case TEXT:
        // for null-terminated values, skip to the end.
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java`
#### Snippet
```java
    switch (poolType) {
      case RoundRobin:
        return new RoundRobinPool<>(poolMaxSize);
      case ThreadLocal:
        return new ThreadLocalPool<>();
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    switch (consistency) {
      case STRONG:
        return Consistency.STRONG;
      case TIMELINE:
        return Consistency.TIMELINE;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    switch (proto) {
      case USE_DEFAULT:
        return Durability.USE_DEFAULT;
      case SKIP_WAL:
        return Durability.SKIP_WAL;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAclReset.java`
#### Snippet
```java
    for (String arg : args) {
      switch (arg) {
        case "-help": {
          printUsageAndExit();
          break;
        }
        case "-set-acls": {
          eraseAcls = false;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      switch (taskType) {
        case READ:
          return read();
        case WRITE:
          return write();
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java`
#### Snippet
```java
          break;
        case MERGE_TABLE_REGIONS_CHECK_CLOSED_REGIONS:
          break;
        case MERGE_TABLE_REGIONS_CLOSE_REGIONS:
          rollbackCloseRegionsForMerge(env);
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java`
#### Snippet
```java
          break;
        case MERGE_TABLE_REGIONS_PREPARE:
          break;
        default:
          throw new UnsupportedOperationException(this + " unhandled state=" + state);
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java`
#### Snippet
```java
         * changed.
         */
        SnapshotSubprocedurePool taskManager2 =
          new SnapshotSubprocedurePool(rss.getServerName().toString(), conf, rss);
        return new FlushSnapshotSubprocedure(member, exnDispatcher, wakeMillis, timeoutMillis,
          involvedRegions, snapshot, taskManager2);

      default:
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (in.getValue()) {
      case 0:
        return KeepDeletedCells.FALSE;
      case 1:
        return KeepDeletedCells.TRUE;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (readType) {
      case DEFAULT:
        return TReadType.DEFAULT;
      case STREAM:
        return TReadType.STREAM;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (in.getValue()) {
      case 0:
        return DataBlockEncoding.NONE;
      case 2:
        return DataBlockEncoding.PREFIX;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return TCompressionAlgorithm.GZ;
      case NONE:
        return TCompressionAlgorithm.NONE;
      case SNAPPY:
        return TCompressionAlgorithm.SNAPPY;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return Compression.Algorithm.GZ;
      case 2:
        return Compression.Algorithm.NONE;
      case 3:
        return Compression.Algorithm.SNAPPY;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (tConsistency.getValue()) {
      case 1:
        return Consistency.STRONG;
      case 2:
        return Consistency.TIMELINE;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return TBloomFilterType.NONE;
      case ROW:
        return TBloomFilterType.ROW;
      case ROWCOL:
        return TBloomFilterType.ROWCOL;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (tDurability.getValue()) {
      case 0:
        return Durability.USE_DEFAULT;
      case 1:
        return Durability.SKIP_WAL;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (in) {
      case FALSE:
        return TKeepDeletedCells.FALSE;
      case TRUE:
        return TKeepDeletedCells.TRUE;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (consistency) {
      case STRONG:
        return TConsistency.STRONG;
      case TIMELINE:
        return TConsistency.TIMELINE;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
    switch (in) {
      case NONE:
        return TDataBlockEncoding.NONE;
      case PREFIX:
        return TDataBlockEncoding.PREFIX;
```

### DuplicateBranchesInSwitch
Branch in 'switch' is a duplicate of the default branch
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        return BloomType.NONE;
      case 1:
        return BloomType.ROW;
      case 2:
        return BloomType.ROWCOL;
```

### DuplicateBranchesInSwitch
Duplicate branch in 'switch'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        case ASYNC_WAL:
          // nothing do to
          break;
        case SYNC_WAL:
          this.wal.sync(txid, false);
```

## RuleId[id=MisspelledEquals]
### MisspelledEquals
`equal()` method should probably be 'equals()'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
    }

    public RecordFilter equal(FieldValue value) {
      return newFilter(false, Operator.EQUAL, value);
    }
```

### MisspelledEquals
`equal()` method should probably be 'equals()'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/RecordFilter.java`
#### Snippet
```java
    }

    public RecordFilter equal(Object value) {
      return equal(field.newValue(value));
    }
```

## RuleId[id=RegExpSimplifiable]
### RegExpSimplifiable
`[\d]` can be simplified to '\\d'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ServerName.java`
#### Snippet
```java
  public static final Pattern SERVERNAME_PATTERN =
    Pattern.compile("[^" + SERVERNAME_SEPARATOR + "]+" + SERVERNAME_SEPARATOR
      + Addressing.VALID_PORT_REGEX + SERVERNAME_SEPARATOR + Addressing.VALID_PORT_REGEX + "$");

  /**
```

### RegExpSimplifiable
`[/]` can be simplified to '/'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java`
#### Snippet
```java
    new Pattern[] { Pattern.compile("^[^-]+-default\\.xml$") };

  private static final Pattern libJarPattern = Pattern.compile("[/]?lib/([^/]+\\.jar)");

  /**
```

### RegExpSimplifiable
`[\\s]` can be simplified to '\\s'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   */
  private static final Pattern CP_HTD_ATTR_VALUE_PATTERN =
    Pattern.compile("(^[^\\|]*)\\|([^\\|]+)\\|[\\s]*([\\d]*)[\\s]*(\\|.*)?$");

  private static final String CP_HTD_ATTR_VALUE_PARAM_KEY_PATTERN = "[^=,]+";
```

### RegExpSimplifiable
`[\\d]` can be simplified to '\\d'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   */
  private static final Pattern CP_HTD_ATTR_VALUE_PATTERN =
    Pattern.compile("(^[^\\|]*)\\|([^\\|]+)\\|[\\s]*([\\d]*)[\\s]*(\\|.*)?$");

  private static final String CP_HTD_ATTR_VALUE_PARAM_KEY_PATTERN = "[^=,]+";
```

### RegExpSimplifiable
`[\\s]` can be simplified to '\\s'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   */
  private static final Pattern CP_HTD_ATTR_VALUE_PATTERN =
    Pattern.compile("(^[^\\|]*)\\|([^\\|]+)\\|[\\s]*([\\d]*)[\\s]*(\\|.*)?$");

  private static final String CP_HTD_ATTR_VALUE_PARAM_KEY_PATTERN = "[^=,]+";
```

## RuleId[id=DuplicateExpressions]
### DuplicateExpressions
Multiple occurrences of `key.toString() + "; " + rp.toString()`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
      // If null here, it means node has been removed because it crashed. This happens when server
      // is expired in ServerManager. ServerCrashProcedure may or may not have run.
      throw new NoServerDispatchException(key.toString() + "; " + rp.toString());
    }
    node.add(rp);
```

### DuplicateExpressions
Multiple occurrences of `key.toString() + "; " + rp.toString()`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
    // Check our node still in the map; could have been removed by #removeNode.
    if (!nodeMap.containsValue(node)) {
      throw new NoNodeDispatchException(key.toString() + "; " + rp.toString());
    }
  }
```

## RuleId[id=StringBufferReplaceableByString]
### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TimeRange.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("maxStamp=");
    sb.append(this.maxStamp);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("Name:");
    sb.append(this.name);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerTaskBuilder.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder(512);
      sb.append(getDescription());
      sb.append(": status=");
```

### StringBufferReplaceableByString
`StringBuilder s` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder s = new StringBuilder();
      s.append('{');
      s.append(HConstants.NAME);
```

### StringBufferReplaceableByString
`StringBuilder s` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    @Override
    public String toStringCustomizedValues() {
      StringBuilder s = new StringBuilder();
      s.append('{');
      s.append(HConstants.NAME);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionStatesCount.java`
#### Snippet
```java
  @Override
  public String toString() {
    final StringBuilder sb = new StringBuilder("RegionStatesCount{");
    sb.append("openRegions=").append(openRegions);
    sb.append(", splitRegions=").append(splitRegions);
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalancerRejection.java`
#### Snippet
```java
        costFuncInfoList = new ArrayList<>();
      }
      costFuncInfoList.add(new StringBuilder().append(funcName).append(" cost:").append(cost)
        .append(" multiplier:").append(multiplier).toString());
    }
```

### StringBufferReplaceableByString
`StringBuilder builder` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionCoprocessorServiceExec.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder builder = new StringBuilder();
    builder.append("region:").append(Bytes.toStringBinary(region)).append(", startKey:")
      .append(Bytes.toStringBinary(startKey)).append(", method:").append(method.getFullName())
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshot.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder(32);
    sb.append("SpaceQuotaSnapshot[policy=").append(quotaStatus).append(", use=");
    sb.append(StringUtils.byteDesc(usage)).append("/");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshot.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder(getClass().getSimpleName());
      sb.append("[policy=").append(policy);
      sb.append(", inViolation=").append(inViolation).append("]");
```

### StringBufferReplaceableByString
`StringBuilder raw` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/NamespacePermission.java`
#### Snippet
```java
  @Override
  protected String rawExpression() {
    StringBuilder raw = new StringBuilder("namespace=").append(namespace).append(", ");
    return raw.toString() + super.rawExpression();
  }
```

### StringBufferReplaceableByString
`StringBuilder str` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/UserPermission.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder str = new StringBuilder("UserPermission: ").append("user=").append(user)
      .append(", ").append(permission.toString());
    return str.toString();
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/snapshot/ClientSnapshotDescriptionUtils.java`
#### Snippet
```java
    }

    return new StringBuilder("{ ss=").append(snapshot.getName()).append(" table=")
      .append(snapshot.hasTable() ? TableName.valueOf(snapshot.getTable()) : "").append(" type=")
      .append(snapshot.getType()).append(" ttl=").append(snapshot.getTtl()).append(" }").toString();
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZNodePaths.java`
#### Snippet
```java
  @Override
  public String toString() {
    return new StringBuilder().append("ZNodePaths [baseZNode=").append(baseZNode)
      .append(", rsZNode=").append(rsZNode).append(", drainingZNode=").append(drainingZNode)
      .append(", masterAddressZNode=").append(masterAddressZNode)
```

### StringBufferReplaceableByString
`StringBuilder builder` can be replaced with 'String'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerDescription.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder builder = new StringBuilder("id : ").append(id);
    builder.append(", enabled : " + enabled);
    builder.append(", config : " + config);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/VersionModel.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("rest ");
    sb.append(restVersion);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Cluster.java`
#### Snippet
```java
   */
  public Cluster remove(String name, int port) {
    StringBuilder sb = new StringBuilder();
    sb.append(name);
    sb.append(':');
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Cluster.java`
#### Snippet
```java
   */
  public Cluster add(String name, int port) {
    StringBuilder sb = new StringBuilder();
    sb.append(name);
    sb.append(':');
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java`
#### Snippet
```java
        "Invalid encoded region name: " + encodedRegionName + ", length should be 32.");
    }
    return new StringBuilder(regionsZNode).append(ZNodePaths.ZNODE_PATH_SEPARATOR)
      .append(encodedRegionName, 0, 2).append(ZNodePaths.ZNODE_PATH_SEPARATOR)
      .append(encodedRegionName, 2, 4).append(ZNodePaths.ZNODE_PATH_SEPARATOR)
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableSplit.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("HBase table split(");
    sb.append("table name: ").append(m_tableName);
```

### StringBufferReplaceableByString
`StringBuilder footerBuilder` can be replaced with 'String'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
  @Override
  protected void printUsage() {
    StringBuilder footerBuilder = new StringBuilder();
    footerBuilder.append("For performance, consider the following configuration properties:\n");
    footerBuilder.append("-Dhbase.client.scanner.caching=100\n");
```

### StringBufferReplaceableByString
`StringBuilder url` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
    throws IOException {
    // Build jmxURL
    StringBuilder url = new StringBuilder();
    url.append("service:jmx:rmi://localhost:");
    url.append(rmiConnectorPort);
```

### StringBufferReplaceableByString
`StringBuilder strBalanceParam` can be replaced with 'String'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java

    // Using to check balance result.
    StringBuilder strBalanceParam = new StringBuilder();
    strBalanceParam.append("Balance parameter: numRegions=").append(numRegions)
      .append(", numServers=").append(numServers).append(", max=").append(max).append(", min=")
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilter.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append(BloomFilterUtil.formatStats(this));
    sb.append(BloomFilterUtil.STATS_RECORD_SEP + "Number of chunks: " + numChunks);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java

  private String getLexicalErrorMessage(Cell cell) {
    StringBuilder sb = new StringBuilder();
    sb.append("Added a key not lexically larger than previous. Current cell = ");
    sb.append(cell);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/CacheFullException.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder(1024);
    sb.append("Allocator requested size ").append(requestedSize);
    sb.append(" for bucket ").append(bucketIndex);
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    ServerName serverName = getServerNameFromWALDirectoryName(path);
    // Try finding the log in separate old log dir
    oldLogDir = new Path(walRootDir, new StringBuilder(HConstants.HREGION_OLDLOGDIR_NAME)
      .append(Path.SEPARATOR).append(serverName.getServerName()).toString());
    archivedLogLocation = new Path(oldLogDir, path.getName());
```

### StringBufferReplaceableByString
`StringBuilder dirName` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
   */
  public static String getWALDirectoryName(final String serverName) {
    StringBuilder dirName = new StringBuilder(HConstants.HREGION_LOGDIR_NAME);
    dirName.append("/");
    dirName.append(serverName);
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  @Override
  public synchronized String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("{ meta => ");
    sb.append((metaEntry != null) ? metaEntry.hri.getRegionNameAsString() : "null");
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/FailedArchiveException.java`
#### Snippet
```java
  @Override
  public String getMessage() {
    return new StringBuilder(super.getMessage()).append("; files=").append(failedFiles).toString();
  }
}
```

### StringBufferReplaceableByString
`StringBuilder builder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
    executor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);
    if (!ERRORS.isEmpty()) {
      StringBuilder builder = new StringBuilder().append("Major compaction failed, there were: ")
        .append(ERRORS.size()).append(" regions / stores that failed compacting\n")
        .append("Failed compaction requests\n").append("--------------------------\n")
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java`
#### Snippet
```java
    @Override
    public String toString() {
      final StringBuilder sb = new StringBuilder(32);
      sb.append(getClass().getSimpleName()).append(": tablesWithTableQuotas=")
        .append(this.tablesWithTableQuotas).append(", tablesWithNamespaceQuotas=")
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder(32);
      sb.append("SizeSnapshotWithTimestamp={size=").append(size).append("B, ");
      sb.append("time=").append(time).append("}");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append(getClass().getSimpleName()).append("[");
    sb.append("tableName=").append(tn).append(", currentSnapshots=");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder();
      return sb.append("StoreFileReference[region=").append(regionName).append(", files=")
        .append(familyToFiles).append("]").toString();
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder(32);
      return sb.append("SnapshotWithSize:[").append(name).append(" ")
        .append(StringUtils.byteDesc(size)).append("]").toString();
```

### StringBufferReplaceableByString
`StringBuilder buf` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationKey.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder buf = new StringBuilder();
    buf.append("AuthenticationKey[").append("id=").append(id).append(", expiration=")
      .append(Instant.ofEpochMilli(this.expirationDate)).append(", obj=").append(super.toString())
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder(512);
    sb.append(getDescription());
    sb.append(": status=");
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerPayload.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder(this.getClass().getSimpleName());
    sb.append("[");
    sb.append("rsName=").append(rsName);
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetricsCoprocessor.java`
#### Snippet
```java

  private static String suffix(String metricName, String cpName) {
    return new StringBuilder(metricName).append(".").append("CP_").append(cpName).toString();
  }

```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
  private String getHFilePath(TableName table, BulkLoadDescriptor bld, String storeFile,
    byte[] family) {
    return new StringBuilder(100).append(table.getNamespaceAsString()).append(Path.SEPARATOR)
      .append(table.getQualifierAsString()).append(Path.SEPARATOR)
      .append(Bytes.toString(bld.getEncodedRegionName().toByteArray())).append(Path.SEPARATOR)
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append("{");

```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append("{");

```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("{");

```

### StringBufferReplaceableByString
`StringBuilder builder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      return null;
    }
    StringBuilder builder = new StringBuilder();
    builder.append("table: ").append(scanner.getRegionInfo().getTable().getNameAsString());
    builder.append(" region: ").append(scanner.getRegionInfo().getRegionNameAsString());
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java

  private static String qualifyMetrics(String prefix, TableName tableName) {
    StringBuilder sb = new StringBuilder();
    sb.append(prefix).append("_");
    sb.append("Namespace_").append(tableName.getNamespaceAsString());
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
  protected final void postSync(long timeInNanos, int handlerSyncs) {
    if (timeInNanos > this.slowSyncNs) {
      String msg = new StringBuilder().append("Slow sync cost: ")
        .append(TimeUnit.NANOSECONDS.toMillis(timeInNanos)).append(" ms, current pipeline: ")
        .append(Arrays.toString(getPipeline())).toString();
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java

  public String getTableListAsString() {
    StringBuilder sb = new StringBuilder();
    sb.append("{");
    sb.append(StringUtils.join(backupTableInfoMap.keySet(), ","));
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java

  public String getStatusAndProgressAsString() {
    StringBuilder sb = new StringBuilder();
    sb.append("id: ").append(getBackupId()).append(" state: ").append(getState())
      .append(" progress: ").append(getProgress());
```

### StringBufferReplaceableByString
`StringBuilder sb` can be replaced with 'String'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupSet.java`
#### Snippet
```java
  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append(name).append("={");
    sb.append(StringUtils.join(tables, ','));
```

### StringBufferReplaceableByString
`StringBuilder` can be replaced with 'String'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    @Override
    public String toString() {
      return new StringBuilder().append("flush result:").append(result).append(", ")
        .append("failureReason:").append(failureReason).append(",").append("flush seq id")
        .append(flushSequenceId).toString();
```

### StringBufferReplaceableByString
`StringBuilder builder` can be replaced with 'String'
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java
    @Override
    public String toString() {
      StringBuilder builder = new StringBuilder(35);
      return builder.append("rowCount=").append(rowCount).append(", cellCount=").append(cellCount)
        .toString();
```

## RuleId[id=NonShortCircuitBoolean]
### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasFilters |= StringUtils.isNotEmpty(regex)`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
  public QuotaFilter setUserFilter(final String regex) {
    this.userRegex = regex;
    hasFilters |= StringUtils.isNotEmpty(regex);
    return this;
  }
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasFilters |= StringUtils.isNotEmpty(regex)`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
  public QuotaFilter setNamespaceFilter(final String regex) {
    this.namespaceRegex = regex;
    hasFilters |= StringUtils.isNotEmpty(regex);
    return this;
  }
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasFilters |= StringUtils.isNotEmpty(regex)`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
  public QuotaFilter setTableFilter(final String regex) {
    this.tableRegex = regex;
    hasFilters |= StringUtils.isNotEmpty(regex);
    return this;
  }
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasFilters |= true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
  public QuotaFilter addTypeFilter(final QuotaType type) {
    this.types.add(type);
    hasFilters |= true;
    return this;
  }
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasFilters |= StringUtils.isNotEmpty(regex)`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
  public QuotaFilter setRegionServerFilter(final String regex) {
    this.regionServerRegex = regex;
    hasFilters |= StringUtils.isNotEmpty(regex);
    return this;
  }
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasSettings |= quotas.hasThrottle()`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  public static boolean isEmptyQuota(final Quotas quotas) {
    boolean hasSettings = false;
    hasSettings |= quotas.hasThrottle();
    hasSettings |= quotas.hasBypassGlobals();
    // Only when there is a space quota, make sure there's actually both fields provided
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasSettings |= quotas.hasBypassGlobals()`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
    boolean hasSettings = false;
    hasSettings |= quotas.hasThrottle();
    hasSettings |= quotas.hasBypassGlobals();
    // Only when there is a space quota, make sure there's actually both fields provided
    // Otherwise, it's a noop.
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasSettings |= (quotas.getSpace().hasSoftLimit() && quotas.getSpace().hasViolationPolicy())`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
    // Otherwise, it's a noop.
    if (quotas.hasSpace()) {
      hasSettings |= (quotas.getSpace().hasSoftLimit() && quotas.getSpace().hasViolationPolicy());
    }
    return !hasSettings;
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `e instanceof JsonMappingException | e instanceof JsonParseException`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
      } else if (
        e instanceof RuntimeException
          || e instanceof JsonMappingException | e instanceof JsonParseException
      ) {
        return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `abortRollback |= !isRunning() || !store.isRunning()`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
        releaseLock(proc, false);
        boolean abortRollback = lockState != LockState.LOCK_ACQUIRED;
        abortRollback |= !isRunning() || !store.isRunning();

        // allows to kill the executor before something is stored to the wal.
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `includesUserTables |= !hri.getTable().isSystemTable()`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java
    for (final RegionInfo hri : service.getAssignedRegions()) {
      cache.refresh(hri);
      includesUserTables |= !hri.getTable().isSystemTable();
    }
    return includesUserTables;
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `cacheBlock &= cacheConf.shouldCacheBlockOnRead(BlockType.META.getCategory())`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
        new BlockCacheKey(name, metaBlockOffset, this.isPrimaryReplicaReader(), BlockType.META);

      cacheBlock &= cacheConf.shouldCacheBlockOnRead(BlockType.META.getCategory());
      HFileBlock cachedBlock =
        getCachedBlock(cacheKey, cacheBlock, false, true, BlockType.META, null);
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `cacheBlock &= cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory())`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    // we can really only check for that if have an expectedBlockType
    if (expectedBlockType != null) {
      cacheBlock &= cacheConf.shouldCacheBlockOnRead(expectedBlockType.getCategory());
    }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `shouldFinishBlock &= blockWriter.checkBoundariesWithPredicate()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
        || blockWriter.blockSizeWritten() >= hFileContext.getBlocksize();
    }
    shouldFinishBlock &= blockWriter.checkBoundariesWithPredicate();
    if (shouldFinishBlock) {
      finishBlock();
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `isSuccessful &= writeRemainingEntryBuffers()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredEditsOutputSink.java`
#### Snippet
```java
      isSuccessful = finishWriterThreads();
    } finally {
      isSuccessful &= writeRemainingEntryBuffers();
    }
    return isSuccessful ? splits : null;
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `isSuccessful &= closeWriters()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java`
#### Snippet
```java
      isSuccessful = finishWriterThreads();
    } finally {
      isSuccessful &= closeWriters();
    }
    return isSuccessful ? splits : null;
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `isSuccessful &= writeRemainingEntryBuffers()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java`
#### Snippet
```java
      isSuccessful = finishWriterThreads();
    } finally {
      isSuccessful &= writeRemainingEntryBuffers();
    }
    return isSuccessful ? splits : null;
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixEmptyMetaCells(boolean shouldFix) {
    fixEmptyMetaCells = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixHFileLinks(boolean shouldFix) {
    fixHFileLinks = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixAssignments(boolean shouldFix) {
    fixAssignments = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixMeta(boolean shouldFix) {
    fixMeta = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setRemoveParents(boolean shouldFix) {
    removeParents = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixHdfsOverlaps(boolean shouldFix) {
    fixHdfsOverlaps = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixHdfsHoles(boolean shouldFix) {
    fixHdfsHoles = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixVersionFile(boolean shouldFix) {
    fixVersionFile = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixTableOrphans(boolean shouldFix) {
    fixTableOrphans = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixSplitParents(boolean shouldFix) {
    fixSplitParents = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixReplication(boolean shouldFix) {
    fixReplication = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixHdfsOrphans(boolean shouldFix) {
    fixHdfsOrphans = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `fixAny |= shouldFix`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  public void setFixReferenceFiles(boolean shouldFix) {
    fixReferenceFiles = shouldFix;
    fixAny |= shouldFix;
  }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `expectedState |= (state == expected[i])`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/ServerStateNode.java`
#### Snippet
```java
    if (expected != null) {
      for (int i = 0; i < expected.length; ++i) {
        expectedState |= (state == expected[i]);
      }
    }
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `useNoop &= tableLimiter.isBypass() && nsLimiter.isBypass() && rsLimiter.isBypass()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java`
#### Snippet
```java
        QuotaLimiter rsLimiter =
          quotaCache.getRegionServerQuotaLimiter(QuotaTableUtil.QUOTA_REGION_SERVER_ROW_KEY);
        useNoop &= tableLimiter.isBypass() && nsLimiter.isBypass() && rsLimiter.isBypass();
        boolean exceedThrottleQuotaEnabled = quotaCache.isExceedThrottleQuotaEnabled();
        if (LOG.isTraceEnabled()) {
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `bypass |= observerOperation.shouldBypass()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
        currentThread.setContextClassLoader(cl);
      }
      bypass |= observerOperation.shouldBypass();
    }

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `bypass |= observerOperation.shouldBypass()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      }
      // Internal to shouldBypass, it checks if obeserverOperation#isBypassable().
      bypass |= observerOperation.shouldBypass();
      observerOperation.postEnvCall();
      if (bypass) {
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `taskGrabbed |= grabTask(ZNodePaths.joinZNode(watcher.getZNodePaths().splitLogZNode, pa...`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
            // don't call ZKSplitLog.getNodeName() because that will lead to
            // double encoding of the path name
            taskGrabbed |=
              grabTask(ZNodePaths.joinZNode(watcher.getZNodePaths().splitLogZNode, paths.get(idx)));
            break;
          } else {
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasAnyWriter |= needEmptyFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
      boolean needEmptyFile = isInMajorRange || isLastWriter;
      existingWriters.add(needEmptyFile ? writerFactory.createWriter() : null);
      hasAnyWriter |= needEmptyFile;
      currentWriterEndKey = (existingWriters.size() + 1 == boundaries.size())
        ? null
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `aboveParallelThreadLimit |= storeAboveThread`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
            : tooBusyStore + "," + store.getColumnFamilyName());
        }
        aboveParallelThreadLimit |= storeAboveThread;
        aboveParallelPrePutLimit |= storeAbovePrePut;

```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `aboveParallelPrePutLimit |= storeAbovePrePut`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
        }
        aboveParallelThreadLimit |= storeAboveThread;
        aboveParallelPrePutLimit |= storeAbovePrePut;

        if (LOG.isTraceEnabled()) {
```

### NonShortCircuitBoolean
Non-short-circuit boolean expression `hasMetaEdit |= entry.edit.isMetaEdit()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationSink.java`
#### Snippet
```java
      toSend.add(entry);
      totalSize += entry.size;
      hasMetaEdit |= entry.edit.isMetaEdit();
      if (toSend.size() >= batchCountCapacity || totalSize >= batchSizeCapacity) {
        break;
```

## RuleId[id=UnnecessaryReturn]
### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsUserSourceImpl.java`
#### Snippet
```java
      // the closed variable and getting the lock.
      if (closed.get()) {
        return;
      }
    }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      LOG.info("Finished subprocedure pid={}, resume processing ppid={}", procedure.getProcId(),
        parent.getProcId());
      return;
    }
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java`
#### Snippet
```java
      }
    } catch (KeeperException.NoNodeException nne) {
      return;
    } catch (InterruptedException ie) {
      interruptedExceptionNoThrow(ie, false);
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
        CreateMode.PERSISTENT);
    } catch (KeeperException.NodeExistsException nee) {
      return;
    } catch (KeeperException.NoNodeException nne) {
      createWithParents(zkw, getParent(znode));
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/WrapperAsyncFSOutput.java`
#### Snippet
```java
    } catch (IOException e) {
      future.completeExceptionally(e);
      return;
    }
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java`
#### Snippet
```java
      } catch (TableNotFoundException e) {
        // Table not found. So just return
        return;
      } catch (IOException e) {
        LOG.error("Error scanning 'labels' table", e);
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java`
#### Snippet
```java
    } catch (IOException ioe) {
      LOG.error("Failed reading 'labels' tags", ioe);
      return;
    } finally {
      if (labelsTable != null) {
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java`
#### Snippet
```java
          + "environment is properly configured. For more information please see\n"
          + "http://hbase.apache.org/book.html#profiler\n");
      return;
    }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java

      if (victimCache == null) {
        return;
      } else if (victimCache instanceof BucketCache) {
        BucketCache victimBucketCache = (BucketCache) victimCache;
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      freeSpace("Used=" + used + " > acceptable=" + acceptableSize());
    }
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
    return () -> {
      freeBucketEntry(bucketEntry);
      return;
    };
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        if (!exist) {
          future.completeExceptionally(new TableNotFoundException(tableName));
          return;
        }
      });
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java`
#### Snippet
```java
    public void updateStore(byte[] encodedRegionName, byte[] familyName, Long sequenceid,
      boolean onlyIfGreater) {
      return;
    }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
        }
      }
      return;
    } catch (FileNotFoundException fnfe) {
      LOG.warn("Mob file " + p + " was missing.  Likely removed due to compaction?");
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
        }
      }
      return;
    } catch (FileNotFoundException fnfe) {
      LOG.warn("HFile " + p + " was missing.  Likely removed due to compaction/split?");
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    }
    if (inMeta && inHdfs && isDeployed && deploymentMatchesMeta && shouldBeDeployed) {
      return;
    } else if (inMeta && inHdfs && !shouldBeDeployed && !isDeployed) {
      LOG.info("Region " + descriptiveName + " is in META, and in a disabled "
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      LOG.info("Region " + descriptiveName + " is in META, and in a disabled "
        + "tabled that is not deployed");
      return;
    } else if (recentlyModified) {
      LOG.warn("Region " + descriptiveName + " was recently modified -- skipping");
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    } else if (recentlyModified) {
      LOG.warn("Region " + descriptiveName + " was recently modified -- skipping");
      return;
    }
    // ========== Cases where the region is not in hbase:meta =============
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
      if (tableDeleted) {
        markTableAsDeleted(iProcTable.getTableName(), proc);
        return;
      }
    } else if (proc instanceof PeerProcedureInterface) {
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
    } else {
      // No cleanup for other procedure types, yet.
      return;
    }
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
    } catch (InterruptedException e) {
      Thread.currentThread().interrupt();
      return;
    }
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
        if (isReachedNode(parent)) {
          receivedReachedGlobalBarrier(path);
          return;
        } else if (isAbortNode(parent)) {
          abort(path);
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
        } else if (isAbortNode(parent)) {
          abort(path);
          return;
        } else if (isAcquiredNode(parent)) {
          startNewSubprocedure(path);
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
  private void deleteNodeFailure(String path) {
    LOG.info("Failed to delete node " + path + " and will retry soon.");
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      // exists or not
    }
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
    // if a deletion fails, TimeoutMonitor will retry the same deletion later
    deleteNode(path, zkretries);
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
        LOG.warn("Deserialization problem", e);
      }
      return;
    }
  }
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      retry_count);
    SplitLogCounters.tot_mgr_node_create_queued.increment();
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java`
#### Snippet
```java
   */
  default void stopReplayingFromWAL() {
    return;
  }
}
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java`
#### Snippet
```java
   */
  default void startReplayingFromWAL() {
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java
    if (segments.size() == 1 && !segments.get(0).isEmpty()) {
      this.snapshot = segments.get(0);
      return;
    } else { // create composite snapshot
      this.snapshot =
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java`
#### Snippet
```java
  @Override
  protected void postUpdate(MutableSegment currentActive) {
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
      // so we have nothing to do on its release
    }
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
        trySetReadyForRolling();
      }
      return;
    }
    // reach here means that we have some unsynced data but haven't reached the batch size yet
```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
    System.err.println("If <input> WAL is compressed, <output> will be decompressed.");
    System.err.println("If <input> WAL is uncompressed, <output> will be compressed.");
    return;
  }

```

### UnnecessaryReturn
`return` is unnecessary as the last statement in a 'void' method
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupObserver.java`
#### Snippet
```java
      }
      tbl.writeFilesForBulkLoadPreCommit(tableName, info.getEncodedNameAsBytes(), family, pairs);
      return;
    }
  }
```

## RuleId[id=SuspiciousListRemoveInLoop]
### SuspiciousListRemoveInLoop
Suspicious 'List.remove()' in loop
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
        clientPortList.add(currentClientPort);
      } else if (clientPortList.get(i) <= 0) { // the list has invalid port, update with valid port
        clientPortList.remove(i);
        clientPortList.add(i, currentClientPort);
      }
```

## RuleId[id=ManualMinMaxCalculation]
### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java`
#### Snippet
```java
  private static void unsafeCopy(Object src, long srcAddr, Object dst, long destAddr, long len) {
    while (len > 0) {
      long size = (len > UNSAFE_COPY_THRESHOLD) ? UNSAFE_COPY_THRESHOLD : len;
      HBasePlatformDependent.copyMemory(src, srcAddr, dst, destAddr, size);
      len -= size;
```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java`
#### Snippet
```java
  public int compareTo(byte[] value, int offset, int length) {
    return Bytes.compareTo(this.value, 0, this.value.length, value, offset,
      this.value.length <= length ? this.value.length : length);
  }

```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListBase.java`
#### Snippet
```java
  @Override
  public String toString() {
    int endIndex = this.size() < MAX_LOG_FILTERS ? this.size() : MAX_LOG_FILTERS;
    return formatLogFilters(filters.subList(0, endIndex));
  }
```

### ManualMinMaxCalculation
Can be replaced with 'Math.max()' call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
    // values than otherwise.
    byte[] result =
      Arrays.copyOf(fuzzyKeyBytes, length > fuzzyKeyBytes.length ? length : fuzzyKeyBytes.length);
    if (reverse && length > fuzzyKeyBytes.length) {
      // we need trailing 0xff's instead of trailing 0x00's
```

### ManualMinMaxCalculation
Can be replaced with 'Math.max()' call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
  @Override
  public ReturnCode filterCell(final Cell c) {
    final int startIndex = lastFoundIndex >= 0 ? lastFoundIndex : 0;
    final int size = fuzzyKeysData.size();
    for (int i = startIndex; i < size + startIndex; i++) {
```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java`
#### Snippet
```java
      throw new IOException("Expecting at least one column");
    }
    int realNumSplits = numSplits > startKeys.length ? startKeys.length : numSplits;
    InputSplit[] splits = new InputSplit[realNumSplits];
    int middle = startKeys.length / realNumSplits;
```

### ManualMinMaxCalculation
Can be replaced with 'Math.max()' call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
    }
    // if n < 1, then still continue using n = 1
    n = n < 1 ? 1 : n;
    List<InputSplit> res = new ArrayList<>(n);
    if (n == 1) {
```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
  private static byte[] getMinimumMidpointArray(ByteBuffer left, int leftOffset, int leftLength,
    ByteBuffer right, int rightOffset, int rightLength) {
    int minLength = leftLength < rightLength ? leftLength : rightLength;
    int diffIdx = 0;
    for (; diffIdx < minLength; diffIdx++) {
```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
  private static byte[] getMinimumMidpointArray(final byte[] leftArray, final int leftOffset,
    final int leftLength, final byte[] rightArray, final int rightOffset, final int rightLength) {
    int minLength = leftLength < rightLength ? leftLength : rightLength;
    int diffIdx = 0;
    for (; diffIdx < minLength; diffIdx++) {
```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
      int traceLogMaxLength = getConf().getInt(TRACE_LOG_MAX_LENGTH, DEFAULT_TRACE_LOG_MAX_LENGTH);
      int truncatedLength =
        strParam.length() < traceLogMaxLength ? strParam.length() : traceLogMaxLength;
      String truncatedFlag = truncatedLength == strParam.length() ? "" : KEY_WORD_TRUNCATED;
      return strParam.subSequence(0, truncatedLength) + truncatedFlag;
```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java

    // With a limit of MAX_SERVER_PER_MESSAGE
    int max = entries.size() > MAX_SERVER_PER_MESSAGE ? MAX_SERVER_PER_MESSAGE : entries.size();
    List<ServerName> res = new ArrayList<>(max);

```

### ManualMinMaxCalculation
Can be replaced with 'Math.max()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java`
#### Snippet
```java
        highestEndKey = ri.getEndKey();
      }
      highestRegionId = ri.getRegionId() > highestRegionId ? ri.getRegionId() : highestRegionId;
    }
    // Merged region is sorted between two merging regions in META
```

### ManualMinMaxCalculation
Can be replaced with 'Math.max()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeImpl.java`
#### Snippet
```java
        new Exception());
    }
    this.size = new AtomicLong(initialSize < 0L ? 0L : initialSize);
  }

```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    // Make sure we do not return more than maximum versions for this store.
    int maxVersions = getColumnFamilyDescriptor().getMaxVersions();
    return wantedVersions > maxVersions ? maxVersions : wantedVersions;
  }

```

### ManualMinMaxCalculation
Can be replaced with 'Math.min()' call
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
              rowCount++;
            } catch (RegionTooBusyException e) {
              pause = (pause * 2 >= maxPause) ? maxPause : pause * 2;
              Threads.sleep(pause);
            }
```

## RuleId[id=AbstractClassNeverImplemented]
### AbstractClassNeverImplemented
Abstract class `Union3` has no concrete subclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union3.java`
#### Snippet
```java
@SuppressWarnings("unchecked")
@InterfaceAudience.Public
public abstract class Union3<A, B, C> extends Union2<A, B> {

  protected final DataType<C> typeC;
```

### AbstractClassNeverImplemented
Abstract class `Union4` has no concrete subclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/Union4.java`
#### Snippet
```java
@SuppressWarnings("unchecked")
@InterfaceAudience.Public
public abstract class Union4<A, B, C, D> extends Union3<A, B, C> {

  protected final DataType<D> typeD;
```

### AbstractClassNeverImplemented
Abstract class `JobUtil` has no concrete subclass
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/mapreduce/JobUtil.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public abstract class JobUtil {

  protected JobUtil() {
```

### AbstractClassNeverImplemented
Abstract class `SyncCoprocessorRpcChannel` has no concrete subclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SyncCoprocessorRpcChannel.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Public
abstract class SyncCoprocessorRpcChannel implements CoprocessorRpcChannel {
  private static final Logger LOG = LoggerFactory.getLogger(SyncCoprocessorRpcChannel.class);

```

### AbstractClassNeverImplemented
Abstract class `Batch` has no concrete subclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Batch.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public abstract class Batch {

  /**
```

### AbstractClassNeverImplemented
Abstract class `TwoPhaseProcedure` has no concrete subclass
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/TwoPhaseProcedure.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public abstract class TwoPhaseProcedure<TEnvironment> extends Procedure<TEnvironment> {
  // TODO (e.g. used by ACLs/VisibilityTags updates)
}
```

### AbstractClassNeverImplemented
Abstract class `OnePhaseProcedure` has no concrete subclass
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/OnePhaseProcedure.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public abstract class OnePhaseProcedure<TEnvironment> extends Procedure<TEnvironment> {
  // TODO (e.g. used by online snapshots)
}
```

### AbstractClassNeverImplemented
Abstract class `LogMonitoring` has no concrete subclass
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/LogMonitoring.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public abstract class LogMonitoring {

  public static void dumpTailOfLogs(PrintWriter out, long tailKb) throws IOException {
```

### AbstractClassNeverImplemented
Abstract class `ModifyRegionUtils` has no concrete subclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public abstract class ModifyRegionUtils {
  private static final Logger LOG = LoggerFactory.getLogger(ModifyRegionUtils.class);

```

### AbstractClassNeverImplemented
Abstract class `JvmVersion` has no concrete subclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Stable
public abstract class JvmVersion {
  private static Set<String> BAD_JVM_VERSIONS = new HashSet<>();
  static {
```

### AbstractClassNeverImplemented
Abstract class `ThreadMonitoring` has no concrete subclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/ThreadMonitoring.java`
#### Snippet
```java

@InterfaceAudience.Private
public abstract class ThreadMonitoring {

  private static final ThreadMXBean threadBean = ManagementFactory.getThreadMXBean();
```

## RuleId[id=BoundedWildcard]
### BoundedWildcard
Can generalize to `? extends V`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/JitterScheduledThreadPoolExecutorImpl.java`
#### Snippet
```java
    private final RunnableScheduledFuture<V> wrapped;

    JitteredRunnableScheduledFuture(RunnableScheduledFuture<V> wrapped) {
      this.wrapped = wrapped;
    }
```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TagUtil.java`
#### Snippet
```java
   * @return the serialized tag data as bytes
   */
  public static byte[] fromList(List<Tag> tags) {
    if (tags == null || tags.isEmpty()) {
      return HConstants.EMPTY_BYTE_ARRAY;
```

### BoundedWildcard
Can generalize to `? super L`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java

  private static <L, R> int compareRows(L left, int loffset, int llength, R right, int roffset,
    int rlength, SearchDelimiter<L> searchLeft, SearchDelimiter<R> searchRight,
    SearchDelimiterInReverse<L> searchInReverseLeft,
    SearchDelimiterInReverse<R> searchInReverseRight, Compare<L, R> comparator) {
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java

  private static <L, R> int compareRows(L left, int loffset, int llength, R right, int roffset,
    int rlength, SearchDelimiter<L> searchLeft, SearchDelimiter<R> searchRight,
    SearchDelimiterInReverse<L> searchInReverseLeft,
    SearchDelimiterInReverse<R> searchInReverseRight, Compare<L, R> comparator) {
```

### BoundedWildcard
Can generalize to `? super L`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java
  private static <L, R> int compareRows(L left, int loffset, int llength, R right, int roffset,
    int rlength, SearchDelimiter<L> searchLeft, SearchDelimiter<R> searchRight,
    SearchDelimiterInReverse<L> searchInReverseLeft,
    SearchDelimiterInReverse<R> searchInReverseRight, Compare<L, R> comparator) {
    int leftDelimiter = searchLeft.search(left, loffset, llength, HConstants.DELIMITER);
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java
    int rlength, SearchDelimiter<L> searchLeft, SearchDelimiter<R> searchRight,
    SearchDelimiterInReverse<L> searchInReverseLeft,
    SearchDelimiterInReverse<R> searchInReverseRight, Compare<L, R> comparator) {
    int leftDelimiter = searchLeft.search(left, loffset, llength, HConstants.DELIMITER);
    int rightDelimiter = searchRight.search(right, roffset, rlength, HConstants.DELIMITER);
```

### BoundedWildcard
Can generalize to `? super L`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java
    int rlength, SearchDelimiter<L> searchLeft, SearchDelimiter<R> searchRight,
    SearchDelimiterInReverse<L> searchInReverseLeft,
    SearchDelimiterInReverse<R> searchInReverseRight, Compare<L, R> comparator) {
    int leftDelimiter = searchLeft.search(left, loffset, llength, HConstants.DELIMITER);
    int rightDelimiter = searchRight.search(right, roffset, rlength, HConstants.DELIMITER);
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/MetaCellComparator.java`
#### Snippet
```java
    int rlength, SearchDelimiter<L> searchLeft, SearchDelimiter<R> searchRight,
    SearchDelimiterInReverse<L> searchInReverseLeft,
    SearchDelimiterInReverse<R> searchInReverseRight, Compare<L, R> comparator) {
    int leftDelimiter = searchLeft.search(left, loffset, llength, HConstants.DELIMITER);
    int rightDelimiter = searchRight.search(right, roffset, rlength, HConstants.DELIMITER);
```

### BoundedWildcard
Can generalize to `? extends Map.Entry`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java`
#### Snippet
```java
   */
  public static void setWithPrefix(Configuration conf, String prefix,
    Iterable<Map.Entry<String, String>> properties) {
    for (Map.Entry<String, String> entry : properties) {
      conf.set(prefix + entry.getKey(), entry.getValue());
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java

  @Deprecated
  public static List<KeyValue> ensureKeyValues(List<Cell> cells) {
    List<KeyValue> lazyList = Lists.transform(cells, new Function<Cell, KeyValue>() {
      @Override
```

### BoundedWildcard
Can generalize to `? super WatchEvent`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/FileChangeWatcher.java`
#### Snippet
```java
    final Consumer<WatchEvent<?>> callback;

    WatcherThread(WatchService watchService, Consumer<WatchEvent<?>> callback) {
      super(THREAD_NAME);
      this.watchService = watchService;
```

### BoundedWildcard
Can generalize to `? super Cell`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   * @return The Key portion of the passed <code>cell</code> as a String.
   */
  public static String getCellKeyAsString(Cell cell, Function<Cell, String> rowConverter) {
    StringBuilder sb = new StringBuilder(rowConverter.apply(cell));
    sb.append('/');
```

### BoundedWildcard
Can generalize to `? super FileChangeWatcher`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java
   */
  public static void enableCertFileReloading(Configuration config,
    AtomicReference<FileChangeWatcher> keystoreWatcher,
    AtomicReference<FileChangeWatcher> trustStoreWatcher, Runnable resetContext)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? super FileChangeWatcher`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java
  public static void enableCertFileReloading(Configuration config,
    AtomicReference<FileChangeWatcher> keystoreWatcher,
    AtomicReference<FileChangeWatcher> trustStoreWatcher, Runnable resetContext)
    throws IOException {
    String keyStoreLocation = config.get(TLS_CONFIG_KEYSTORE_LOCATION, "");
```

### BoundedWildcard
Can generalize to `? super ByteBuffer`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java`
#### Snippet
```java

  @Override
  public void asSubByteBuffer(int offset, int length, ObjectIntPair<ByteBuffer> pair) {
    checkRefCount();
    // Just return the single BB that is available
```

### BoundedWildcard
Can generalize to `? extends ByteRange`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java`
#### Snippet
```java
  }

  public static ArrayList<byte[]> copyToNewArrays(Collection<ByteRange> ranges) {
    if (ranges == null) {
      return new ArrayList<>(0);
```

### BoundedWildcard
Can generalize to `? super ByteBuffer`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
   */
  @Override
  public void asSubByteBuffer(int offset, int length, ObjectIntPair<ByteBuffer> pair) {
    checkRefCount();
    if (this.itemBeginPos[this.curItemIndex] <= offset) {
```

### BoundedWildcard
Can generalize to `? extends V`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcurrentMapUtils.java`
#### Snippet
```java
   */
  public static <K, V> V computeIfAbsentEx(ConcurrentMap<K, V> map, K key,
    IOExceptionSupplier<V> supplier) throws IOException {
    V v, newValue;
    return ((v = map.get(key)) == null && (newValue = supplier.get()) != null
```

### BoundedWildcard
Can generalize to `? extends V`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcurrentMapUtils.java`
#### Snippet
```java
  }

  public static <K, V> V computeIfAbsent(ConcurrentMap<K, V> map, K key, Supplier<V> supplier,
    Runnable actionIfAbsent) {
    V v = map.get(key);
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MovingAverage.java`
#### Snippet
```java
   * @return T it refers to the original return type of the measurable method
   */
  public T measure(TimeMeasurable<T> measurable) {
    long startTime = start();
    LOG.trace("{} - start to measure at: {} ns.", label, startTime);
```

### BoundedWildcard
Can generalize to `? super ByteBuffer`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferArray.java`
#### Snippet
```java
   * @return the length of bytes we transferred.
   */
  private int internalTransfer(long offset, ByteBuff b, BiConsumer<ByteBuffer, ByteBuff> transfer) {
    int expectedTransferLen = b.remaining();
    if (expectedTransferLen == 0) {
```

### BoundedWildcard
Can generalize to `? super ByteBuff`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferArray.java`
#### Snippet
```java
   * @return the length of bytes we transferred.
   */
  private int internalTransfer(long offset, ByteBuff b, BiConsumer<ByteBuffer, ByteBuff> transfer) {
    int expectedTransferLen = b.remaining();
    if (expectedTransferLen == 0) {
```

### BoundedWildcard
Can generalize to `? super TNode`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
     */
    public void seekTo(final TNode root, final Object key,
      final AvlKeyComparator<TNode> keyComparator) {
      current = null;
      height = 0;
```

### BoundedWildcard
Can generalize to `? super TNode`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
     */
    public static <TNode extends AvlNode> TNode remove(TNode root, Object key,
      final AvlKeyComparator<TNode> keyComparator, final AtomicBoolean removed) {
      if (root == null) return null;

```

### BoundedWildcard
Can generalize to `? super TNode`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
     */
    public static <TNode extends AvlNode> TNode get(TNode root, final Object key,
      final AvlKeyComparator<TNode> keyComparator) {
      while (root != null) {
        int cmp = keyComparator.compareKey(root, key);
```

### BoundedWildcard
Can generalize to `? super TNode`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
     */
    public static <TNode extends AvlNode> TNode insert(TNode root, Object key,
      final AvlKeyComparator<TNode> keyComparator,
      final AvlInsertOrReplace<TNode> insertOrReplace) {
      if (root == null) {
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java

  // Make this private because we don't want to expose the refCnt related wrap method to upstream.
  private static ByteBuff wrap(List<ByteBuffer> buffers, RefCnt refCnt) {
    if (buffers == null || buffers.size() == 0) {
      throw new IllegalArgumentException("buffers shouldn't be null or empty");
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/FutureUtils.java`
#### Snippet
```java
   * the callbacks in the given {@code executor}.
   */
  public static <T> CompletableFuture<T> wrapFuture(CompletableFuture<T> future,
    Executor executor) {
    CompletableFuture<T> wrappedFuture = new CompletableFuture<>();
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/FutureUtils.java`
#### Snippet
```java
   * CompletableFuture.
   */
  public static <T> CompletableFuture<List<T>> allOf(List<CompletableFuture<T>> futures) {
    return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
      .thenApply(v -> futures.stream().map(f -> f.getNow(null)).collect(toList()));
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   * Trace an asynchronous operation for a table.
   */
  public static <T> CompletableFuture<T> tracedFuture(Supplier<CompletableFuture<T>> action,
    Supplier<Span> spanSupplier) {
    Span span = spanSupplier.get();
```

### BoundedWildcard
Can generalize to `? extends Span`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   */
  public static <T> CompletableFuture<T> tracedFuture(Supplier<CompletableFuture<T>> action,
    Supplier<Span> spanSupplier) {
    Span span = spanSupplier.get();
    try (Scope ignored = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends Span`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   */
  public static Runnable tracedRunnable(final Runnable runnable,
    final Supplier<Span> spanSupplier) {
    // N.B. This method name follows the convention of this class, i.e., tracedFuture, rather than
    // the convention of the OpenTelemetry classes, i.e., Context#wrap.
```

### BoundedWildcard
Can generalize to `? extends Span`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java

  public static <R, T extends Throwable> R trace(final ThrowingCallable<R, T> callable,
    final Supplier<Span> spanSupplier) throws T {
    Span span = spanSupplier.get();
    try (Scope ignored = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends Span`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   */
  public static <T extends Throwable> void trace(final ThrowingRunnable<T> runnable,
    final Supplier<Span> spanSupplier) throws T {
    Span span = spanSupplier.get();
    try (Scope ignored = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends List`>
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   */
  public static <T> List<CompletableFuture<T>>
    tracedFutures(Supplier<List<CompletableFuture<T>>> action, Supplier<Span> spanSupplier) {
    Span span = spanSupplier.get();
    try (Scope ignored = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends Span`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   */
  public static <T> List<CompletableFuture<T>>
    tracedFutures(Supplier<List<CompletableFuture<T>>> action, Supplier<Span> spanSupplier) {
    Span span = spanSupplier.get();
    try (Scope ignored = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
   * Trace an asynchronous operation.
   */
  public static <T> CompletableFuture<T> tracedFuture(Supplier<CompletableFuture<T>> action,
    String spanName) {
    Span span = createSpan(spanName);
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
    }

    public SecureHadoopUser(UserGroupInformation ugi, LoadingCache<String, String[]> cache) {
      this.ugi = ugi;
      this.cache = cache;
```

### BoundedWildcard
Can generalize to `? extends V`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    private final ArrayHolder<K, V> holder;

    private ArrayValueIterator(ArrayHolder<K, V> holder) {
      this.holder = holder;
      index = holder.startIndex;
```

### BoundedWildcard
Can generalize to `? extends K`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
    private final ArrayHolder<K, V> holder;

    private ArrayKeyIterator(ArrayHolder<K, V> holder) {
      this.holder = holder;
      index = holder.startIndex;
```

### BoundedWildcard
Can generalize to `? extends Entry`>
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterQuotaSourceImpl.java`
#### Snippet
```java
   * Summarizes the usage and limit for many targets (table or namespace) into JSON.
   */
  private String generateJsonQuotaSummary(Iterable<Entry<String, Entry<Long, Long>>> data,
    String target) {
    StringBuilder sb = new StringBuilder();
```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    final byte[] family, final int foffset, int flength, final Object qualifier, final int qoffset,
    int qlength, final long timestamp, final Type type, final Object value, final int voffset,
    int vlength, List<Tag> tags) {

    checkParameters(row, rlength, family, flength, qlength, vlength);
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/DynamicMetricsRegistry.java`
#### Snippet
```java

  private <T extends MutableMetric> T addNewMetricIfAbsent(String name, T ret,
    Class<T> metricClass) {
    // If the value we get back is null then the put was successful and we will
    // return that. Otherwise metric should contain the thing that was in
```

### BoundedWildcard
Can generalize to `? extends Address`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java`
#### Snippet
```java

  /** Adds the given servers to the group. */
  public void addAllServers(Collection<Address> hostPort) {
    servers.addAll(hostPort);
  }
```

### BoundedWildcard
Can generalize to `? super UserGroupInformation`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeSaslServerAuthenticationProvider.java`
#### Snippet
```java
    private final Map<String, char[]> passwordDatabase;

    public ShadeSaslServerCallbackHandler(AtomicReference<UserGroupInformation> attemptingUser,
      Map<String, char[]> passwordDatabase) {
      this.attemptingUser = attemptingUser;
```

### BoundedWildcard
Can generalize to `? extends TRowResult`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
  }

  private void printRow(List<TRowResult> rows) {
    for (TRowResult rowResult : rows) {
      printRow(rowResult);
```

### BoundedWildcard
Can generalize to `? extends TCell`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
  }

  private void printVersions(ByteBuffer row, List<TCell> versions) {
    StringBuilder rowStr = new StringBuilder();

```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/WriteHeavyIncrementObserver.java`
#### Snippet
```java

  @Override
  public Result preIncrement(ObserverContext<RegionCoprocessorEnvironment> c, Increment increment)
    throws IOException {
    byte[] row = increment.getRow();
```

### BoundedWildcard
Can generalize to `? super Cell`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/WriteHeavyIncrementObserver.java`
#### Snippet
```java

      @Override
      public boolean next(List<Cell> result, ScannerContext scannerContext) throws IOException {
        boolean moreRows = scanner.next(srcResult, scannerContext);
        if (srcResult.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/WriteHeavyIncrementObserver.java`
#### Snippet
```java

  @Override
  public void preGetOp(ObserverContext<RegionCoprocessorEnvironment> c, Get get, List<Cell> result)
    throws IOException {
    Scan scan =
```

### BoundedWildcard
Can generalize to `? super Cell`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/WriteHeavyIncrementObserver.java`
#### Snippet
```java

  @Override
  public void preGetOp(ObserverContext<RegionCoprocessorEnvironment> c, Get get, List<Cell> result)
    throws IOException {
    Scan scan =
```

### BoundedWildcard
Can generalize to `? extends FieldInfo`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/mode/RegionModeStrategy.java`
#### Snippet
```java
   * @return records after selecting required fields and adding count field
   */
  List<Record> selectModeFieldsAndAddCountField(List<FieldInfo> fieldInfos, List<Record> records,
    Field countField) {

```

### BoundedWildcard
Can generalize to `? extends Header`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/top/TopScreenView.java`
#### Snippet
```java
  }

  private void showHeaders(List<Header> headers) {
    String header = headers.stream().map(h -> String.format(h.format(), h.getField().getHeader()))
      .collect(Collectors.joining(" "));
```

### BoundedWildcard
Can generalize to `? extends Header`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/top/TopScreenView.java`
#### Snippet
```java
  }

  private void showRecords(List<Header> headers, List<Record> records, Record selectedRecord) {
    TerminalPrinter printer = getTerminalPrinter(RECORD_START_ROW);
    int size;
```

### BoundedWildcard
Can generalize to `? super KeyPress`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/KeyPressGenerator.java`
#### Snippet
```java
  private int param2;

  public KeyPressGenerator(InputStream inputStream, Queue<KeyPress> keyPressQueue) {
    this.inputStream = inputStream;
    input = new InputStreamReader(inputStream, StandardCharsets.UTF_8);
```

### BoundedWildcard
Can generalize to `? super AdvancedScanResultConsumer`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java`
#### Snippet
```java
   * @param visitor   Visitor invoked against each row
   */
  private static CompletableFuture<Void> scanMeta(AsyncTable<AdvancedScanResultConsumer> metaTable,
    byte[] startRow, byte[] stopRow, QueryType type, int maxRows, final Visitor visitor) {
    int rowUpperLimit = maxRows > 0 ? maxRows : Integer.MAX_VALUE;
```

### BoundedWildcard
Can generalize to `? extends UserMetrics`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
  }

  public ServerMetricsBuilder setUserMetrics(List<UserMetrics> value) {
    value.forEach(v -> this.userMetrics.put(v.getUserName(), v));
    return this;
```

### BoundedWildcard
Can generalize to `? extends RegionMetrics`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
  }

  public ServerMetricsBuilder setRegionMetrics(List<RegionMetrics> value) {
    value.forEach(v -> this.regionStatus.put(v.getRegionName(), v));
    return this;
```

### BoundedWildcard
Can generalize to `? extends ServerTask`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
  }

  public ServerMetricsBuilder setTasks(List<ServerTask> tasks) {
    this.tasks.addAll(tasks);
    return this;
```

### BoundedWildcard
Can generalize to `? super Message`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java

  private void onCallFinished(Call call, HBaseRpcController hrc, Address addr,
    RpcCallback<Message> callback) {
    call.callStats.setCallTimeMs(EnvironmentEdgeManager.currentTime() - call.getStartTime());
    if (metrics != null) {
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java`
#### Snippet
```java

    @Override
    public R getOrCreate(PoolResourceSupplier<R> supplier) throws IOException {
      int size = resources.size();
      R resource;
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/PoolMap.java`
#### Snippet
```java

    @Override
    public R getOrCreate(PoolResourceSupplier<R> supplier) throws IOException {
      Thread myself = Thread.currentThread();
      R resource = resources.get(myself);
```

### BoundedWildcard
Can generalize to `? super Throwable`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java`
#### Snippet
```java

  protected final void onError(Throwable t, Supplier<String> errMsg,
    Consumer<Throwable> updateCachedLocation) {
    if (future.isDone()) {
      // Give up if the future is already done, this is possible if user has already canceled the
```

### BoundedWildcard
Can generalize to `? extends CellScannable`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  }

  HBaseRpcControllerImpl(RegionInfo regionInfo, final List<CellScannable> cellIterables) {
    this.cellScanner = cellIterables == null ? null : CellUtil.createCellScanner(cellIterables);
    this.regionInfo = null;
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SnapshotDescription.java`
#### Snippet
```java
  }

  private long getLongFromSnapshotProps(Map<String, Object> snapshotProps, String property) {
    return MapUtils.getLongValue(snapshotProps, property, -1);
  }
```

### BoundedWildcard
Can generalize to `? super HRegionLocation`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java`
#### Snippet
```java

  static void updateCachedLocationOnError(HRegionLocation loc, Throwable exception,
    Function<HRegionLocation, HRegionLocation> cachedLocationSupplier,
    Consumer<HRegionLocation> addToCache, Consumer<HRegionLocation> removeFromCache,
    MetricsConnection metrics) {
```

### BoundedWildcard
Can generalize to `? extends HRegionLocation`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java`
#### Snippet
```java

  static void updateCachedLocationOnError(HRegionLocation loc, Throwable exception,
    Function<HRegionLocation, HRegionLocation> cachedLocationSupplier,
    Consumer<HRegionLocation> addToCache, Consumer<HRegionLocation> removeFromCache,
    MetricsConnection metrics) {
```

### BoundedWildcard
Can generalize to `? super HRegionLocation`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java`
#### Snippet
```java
  static void updateCachedLocationOnError(HRegionLocation loc, Throwable exception,
    Function<HRegionLocation, HRegionLocation> cachedLocationSupplier,
    Consumer<HRegionLocation> addToCache, Consumer<HRegionLocation> removeFromCache,
    MetricsConnection metrics) {
    HRegionLocation oldLoc = cachedLocationSupplier.apply(loc);
```

### BoundedWildcard
Can generalize to `? super HRegionLocation`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocatorHelper.java`
#### Snippet
```java
  static void updateCachedLocationOnError(HRegionLocation loc, Throwable exception,
    Function<HRegionLocation, HRegionLocation> cachedLocationSupplier,
    Consumer<HRegionLocation> addToCache, Consumer<HRegionLocation> removeFromCache,
    MetricsConnection metrics) {
    HRegionLocation oldLoc = cachedLocationSupplier.apply(loc);
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
   * Get a metric for {@code key} from {@code map}, or create it with {@code factory}.
   */
  private <T> T getMetric(String key, ConcurrentMap<String, T> map, NewMetric<T> factory) {
    return computeIfAbsent(map, key, () -> factory.newMetric(getClass(), key, scope));
  }
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
   * Get a metric for {@code key} from {@code map}, or create it with {@code factory}.
   */
  private <T> T getMetric(String key, ConcurrentMap<String, T> map, NewMetric<T> factory) {
    return computeIfAbsent(map, key, () -> factory.newMetric(getClass(), key, scope));
  }
```

### BoundedWildcard
Can generalize to `? extends Row`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
  }

  public RetriesExhaustedWithDetailsException(List<Throwable> exceptions, List<Row> actions,
    List<String> hostnameAndPort) {
    super("Failed " + exceptions.size() + " action" + pluralize(exceptions) + ": "
```

### BoundedWildcard
Can generalize to `? extends Throwable`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
  }

  public static Map<String, Integer> classifyExs(List<Throwable> ths) {
    Map<String, Integer> cls = new HashMap<>();
    for (Throwable t : ths) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
  // We will make use of the ServerStatisticTracker to determine whether we need to delay a bit,
  // based on the load of the region server and the region.
  private void sendOrDelay(Map<ServerName, ServerRequest> actionsByServer, int tries) {
    Optional<MetricsConnection> metrics = conn.getConnectionMetrics();
    Optional<ServerStatisticTracker> optStats = conn.getStatisticsTracker();
```

### BoundedWildcard
Can generalize to `? extends Stream`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
  }

  private void logException(int tries, Supplier<Stream<RegionRequest>> regionsSupplier,
    Throwable error, ServerName serverName) {
    if (tries > startLogErrorsCnt) {
```

### BoundedWildcard
Can generalize to `? extends Action`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
  }

  private void failAll(Stream<Action> actions, int tries, Throwable error, ServerName serverName) {
    long currentTime = EnvironmentEdgeManager.currentTime();
    String extras = getExtraContextForError(serverName);
```

### BoundedWildcard
Can generalize to `? super Action`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
  @SuppressWarnings("unchecked")
  private void onComplete(Action action, RegionRequest regionReq, int tries, ServerName serverName,
    RegionResult regionResult, List<Action> failedActions, Throwable regionException,
    MutableBoolean retryImmediately) {
    Object result = regionResult.result.getOrDefault(action.getOriginalIndex(), regionException);
```

### BoundedWildcard
Can generalize to `? extends Action`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
  }

  private void failAll(Stream<Action> actions, int tries) {
    actions.forEach(action -> {
      CompletableFuture<T> future = action2Future.get(action);
```

### BoundedWildcard
Can generalize to `? extends Action`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
  }

  private void addError(Iterable<Action> actions, Throwable error, ServerName serverName) {
    actions.forEach(action -> addError(action, error, serverName));
  }
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  }

  public ClusterMetricsBuilder setLiveServerMetrics(Map<ServerName, ServerMetrics> value) {
    liveServerMetrics.putAll(value);
    return this;
```

### BoundedWildcard
Can generalize to `? extends ServerMetrics`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
  }

  public ClusterMetricsBuilder setLiveServerMetrics(Map<ServerName, ServerMetrics> value) {
    liveServerMetrics.putAll(value);
    return this;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      ServerName masterName, List<ServerName> backupMasterNames,
      List<RegionState> regionsInTransition, String clusterId, List<String> masterCoprocessorNames,
      Boolean balancerOn, int masterInfoPort, List<ServerName> serversName,
      Map<TableName, RegionStatesCount> tableRegionStatesCount, List<ServerTask> masterTasks,
      List<ServerName> decommissionedServerNames) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    }

    private <T> T getOrDefault(Bytes key, Function<byte[], T> function, T defaultValue) {
      Bytes value = values.get(key);
      if (value == null) {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    }

    private <T> T getStringOrDefault(Bytes key, Function<String, T> function, T defaultValue) {
      return getOrDefault(key, b -> function.apply(Bytes.toString(b)), defaultValue);
    }
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
    }

    private <T> T getStringOrDefault(Bytes key, Function<String, T> function, T defaultValue) {
      return getOrDefault(key, b -> function.apply(Bytes.toString(b)), defaultValue);
    }
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    }

    private <T> T getOrDefault(Bytes key, Function<String, T> function, T defaultValue) {
      Bytes value = values.get(key);
      if (value == null) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    }

    private <T> T getOrDefault(Bytes key, Function<String, T> function, T defaultValue) {
      Bytes value = values.get(key);
      if (value == null) {
```

### BoundedWildcard
Can generalize to `? super Bytes`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  }

  public TableDescriptorBuilder removeValue(BiPredicate<Bytes, Bytes> predicate) {
    List<Bytes> toRemove =
      desc.getValues().entrySet().stream().filter(e -> predicate.test(e.getKey(), e.getValue()))
```

### BoundedWildcard
Can generalize to `? super Bytes`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  }

  public TableDescriptorBuilder removeValue(BiPredicate<Bytes, Bytes> predicate) {
    List<Bytes> toRemove =
      desc.getValues().entrySet().stream().filter(e -> predicate.test(e.getKey(), e.getValue()))
```

### BoundedWildcard
Can generalize to `? extends CoprocessorDescriptor`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  }

  public TableDescriptorBuilder setCoprocessors(Collection<CoprocessorDescriptor> cpDescs)
    throws IOException {
    for (CoprocessorDescriptor cpDesc : cpDescs) {
```

### BoundedWildcard
Can generalize to `? extends ColumnFamilyDescriptor`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java

  public TableDescriptorBuilder
    setColumnFamilies(final Collection<ColumnFamilyDescriptor> families) {
    families.forEach(desc::setColumnFamily);
    return this;
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
  @Override
  public void waitForMaximumCurrentTasks(long max, long id, int periodToTrigger,
    Consumer<Long> trigger) throws InterruptedIOException {
    assert max >= 0;
    long lastLog = EnvironmentEdgeManager.currentTime();
```

### BoundedWildcard
Can generalize to `? extends RowChecker`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
  }

  static Checker newChecker(List<RowChecker> checkers) {
    return new Checker() {
      private boolean isEnd = false;
```

### BoundedWildcard
Can generalize to `? extends AtomicInteger`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
    TaskCountChecker(final int maxTotalConcurrentTasks, final int maxConcurrentTasksPerServer,
      final int maxConcurrentTasksPerRegion, final AtomicLong tasksInProgress,
      final Map<ServerName, AtomicInteger> taskCounterPerServer,
      final Map<byte[], AtomicInteger> taskCounterPerRegion) {
      this.maxTotalConcurrentTasks = maxTotalConcurrentTasks;
```

### BoundedWildcard
Can generalize to `? extends AtomicInteger`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
      final int maxConcurrentTasksPerRegion, final AtomicLong tasksInProgress,
      final Map<ServerName, AtomicInteger> taskCounterPerServer,
      final Map<byte[], AtomicInteger> taskCounterPerRegion) {
      this.maxTotalConcurrentTasks = maxTotalConcurrentTasks;
      this.maxConcurrentTasksPerRegion = maxConcurrentTasksPerRegion;
```

### BoundedWildcard
Can generalize to `? super RegionLocationsFutureResult`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java

    private boolean tryComplete(LocateRequest req, CompletableFuture<RegionLocations> future,
      RegionLocations locations, List<RegionLocationsFutureResult> futureResultList) {
      if (future.isDone()) {
        return true;
```

### BoundedWildcard
Can generalize to `? super RegionLocations`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    private final Throwable e;

    public RegionLocationsFutureResult(CompletableFuture<RegionLocations> future,
      RegionLocations result, Throwable e) {
      this.future = future;
```

### BoundedWildcard
Can generalize to `? super ColumnFamilyDescriptor`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptor.java`
#### Snippet
```java

  static Comparator<TableDescriptor>
    getComparator(Comparator<ColumnFamilyDescriptor> cfComparator) {
    return (TableDescriptor lhs, TableDescriptor rhs) -> {
      int result = lhs.getTableName().compareTo(rhs.getTableName());
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java`
#### Snippet
```java
   * logging.
   */
  static String getShortNameToLog(final List<RegionInfo> ris) {
    return ris.stream().map(RegionInfo::getEncodedName).collect(Collectors.toList()).toString();
  }
```

### BoundedWildcard
Can generalize to `? extends Throwable`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java`
#### Snippet
```java

  private static String getMessage(String callableVitals, int numTries,
    List<Throwable> exceptions) {
    StringBuilder buffer = new StringBuilder("Failed contacting ");
    buffer.append(callableVitals);
```

### BoundedWildcard
Can generalize to `? extends ThrowableWithExtraContext`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java`
#### Snippet
```java

  private static String getMessage(final int numRetries,
    final List<ThrowableWithExtraContext> exceptions) {
    StringBuilder buffer = new StringBuilder("Failed after attempts=");
    buffer.append(numRetries + 1);
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocator.java`
#### Snippet
```java
  }

  private <T> CompletableFuture<T> tracedLocationFuture(Supplier<CompletableFuture<T>> action,
    Function<T, List<String>> getRegionNames, Supplier<Span> spanSupplier) {
    final Span span = spanSupplier.get();
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocator.java`
#### Snippet
```java

  private <T> CompletableFuture<T> tracedLocationFuture(Supplier<CompletableFuture<T>> action,
    Function<T, List<String>> getRegionNames, Supplier<Span> spanSupplier) {
    final Span span = spanSupplier.get();
    try (Scope scope = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocator.java`
#### Snippet
```java

  private <T> CompletableFuture<T> tracedLocationFuture(Supplier<CompletableFuture<T>> action,
    Function<T, List<String>> getRegionNames, Supplier<Span> spanSupplier) {
    final Span span = spanSupplier.get();
    try (Scope scope = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends Span`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionLocator.java`
#### Snippet
```java

  private <T> CompletableFuture<T> tracedLocationFuture(Supplier<CompletableFuture<T>> action,
    Function<T, List<String>> getRegionNames, Supplier<Span> spanSupplier) {
    final Span span = spanSupplier.get();
    try (Scope scope = span.makeCurrent()) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
  }

  private <T> CompletableFuture<T> getAndConvert(String path, Converter<T> converter) {
    CompletableFuture<T> future = new CompletableFuture<>();
    addListener(zk.get(path), (data, error) -> {
```

### BoundedWildcard
Can generalize to `? super RegionLocations`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java

  private static void tryComplete(MutableInt remaining, HRegionLocation[] locs,
    CompletableFuture<RegionLocations> future) {
    remaining.decrement();
    if (remaining.intValue() > 0) {
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  static <T> CompletableFuture<T> getOrFetch(AtomicReference<T> cacheRef,
    AtomicReference<CompletableFuture<T>> futureRef, boolean reload,
    Supplier<CompletableFuture<T>> fetch, Predicate<T> validator, String type) {
    for (;;) {
      if (!reload) {
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  static <T> CompletableFuture<T> getOrFetch(AtomicReference<T> cacheRef,
    AtomicReference<CompletableFuture<T>> futureRef, boolean reload,
    Supplier<CompletableFuture<T>> fetch, Predicate<T> validator, String type) {
    for (;;) {
      if (!reload) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
   * increase the hedge read related metrics.
   */
  private static <T> void connect(CompletableFuture<T> srcFuture, CompletableFuture<T> dstFuture,
    Optional<MetricsConnection> metrics) {
    addListener(srcFuture, (r, e) -> {
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
   * increase the hedge read related metrics.
   */
  private static <T> void connect(CompletableFuture<T> srcFuture, CompletableFuture<T> dstFuture,
    Optional<MetricsConnection> metrics) {
    addListener(srcFuture, (r, e) -> {
```

### BoundedWildcard
Can generalize to `? super Integer`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java

  private static <T> void sendRequestsToSecondaryReplicas(
    Function<Integer, CompletableFuture<T>> requestReplica, RegionLocations locs,
    CompletableFuture<T> future, Optional<MetricsConnection> metrics) {
    if (future.isDone()) {
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java

  private static <T> void sendRequestsToSecondaryReplicas(
    Function<Integer, CompletableFuture<T>> requestReplica, RegionLocations locs,
    CompletableFuture<T> future, Optional<MetricsConnection> metrics) {
    if (future.isDone()) {
```

### BoundedWildcard
Can generalize to `? extends PREQ`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  static <REQ, PREQ, PRESP, RESP> CompletableFuture<RESP> call(HBaseRpcController controller,
    HRegionLocation loc, ClientService.Interface stub, REQ req,
    Converter<PREQ, byte[], REQ> reqConvert, RpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, HBaseRpcController, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
```

### BoundedWildcard
Can generalize to `? super PREQ`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  static <REQ, PREQ, PRESP, RESP> CompletableFuture<RESP> call(HBaseRpcController controller,
    HRegionLocation loc, ClientService.Interface stub, REQ req,
    Converter<PREQ, byte[], REQ> reqConvert, RpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, HBaseRpcController, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
```

### BoundedWildcard
Can generalize to `? extends RESP`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    HRegionLocation loc, ClientService.Interface stub, REQ req,
    Converter<PREQ, byte[], REQ> reqConvert, RpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, HBaseRpcController, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    try {
```

### BoundedWildcard
Can generalize to `? super HBaseRpcController`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    HRegionLocation loc, ClientService.Interface stub, REQ req,
    Converter<PREQ, byte[], REQ> reqConvert, RpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, HBaseRpcController, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    try {
```

### BoundedWildcard
Can generalize to `? super PRESP`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
    HRegionLocation loc, ClientService.Interface stub, REQ req,
    Converter<PREQ, byte[], REQ> reqConvert, RpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, HBaseRpcController, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    try {
```

### BoundedWildcard
Can generalize to `? extends Get`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java`
#### Snippet
```java
  }

  static List<Get> toCheckExistenceOnly(List<Get> gets) {
    return gets.stream().map(ConnectionUtils::toCheckExistenceOnly).collect(toList());
  }
```

### BoundedWildcard
Can generalize to `? super AttributeKey`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/TableSpanBuilder.java`
#### Snippet
```java
   * @param tableName  the source of attribute values.
   */
  static void populateTableNameAttributes(final Map<AttributeKey<?>, Object> attributes,
    final TableName tableName) {
    attributes.put(DB_NAME, tableName.getNamespaceAsString());
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractRpcBasedConnectionRegistry.java`
#### Snippet
```java
   * points have been tried and all of them are failed, we will fail the future.
   */
  private <T extends Message> void groupCall(CompletableFuture<T> future, Set<ServerName> servers,
    List<ClientMetaService.Interface> stubs, int startIndexInclusive, Callable<T> callable,
    Predicate<T> isValidResp, String debug, ConcurrentLinkedQueue<Throwable> errors) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractRpcBasedConnectionRegistry.java`
#### Snippet
```java
   */
  private <T extends Message> void groupCall(CompletableFuture<T> future, Set<ServerName> servers,
    List<ClientMetaService.Interface> stubs, int startIndexInclusive, Callable<T> callable,
    Predicate<T> isValidResp, String debug, ConcurrentLinkedQueue<Throwable> errors) {
    int endIndexExclusive = Math.min(startIndexInclusive + hedgedReadFanOut, stubs.size());
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractRpcBasedConnectionRegistry.java`
#### Snippet
```java
  private <T extends Message> void groupCall(CompletableFuture<T> future, Set<ServerName> servers,
    List<ClientMetaService.Interface> stubs, int startIndexInclusive, Callable<T> callable,
    Predicate<T> isValidResp, String debug, ConcurrentLinkedQueue<Throwable> errors) {
    int endIndexExclusive = Math.min(startIndexInclusive + hedgedReadFanOut, stubs.size());
    AtomicInteger remaining = new AtomicInteger(endIndexExclusive - startIndexInclusive);
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractRpcBasedConnectionRegistry.java`
#### Snippet
```java

  private <T extends Message> CompletableFuture<T> call(ClientMetaService.Interface stub,
    Callable<T> callable) {
    HBaseRpcController controller = rpcControllerFactory.newController();
    CompletableFuture<T> future = new CompletableFuture<>();
```

### BoundedWildcard
Can generalize to `? super AttributeKey`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/ConnectionSpanBuilder.java`
#### Snippet
```java
   * @param userSupplier             the source of the {@code db.user} attribute value.
   */
  static void populateConnectionAttributes(final Map<AttributeKey<?>, Object> attributes,
    final Supplier<String> connectionStringSupplier, final Supplier<User> userSupplier) {
    attributes.put(DB_SYSTEM, DB_SYSTEM_VALUE);
```

### BoundedWildcard
Can generalize to `? super AttributeKey`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/IpcClientSpanBuilder.java`
#### Snippet
```java
   * @param md         the source of the RPC attribute values.
   */
  static void populateMethodDescriptorAttributes(final Map<AttributeKey<?>, Object> attributes,
    final Descriptors.MethodDescriptor md) {
    final String packageAndService = getRpcPackageAndService(md.getService());
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableImpl.java`
#### Snippet
```java
  public <S, R> CoprocessorServiceBuilder<S, R> coprocessorService(
    Function<RpcChannel, S> stubMaker, ServiceCaller<S, R> callable,
    CoprocessorCallback<R> callback) {
    final Context context = Context.current();
    CoprocessorCallback<R> wrappedCallback = new CoprocessorCallback<R>() {
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java`
#### Snippet
```java

  @Override
  public void filterRowCells(List<Cell> kvs) {
    kvs.removeIf(kv -> !stampSet.contains(kv.getTimestamp()));
  }
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
   * @return the evaluated filter
   */
  public static Filter popArguments(Stack<ByteBuffer> operatorStack, Stack<Filter> filterStack) {
    ByteBuffer argumentOnTopOfStack = operatorStack.peek();

```

### BoundedWildcard
Can generalize to `? extends Filter`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
   * @return the evaluated filter
   */
  public static Filter popArguments(Stack<ByteBuffer> operatorStack, Stack<Filter> filterStack) {
    ByteBuffer argumentOnTopOfStack = operatorStack.peek();

```

### BoundedWildcard
Can generalize to `? extends RowRange`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
     * that we can use Collections.binarySearch().
     */
    static List<ReversedRowRange> flipAndReverseRanges(List<RowRange> ranges) {
      List<ReversedRowRange> flippedRanges = new ArrayList<>(ranges.size());
      for (int i = ranges.size() - 1; i >= 0; i--) {
```

### BoundedWildcard
Can generalize to `? extends RowRange`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
  }

  private static void throwExceptionForInvalidRanges(List<RowRange> invalidRanges,
    boolean details) {
    StringBuilder sb = new StringBuilder();
```

### BoundedWildcard
Can generalize to `? extends RowRange`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
   * @return the ranges after sort and merge.
   */
  public static List<RowRange> sortAndMerge(List<RowRange> ranges) {
    if (ranges.isEmpty()) {
      throw new IllegalArgumentException("No ranges found.");
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java

  private static Map<TableName, List<String>>
    copyTableCFsMap(Map<TableName, List<String>> preTableCfs) {
    Map<TableName, List<String>> newTableCfs = new HashMap<>();
    preTableCfs.forEach(
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java

  public static ReplicationPeerConfig removeTableCFsFromReplicationPeerConfig(
    Map<TableName, List<String>> tableCfs, ReplicationPeerConfig peerConfig, String id)
    throws ReplicationException {
    Map<TableName, List<String>> preTableCfs = peerConfig.getTableCFsMap();
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java

  public static ReplicationPeerConfig removeExcludeTableCFsFromReplicationPeerConfig(
    Map<TableName, List<String>> excludeTableCfs, ReplicationPeerConfig peerConfig, String id)
    throws ReplicationException {
    if (excludeTableCfs == null) {
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java

  private static Map<TableName, List<String>>
    mergeTableCFs(Map<TableName, List<String>> preTableCfs, Map<TableName, List<String>> tableCfs) {
    Map<TableName, List<String>> newTableCfs = copyTableCFsMap(preTableCfs);
    for (Map.Entry<TableName, ? extends Collection<String>> entry : tableCfs.entrySet()) {
```

### BoundedWildcard
Can generalize to `? extends ExecutorService`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java

  TableOverAsyncTable(AsyncConnectionImpl conn, AsyncTable<?> table,
    IOExceptionSupplier<ExecutorService> poolSupplier) {
    this.conn = conn;
    this.table = table;
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java

  private <R> void coprocessorService(String serviceName, byte[] startKey, byte[] endKey,
    Batch.Callback<R> callback, StubCall<R> call) throws Throwable {
    // get regions covered by the row range
    ExecutorService pool = Context.current().wrap(this.poolSupplier.get());
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java

  private <R> void coprocessorService(String serviceName, byte[] startKey, byte[] endKey,
    Batch.Callback<R> callback, StubCall<R> call) throws Throwable {
    // get regions covered by the row range
    ExecutorService pool = Context.current().wrap(this.poolSupplier.get());
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java

  @Override
  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
    byte[] endKey, Batch.Call<T, R> callable, Batch.Callback<R> callback)
    throws ServiceException, Throwable {
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  @Override
  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
    byte[] endKey, Batch.Call<T, R> callable, Batch.Callback<R> callback)
    throws ServiceException, Throwable {
    final Supplier<Span> supplier = new TableOperationSpanBuilder(conn)
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  @Override
  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
    byte[] endKey, Batch.Call<T, R> callable, Batch.Callback<R> callback)
    throws ServiceException, Throwable {
    final Supplier<Span> supplier = new TableOperationSpanBuilder(conn)
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  @Override
  public <T extends Service, R> void coprocessorService(Class<T> service, byte[] startKey,
    byte[] endKey, Batch.Call<T, R> callable, Batch.Callback<R> callback)
    throws ServiceException, Throwable {
    final Supplier<Span> supplier = new TableOperationSpanBuilder(conn)
```

### BoundedWildcard
Can generalize to `? extends Filter`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListBase.java`
#### Snippet
```java
  }

  protected static boolean checkAndGetReversed(List<Filter> rowFilters, boolean defaultValue) {
    if (rowFilters.isEmpty()) {
      return defaultValue;
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java
  // Here we remove from row all key values from testing column
  @Override
  public void filterRowCells(List<Cell> kvs) {
    Iterator<? extends Cell> it = kvs.iterator();
    while (it.hasNext()) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
  private RowTracker tracker;

  public FuzzyRowFilter(List<Pair<byte[], byte[]>> fuzzyKeysData) {
    List<Pair<byte[], byte[]>> fuzzyKeyDataCopy = new ArrayList<>(fuzzyKeysData.size());

```

### BoundedWildcard
Can generalize to `? super Boolean`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/NettyHBaseSaslRpcClientHandler.java`
#### Snippet
```java
   *                    simple.
   */
  public NettyHBaseSaslRpcClientHandler(Promise<Boolean> saslPromise, UserGroupInformation ugi,
    SaslClientAuthenticationProvider provider, Token<? extends TokenIdentifier> token,
    InetAddress serverAddr, SecurityInfo securityInfo, boolean fallbackAllowed, Configuration conf)
```

### BoundedWildcard
Can generalize to `? extends SaslClientAuthenticationProvider`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/BuiltInProviderSelector.java`
#### Snippet
```java
  @Override
  public void configure(Configuration conf,
    Collection<SaslClientAuthenticationProvider> providers) {
    if (this.conf != null) {
      throw new IllegalStateException("configure() should only be called once");
```

### BoundedWildcard
Can generalize to `? super Byte`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/SaslClientAuthenticationProviders.java`
#### Snippet
```java
   */
  static void addProviderIfNotExists(SaslClientAuthenticationProvider provider,
    HashMap<Byte, SaslClientAuthenticationProvider> providers) {
    Byte code = provider.getSaslAuthMethod().getCode();
    SaslClientAuthenticationProvider existingProvider = providers.get(code);
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java`
#### Snippet
```java
    private int retries;

    protected ZKTask(String path, CompletableFuture<T> future, String operationType) {
      this.path = path;
      this.future = future;
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java

  private Map<TableName, List<String>>
    unmodifiableTableCFsMap(Map<TableName, List<String>> tableCFsMap) {
    Map<TableName, List<String>> newTableCFsMap = new HashMap<>();
    tableCFsMap.forEach((table, cfs) -> newTableCFsMap.put(table,
```

### BoundedWildcard
Can generalize to `? extends Get`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  @Override
  public List<CompletableFuture<Result>> get(List<Get> gets) {
    final Supplier<Span> supplier = newTableOperationSpanBuilder().setOperation(gets)
      .setContainerOperations(HBaseSemanticAttributes.Operation.GET);
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  private <S, R> void onLocateComplete(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, CoprocessorCallback<R> callback, List<HRegionLocation> locs,
    byte[] endKey, boolean endKeyInclusive, AtomicBoolean locateFinished,
    AtomicInteger unfinishedRequest, HRegionLocation loc, Throwable error) {
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  private <S, R> void onLocateComplete(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, CoprocessorCallback<R> callback, List<HRegionLocation> locs,
    byte[] endKey, boolean endKeyInclusive, AtomicBoolean locateFinished,
    AtomicInteger unfinishedRequest, HRegionLocation loc, Throwable error) {
```

### BoundedWildcard
Can generalize to `? extends Put`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  @Override
  public List<CompletableFuture<Void>> put(List<Put> puts) {
    final Supplier<Span> supplier = newTableOperationSpanBuilder().setOperation(puts)
      .setContainerOperations(HBaseSemanticAttributes.Operation.PUT);
```

### BoundedWildcard
Can generalize to `? super RpcChannel`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
  }

  private <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, RegionInfo region, byte[] row) {
    RegionCoprocessorRpcChannelImpl channel = new RegionCoprocessorRpcChannelImpl(conn, tableName,
```

### BoundedWildcard
Can generalize to `? extends S`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
  }

  private <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, RegionInfo region, byte[] row) {
    RegionCoprocessorRpcChannelImpl channel = new RegionCoprocessorRpcChannelImpl(conn, tableName,
```

### BoundedWildcard
Can generalize to `? super S`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  private <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, RegionInfo region, byte[] row) {
    RegionCoprocessorRpcChannelImpl channel = new RegionCoprocessorRpcChannelImpl(conn, tableName,
      region, row, rpcTimeoutNs, operationTimeoutNs);
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  private <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, RegionInfo region, byte[] row) {
    RegionCoprocessorRpcChannelImpl channel = new RegionCoprocessorRpcChannelImpl(conn, tableName,
      region, row, rpcTimeoutNs, operationTimeoutNs);
```

### BoundedWildcard
Can generalize to `? extends Delete`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java

  @Override
  public List<CompletableFuture<Void>> delete(List<Delete> deletes) {
    final Supplier<Span> supplier = newTableOperationSpanBuilder().setOperation(deletes)
      .setContainerOperations(HBaseSemanticAttributes.Operation.DELETE);
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockAndQueue.java`
#### Snippet
```java
  // ======================================================================

  public LockAndQueue(Function<Long, Procedure<?>> procedureRetriever) {
    this.procedureRetriever = procedureRetriever;
  }
```

### BoundedWildcard
Can generalize to `? extends Procedure`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockAndQueue.java`
#### Snippet
```java
  // ======================================================================

  public LockAndQueue(Function<Long, Procedure<?>> procedureRetriever) {
    this.procedureRetriever = procedureRetriever;
  }
```

### BoundedWildcard
Can generalize to `? super Procedure`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockAndQueue.java`
#### Snippet
```java

  @SuppressWarnings("rawtypes")
  public Stream<Procedure> filterWaitingQueue(Predicate<Procedure> predicate) {
    return queue.stream().filter(predicate);
  }
```

### BoundedWildcard
Can generalize to `? extends Procedure`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
   * Helper to lookup the root Procedure ID given a specified procedure.
   */
  protected static <T> Long getRootProcedureId(Map<Long, Procedure<T>> procedures,
    Procedure<T> proc) {
    while (proc.hasParent()) {
```

### BoundedWildcard
Can generalize to `? extends RemoteOperation`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java

  protected <T extends RemoteOperation> List<T> fetchType(
    final ArrayListMultimap<Class<?>, RemoteOperation> requestByType, final Class<T> type) {
    return (List<T>) requestByType.removeAll(type);
  }
```

### BoundedWildcard
Can generalize to `? extends RemoteProcedure`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java

  protected ArrayListMultimap<Class<?>, RemoteOperation> buildAndGroupRequestByType(final TEnv env,
    final TRemote remote, final Set<RemoteProcedure> remoteProcedures) {
    final ArrayListMultimap<Class<?>, RemoteOperation> requestByType = ArrayListMultimap.create();
    for (RemoteProcedure proc : remoteProcedures) {
```

### BoundedWildcard
Can generalize to `? extends Procedure`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java

  @Override
  public void addFront(Iterator<Procedure> procedureIterator) {
    schedLock();
    try {
```

### BoundedWildcard
Can generalize to `? super TEnvironment`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/TimeoutExecutorThread.java`
#### Snippet
```java
  }

  private void executeInMemoryChore(ProcedureInMemoryChore<TEnvironment> chore) {
    if (!chore.isWaiting()) {
      return;
```

### BoundedWildcard
Can generalize to `? extends ProcedureWALFile`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALFormat.java`
#### Snippet
```java
   * procedures.
   */
  public static void load(Iterator<ProcedureWALFile> logs, ProcedureStoreTracker tracker,
    Loader loader) throws IOException {
    ProcedureWALFormatReader reader = new ProcedureWALFormatReader(tracker, loader);
```

### BoundedWildcard
Can generalize to `? super TEnvironment`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  }

  private void restoreLock(Procedure<TEnvironment> proc, Set<Long> restored) {
    proc.restoreLock(getEnvironment());
    restored.add(proc.getProcId());
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  }

  private void restoreLock(Procedure<TEnvironment> proc, Set<Long> restored) {
    proc.restoreLock(getEnvironment());
    restored.add(proc.getProcId());
```

### BoundedWildcard
Can generalize to `? super TEnvironment`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  }

  private void releaseLock(Procedure<TEnvironment> proc, boolean force) {
    TEnvironment env = getEnvironment();
    // For how the framework works, we know that we will always have the lock
```

### BoundedWildcard
Can generalize to `? super TEnvironment`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  }

  private LockState acquireLock(Procedure<TEnvironment> proc) {
    TEnvironment env = getEnvironment();
    // if holdLock is true, then maybe we already have the lock, so just return LOCK_ACQUIRED if
```

### BoundedWildcard
Can generalize to `? extends Procedure`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  }

  private void restoreLocks(Deque<Procedure<TEnvironment>> stack, Set<Long> restored) {
    while (!stack.isEmpty()) {
      restoreLock(stack.pop(), restored);
```

### BoundedWildcard
Can generalize to `? extends KeyStore`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java

  private void initialize(Cluster cluster, Configuration conf, boolean sslEnabled,
    Optional<KeyStore> trustStore) {
    this.cluster = cluster;
    this.conf = conf;
```

### BoundedWildcard
Can generalize to `? extends Op`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java
   * actually pass to multi (need to do this in order to appendMetaData).
   */
  private Iterable<Op> prepareZKMulti(Iterable<Op> ops) throws UnsupportedOperationException {
    if (ops == null) {
      return null;
```

### BoundedWildcard
Can generalize to `? extends ACL`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java`
#### Snippet
```java
   * Validate whether ACL set for all superusers.
   */
  private boolean checkACLForSuperUsers(String[] superUsers, List<ACL> acls) {
    for (String user : superUsers) {
      boolean hasAccess = false;
```

### BoundedWildcard
Can generalize to `? super BitSetNode`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java

  private void setDeleteIf(ProcedureStoreTracker tracker,
    BiFunction<BitSetNode, Long, Boolean> func) {
    BitSetNode trackerNode = null;
    for (BitSetNode node : map.values()) {
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java

  private void setDeleteIf(ProcedureStoreTracker tracker,
    BiFunction<BitSetNode, Long, Boolean> func) {
    BitSetNode trackerNode = null;
    for (BitSetNode node : map.values()) {
```

### BoundedWildcard
Can generalize to `? extends MetricRegistries`
in `hbase-metrics-api/src/main/java/org/apache/hadoop/hbase/metrics/MetricRegistriesLoader.java`
#### Snippet
```java
   * @return A {@link MetricRegistries} implementation.
   */
  static MetricRegistries load(List<MetricRegistries> availableImplementations) {

    if (availableImplementations.size() == 1) {
```

### BoundedWildcard
Can generalize to `? extends ZKUtilOp`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
  }

  private static void processSequentially(ZKWatcher zkw, List<ZKUtilOp> ops)
    throws KeeperException, NoNodeException {
    for (ZKUtilOp op : ops) {
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java`
#### Snippet
```java
   */
  private static void extractDeadServersFromZNodeString(String deadServerListStr,
    List<ServerName> result) {
    if (deadServerListStr == null || result == null || deadServerListStr.isEmpty()) {
      return;
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/WrapperAsyncFSOutput.java`
#### Snippet
```java
  }

  private void flush0(CompletableFuture<Long> future, ByteArrayOutputStream buffer, boolean sync) {
    try {
      if (buffer.size() > 0) {
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationUtils.java`
#### Snippet
```java

  private static boolean isTableCFsEqual(Map<TableName, List<String>> tableCFs1,
    Map<TableName, List<String>> tableCFs2) {
    if (tableCFs1 == null) {
      return tableCFs2 == null;
```

### BoundedWildcard
Can generalize to `? extends ImmutableBytesWritable`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/MobRefReporter.java`
#### Snippet
```java
     * of base64 encoded row keys
     */
    private Text encodeRows(Context context, Text key, Iterable<ImmutableBytesWritable> rows)
      throws IOException {
      StringBuilder sb = new StringBuilder(key.toString());
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java`
#### Snippet
```java

  @Override
  public void addHFileRefs(String peerId, List<Pair<Path, Path>> pairs)
    throws ReplicationException {
    String peerNode = getHFileRefsPeerNode(peerId);
```

### BoundedWildcard
Can generalize to `? super ZKUtilOp`
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java`
#### Snippet
```java

  private void addLastSeqIdsToOps(String queueId, Map<String, Long> lastSeqIds,
    List<ZKUtilOp> listOfOps) throws KeeperException, ReplicationException {
    String peerId = new ReplicationQueueInfo(queueId).getPeerId();
    for (Entry<String, Long> lastSeqEntry : lastSeqIds.entrySet()) {
```

### BoundedWildcard
Can generalize to `? super Channel`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java

  private static void processWriteBlockResponse(Channel channel, DatanodeInfo dnInfo,
    Promise<Channel> promise, int timeoutMs) {
    channel.pipeline().addLast(new IdleStateHandler(timeoutMs, 0, 0, TimeUnit.MILLISECONDS),
      new ProtobufVarint32FrameDecoder(),
```

### BoundedWildcard
Can generalize to `? extends Throwable`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java`
#### Snippet
```java
  }

  private void failWaitingAckQueue(Channel channel, Supplier<Throwable> errorSupplier) {
    Throwable error = errorSupplier.get();
    for (Iterator<Callback> iter = waitingAckQueue.iterator(); iter.hasNext();) {
```

### BoundedWildcard
Can generalize to `? super FileStatus`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java`
#### Snippet
```java
  }

  static void addFile(List<FileStatus> result, LocatedFileStatus lfs, long startTime,
    long endTime) {
    long timestamp = AbstractFSWALProvider.getTimestamp(lfs.getPath().getName());
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      private int index = -1;

      ExportSnapshotRecordReader(final List<Pair<BytesWritable, Long>> files) {
        this.files = files;
        for (Pair<BytesWritable, Long> fileInfo : files) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  }

  private void setConfigParallel(FileSystem outputFs, List<Path> traversedPath,
    BiConsumer<FileSystem, Path> task, Configuration conf) throws IOException {
    ExecutorService pool = Executors
```

### BoundedWildcard
Can generalize to `? super FileSystem`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java

  private void setConfigParallel(FileSystem outputFs, List<Path> traversedPath,
    BiConsumer<FileSystem, Path> task, Configuration conf) throws IOException {
    ExecutorService pool = Executors
      .newFixedThreadPool(conf.getInt(CONF_COPY_MANIFEST_THREADS, DEFAULT_COPY_MANIFEST_THREADS));
```

### BoundedWildcard
Can generalize to `? super Path`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java

  private void setConfigParallel(FileSystem outputFs, List<Path> traversedPath,
    BiConsumer<FileSystem, Path> task, Configuration conf) throws IOException {
    ExecutorService pool = Executors
      .newFixedThreadPool(conf.getInt(CONF_COPY_MANIFEST_THREADS, DEFAULT_COPY_MANIFEST_THREADS));
```

### BoundedWildcard
Can generalize to `? extends Result`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
      private Result nextRowResult;

      public CellScanner(Iterator<Result> results) {
        this.results = results;
      }
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java

  public static List<InputSplit> getSplits(Scan scan, SnapshotManifest manifest,
    List<RegionInfo> regionManifests, Path restoreDir, Configuration conf,
    RegionSplitter.SplitAlgorithm sa, int numSplits) throws IOException {
    // load table descriptor
```

### BoundedWildcard
Can generalize to `? extends RegionLocator`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
   * Return the start keys of all of the regions in this table, as a list of ImmutableBytesWritable.
   */
  private static List<ImmutableBytesWritable> getRegionStartKeys(List<RegionLocator> regionLocators,
    boolean writeMultipleTables) throws IOException {

```

### BoundedWildcard
Can generalize to `? extends ImmutableBytesWritable`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
  @SuppressWarnings("deprecation")
  private static void writePartitions(Configuration conf, Path partitionsPath,
    List<ImmutableBytesWritable> startKeys, boolean writeMultipleTables) throws IOException {
    LOG.info("Writing partition information to " + partitionsPath);
    if (startKeys.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? super ColumnFamilyDescriptor`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
      value = "RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE")
  @InterfaceAudience.Private
  static String serializeColumnFamilyAttribute(Function<ColumnFamilyDescriptor, String> fn,
    List<TableDescriptor> allTables) throws UnsupportedEncodingException {
    StringBuilder attributeValue = new StringBuilder();
```

### BoundedWildcard
Can generalize to `? extends TableDescriptor`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
  @InterfaceAudience.Private
  static String serializeColumnFamilyAttribute(Function<ColumnFamilyDescriptor, String> fn,
    List<TableDescriptor> allTables) throws UnsupportedEncodingException {
    StringBuilder attributeValue = new StringBuilder();
    int i = 0;
```

### BoundedWildcard
Can generalize to `? extends TableInfo`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
  }

  static void configureIncrementalLoad(Job job, List<TableInfo> multiTableInfo,
    Class<? extends OutputFormat<?, ?>> cls) throws IOException {
    Configuration conf = job.getConfiguration();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
   * Execute compaction, from this client, one path at the time.
   */
  private int doClient(final FileSystem fs, final Set<Path> toCompactDirs,
    final boolean compactOnce, final boolean major) throws IOException {
    CompactionWorker worker = new CompactionWorker(fs, getConf());
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
     */
    public static List<Path> createInputFile(final FileSystem fs, final FileSystem stagingFs,
      final Path path, final Set<Path> toCompactDirs) throws IOException {
      // Extract the list of store dirs
      List<Path> storeDirs = new LinkedList<>();
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/jersey/SupplierFactoryAdapter.java`
#### Snippet
```java
  private final Supplier<T> supplier;

  public SupplierFactoryAdapter(Supplier<T> supplier) {
    this.supplier = supplier;
  }
```

### BoundedWildcard
Can generalize to `? extends Scan`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
   * @throws IOException When setting up the details fails.
   */
  public static void initTableMapperJob(List<Scan> scans, Class<? extends TableMapper> mapper,
    Class<?> outputKeyClass, Class<?> outputValueClass, Job job, boolean addDependencyJars,
    boolean initCredentials) throws IOException {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
   * @param packagedClasses map[class -> jar]
   */
  private static void updateMap(String jar, Map<String, String> packagedClasses)
    throws IOException {
    if (null == jar || jar.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
   * @param packagedClasses map[class -> jar]
   */
  private static void updateMap(String jar, Map<String, String> packagedClasses)
    throws IOException {
    if (null == jar || jar.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/RackManager.java`
#### Snippet
```java
   * @return list of racks for the given list of servers
   */
  public List<String> getRack(List<ServerName> servers) {
    // just a note - switchMapping caches results (at least the implementation should unless the
    // resolution is really a lightweight process)
```

### BoundedWildcard
Can generalize to `? extends BalancerRegionLoad`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFromRegionLoadFunction.java`
#### Snippet
```java
  }

  protected double getRegionLoadCost(Collection<BalancerRegionLoad> regionLoadList) {
    double cost = 0;
    for (BalancerRegionLoad rl : regionLoadList) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java
  }

  void refreshAndWait(Collection<RegionInfo> hris) {
    ArrayList<ListenableFuture<HDFSBlocksDistribution>> regionLocationFutures =
      new ArrayList<>(hris.size());
```

### BoundedWildcard
Can generalize to `? extends ServerMetrics`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java

  private float getOldLocality(ServerName newServer, byte[] regionName,
    Map<ServerName, ServerMetrics> oldServers) {
    ServerMetrics serverMetrics = oldServers.get(newServer);
    if (serverMetrics == null) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  }

  private void printHServerAddressSet(Set<ServerName> serverSet) {
    if (serverSet == null) {
      return;
```

### BoundedWildcard
Can generalize to `? extends Map`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  public void fillUp(TableName tableName, SnapshotOfRegionAssignmentFromMeta snapshot,
    Map<String, Map<String, Float>> regionLocalityMap) {
    // Set the table name
    this.tableName = tableName;
```

### BoundedWildcard
Can generalize to `? extends CostFunction`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java

  private void sendRejectionReasonToRingBuffer(Supplier<String> reason,
    List<CostFunction> costFunctions) {
    provider.recordBalancerRejection(() -> {
      BalancerRejection.Builder builder = new BalancerRejection.Builder().setReason(reason.get());
```

### BoundedWildcard
Can generalize to `? extends RegionPlan`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
  }

  private void sendRegionPlansToRingBuffer(List<RegionPlan> plans, double currentCost,
    double initCost, String initFunctionTotalCosts, long step) {
    provider.recordBalancerDecision(() -> {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java`
#### Snippet
```java
   * size 3.
   */
  public static Position getFavoredServerPosition(List<ServerName> favoredNodes,
    ServerName server) {
    if (
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java`
#### Snippet
```java
   * any region, the replica count would be [0, 0, 0].
   */
  public synchronized Map<ServerName, List<Integer>> getReplicaLoad(List<ServerName> servers) {
    Map<ServerName, List<Integer>> result = Maps.newHashMap();
    for (ServerName sn : servers) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java`
#### Snippet
```java
  }

  private synchronized void addToReplicaLoad(RegionInfo hri, List<ServerName> servers) {
    ServerName serverToUse =
      ServerName.valueOf(servers.get(PRIMARY.ordinal()).getAddress().toString(), NON_STARTCODE);
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java`
#### Snippet
```java
   * @return set of regions for which favored nodes is not applicable
   */
  public static Set<RegionInfo> filterNonFNApplicableRegions(Collection<RegionInfo> regions) {
    return regions.stream().filter(r -> !isFavoredNodeApplicable(r)).collect(Collectors.toSet());
  }
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesManager.java`
#### Snippet
```java
  }

  public synchronized void deleteFavoredNodesForRegions(Collection<RegionInfo> regionInfoList) {
    for (RegionInfo regionInfo : regionInfoList) {
      deleteFavoredNodesForRegion(regionInfo);
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
   */
  private ServerName randomAssignment(BalancerClusterState cluster, RegionInfo regionInfo,
    List<ServerName> servers) {
    int numServers = servers.size(); // servers is not null, numServers > 1
    ServerName sn = null;
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
   * Round-robin a list of regions to a list of servers
   */
  private void roundRobinAssignment(BalancerClusterState cluster, List<RegionInfo> regions,
    List<ServerName> servers, Map<ServerName, List<RegionInfo>> assignments) {
    Random rand = ThreadLocalRandom.current();
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
   */
  private void roundRobinAssignment(BalancerClusterState cluster, List<RegionInfo> regions,
    List<ServerName> servers, Map<ServerName, List<RegionInfo>> assignments) {
    Random rand = ThreadLocalRandom.current();
    List<RegionInfo> unassignedRegions = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
   */
  private void roundRobinAssignment(BalancerClusterState cluster, List<RegionInfo> regions,
    List<ServerName> servers, Map<ServerName, List<RegionInfo>> assignments) {
    Random rand = ThreadLocalRandom.current();
    List<RegionInfo> unassignedRegions = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
  }

  private BalancerClusterState createCluster(List<ServerName> servers,
    Collection<RegionInfo> regions) throws HBaseIOException {
    boolean hasRegionReplica = false;
```

### BoundedWildcard
Can generalize to `? extends Map`>
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java

  protected final Map<ServerName, List<RegionInfo>>
    toEnsumbleTableLoad(Map<TableName, Map<ServerName, List<RegionInfo>>> LoadOfAllTable) {
    Map<ServerName, List<RegionInfo>> returnMap = new TreeMap<>();
    for (Map<ServerName, List<RegionInfo>> serverNameListMap : LoadOfAllTable.values()) {
```

### BoundedWildcard
Can generalize to `? extends RegionPlan`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
   * Add a region from the head or tail to the List of regions to return.
   */
  private void addRegionPlan(final MinMaxPriorityQueue<RegionPlan> regionsToMove,
    final boolean fetchFromTail, final ServerName sn, List<RegionPlan> regionsToReturn) {
    RegionPlan rp = null;
```

### BoundedWildcard
Can generalize to `? super RegionPlan`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
   */
  private void addRegionPlan(final MinMaxPriorityQueue<RegionPlan> regionsToMove,
    final boolean fetchFromTail, final ServerName sn, List<RegionPlan> regionsToReturn) {
    RegionPlan rp = null;
    if (!fetchFromTail) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
   * ServerName with the correct start code from the list of provided servers.
   */
  private ServerName getServerFromFavoredNode(List<ServerName> servers, ServerName fn) {
    for (ServerName server : servers) {
      if (ServerName.isSameAddress(fn, server)) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
  @Override
  @NonNull
  public Map<ServerName, List<RegionInfo>> roundRobinAssignment(List<RegionInfo> regions,
    List<ServerName> servers) throws HBaseIOException {
    metricsBalancer.incrMiscInvocations();
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
    }

    private int getDifferentFavoredNode(BalancerClusterState cluster, List<ServerName> favoredNodes,
      int currentServer) {
      List<Integer> fnIndex = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
   * Return list of favored nodes that are online.
   */
  private List<ServerName> getOnlineFavoredNodes(List<ServerName> onlineServers,
    List<ServerName> serversWithoutStartCodes) {
    if (serversWithoutStartCodes == null) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
   */
  private List<ServerName> getOnlineFavoredNodes(List<ServerName> onlineServers,
    List<ServerName> serversWithoutStartCodes) {
    if (serversWithoutStartCodes == null) {
      return null;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java

    private int pickLeastLoadedFNServer(final BalancerClusterState cluster,
      List<ServerName> favoredNodes, int currentServerIndex) {
      List<Integer> fnIndex = new ArrayList<>();
      for (ServerName sn : favoredNodes) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java

  private Set<ServerName> getInheritedFNForDaughter(FavoredNodeAssignmentHelper helper,
    List<ServerName> parentFavoredNodes, Position primary, Position secondary) throws IOException {

    Set<ServerName> daughterFN = Sets.newLinkedHashSet();
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
  }

  private void addRegionToMap(Map<ServerName, List<RegionInfo>> assignmentMapForFavoredNodes,
    RegionInfo region, ServerName host) {
    List<RegionInfo> regionsOnServer = assignmentMapForFavoredNodes.get(host);
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
   */
  private Pair<Map<ServerName, List<RegionInfo>>, List<RegionInfo>>
    segregateRegionsAndAssignRegionsWithFavoredNodes(Collection<RegionInfo> regions,
      List<ServerName> onlineServers) throws HBaseIOException {

```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
  }

  private void mergeAssignmentMaps(Map<ServerName, List<RegionInfo>> assignmentMap,
    Map<ServerName, List<RegionInfo>> otherAssignments) {

```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java

  private Pair<Map<ServerName, List<RegionInfo>>, List<RegionInfo>>
    segregateRegionsAndAssignRegionsWithFavoredNodes(List<RegionInfo> regions,
      List<ServerName> availableServers) {
    Map<ServerName, List<RegionInfo>> assignmentMapForFavoredNodes =
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java

  private Set<ServerName> getInheritedFNForDaughter(FavoredNodeAssignmentHelper helper,
    List<ServerName> parentFavoredNodes, Position primary, Position secondary) throws IOException {

    Set<ServerName> daughterFN = Sets.newLinkedHashSet();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java

  private void assignSecondaryAndTertiaryNodesForRegion(
    FavoredNodeAssignmentHelper assignmentHelper, List<RegionInfo> regions,
    Map<RegionInfo, ServerName> primaryRSMap) throws IOException {
    // figure the secondary and tertiary RSs
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java
  }

  private void addRegionToMap(Map<ServerName, List<RegionInfo>> assignmentMapForFavoredNodes,
    RegionInfo region, ServerName host) {
    List<RegionInfo> regionsOnServer = assignmentMapForFavoredNodes.get(host);
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java
  // that matched (the favoredNode will have a startcode of -1 but we want the real
  // server with the legit startcode
  private ServerName availableServersContains(List<ServerName> servers, ServerName favoredNode) {
    for (ServerName server : servers) {
      if (ServerName.isSameAddress(favoredNode, server)) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java`
#### Snippet
```java

  private static List<RegionInfo>
    getListOfRegionInfos(final List<Pair<RegionInfo, ServerName>> pairs) {
    if (pairs == null || pairs.isEmpty()) {
      return Collections.emptyList();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java`
#### Snippet
```java
   * @throws IOException if problem connecting or updating meta
   */
  public static void addRegionsToMeta(Connection connection, List<RegionInfo> regionInfos,
    int regionReplication, long ts) throws IOException {
    List<Put> puts = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends Deque`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
  /** Helper for Cluster constructor to handle a region */
  private void registerRegion(RegionInfo region, int regionIndex, int serverIndex,
    Map<String, Deque<BalancerRegionLoad>> loads, RegionHDFSBlockLocationFinder regionFinder) {
    String tableName = region.getTable().getNameAsString();
    if (!tablesToIndex.containsKey(tableName)) {
```

### BoundedWildcard
Can generalize to `? extends CachedBlock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java`
#### Snippet
```java

  /** Returns A JSON String of <code>filename</code> and counts of <code>blocks</code> */
  public static String toJSON(String filename, NavigableSet<CachedBlock> blocks)
    throws IOException {
    CachedBlockCountsPerFile counts = new CachedBlockCountsPerFile(filename);
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketEntry.java`
#### Snippet
```java
  }

  <T> T withWriteLock(IdReadWriteLock<Long> offsetLock, BucketEntryHandler<T> handler) {
    ReentrantReadWriteLock lock = offsetLock.getLock(this.offset());
    try {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  }

  public static String getFavoredNodesAsString(List<ServerName> nodes) {
    StringBuilder strBuf = new StringBuilder();
    int i = 0;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
   * when we would like to find a replacement node.
   */
  private ServerName generateMissingFavoredNodeSingleRack(List<ServerName> favoredNodes,
    List<ServerName> excludeNodes) throws IOException {
    ServerName newServer = null;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
   */
  private ServerName generateMissingFavoredNodeSingleRack(List<ServerName> favoredNodes,
    List<ServerName> excludeNodes) throws IOException {
    ServerName newServer = null;
    Set<ServerName> excludeFNSet = Sets.newHashSet(favoredNodes);
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  // placement could be r2:s5, <skip-r3>, r4:s5, r1:s5, r2:s6, <skip-r3> ...
  // The regions should be distributed proportionately to the racksizes
  public void placePrimaryRSAsRoundRobin(Map<ServerName, List<RegionInfo>> assignmentMap,
    Map<RegionInfo, ServerName> primaryRSMap, List<RegionInfo> regions) {
    List<String> rackList = new ArrayList<>(rackToRegionServerMap.size());
```

### BoundedWildcard
Can generalize to `? super RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  // The regions should be distributed proportionately to the racksizes
  public void placePrimaryRSAsRoundRobin(Map<ServerName, List<RegionInfo>> assignmentMap,
    Map<RegionInfo, ServerName> primaryRSMap, List<RegionInfo> regions) {
    List<String> rackList = new ArrayList<>(rackToRegionServerMap.size());
    rackList.addAll(rackToRegionServerMap.keySet());
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  // The regions should be distributed proportionately to the racksizes
  public void placePrimaryRSAsRoundRobin(Map<ServerName, List<RegionInfo>> assignmentMap,
    Map<RegionInfo, ServerName> primaryRSMap, List<RegionInfo> regions) {
    List<String> rackList = new ArrayList<>(rackToRegionServerMap.size());
    rackList.addAll(rackToRegionServerMap.keySet());
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  // The regions should be distributed proportionately to the racksizes
  public void placePrimaryRSAsRoundRobin(Map<ServerName, List<RegionInfo>> assignmentMap,
    Map<RegionInfo, ServerName> primaryRSMap, List<RegionInfo> regions) {
    List<String> rackList = new ArrayList<>(rackToRegionServerMap.size());
    rackList.addAll(rackToRegionServerMap.keySet());
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
   * @param skipServerSet the server shouldn't belong to this set
   */
  protected ServerName getOneRandomServer(String rack, Set<ServerName> skipServerSet) {

    // Is the rack valid? Do we recognize it?
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  }

  public FavoredNodeAssignmentHelper(final List<ServerName> servers,
    final RackManager rackManager) {
    this.servers = servers;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
   * generate a node within a few iterations, ideally 1.
   */
  private ServerName generateMissingFavoredNodeMultiRack(List<ServerName> favoredNodes,
    List<ServerName> excludeNodes) throws IOException {

```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
   */
  private ServerName generateMissingFavoredNodeMultiRack(List<ServerName> favoredNodes,
    List<ServerName> excludeNodes) throws IOException {

    Set<String> racks = Sets.newHashSet();
```

### BoundedWildcard
Can generalize to `? extends Set`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java

  private ServerName[] multiRackCaseWithRestrictions(
    Map<ServerName, Set<RegionInfo>> serverToPrimaries,
    Map<RegionInfo, ServerName[]> secondaryAndTertiaryMap, String primaryRack, ServerName primaryRS,
    RegionInfo regionInfo) throws IOException {
```

### BoundedWildcard
Can generalize to `? extends KeyValueStats`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    private final DateFormat dateFormat;

    private SimpleReporter(PrintStream output, List<KeyValueStats> stats, Locale locale,
      TimeZone timeZone) {
      this.output = output;
```

### BoundedWildcard
Can generalize to `? extends BlockingQueue`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcExecutor.java`
#### Snippet
```java
   */
  protected void startHandlers(final String nameSuffix, final int numHandlers,
    final List<BlockingQueue<CallRunner>> callQueues, final int qindex, final int qsize,
    final int port, final AtomicInteger activeHandlerCount) {
    final String threadPrefix = name + Strings.nullToEmpty(nameSuffix);
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java

  static ByteBuffer createHeaderAndMessageBytes(Message result, Message header, int cellBlockSize,
    List<ByteBuffer> cellBlock) throws IOException {
    // Organize the response as a set of bytebuffers rather than collect it all together inside
    // one big byte array; save on allocations.
```

### BoundedWildcard
Can generalize to `? super RAMQueueEntry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
     * Notice, don't change its reference count in the {@link Consumer}
     */
    public boolean remove(BlockCacheKey key, Consumer<RAMQueueEntry> action) {
      RAMQueueEntry previous = delegate.remove(key);
      action.accept(previous);
```

### BoundedWildcard
Can generalize to `? extends RAMQueueEntry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
   *                interference expected.
   */
  void doDrain(final List<RAMQueueEntry> entries, ByteBuffer metaBuff) throws InterruptedException {
    if (entries.isEmpty()) {
      return;
```

### BoundedWildcard
Can generalize to `? extends RAMQueueEntry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
   * @return {@code receptacle} laden with elements taken from the queue or empty if none found.
   */
  static List<RAMQueueEntry> getRAMQueueEntries(BlockingQueue<RAMQueueEntry> q,
    List<RAMQueueEntry> receptacle) throws InterruptedException {
    // Clear sets all entries to null and sets size to 0. We retain allocations. Presume it
```

### BoundedWildcard
Can generalize to `? extends CallRunner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcHandler.java`
#### Snippet
```java

  RpcHandler(final String name, final double handlerFailureThreshhold, final int handlerCount,
    final BlockingQueue<CallRunner> q, final AtomicInteger activeHandlerCount,
    final AtomicInteger failedHandlerCount, final Abortable abortable) {
    super(name);
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  private CompletableFuture<List<LogEntry>> getSlowLogResponses(
    final Map<String, Object> filterParams, final Set<ServerName> serverNames, final int limit,
    final String logType) {
    if (CollectionUtils.isEmpty(serverNames)) {
```

### BoundedWildcard
Can generalize to `? super RpcChannel`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, ServerName serverName) {
    RegionServerCoprocessorRpcChannelImpl channel = new RegionServerCoprocessorRpcChannelImpl(
```

### BoundedWildcard
Can generalize to `? extends S`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, ServerName serverName) {
    RegionServerCoprocessorRpcChannelImpl channel = new RegionServerCoprocessorRpcChannelImpl(
```

### BoundedWildcard
Can generalize to `? super S`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, ServerName serverName) {
    RegionServerCoprocessorRpcChannelImpl channel = new RegionServerCoprocessorRpcChannelImpl(
      this.<Message> newServerCaller().serverName(serverName));
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable, ServerName serverName) {
    RegionServerCoprocessorRpcChannelImpl channel = new RegionServerCoprocessorRpcChannelImpl(
      this.<Message> newServerCaller().serverName(serverName));
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public CompletableFuture<List<Boolean>>
    clearSlowLogResponses(@Nullable Set<ServerName> serverNames) {
    if (CollectionUtils.isEmpty(serverNames)) {
      return CompletableFuture.completedFuture(Collections.emptyList());
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  private void checkAndGetTableName(byte[] encodeRegionName, AtomicReference<TableName> tableName,
    CompletableFuture<TableName> result) {
    addListener(getRegionLocation(encodeRegionName), (location, err) -> {
      if (err != null) {
```

### BoundedWildcard
Can generalize to `? extends RESP`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  private <PREQ, PRESP, RESP> CompletableFuture<RESP> adminCall(HBaseRpcController controller,
    AdminService.Interface stub, PREQ preq, AdminRpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    rpcCall.call(stub, controller, preq, new RpcCallback<PRESP>() {
```

### BoundedWildcard
Can generalize to `? super PRESP`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  private <PREQ, PRESP, RESP> CompletableFuture<RESP> adminCall(HBaseRpcController controller,
    AdminService.Interface stub, PREQ preq, AdminRpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    rpcCall.call(stub, controller, preq, new RpcCallback<PRESP>() {
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  }

  private <T> void completeConditionalOnFuture(CompletableFuture<T> dependentFuture,
    CompletableFuture<T> parentFuture) {
    addListener(parentFuture, (res, err) -> {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  private <T> void completeConditionalOnFuture(CompletableFuture<T> dependentFuture,
    CompletableFuture<T> parentFuture) {
    addListener(parentFuture, (res, err) -> {
      if (err != null) {
```

### BoundedWildcard
Can generalize to `? super MasterRequestCallerBuilder`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  private <PREQ, PRESP> CompletableFuture<Void> procedureCall(
    Consumer<MasterRequestCallerBuilder<?>> prioritySetter, PREQ preq,
    MasterRpcCall<PRESP, PREQ> rpcCall, Converter<Long, PRESP> respConverter,
    ProcedureBiConsumer consumer) {
```

### BoundedWildcard
Can generalize to `? extends RESP`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  private <PREQ, PRESP, RESP> CompletableFuture<RESP> call(HBaseRpcController controller,
    MasterService.Interface stub, PREQ preq, MasterRpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    rpcCall.call(stub, controller, preq, new RpcCallback<PRESP>() {
```

### BoundedWildcard
Can generalize to `? super PRESP`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  private <PREQ, PRESP, RESP> CompletableFuture<RESP> call(HBaseRpcController controller,
    MasterService.Interface stub, PREQ preq, MasterRpcCall<PRESP, PREQ> rpcCall,
    Converter<RESP, PRESP> respConverter) {
    CompletableFuture<RESP> future = new CompletableFuture<>();
    rpcCall.call(stub, controller, preq, new RpcCallback<PRESP>() {
```

### BoundedWildcard
Can generalize to `? super RpcChannel`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable) {
    MasterCoprocessorRpcChannelImpl channel =
```

### BoundedWildcard
Can generalize to `? extends S`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable) {
    MasterCoprocessorRpcChannelImpl channel =
```

### BoundedWildcard
Can generalize to `? super S`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable) {
    MasterCoprocessorRpcChannelImpl channel =
      new MasterCoprocessorRpcChannelImpl(this.<Message> newMasterCaller());
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  @Override
  public <S, R> CompletableFuture<R> coprocessorService(Function<RpcChannel, S> stubMaker,
    ServiceCaller<S, R> callable) {
    MasterCoprocessorRpcChannelImpl channel =
      new MasterCoprocessorRpcChannelImpl(this.<Message> newMasterCaller());
```

### BoundedWildcard
Can generalize to `? extends CompletableFuture`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java

  private static <T> CompletableFuture<List<T>>
    convertToFutureOfList(List<CompletableFuture<T>> futures) {
    CompletableFuture<Void> allDoneFuture =
      CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
```

### BoundedWildcard
Can generalize to `? extends Runnable`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java`
#### Snippet
```java
  }

  protected void updateMethodCountAndSizeByQueue(BlockingQueue<Runnable> queue,
    HashMap<String, Long> methodCount, HashMap<String, Long> methodSize) {
    for (Runnable r : queue) {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java`
#### Snippet
```java

  protected void updateMethodCountAndSizeByQueue(BlockingQueue<Runnable> queue,
    HashMap<String, Long> methodCount, HashMap<String, Long> methodSize) {
    for (Runnable r : queue) {
      FifoCallRunner mcr = (FifoCallRunner) r;
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/FifoRpcScheduler.java`
#### Snippet
```java

  protected void updateMethodCountAndSizeByQueue(BlockingQueue<Runnable> queue,
    HashMap<String, Long> methodCount, HashMap<String, Long> methodSize) {
    for (Runnable r : queue) {
      FifoCallRunner mcr = (FifoCallRunner) r;
```

### BoundedWildcard
Can generalize to `? extends BlockingServiceAndInterface`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServerFactory.java`
#### Snippet
```java

  public static RpcServer createRpcServer(final Server server, final String name,
    final List<BlockingServiceAndInterface> services, final InetSocketAddress bindAddress,
    Configuration conf, RpcScheduler scheduler, boolean reservoirEnabled) throws IOException {
    String rpcServerClass =
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
   */
  public void archiveMobFiles(Configuration conf, TableName tableName, byte[] family,
    List<Path> storeFiles) throws IOException {

    if (storeFiles.size() == 0) {
```

### BoundedWildcard
Can generalize to `? extends BlockingServiceAndInterface`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   */
  protected static BlockingServiceAndInterface getServiceAndInterface(
    final List<BlockingServiceAndInterface> services, final String serviceName) {
    for (BlockingServiceAndInterface bs : services) {
      if (bs.getBlockingService().getDescriptorForType().getName().equals(serviceName)) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
   */
  public void archiveMobFiles(Configuration conf, TableName tableName, byte[] family,
    List<Path> storeFiles) throws IOException {

    if (storeFiles.size() == 0) {
```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java
   * @return True if the list has a mob reference tag, false if it doesn't.
   */
  public static boolean hasMobReferenceTag(List<Tag> tags) {
    if (!tags.isEmpty()) {
      for (Tag tag : tags) {
```

### BoundedWildcard
Can generalize to `? super IOException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java

  protected Path closeRecoveredEditsWriter(RecoveredEditsWriter editsWriter,
    List<IOException> thrown) throws IOException {
    try {
      editsWriter.writer.close();
```

### BoundedWildcard
Can generalize to `? extends WAL.Entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
    }

    void writeRegionEntries(List<WAL.Entry> entries) throws IOException {
      long startTime = System.nanoTime();
      int editsCount = 0;
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java

  @InterfaceAudience.Private
  public WALEdit add(List<Cell> cells) {
    if (cells == null || cells.isEmpty()) {
      return this;
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/WriteSinkCoprocessor.java`
#### Snippet
```java

  @Override
  public void preOpen(ObserverContext<RegionCoprocessorEnvironment> e) throws IOException {
    regionName = e.getEnvironment().getRegion().getRegionInfo().getRegionNameAsString();
  }
```

### BoundedWildcard
Can generalize to `? extends FileSystem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
  @Deprecated
  public static long getMaxRegionSequenceId(Configuration conf, RegionInfo region,
    IOExceptionSupplier<FileSystem> rootFsSupplier, IOExceptionSupplier<FileSystem> walFsSupplier)
    throws IOException {
    FileSystem rootFs = rootFsSupplier.get();
```

### BoundedWildcard
Can generalize to `? extends FileSystem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
  @Deprecated
  public static long getMaxRegionSequenceId(Configuration conf, RegionInfo region,
    IOExceptionSupplier<FileSystem> rootFsSupplier, IOExceptionSupplier<FileSystem> walFsSupplier)
    throws IOException {
    FileSystem rootFs = rootFsSupplier.get();
```

### BoundedWildcard
Can generalize to `? extends StoreFileScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java

    @Override
    public InternalScanner createScanner(ScanInfo scanInfo, List<StoreFileScanner> scanners,
      ScanType scanType, FileDetails fd, long smallestReadPoint) throws IOException {
      return new StoreScanner(store, scanInfo, scanners, scanType, smallestReadPoint,
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
  private StoreFileWriter switchToNewMobWriter(StoreFileWriter mobFileWriter, FileDetails fd,
    long mobCells, boolean major, CompactionRequestImpl request,
    List<String> committedMobWriterFileNames) throws IOException {
    commitOrAbortMobWriter(mobFileWriter, fd.maxSeqId, mobCells, major);
    committedMobWriterFileNames.add(mobFileWriter.getPath().getName());
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
  }

  private void internalGet(Path dir, Map<String, TableDescriptor> tds, AtomicBoolean allvisited) {
    TableDescriptor htd = get(CommonFSUtils.getTableName(dir));
    if (htd == null) {
```

### BoundedWildcard
Can generalize to `? super TableDescriptor`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
  }

  private void internalGet(Path dir, Map<String, TableDescriptor> tds, AtomicBoolean allvisited) {
    TableDescriptor htd = get(CommonFSUtils.getTableName(dir));
    if (htd == null) {
```

### BoundedWildcard
Can generalize to `? extends Map.Entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConfigurationUtil.java`
#### Snippet
```java
   */
  public static void setKeyValues(Configuration conf, String key,
    Collection<Map.Entry<String, String>> keyValues, char delimiter) {
    List<String> serializedKvps = Lists.newArrayList();

```

### BoundedWildcard
Can generalize to `? super RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithoutAck.java`
#### Snippet
```java

  MoveWithoutAck(Admin admin, RegionInfo regionInfo, ServerName sourceServer,
    ServerName targetServer, List<RegionInfo> movedRegions) {
    this.admin = admin;
    this.region = regionInfo;
```

### BoundedWildcard
Can generalize to `? extends JVMClusterUtil.MasterThread`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java

  private static JVMClusterUtil.MasterThread
    findActiveMaster(List<JVMClusterUtil.MasterThread> masters) {
    for (JVMClusterUtil.MasterThread t : masters) {
      if (t.master.isActiveMaster()) {
```

### BoundedWildcard
Can generalize to `? extends MasterThread`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
  /**
   *   */
  public static void shutdown(final List<MasterThread> masters,
    final List<RegionServerThread> regionservers) {
    LOG.debug("Shutting down HBase Cluster");
```

### BoundedWildcard
Can generalize to `? extends RegionServerThread`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
   *   */
  public static void shutdown(final List<MasterThread> masters,
    final List<RegionServerThread> regionservers) {
    LOG.debug("Shutting down HBase Cluster");
    if (masters != null) {
```

### BoundedWildcard
Can generalize to `? extends JVMClusterUtil.RegionServerThread`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
   */
  public static String startup(final List<JVMClusterUtil.MasterThread> masters,
    final List<JVMClusterUtil.RegionServerThread> regionservers) throws IOException {
    // Implementation note: This method relies on timed sleeps in a loop. It's not great, and
    // should probably be re-written to use actual synchronization objects, but it's ok for now
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
   */
  public static void editRegions(final ThreadPoolExecutor exec,
    final Collection<RegionInfo> regions, final RegionEditTask task) throws IOException {
    final ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<>(exec);
    for (final RegionInfo hri : regions) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
  }

  static LinkedList<Pair<byte[], byte[]>> splitScan(LinkedList<Pair<byte[], byte[]>> regionList,
    final Connection connection, final TableName tableName, SplitAlgorithm splitAlgo)
    throws IOException, InterruptedException {
```

### BoundedWildcard
Can generalize to `? super CoprocessorViolation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java`
#### Snippet
```java

  private void validate(ClassLoader classLoader, String className,
    List<CoprocessorViolation> violations) {
    LOG.debug("Validating class '{}'.", className);

```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java`
#### Snippet
```java
  }

  public LossyCounting(String name, double errorRate, LossyCountingListener<T> listener) {
    this.name = name;
    if (errorRate < 0.0 || errorRate > 1.0) {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java`
#### Snippet
```java
  private Map<String, Map<String, Float>> regionDegreeLocalityMapping;

  FSRegionScanner(FileSystem fs, Path regionPath, Map<String, String> regionToBestLocalityRSMapping,
    Map<String, Map<String, Float>> regionDegreeLocalityMapping) {
    this.fs = fs;
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java`
#### Snippet
```java
  private Map<String, Map<String, Float>> regionDegreeLocalityMapping;

  FSRegionScanner(FileSystem fs, Path regionPath, Map<String, String> regionToBestLocalityRSMapping,
    Map<String, Map<String, Float>> regionDegreeLocalityMapping) {
    this.fs = fs;
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java`
#### Snippet
```java

  FSRegionScanner(FileSystem fs, Path regionPath, Map<String, String> regionToBestLocalityRSMapping,
    Map<String, Map<String, Float>> regionDegreeLocalityMapping) {
    this.fs = fs;
    this.regionPath = regionPath;
```

### BoundedWildcard
Can generalize to `? super Map`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java`
#### Snippet
```java

  FSRegionScanner(FileSystem fs, Path regionPath, Map<String, String> regionToBestLocalityRSMapping,
    Map<String, Map<String, Float>> regionDegreeLocalityMapping) {
    this.fs = fs;
    this.regionPath = regionPath;
```

### BoundedWildcard
Can generalize to `? extends R`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java
   * @return a list of ranges that overlap with most others
   */
  public static <R extends KeyRange> List<R> findBigRanges(Collection<R> bigOverlap, int count) {
    List<R> bigRanges = new ArrayList<>();

```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java
  private final static byte[] ENDKEY = null;

  public RegionSplitCalculator(Comparator<R> cmp) {
    rangeCmp = cmp;
  }
```

### BoundedWildcard
Can generalize to `? super RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/MoveWithAck.java`
#### Snippet
```java

  MoveWithAck(Connection conn, RegionInfo regionInfo, ServerName sourceServer,
    ServerName targetServer, List<RegionInfo> movedRegions) throws IOException {
    this.conn = conn;
    this.region = regionInfo;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java`
#### Snippet
```java
   */
  public static void fixMultiAssignment(Connection connection, RegionInfo region,
    List<ServerName> servers) throws IOException, KeeperException, InterruptedException {
    // Close region on the servers silently
    for (ServerName server : servers) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
   * lines
   */
  private void writeFile(String filename, List<RegionInfo> movedRegions) throws IOException {
    try (DataOutputStream dos =
      new DataOutputStream(new BufferedOutputStream(new FileOutputStream(filename)))) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
  }

  private void loadRegions(List<RegionInfo> regionsToMove) throws Exception {
    ServerName server = getTargetServer();
    List<RegionInfo> movedRegions = Collections.synchronizedList(new ArrayList<>());
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
  }

  private void submitRegionMovesWhileUnloading(ServerName server, List<ServerName> regionServers,
    List<RegionInfo> movedRegions, List<RegionInfo> regionsToMove) throws Exception {
    final ExecutorService moveRegionsPool = Executors.newFixedThreadPool(this.maxthreads);
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java

  private void submitRegionMovesWhileUnloading(ServerName server, List<ServerName> regionServers,
    List<RegionInfo> movedRegions, List<RegionInfo> regionsToMove) throws Exception {
    final ExecutorService moveRegionsPool = Executors.newFixedThreadPool(this.maxthreads);
    List<Future<Boolean>> taskList = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends Future`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java

  private void waitMoveTasksToFinish(ExecutorService moveRegionsPool,
    List<Future<Boolean>> taskList, long timeoutInSeconds) throws Exception {
    try {
      if (!moveRegionsPool.awaitTermination(timeoutInSeconds, TimeUnit.SECONDS)) {
```

### BoundedWildcard
Can generalize to `? extends HbckRegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
  }

  private void dumpOverlapProblems(Multimap<byte[], HbckRegionInfo> regions) {
    // we display this way because the last end key should be displayed as
    // well.
```

### BoundedWildcard
Can generalize to `? extends HbckRegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java

  public synchronized ImmutableList<RegionInfo>
    getRegionsFromMeta(TreeMap<String, HbckRegionInfo> regionInfoMap) {
    // lazy loaded, synchronized to ensure a single load
    if (regionsFromMeta == null) {
```

### BoundedWildcard
Can generalize to `? extends HbckRegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
    }

    void removeParentsAndFixSplits(Collection<HbckRegionInfo> overlap) throws IOException {
      Pair<byte[], byte[]> range = null;
      HbckRegionInfo parent = null;
```

### BoundedWildcard
Can generalize to `? extends HbckRegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
    }

    void mergeOverlaps(Collection<HbckRegionInfo> overlap) throws IOException {
      String thread = Thread.currentThread().getName();
      LOG.info(
```

### BoundedWildcard
Can generalize to `? extends HbckRegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
   * This dumps data in a visually reasonable way for visual debugging
   */
  private void dump(SortedSet<byte[]> splits, Multimap<byte[], HbckRegionInfo> regions) {
    // we display this way because the last end key should be displayed as well.
    StringBuilder sb = new StringBuilder();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java`
#### Snippet
```java
   * Check the specified table dirs for bad hfiles.
   */
  public void checkTables(Collection<Path> tables) throws IOException {
    for (Path t : tables) {
      checkTableDir(t);
```

### BoundedWildcard
Can generalize to `? extends StoreFileInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactionTTLRequest.java`
#### Snippet
```java
  @Override
  protected boolean shouldIncludeStore(HRegionFileSystem fileSystem, String family,
    Collection<StoreFileInfo> storeFiles, long ts) throws IOException {

    for (StoreFileInfo storeFile : storeFiles) {
```

### BoundedWildcard
Can generalize to `? extends StoreFileInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactionRequest.java`
#### Snippet
```java

  protected boolean shouldIncludeStore(HRegionFileSystem fileSystem, String family,
    Collection<StoreFileInfo> storeFiles, long ts) throws IOException {

    for (StoreFileInfo storeFile : storeFiles) {
```

### BoundedWildcard
Can generalize to `? extends HbckTableInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * Prints summary of all tables found on the system.
   */
  private void printTableSummary(SortedMap<TableName, HbckTableInfo> tablesInfo) {
    StringBuilder sb = new StringBuilder();
    int numOfSkippedRegions;
```

### BoundedWildcard
Can generalize to `? extends CheckRegionConsistencyWorkItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   */
  private void
    checkRegionConsistencyConcurrently(final List<CheckRegionConsistencyWorkItem> workItems)
      throws IOException, KeeperException, InterruptedException {
    if (workItems.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * @throws IOException if a remote or network exception occurs
   */
  void processRegionServers(Collection<ServerName> regionServerList)
    throws IOException, InterruptedException {

```

### BoundedWildcard
Can generalize to `? extends HbckRegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * Iterates through the list of all orphan/invalid regiondirs.
   */
  private void adoptHdfsOrphans(Collection<HbckRegionInfo> orphanHdfsDirs) throws IOException {
    for (HbckRegionInfo hi : orphanHdfsDirs) {
      LOG.info("Attempting to handle orphan hdfs dir: " + hi.getHdfsRegionDir());
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    }

    private List<RegionInfo> filterRegions(List<RegionInfo> regions) {
      List<RegionInfo> ret = Lists.newArrayList();
      for (RegionInfo hri : regions) {
```

### BoundedWildcard
Can generalize to `? extends Future`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
  }

  private boolean futuresComplete(List<Future<?>> futures) {
    futures.removeIf(Future::isDone);
    return futures.isEmpty();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
   */
  public static void archiveRegions(Configuration conf, FileSystem fs, Path rootDir, Path tableDir,
    List<Path> regionDirList) throws IOException {
    List<Future<Void>> futures = new ArrayList<>(regionDirList.size());
    for (Path regionDir : regionDirList) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
   *                     throwing the exception, rather than failing at the first file.
   */
  private static void deleteStoreFilesWithoutArchiving(Collection<HStoreFile> compactedFiles)
    throws IOException {
    LOG.debug("Deleting files without archiving.");
```

### BoundedWildcard
Can generalize to `? extends File`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
   */
  private static List<File> resolveAndArchive(FileSystem fs, Path baseArchiveDir,
    Collection<File> toArchive, long start) throws IOException {
    // short circuit if no files to move
    if (toArchive.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java`
#### Snippet
```java
   */
  private boolean cleanOldServerName(ServerName newServerName,
    Iterator<ServerName> deadServerIterator) {
    ServerName sn = deadServerIterator.next();
    if (ServerName.isSameAddress(sn, newServerName)) {
```

### BoundedWildcard
Can generalize to `? extends JVMClusterUtil.RegionServerThread`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java

  private static void
    closeAllRegionServerThreads(List<JVMClusterUtil.RegionServerThread> regionservers) {
    for (JVMClusterUtil.RegionServerThread t : regionservers) {
      t.getRegionServer().stop("HMaster Aborted; Bringing down regions servers");
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitWALManager.java`
#### Snippet
```java
  }

  List<Procedure> createSplitWALProcedures(List<FileStatus> splittingWALs,
    ServerName crashedServer) {
    return splittingWALs.stream()
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryChore.java`
#### Snippet
```java

  private void prepareTableToReopenRegionsMap(
    final Map<TableName, List<byte[]>> tableToReopenRegionsMap, final byte[] regionName,
    final int regionStoreRefCount) {
    final RegionInfo regionInfo = hMaster.getAssignmentManager().getRegionInfo(regionName);
```

### BoundedWildcard
Can generalize to `? extends ServerMetrics`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionsRecoveryChore.java`
#### Snippet
```java

  private Map<TableName, List<byte[]>>
    getTableToRegionsByRefCount(final Map<ServerName, ServerMetrics> serverMetricsMap) {
    final Map<TableName, List<byte[]>> tableToReopenRegionsMap = new HashMap<>();
    for (ServerMetrics serverMetrics : serverMetricsMap.values()) {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionServerTracker.java`
#### Snippet
```java
  // execute the operations which are only needed for active masters, such as expire old servers,
  // add new servers, etc.
  private void processAsActiveMaster(Set<ServerName> newServers) {
    Set<ServerName> oldServers = regionServers;
    ServerManager serverManager = server.getServerManager();
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
   * @return region index
   */
  private int getRegionIndex(List<Pair<byte[], byte[]>> startEndKeys, byte[] key) {
    int idx = Collections.binarySearch(startEndKeys, Pair.newPair(key, HConstants.EMPTY_END_ROW),
      (p1, p2) -> Bytes.compareTo(p1.getFirst(), p2.getFirst()));
```

### BoundedWildcard
Can generalize to `? super LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  @InterfaceAudience.Private
  protected void bulkLoadPhase(AsyncClusterConnection conn, TableName tableName,
    Deque<LoadQueueItem> queue, Multimap<ByteBuffer, LoadQueueItem> regionGroups, boolean copyFiles,
    Map<LoadQueueItem, ByteBuffer> item2RegionMap) throws IOException {
    // atomically bulk load the groups.
```

### BoundedWildcard
Can generalize to `? super LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  protected void bulkLoadPhase(AsyncClusterConnection conn, TableName tableName,
    Deque<LoadQueueItem> queue, Multimap<ByteBuffer, LoadQueueItem> regionGroups, boolean copyFiles,
    Map<LoadQueueItem, ByteBuffer> item2RegionMap) throws IOException {
    // atomically bulk load the groups.
    List<Future<Collection<LoadQueueItem>>> loadingFutures = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? super ByteBuffer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  protected void bulkLoadPhase(AsyncClusterConnection conn, TableName tableName,
    Deque<LoadQueueItem> queue, Multimap<ByteBuffer, LoadQueueItem> regionGroups, boolean copyFiles,
    Map<LoadQueueItem, ByteBuffer> item2RegionMap) throws IOException {
    // atomically bulk load the groups.
    List<Future<Collection<LoadQueueItem>>> loadingFutures = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
   * next region. 3) if the endkey of the last region is not empty.
   */
  private void checkRegionIndexValid(int idx, List<Pair<byte[], byte[]>> startEndKeys,
    TableName tableName) throws IOException {
    if (idx < 0) {
```

### BoundedWildcard
Can generalize to `? super LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
   * Populate the Queue with given HFiles
   */
  private static void populateLoadQueue(Deque<LoadQueueItem> ret, Map<byte[], List<Path>> map) {
    map.forEach((k, v) -> v.stream().map(p -> new LoadQueueItem(k, p)).forEachOrdered(ret::add));
  }
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
   * Populate the Queue with given HFiles
   */
  private static void populateLoadQueue(Deque<LoadQueueItem> ret, Map<byte[], List<Path>> map) {
    map.forEach((k, v) -> v.stream().map(p -> new LoadQueueItem(k, p)).forEachOrdered(ret::add));
  }
```

### BoundedWildcard
Can generalize to `? extends LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java

  private Map<byte[], Collection<LoadQueueItem>>
    groupByFamilies(Collection<LoadQueueItem> itemsInRegion) {
    Map<byte[], Collection<LoadQueueItem>> families2Queue = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    itemsInRegion.forEach(item -> families2Queue
```

### BoundedWildcard
Can generalize to `? extends LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  }

  private void cleanup(AsyncClusterConnection conn, TableName tableName, Deque<LoadQueueItem> queue,
    ExecutorService pool) throws IOException {
    fsDelegationToken.releaseDelegationToken();
```

### BoundedWildcard
Can generalize to `? super LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
   * Walk the given directory for all HFiles, and return a Queue containing all such files.
   */
  private static void discoverLoadQueue(Configuration conf, Deque<LoadQueueItem> ret, Path hfofDir,
    boolean validateHFile) throws IOException {
    visitBulkHFiles(hfofDir.getFileSystem(conf), hfofDir, new BulkHFileVisitor<byte[]>() {
```

### BoundedWildcard
Can generalize to `? super ByteBuffer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  @InterfaceAudience.Private
  protected Pair<List<LoadQueueItem>, String> groupOrSplit(AsyncClusterConnection conn,
    TableName tableName, Multimap<ByteBuffer, LoadQueueItem> regionGroups, LoadQueueItem item,
    List<Pair<byte[], byte[]>> startEndKeys) throws IOException {
    Path hfilePath = item.getFilePath();
```

### BoundedWildcard
Can generalize to `? super LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  @InterfaceAudience.Private
  protected Pair<List<LoadQueueItem>, String> groupOrSplit(AsyncClusterConnection conn,
    TableName tableName, Multimap<ByteBuffer, LoadQueueItem> regionGroups, LoadQueueItem item,
    List<Pair<byte[], byte[]>> startEndKeys) throws IOException {
    Path hfilePath = item.getFilePath();
```

### BoundedWildcard
Can generalize to `? extends LoadQueueItem`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
   */
  private static void validateFamiliesInHFiles(TableDescriptor tableDesc,
    Deque<LoadQueueItem> queue, boolean silence) throws IOException {
    Set<String> familyNames = Arrays.stream(tableDesc.getColumnFamilies())
      .map(ColumnFamilyDescriptor::getNameAsString).collect(Collectors.toSet());
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
   * @return A copy of the internal list of online servers matched by the predicator
   */
  public List<ServerName> getOnlineServersListWithPredicator(List<ServerName> keys,
    Predicate<ServerMetrics> idleServerPredicator) {
    List<ServerName> names = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? super ServerMetrics`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
   */
  public List<ServerName> getOnlineServersListWithPredicator(List<ServerName> keys,
    Predicate<ServerMetrics> idleServerPredicator) {
    List<ServerName> names = new ArrayList<>();
    if (keys != null && idleServerPredicator != null) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
   * Called by delete table and similar to notify the ServerManager that a region was removed.
   */
  public void removeRegions(final List<RegionInfo> regions) {
    for (RegionInfo hri : regions) {
      removeRegion(hri);
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
   * @param liveServersFromWALDir the live region servers from wal directory.
   */
  void findDeadServersAndProcess(Set<ServerName> deadServersFromPE,
    Set<ServerName> liveServersFromWALDir) {
    deadServersFromPE.forEach(deadservers::putIfAbsent);
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
   */
  void findDeadServersAndProcess(Set<ServerName> deadServersFromPE,
    Set<ServerName> liveServersFromWALDir) {
    deadServersFromPE.forEach(deadservers::putIfAbsent);
    liveServersFromWALDir.stream().filter(sn -> !onlineServers.containsKey(sn))
```

### BoundedWildcard
Can generalize to `? extends Map`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java

  public void printLocalityAndDispersionForCurrentPlan(
    Map<String, Map<String, Float>> regionLocalityMap) throws IOException {
    SnapshotOfRegionAssignmentFromMeta snapshot = this.getRegionAssignmentSnapshot();
    FavoredNodesPlan assignmentPlan = snapshot.getExistingAssignmentPlan();
```

### BoundedWildcard
Can generalize to `? extends Map`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
   */
  public void checkDifferencesWithOldPlan(Map<TableName, Integer> movesPerTable,
    Map<String, Map<String, Float>> regionLocalityMap, FavoredNodesPlan newPlan)
    throws IOException {
    // localities for primary, secondary and tertiary
```

### BoundedWildcard
Can generalize to `? extends Map`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
  private void genAssignmentPlan(TableName tableName,
    SnapshotOfRegionAssignmentFromMeta assignmentSnapshot,
    Map<String, Map<String, Float>> regionLocalityMap, FavoredNodesPlan plan,
    boolean munkresForSecondaryAndTertiary) throws IOException {
    // Get the all the regions for the current table
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
      justification = "We only release this lock when we set it. Updates to code "
        + "that uses it should verify use of the guard boolean.")
  List<Path> getLogDirs(final Set<ServerName> serverNames) throws IOException {
    List<Path> logDirs = new ArrayList<>();
    boolean needReleaseLock = false;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
  }

  void handleDeadWorkers(Set<ServerName> serverNames) {
    synchronized (deadWorkersLock) {
      if (deadWorkers == null) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
   * for tests.
   */
  public static List<FileStatus> getFileList(final Configuration conf, final List<Path> logDirs,
    final PathFilter filter) throws IOException {
    List<FileStatus> fileStatus = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java`
#### Snippet
```java

  @Override
  protected int deleteFiles(Iterable<FileStatus> filesToDelete) {
    List<CleanerContext> results = new ArrayList<>();
    for (FileStatus file : filesToDelete) {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessor`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
   * implements the provided {@code service}.
   */
  boolean checkCoprocessorWithService(List<MasterCoprocessor> coprocessorsToCheck,
    Class<?> service) {
    if (coprocessorsToCheck == null || coprocessorsToCheck.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   *         Does NOT check the pairs themselves overlap.
   */
  static boolean isOverlap(RegionInfo ri, Pair<RegionInfo, RegionInfo> pair) {
    if (ri == null || pair == null) {
      // Can't be an overlap in either of these cases.
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   *         Does NOT check the pairs themselves overlap.
   */
  static boolean isOverlap(RegionInfo ri, Pair<RegionInfo, RegionInfo> pair) {
    if (ri == null || pair == null) {
      // Can't be an overlap in either of these cases.
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   */
  static List<SortedSet<RegionInfo>> calculateMerges(int maxMergeCount,
    List<Pair<RegionInfo, RegionInfo>> overlaps) {
    if (overlaps.isEmpty()) {
      LOG.debug("No overlaps.");
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   *         in {@code hole}.
   */
  private static Optional<RegionInfo> getHoleCover(Pair<RegionInfo, RegionInfo> hole) {
    final RegionInfo left = hole.getFirst();
    final RegionInfo right = hole.getSecond();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   *         in {@code hole}.
   */
  private static Optional<RegionInfo> getHoleCover(Pair<RegionInfo, RegionInfo> hole) {
    final RegionInfo left = hole.getFirst();
    final RegionInfo right = hole.getSecond();
```

### BoundedWildcard
Can generalize to `? super SortedSet`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
  }

  private static void calculateTableMerges(int maxMergeCount, List<SortedSet<RegionInfo>> merges,
    Collection<Pair<RegionInfo, RegionInfo>> overlaps) {
    SortedSet<RegionInfo> currentMergeSet = new TreeSet<>();
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java

  private static void calculateTableMerges(int maxMergeCount, List<SortedSet<RegionInfo>> merges,
    Collection<Pair<RegionInfo, RegionInfo>> overlaps) {
    SortedSet<RegionInfo> currentMergeSet = new TreeSet<>();
    HashSet<RegionInfo> regionsInMergeSet = new HashSet<>();
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   */
  private static List<RegionInfo>
    createRegionInfosForHoles(final List<Pair<RegionInfo, RegionInfo>> holes) {
    final List<RegionInfo> newRegionInfos = holes.stream().map(MetaFixer::getHoleCover)
      .filter(Optional::isPresent).map(Optional::get).collect(Collectors.toList());
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
   */
  private static List<RegionInfo> createMetaEntries(final MasterServices masterServices,
    final List<RegionInfo> newRegionInfos) {

    final List<Either<List<RegionInfo>, IOException>> addMetaEntriesResults =
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MasterClusterInfoProvider.java`
#### Snippet
```java

  @Override
  public boolean hasRegionReplica(Collection<RegionInfo> regions) throws IOException {
    TableDescriptors tds = services.getTableDescriptors();
    if (tds == null) {
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
   * @return true iff successfully deleted all files
   */
  private boolean checkAndDeleteFiles(List<FileStatus> files) {
    if (files == null) {
      return true;
```

### BoundedWildcard
Can generalize to `? super Boolean`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
   * calling result.get().
   */
  private void traverseAndDelete(Path dir, boolean root, CompletableFuture<Boolean> result) {
    try {
      // Step.1: List all files under the given directory.
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
   * @return number of deleted files
   */
  protected int deleteFiles(Iterable<FileStatus> filesToDelete) {
    int deletedFileCount = 0;
    for (FileStatus file : filesToDelete) {
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
   * @param dirs the list to sort, element in it should be directory (not file)
   */
  private void sortByConsumedSpace(List<FileStatus> dirs) {
    if (dirs == null || dirs.size() < 2) {
      // no need to sort for empty or single directory
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java`
#### Snippet
```java
   * @throws IOException if there is an unexpected error reaching the filesystem.
   */
  public Iterable<FileStatus> getUnreferencedFiles(List<FileStatus> files,
    final SnapshotManager snapshotManager) throws IOException {
    List<FileStatus> unReferencedFiles = Lists.newArrayList();
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java`
#### Snippet
```java
  // easier to keep them serial though
  @Override
  public void snapshotRegions(List<Pair<RegionInfo, ServerName>> regionsAndLocations)
    throws IOException, KeeperException {
    try {
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotHFileCleaner.java`
#### Snippet
```java

  @Override
  public Iterable<FileStatus> getDeletableFiles(Iterable<FileStatus> files) {
    // The Iterable is lazy evaluated, so if we just pass this Iterable in, we will access the HFile
    // storage inside the snapshot lock, which could take a lot of time (for example, several
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MaintenanceLoadBalancer.java`
#### Snippet
```java

  @Override
  public ServerName randomAssignment(RegionInfo regionInfo, List<ServerName> servers)
    throws IOException {
    // should only have 1 region server in maintenance mode
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MaintenanceLoadBalancer.java`
#### Snippet
```java
  }

  private Map<ServerName, List<RegionInfo>> assign(Collection<RegionInfo> regions,
    List<ServerName> servers) {
    // should only have 1 region server in maintenance mode
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MaintenanceLoadBalancer.java`
#### Snippet
```java

  private Map<ServerName, List<RegionInfo>> assign(Collection<RegionInfo> regions,
    List<ServerName> servers) {
    // should only have 1 region server in maintenance mode
    assert servers.size() == 1;
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.java`
#### Snippet
```java
   **/
  public static void deleteColumnFamilyFromFileSystem(final MasterProcedureEnv env,
    final TableName tableName, final List<RegionInfo> regionInfoList, final byte[] familyName,
    final boolean hasMob) throws IOException {
    final MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();
```

### BoundedWildcard
Can generalize to `? super Future`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java

  private static List<Path> copyFiles(FileSystem srcFS, Path src, FileSystem dstFS, Path dst,
    Configuration conf, ExecutorService pool, List<Future<Void>> futures) throws IOException {
    List<Path> traversedPaths = new ArrayList<>();
    traversedPaths.add(dst);
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
   * @return A list of FileStatuses
   */
  public static List<FileStatus> filterFileStatuses(Iterator<FileStatus> input,
    FileStatusFilter filter) {
    if (input == null) return null;
```

### BoundedWildcard
Can generalize to `? extends RegionPlan`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
   * @return succeeded plans
   */
  public List<RegionPlan> executeRegionPlansWithThrottling(List<RegionPlan> plans) {
    List<RegionPlan> successRegionPlans = new ArrayList<>();
    int maxRegionsInTransition = getMaxRegionsInTransition();
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
   * @param servers Region servers to decommission.
   */
  public void decommissionRegionServers(final List<ServerName> servers, final boolean offload)
    throws IOException {
    List<ServerName> serversAdded = new ArrayList<>(servers.size());
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java`
#### Snippet
```java

  /** Returns Maximum region replica id found in passed list of regions. */
  private static int getMaxReplicaId(List<RegionInfo> regions) {
    int max = 0;
    for (RegionInfo regionInfo : regions) {
```

### BoundedWildcard
Can generalize to `? extends RemoteProcedure`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
  @Override
  protected void abortPendingOperations(final ServerName serverName,
    final Set<RemoteProcedure> operations) {
    // TODO: Replace with a ServerNotOnlineException()
    final IOException e = new DoNotRetryIOException("server not online " + serverName);
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java

  protected static void waitRegionInTransition(final MasterProcedureEnv env,
    final List<RegionInfo> regions) throws IOException {
    final RegionStates states = env.getAssignmentManager().getRegionStates();
    for (final RegionInfo region : regions) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java`
#### Snippet
```java
  }

  private static List<RegionInfo> recreateRegionInfo(final List<RegionInfo> regions) {
    ArrayList<RegionInfo> newRegions = new ArrayList<>(regions.size());
    for (RegionInfo hri : regions) {
```

### BoundedWildcard
Can generalize to `? extends FileStatus`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java

  @Override
  public int deleteFiles(Iterable<FileStatus> filesToDelete) {
    int deletedFiles = 0;
    List<HFileDeleteTask> tasks = new ArrayList<HFileDeleteTask>();
```

### BoundedWildcard
Can generalize to `? super LockedResource`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
  }

  private <T> void addToLockedResources(List<LockedResource> lockedResources,
    Map<T, LockAndQueue> locks, Function<T, String> keyTransformer,
    LockedResourceType resourcesType) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java

  private <T> void addToLockedResources(List<LockedResource> lockedResources,
    Map<T, LockAndQueue> locks, Function<T, String> keyTransformer,
    LockedResourceType resourcesType) {
    locks.entrySet().stream().filter(e -> e.getValue().isLocked())
```

### BoundedWildcard
Can generalize to `? extends LockAndQueue`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java

  private <T> void addToLockedResources(List<LockedResource> lockedResources,
    Map<T, LockAndQueue> locks, Function<T, String> keyTransformer,
    LockedResourceType resourcesType) {
    locks.entrySet().stream().filter(e -> e.getValue().isLocked())
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java

  private <T> void addToLockedResources(List<LockedResource> lockedResources,
    Map<T, LockAndQueue> locks, Function<T, String> keyTransformer,
    LockedResourceType resourcesType) {
    locks.entrySet().stream().filter(e -> e.getValue().isLocked())
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java`
#### Snippet
```java
   * it.
   */
  private void assignRegions(MasterProcedureEnv env, List<RegionInfo> regions) throws IOException {
    AssignmentManager am = env.getMasterServices().getAssignmentManager();
    boolean retainAssignment = env.getMasterConfiguration().getBoolean(MASTER_SCP_RETAIN_ASSIGNMENT,
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java
   * @param regionReplication the number of region replications
   */
  private void addRegionsToInMemoryStates(List<RegionInfo> regionInfos, MasterProcedureEnv env,
    int regionReplication) {
    AssignmentManager am = env.getAssignmentManager();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java`
#### Snippet
```java

  protected static void deleteFromFs(final MasterProcedureEnv env, final TableName tableName,
    final List<RegionInfo> regions, final boolean archive) throws IOException {
    final MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();
    final FileSystem fs = mfs.getFileSystem();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
   */
  private static TransitRegionStateProcedure[] createRoundRobinAssignProcedures(
    MasterProcedureEnv env, List<RegionInfo> regions, int regionReplication,
    List<ServerName> serversToExclude, boolean ignoreIfInTransition) {
    List<RegionInfo> regionsAndReplicas = new ArrayList<>(regions);
```

### BoundedWildcard
Can generalize to `? extends RegionStateNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
  }

  private static void lock(List<RegionStateNode> regionNodes) {
    regionNodes.iterator().forEachRemaining(RegionStateNode::lock);
  }
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java

  static TransitRegionStateProcedure[] createUnassignProceduresForSplitOrMerge(
    MasterProcedureEnv env, Stream<RegionInfo> regions, int regionReplication) throws IOException {
    List<RegionStateNode> regionNodes = regions
      .flatMap(hri -> IntStream.range(0, regionReplication)
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
  }

  static void removeNonDefaultReplicas(MasterProcedureEnv env, Stream<RegionInfo> regions,
    int regionReplication) {
    // Remove from in-memory states
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
   */
  private static TransitRegionStateProcedure[] createAssignProcedures(MasterProcedureEnv env,
    List<RegionInfo> regions, int regionReplication, ServerName targetServer,
    boolean ignoreIfInTransition) {
    // create the assign procs only for the primary region using the targetServer
```

### BoundedWildcard
Can generalize to `? extends NormalizationTarget`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/MergeNormalizationPlan.java`
#### Snippet
```java
    private final List<NormalizationTarget> normalizationTargets = new LinkedList<>();

    public Builder setTargets(final List<NormalizationTarget> targets) {
      normalizationTargets.clear();
      normalizationTargets.addAll(targets);
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java`
#### Snippet
```java
  RegionNormalizerManager(@NonNull final RegionNormalizerStateStore regionNormalizerStateStore,
    @Nullable final RegionNormalizerChore regionNormalizerChore,
    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,
    @Nullable final RegionNormalizerWorker worker) {
    this.regionNormalizerStateStore = regionNormalizerStateStore;
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  }

  public void deleteRegions(final List<RegionInfo> regionInfos) {
    regionInfos.forEach(this::deleteRegion);
  }
```

### BoundedWildcard
Can generalize to `? super ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  }

  private void createSnapshot(RegionStateNode node, Map<ServerName, List<RegionInfo>> result) {
    final ServerName serverName = node.getRegionLocation();
    if (serverName == null) {
```

### BoundedWildcard
Can generalize to `? super RegionStateNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java

  /** Returns Return the regions of the table and filter them. */
  private List<RegionInfo> getRegionsOfTable(TableName table, Predicate<RegionStateNode> filter) {
    return getTableRegionStateNodes(table).stream().filter(filter).map(n -> n.getRegionInfo())
      .collect(Collectors.toList());
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
   */
  public Map<TableName, Map<ServerName, List<RegionInfo>>>
    getAssignmentsForBalancer(TableStateManager tableStateManager, List<ServerName> onlineServers) {
    final Map<TableName, Map<ServerName, List<RegionInfo>>> result = new HashMap<>();
    for (RegionStateNode node : regionsMap.values()) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java

  public Map<ServerName, List<RegionInfo>>
    getSnapShotOfAssignment(final Collection<RegionInfo> regions) {
    final Map<ServerName, List<RegionInfo>> result = new HashMap<ServerName, List<RegionInfo>>();
    if (regions != null) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java`
#### Snippet
```java
   * @see TableDescriptor#getNormalizerTargetRegionCount()
   */
  private double getAverageRegionSizeMb(final List<RegionInfo> tableRegions,
    final TableDescriptor tableDescriptor) {
    if (isEmpty(tableRegions)) {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java`
#### Snippet
```java
    }

    public <T> T getOrDefault(String key, Function<String, T> function, T defaultValue) {
      String value = tableDescriptor.getValue(key);
      if (value == null) {
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java`
#### Snippet
```java
    }

    public <T> T getOrDefault(String key, Function<String, T> function, T defaultValue) {
      String value = tableDescriptor.getValue(key);
      if (value == null) {
```

### BoundedWildcard
Can generalize to `? extends NormalizationPlan`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java`
#### Snippet
```java
  }

  private void submitPlans(final List<NormalizationPlan> plans) {
    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit
    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java`
#### Snippet
```java
  }

  static Put addMergeRegions(Put put, Collection<RegionInfo> mergeRegions) throws IOException {
    int limit = 10000; // Arbitrary limit. No room in our formatted 'task0000' below for more.
    int max = mergeRegions.size();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java`
#### Snippet
```java
  }

  private void deleteRegions(List<RegionInfo> regions, long ts) throws IOException {
    List<Delete> deletes = new ArrayList<>(regions.size());
    for (RegionInfo hri : regions) {
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java`
#### Snippet
```java
     * Computes the number of regions reported for a table.
     */
    int getNumReportedRegions(TableName table, QuotaSnapshotStore<TableName> tableStore)
      throws IOException {
      return Iterables.size(tableStore.filterBySubject(table));
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java`
#### Snippet
```java
   */
  void updateNamespaceQuota(String namespace, SpaceQuotaSnapshot currentSnapshot,
    SpaceQuotaSnapshot targetSnapshot, final Multimap<String, TableName> tablesByNamespace)
    throws IOException {
    final SpaceQuotaStatus targetStatus = targetSnapshot.getQuotaStatus();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaObserverChore.java`
#### Snippet
```java
  }

  void initializeSnapshotStores(Map<RegionInfo, Long> regionSizes) {
    Map<RegionInfo, Long> immutableRegionSpaceUse = Collections.unmodifiableMap(regionSizes);
    if (tableSnapshotStore == null) {
```

### BoundedWildcard
Can generalize to `? extends Result`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaUtil.java`
#### Snippet
```java
  }

  public static long calculateResultSize(final List<Result> results) {
    long size = 0;
    for (Result result : results) {
```

### BoundedWildcard
Can generalize to `? extends TableDescriptor`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupUtil.java`
#### Snippet
```java
   */
  @SuppressWarnings("deprecation")
  public static RSGroupInfo fillTables(RSGroupInfo rsGroupInfo, Collection<TableDescriptor> tds) {
    RSGroupInfo newRsGroupInfo = new RSGroupInfo(rsGroupInfo);
    Predicate<TableDescriptor> filter;
```

### BoundedWildcard
Can generalize to `? extends StoreFileReference`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
   * Computes the size of each store file in {@code storeFileNames}
   */
  long getSizeOfStoreFiles(TableName tn, Set<StoreFileReference> storeFileNames) {
    return storeFileNames.stream()
      .collect(Collectors.summingLong((sfr) -> getSizeOfStoreFile(tn, sfr)));
```

### BoundedWildcard
Can generalize to `? extends Entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
   */
  void groupArchivedFiledBySnapshotAndRecordSize(List<String> snapshots,
    Set<Entry<String, Long>> fileSizes) throws IOException {
    // Make a copy as we'll modify it.
    final Map<String, Long> filesToUpdate = new HashMap<>(fileSizes.size());
```

### BoundedWildcard
Can generalize to `? extends SnapshotWithSize`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java
   * Writes the snapshot sizes to the provided {@code table}.
   */
  void persistSnapshotSizes(Table table, List<SnapshotWithSize> snapshotSizes) throws IOException {
    // Convert each entry in the map to a Put and write them to the quota table
    table.put(snapshotSizes.stream()
```

### BoundedWildcard
Can generalize to `? extends SpaceViolationPolicyEnforcement`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/ActivePolicyEnforcement.java`
#### Snippet
```java
  }

  public ActivePolicyEnforcement(Map<TableName, SpaceViolationPolicyEnforcement> activePolicies,
    Map<TableName, SpaceQuotaSnapshot> snapshots, RegionServerServices rss,
    SpaceViolationPolicyEnforcementFactory factory) {
```

### BoundedWildcard
Can generalize to `? extends SpaceQuotaSnapshot`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/ActivePolicyEnforcement.java`
#### Snippet
```java

  public ActivePolicyEnforcement(Map<TableName, SpaceViolationPolicyEnforcement> activePolicies,
    Map<TableName, SpaceQuotaSnapshot> snapshots, RegionServerServices rss,
    SpaceViolationPolicyEnforcementFactory factory) {
    this.activePolicies = activePolicies;
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SnapshotQuotaObserverChore.java`
#### Snippet
```java
   * @param snapshotsToComputeSize list of snapshots to be persisted
   */
  void pruneTableSnapshots(Multimap<TableName, String> snapshotsToComputeSize) throws IOException {
    Multimap<TableName, String> existingSnapshotEntries = QuotaTableUtil.getTableSnapshots(conn);
    Multimap<TableName, String> snapshotEntriesToRemove = HashMultimap.create();
```

### BoundedWildcard
Can generalize to `? extends ZKUtil.NodeAndData`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java`
#### Snippet
```java
  }

  private void refreshNodes(List<ZKUtil.NodeAndData> nodes) {
    for (ZKUtil.NodeAndData n : nodes) {
      String path = n.getNode();
```

### BoundedWildcard
Can generalize to `? extends NamespacePermission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
  }

  private boolean authorizeNamespace(Set<NamespacePermission> permissions, String namespace,
    Permission.Action action) {
    if (permissions == null) {
```

### BoundedWildcard
Can generalize to `? extends Permission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
   * @param globalPerms new global permissions
   */
  private void updateGlobalCache(ListMultimap<String, Permission> globalPerms) {
    globalCache.clear();
    for (String name : globalPerms.keySet()) {
```

### BoundedWildcard
Can generalize to `? extends TablePermission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
  }

  private boolean authorizeTable(Set<TablePermission> permissions, TableName table, byte[] family,
    byte[] qualifier, Permission.Action action) {
    if (permissions == null) {
```

### BoundedWildcard
Can generalize to `? extends TablePermission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
  }

  private boolean hasAccessTable(Set<TablePermission> permissions, Permission.Action action) {
    if (permissions == null) {
      return false;
```

### BoundedWildcard
Can generalize to `? extends Permission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
   * @param tablePerms new table permissions
   */
  private void updateTableCache(TableName table, ListMultimap<String, Permission> tablePerms) {
    PermissionCache<TablePermission> cacheToUpdate =
      tableCache.getOrDefault(table, new PermissionCache<>());
```

### BoundedWildcard
Can generalize to `? extends Permission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
   * @param nsPerms   new namespace permissions
   */
  private void updateNamespaceCache(String namespace, ListMultimap<String, Permission> nsPerms) {
    PermissionCache<NamespacePermission> cacheToUpdate =
      namespaceCache.getOrDefault(namespace, new PermissionCache<>());
```

### BoundedWildcard
Can generalize to `? extends TablePermission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthManager.java`
#### Snippet
```java
  }

  private boolean authorizeFamily(Set<TablePermission> permissions, TableName table, byte[] family,
    Permission.Action action) {
    if (permissions == null) {
```

### BoundedWildcard
Can generalize to `? extends ZKUtil.NodeAndData`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
  }

  private void refreshNodes(List<ZKUtil.NodeAndData> nodes) {
    for (ZKUtil.NodeAndData n : nodes) {
      if (Thread.interrupted()) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java

  private List<Pair<List<RegionInfo>, List<ServerName>>> generateGroupAssignments(
    List<RegionInfo> regions, List<ServerName> servers) throws HBaseIOException {
    try {
      ListMultimap<String, RegionInfo> regionMap = ArrayListMultimap.create();
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
   * @return the list
   */
  private List<ServerName> filterServers(Set<Address> servers, List<ServerName> onlineServers) {
    ArrayList<ServerName> finalList = new ArrayList<>();
    for (ServerName onlineServer : onlineServers) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
  @Override
  @NonNull
  public Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,
    List<ServerName> servers) throws HBaseIOException {
    try {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
  @Override
  @NonNull
  public Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,
    List<ServerName> servers) throws HBaseIOException {
    try {
```

### BoundedWildcard
Can generalize to `? extends UserGroupInformation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/AttemptingUserProvidingSaslServer.java`
#### Snippet
```java

  public AttemptingUserProvidingSaslServer(SaslServer saslServer,
    Supplier<UserGroupInformation> producer) {
    this.saslServer = saslServer;
    this.producer = producer;
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postModifyTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
    TableName tableName, TableDescriptor oldDescriptor, TableDescriptor currentDescriptor)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postDeleteNamespace(ObserverContext<MasterCoprocessorEnvironment> ctx,
    String namespace) throws IOException {
    if (checkInitialized("deleteNamespace " + namespace)) {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void preMasterInitialization(ObserverContext<MasterCoprocessorEnvironment> c)
    throws IOException {
    if (
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postGrant(ObserverContext<MasterCoprocessorEnvironment> c,
    UserPermission userPermission, boolean mergeExistingPermissions) throws IOException {
    if (
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postRevoke(ObserverContext<MasterCoprocessorEnvironment> c,
    UserPermission userPermission) throws IOException {
    if (checkInitialized("revoke " + userPermission)) {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postCompletedCreateTableAction(ObserverContext<MasterCoprocessorEnvironment> c,
    TableDescriptor desc, RegionInfo[] regions) throws IOException {
    if (needHandleTableHdfsAcl(desc, "createTable " + desc.getTableName())) {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postStartMaster(ObserverContext<MasterCoprocessorEnvironment> c) throws IOException {
    if (!initialized) {
      return;
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  @Override
  public void postCompletedDeleteTableAction(ObserverContext<MasterCoprocessorEnvironment> ctx,
    TableName tableName) throws IOException {
    if (!tableName.isSystemTable() && checkInitialized("deleteTable " + tableName)) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
   * @see #createAssignProcedures(Map)
   */
  public TransitRegionStateProcedure[] createAssignProcedures(List<RegionInfo> hris) {
    return hris.stream().map(hri -> regionStates.getOrCreateRegionStateNode(hri))
      .map(regionNode -> createAssignProcedure(regionNode, null)).sorted(AssignmentManager::compare)
```

### BoundedWildcard
Can generalize to `? extends RegionState`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    }

    private void update(final Collection<RegionState> regions, final long currentTime) {
      for (RegionState state : regions) {
        totalRITs++;
```

### BoundedWildcard
Can generalize to `? extends RegionStateNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  }

  private void acceptPlan(final HashMap<RegionInfo, RegionStateNode> regions,
    final Map<ServerName, List<RegionInfo>> plan) throws HBaseIOException {
    final ProcedureEvent<?>[] events = new ProcedureEvent[regions.size()];
```

### BoundedWildcard
Can generalize to `? extends RegionStateNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  }

  private boolean containsBogusAssignments(Map<RegionInfo, RegionStateNode> regions,
    List<RegionInfo> hirs) {
    for (RegionInfo ri : hirs) {
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java

  private boolean containsBogusAssignments(Map<RegionInfo, RegionStateNode> regions,
    List<RegionInfo> hirs) {
    for (RegionInfo ri : hirs) {
      if (
```

### BoundedWildcard
Can generalize to `? extends TransitRegionStateProcedure`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
   * the very beginning, before we start processing any procedures.
   */
  public void setupRIT(List<TransitRegionStateProcedure> procs) {
    procs.forEach(proc -> {
      RegionInfo regionInfo = proc.getRegion();
```

### BoundedWildcard
Can generalize to `? extends RegionStateNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  }

  private void addToPendingAssignment(final HashMap<RegionInfo, RegionStateNode> regions,
    final Collection<RegionInfo> pendingRegions) {
    assignQueueLock.lock();
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java

  private void addToPendingAssignment(final HashMap<RegionInfo, RegionStateNode> regions,
    final Collection<RegionInfo> pendingRegions) {
    assignQueueLock.lock();
    try {
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
   */
  private TransitRegionStateProcedure[]
    createAssignProcedures(Map<ServerName, List<RegionInfo>> assignments) {
    return assignments.entrySet().stream()
      .flatMap(e -> e.getValue().stream().map(hri -> regionStates.getOrCreateRegionStateNode(hri))
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
   */
  private TransitRegionStateProcedure[]
    createAssignProcedures(Map<ServerName, List<RegionInfo>> assignments) {
    return assignments.entrySet().stream()
      .flatMap(e -> e.getValue().stream().map(hri -> regionStates.getOrCreateRegionStateNode(hri))
```

### BoundedWildcard
Can generalize to `? super Byte`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/SaslServerAuthenticationProviders.java`
#### Snippet
```java
   */
  static void addProviderIfNotExists(SaslServerAuthenticationProvider provider,
    HashMap<Byte, SaslServerAuthenticationProvider> providers) {
    final byte newProviderAuthCode = provider.getSaslAuthMethod().getCode();
    final SaslServerAuthenticationProvider alreadyRegisteredProvider =
```

### BoundedWildcard
Can generalize to `? super TokenIdentifier`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslServerAuthenticationProvider.java`
#### Snippet
```java
    private final AtomicReference<UserGroupInformation> attemptingUser;

    public SaslDigestCallbackHandler(SecretManager<TokenIdentifier> secretManager,
      AtomicReference<UserGroupInformation> attemptingUser) {
      this.secretManager = secretManager;
```

### BoundedWildcard
Can generalize to `? super UserGroupInformation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslServerAuthenticationProvider.java`
#### Snippet
```java

    public SaslDigestCallbackHandler(SecretManager<TokenIdentifier> secretManager,
      AtomicReference<UserGroupInformation> attemptingUser) {
      this.secretManager = secretManager;
      this.attemptingUser = attemptingUser;
```

### BoundedWildcard
Can generalize to `? extends ExpressionNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionExpander.java`
#### Snippet
```java
  }

  private boolean isToBeExpanded(List<ExpressionNode> childExps) {
    for (ExpressionNode exp : childExps) {
      if (!exp.isSingleNode()) {
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java

  private <T> void moveRegionsBetweenGroups(Set<T> regionsOwners, Set<Address> newRegionsOwners,
    String targetGroupName, String sourceGroupName, Function<T, List<RegionInfo>> getRegionsInfo,
    Function<RegionInfo, Boolean> validation) throws IOException {
    // Get server names corresponding to given Addresses
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java

  private <T> void moveRegionsBetweenGroups(Set<T> regionsOwners, Set<Address> newRegionsOwners,
    String targetGroupName, String sourceGroupName, Function<T, List<RegionInfo>> getRegionsInfo,
    Function<RegionInfo, Boolean> validation) throws IOException {
    // Get server names corresponding to given Addresses
```

### BoundedWildcard
Can generalize to `? super RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
  private <T> void moveRegionsBetweenGroups(Set<T> regionsOwners, Set<Address> newRegionsOwners,
    String targetGroupName, String sourceGroupName, Function<T, List<RegionInfo>> getRegionsInfo,
    Function<RegionInfo, Boolean> validation) throws IOException {
    // Get server names corresponding to given Addresses
    List<ServerName> movedServerNames = new ArrayList<>(regionsOwners.size());
```

### BoundedWildcard
Can generalize to `? extends RSGroupInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java

  // Called by ServerEventsListenerThread. Presume it has lock on this manager when it runs.
  private SortedSet<Address> getDefaultServers(List<RSGroupInfo> rsGroupInfoList) {
    // Build a list of servers in other groups than default group, from rsGroupMap
    Set<Address> serversInOtherGroup = new HashSet<>();
```

### BoundedWildcard
Can generalize to `? extends RSGroupInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
  }

  private void migrate(Collection<RSGroupInfo> groupList) {
    TableDescriptors tds = masterServices.getTableDescriptors();
    ProcedureExecutor<MasterProcedureEnv> procExec = masterServices.getMasterProcedureExecutor();
```

### BoundedWildcard
Can generalize to `? extends Address`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
   * @throws IOException if moving the server and tables fail
   */
  private void moveServerRegionsFromGroup(Set<Address> movedServers, Set<Address> srcGrpServers,
    String targetGroupName, String sourceGroupName) throws IOException {
    moveRegionsBetweenGroups(movedServers, srcGrpServers, targetGroupName, sourceGroupName,
```

### BoundedWildcard
Can generalize to `? extends Address`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
   * @param servers servers to remove
   */
  private void checkForDeadOrOnlineServers(Set<Address> servers) throws IOException {
    // This ugliness is because we only have Address, not ServerName.
    Set<Address> onlineServers = new HashSet<>();
```

### BoundedWildcard
Can generalize to `? extends Pair`>
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
   * even if some region movement fails.
   */
  private void waitForRegionMovement(List<Pair<RegionInfo, Future<byte[]>>> regionMoveFutures,
    Set<String> failedRegions, String sourceGroupName, int retryCount) {
    LOG.info("Moving {} region(s) to group {}, current retry={}", regionMoveFutures.size(),
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
   */
  private void waitForRegionMovement(List<Pair<RegionInfo, Future<byte[]>>> regionMoveFutures,
    Set<String> failedRegions, String sourceGroupName, int retryCount) {
    LOG.info("Moving {} region(s) to group {}, current retry={}", regionMoveFutures.size(),
      sourceGroupName, retryCount);
```

### BoundedWildcard
Can generalize to `? extends Address`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
  }

  private void checkOnlineServersOnly(Set<Address> servers) throws IOException {
    // This uglyness is because we only have Address, not ServerName.
    // Online servers are keyed by ServerName.
```

### BoundedWildcard
Can generalize to `? extends Address`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
  }

  public synchronized Set<Address> moveServers(Set<Address> servers, String srcGroup,
    String dstGroup) throws IOException {
    RSGroupInfo src = getRSGroupInfo(srcGroup);
```

### BoundedWildcard
Can generalize to `? super RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
  }

  private void addRegion(final LinkedList<RegionInfo> regions, RegionInfo hri) {
    // If meta, move it last otherwise other unassigns fail because meta is not
    // online for them to update state in. This is dodgy. Needs to be made more
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java

  private boolean isTableInGroup(TableName tableName, String groupName,
    Set<TableName> tablesInGroupCache) throws IOException {
    if (tablesInGroupCache.contains(tableName)) {
      return true;
```

### BoundedWildcard
Can generalize to `? extends ExpressionNode`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.java`
#### Snippet
```java
  }

  public void addChildExps(List<ExpressionNode> exps) {
    this.childExps.addAll(exps);
  }
```

### BoundedWildcard
Can generalize to `? super Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
   * @return The visibility tags serialization format
   */
  public static Byte extractVisibilityTags(Cell cell, List<Tag> tags) {
    Byte serializationFormat = null;
    Iterator<Tag> tagsIterator = PrivateCellUtil.tagsIterator(cell);
```

### BoundedWildcard
Can generalize to `? super Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
   *         no visibility tag found
   */
  public static Byte extractAndPartitionTags(Cell cell, List<Tag> visTags, List<Tag> nonVisTags) {
    Byte serializationFormat = null;
    Iterator<Tag> tagsIterator = PrivateCellUtil.tagsIterator(cell);
```

### BoundedWildcard
Can generalize to `? super Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
   *         no visibility tag found
   */
  public static Byte extractAndPartitionTags(Cell cell, List<Tag> visTags, List<Tag> nonVisTags) {
    Byte serializationFormat = null;
    Iterator<Tag> tagsIterator = PrivateCellUtil.tagsIterator(cell);
```

### BoundedWildcard
Can generalize to `? super Integer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
  }

  private static void getLabelOrdinals(ExpressionNode node, List<Integer> labelOrdinals,
    Set<Integer> auths, boolean checkAuths, VisibilityLabelOrdinalProvider ordinalProvider)
    throws IOException, InvalidLabelException {
```

### BoundedWildcard
Can generalize to `? extends StoreFileInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifest.java`
#### Snippet
```java

  private void addReferenceFiles(RegionVisitor visitor, Object regionData, Object familyData,
    Collection<StoreFileInfo> storeFiles, boolean isMob) throws IOException {
    final String fileType = isMob ? "mob file" : "hfile";

```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
   * @return - the modified visibility expression as byte[]
   */
  private byte[] createModifiedVisExpression(final List<Tag> tags) throws IOException {
    StringBuilder visibilityString = new StringBuilder();
    for (Tag tag : tags) {
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
   * @return true when all the visibility tags in Put matches with visibility tags in Delete.
   */
  private static boolean compareTagsOrdinals(List<List<Integer>> putVisTags,
    List<List<Integer>> deleteVisTags) {
    boolean matchFound = false;
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
   */
  private static boolean compareTagsOrdinals(List<List<Integer>> putVisTags,
    List<List<Integer>> deleteVisTags) {
    boolean matchFound = false;
    if (deleteVisTags.size() == putVisTags.size()) {
```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
  }

  private static List<List<Integer>> sortTagsBasedOnOrdinal(List<Tag> tags) throws IOException {
    List<List<Integer>> fullTagsList = new ArrayList<>();
    for (Tag tag : tags) {
```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
   *         is used when both the set of tags are sorted based on the label ordinal.
   */
  private static boolean matchOrdinalSortedVisibilityTags(List<Tag> putVisTags,
    List<Tag> deleteVisTags) {
    boolean matchFound = false;
```

### BoundedWildcard
Can generalize to `? extends Tag`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
   */
  private static boolean matchOrdinalSortedVisibilityTags(List<Tag> putVisTags,
    List<Tag> deleteVisTags) {
    boolean matchFound = false;
    // If the size does not match. Definitely we are not comparing the equal tags.
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java

  protected Pair<Map<String, Integer>, Map<String, List<Integer>>>
    extractLabelsAndAuths(List<List<Cell>> labelDetails) {
    Map<String, Integer> labels = new HashMap<>();
    Map<String, List<Integer>> userAuths = new HashMap<>();
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
  }

  protected void addSystemLabel(Region region, Map<String, Integer> labels,
    Map<String, List<Integer>> userAuths) throws IOException {
    if (!labels.containsKey(SYSTEM_LABEL)) {
```

### BoundedWildcard
Can generalize to `? super Integer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
  }

  protected void addSystemLabel(Region region, Map<String, Integer> labels,
    Map<String, List<Integer>> userAuths) throws IOException {
    if (!labels.containsKey(SYSTEM_LABEL)) {
```

### BoundedWildcard
Can generalize to `? super List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
  }

  private static void getSortedTagOrdinals(List<List<Integer>> fullTagsList, Tag tag)
    throws IOException {
    List<Integer> tagsOrdinalInSortedOrder = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void prePrepareTimeStampForDeleteVersion(ObserverContext<RegionCoprocessorEnvironment> ctx,
    Mutation delete, Cell cell, byte[] byteNow, Get get) throws IOException {
    // Nothing to do if we are not filtering by visibility
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void preGetOp(ObserverContext<RegionCoprocessorEnvironment> e, Get get, List<Cell> results)
    throws IOException {
    if (!initialized) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  public List<Pair<Cell, Cell>> postIncrementBeforeWAL(
    ObserverContext<RegionCoprocessorEnvironment> ctx, Mutation mutation,
    List<Pair<Cell, Cell>> cellPairs) throws IOException {
    List<Pair<Cell, Cell>> resultPairs = new ArrayList<>(cellPairs.size());
    for (Pair<Cell, Cell> pair : cellPairs) {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> c,
    MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
    if (c.getEnvironment().getRegion().getRegionInfo().getTable().isSystemTable()) {
```

### BoundedWildcard
Can generalize to `? extends Mutation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  @Override
  public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> c,
    MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
    if (c.getEnvironment().getRegion().getRegionInfo().getTable().isSystemTable()) {
      return;
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void postOpen(ObserverContext<RegionCoprocessorEnvironment> e) {
    // Read the entire labels table and populate the zk
    if (e.getEnvironment().getRegion().getRegionInfo().getTable().equals(LABELS_TABLE_NAME)) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  public List<Pair<Cell, Cell>> postAppendBeforeWAL(
    ObserverContext<RegionCoprocessorEnvironment> ctx, Mutation mutation,
    List<Pair<Cell, Cell>> cellPairs) throws IOException {
    List<Pair<Cell, Cell>> resultPairs = new ArrayList<>(cellPairs.size());
    for (Pair<Cell, Cell> pair : cellPairs) {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  @Override
  public DeleteTracker postInstantiateDeleteTracker(
    ObserverContext<RegionCoprocessorEnvironment> ctx, DeleteTracker delTracker)
    throws IOException {
    // Nothing to do if we are not filtering by visibility
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void preScannerOpen(ObserverContext<RegionCoprocessorEnvironment> e, Scan scan)
    throws IOException {
    if (!initialized) {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java

  @Override
  public void postStartMaster(ObserverContext<MasterCoprocessorEnvironment> ctx)
    throws IOException {
    // Need to create the new system table for labels here
```

### BoundedWildcard
Can generalize to `? extends RegionInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    }

    public void updateMetaParentRegions(Connection connection, final List<RegionInfo> regionInfos)
      throws IOException {
      if (regionInfos == null || parentsMap.isEmpty()) return;
```

### BoundedWildcard
Can generalize to `? extends HRegion`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java`
#### Snippet
```java

  public FlushTableSubprocedure(ProcedureMember member, ForeignExceptionDispatcher errorListener,
    long wakeFrequency, long timeout, List<HRegion> regions, String table, String family,
    FlushTableSubprocedurePool taskManager) {
    super(member, table, errorListener, wakeFrequency, timeout);
```

### BoundedWildcard
Can generalize to `? extends HDFSAclOperation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
  }

  private CompletableFuture<Void> handleHDFSAclSequential(List<HDFSAclOperation> operations) {
    return CompletableFuture.supplyAsync(() -> {
      try {
```

### BoundedWildcard
Can generalize to `? extends HDFSAclOperation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
  }

  private CompletableFuture<Void> handleHDFSAclParallel(List<HDFSAclOperation> operations) {
    List<CompletableFuture<Void>> futures =
      operations.stream().map(this::handleHDFSAcl).collect(Collectors.toList());
```

### BoundedWildcard
Can generalize to `? extends UserPermission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java

  private Set<String>
    getUsersWithReadAction(ListMultimap<String, UserPermission> permissionMultimap) {
    return permissionMultimap.entries().stream()
      .filter(entry -> checkUserPermission(entry.getValue())).map(Map.Entry::getKey)
```

### BoundedWildcard
Can generalize to `? extends TaskAndWeakRefPair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java`
#### Snippet
```java
  }

  private static void processTasks(Iterable<TaskAndWeakRefPair> tasks, TaskFilter filter,
    List<MonitoredTask> results) {
    for (TaskAndWeakRefPair task : tasks) {
```

### BoundedWildcard
Can generalize to `? super MonitoredTask`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java`
#### Snippet
```java

  private static void processTasks(Iterable<TaskAndWeakRefPair> tasks, TaskFilter filter,
    List<MonitoredTask> results) {
    for (TaskAndWeakRefPair task : tasks) {
      MonitoredTask t = task.get();
```

### BoundedWildcard
Can generalize to `? extends WALEventTrackerPayload`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerTableAccessor.java`
#### Snippet
```java
   * @param connection       Connection to use.
   */
  public static void addWalEventTrackerRows(Queue<WALEventTrackerPayload> walEventPayloads,
    final Connection connection) throws Exception {
    List<Put> puts = new ArrayList<>(walEventPayloads.size());
```

### BoundedWildcard
Can generalize to `? super Mutation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionProcedureStore.java`
#### Snippet
```java
  // As we need to keep the max procedure id, here we can not simply delete the procedure, just fill
  // the proc column with an empty array.
  private void serializeDelete(long procId, List<Mutation> mutations, List<byte[]> rowsToLock) {
    byte[] row = Bytes.toBytes(procId);
    mutations.add(new Put(row).addColumn(PROC_FAMILY, PROC_QUALIFIER, EMPTY_BYTE_ARRAY));
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
    }

    private boolean isMetaTableOp(ObserverContext<RegionCoprocessorEnvironment> e) {
      return TableName.META_TABLE_NAME.equals(e.getEnvironment().getRegionInfo().getTable());
    }
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationObserver.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "NP_NULL_ON_SOME_PATH",
      justification = "NPE should never happen; if it does it is a bigger issue")
  public void preCommitStoreFile(final ObserverContext<RegionCoprocessorEnvironment> ctx,
    final byte[] family, final List<Pair<Path, Path>> pairs) throws IOException {
    RegionCoprocessorEnvironment env = ctx.getEnvironment();
```

### BoundedWildcard
Can generalize to `? super Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java`
#### Snippet
```java
   *                                WALEntries so they never make it out of this ReplicationSource.
   */
  ReplicationSource(Predicate<Path> replicateWAL, List<WALEntryFilter> baseFilterOutWALEntries) {
    this.filterInWALs = replicateWAL;
    this.baseFilterOutWALEntries = Collections.unmodifiableList(baseFilterOutWALEntries);
```

### BoundedWildcard
Can generalize to `? super SyncReplicationState`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SyncReplicationPeerInfoProviderImpl.java`
#### Snippet
```java
  @Override
  public boolean checkState(TableName table,
    BiPredicate<SyncReplicationState, SyncReplicationState> checker) {
    String peerId = mapping.getPeerId(table);
    if (peerId == null) {
```

### BoundedWildcard
Can generalize to `? super SyncReplicationState`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SyncReplicationPeerInfoProviderImpl.java`
#### Snippet
```java
  @Override
  public boolean checkState(TableName table,
    BiPredicate<SyncReplicationState, SyncReplicationState> checker) {
    String peerId = mapping.getPeerId(table);
    if (peerId == null) {
```

### BoundedWildcard
Can generalize to `? extends Entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
   */
  private CompletableFuture<Integer> serialReplicateRegionEntries(
    PeekingIterator<Entry> walEntryPeekingIterator, int batchIndex, int timeout) {
    if (!walEntryPeekingIterator.hasNext()) {
      return CompletableFuture.completedFuture(batchIndex);
```

### BoundedWildcard
Can generalize to `? extends Entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  }

  private List<List<Entry>> createSerialBatches(final List<Entry> entries) {
    Map<byte[], List<Entry>> regionEntries = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    for (Entry e : entries) {
```

### BoundedWildcard
Can generalize to `? extends Entry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  }

  private List<List<Entry>> createParallelBatches(final List<Entry> entries) {
    int numSinks = Math.max(getNumSinks(), 1);
    int n = Math.min(Math.min(this.maxThreads, entries.size() / 100 + 1), numSinks);
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
   * @param batchRowSizeThreshold rowSize threshold for batch mutation
   */
  private void batch(TableName tableName, Collection<List<Row>> allRows, int batchRowSizeThreshold)
    throws IOException {
    if (allRows.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? super Pair`>
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java

  private void addFamilyAndItsHFilePathToTableInMap(byte[] family, String pathToHfileFromNS,
    List<Pair<byte[], List<String>>> familyHFilePathsList) {
    List<String> hfilePaths = new ArrayList<>(1);
    hfilePaths.add(pathToHfileFromNS);
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java

  private void addNewTableEntryInMap(
    final Map<String, List<Pair<byte[], List<String>>>> bulkLoadHFileMap, byte[] family,
    String pathToHfileFromNS, String tableName) {
    List<String> hfilePaths = new ArrayList<>(1);
```

### BoundedWildcard
Can generalize to `? super List`>>
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java

  private void addNewTableEntryInMap(
    final Map<String, List<Pair<byte[], List<String>>>> bulkLoadHFileMap, byte[] family,
    String pathToHfileFromNS, String tableName) {
    List<String> hfilePaths = new ArrayList<>(1);
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java`
#### Snippet
```java
   * @param walEntries List of pairs of WAL entry and it's size
   */
  public void updateTableLevelMetrics(List<Pair<Entry, Long>> walEntries) {
    for (Pair<Entry, Long> walEntryWithSize : walEntries) {
      Entry entry = walEntryWithSize.getFirst();
```

### BoundedWildcard
Can generalize to `? extends ReplicationPeerDescription`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
  }

  public String dumpPeersState(List<ReplicationPeerDescription> peers) throws Exception {
    Map<String, String> currentConf;
    StringBuilder sb = new StringBuilder();
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java`
#### Snippet
```java
   * @return The scanner; null if coprocessor is canceling the flush.
   */
  protected final InternalScanner createScanner(List<KeyValueScanner> snapshotScanners,
    FlushLifeCycleTracker tracker) throws IOException {
    ScanInfo scanInfo;
```

### BoundedWildcard
Can generalize to `? extends ImmutableSegment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreMergerSegmentsIterator.java`
#### Snippet
```java

  // C-tor
  public MemStoreMergerSegmentsIterator(List<ImmutableSegment> segments, CellComparator comparator,
    int compactionKVMax) throws IOException {
    super(compactionKVMax);
```

### BoundedWildcard
Can generalize to `? super KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java

  protected static void addToScanners(Segment segment, long readPt,
    List<KeyValueScanner> scanners) {
    if (!segment.isEmpty()) {
      scanners.add(segment.getScanner(readPt));
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java

  @Override
  public void add(Iterable<Cell> cells, MemStoreSizing memstoreSizing) {
    for (Cell cell : cells) {
      add(cell, memstoreSizing);
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java

  @Override
  public void upsert(Iterable<Cell> cells, long readpoint, MemStoreSizing memstoreSizing) {
    for (Cell cell : cells) {
      upsert(cell, readpoint, memstoreSizing);
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedRegionScannerImpl.java`
#### Snippet
```java

  @Override
  protected void initializeKVHeap(List<KeyValueScanner> scanners,
    List<KeyValueScanner> joinedScanners, HRegion region) throws IOException {
    this.storeHeap = new ReversedKeyValueHeap(scanners, comparator);
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void preGetTableDescriptors(ObserverContext<MasterCoprocessorEnvironment> ctx,
    List<TableName> tableNamesList, List<TableDescriptor> descriptors, String regex)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public Result preAppend(ObserverContext<RegionCoprocessorEnvironment> c, Append append)
    throws IOException {
    User user = getActiveUser(c);
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postDeleteNamespace(ObserverContext<MasterCoprocessorEnvironment> ctx,
    final String namespace) throws IOException {
    final Configuration conf = ctx.getEnvironment().getConfiguration();
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postStartMaster(ObserverContext<MasterCoprocessorEnvironment> ctx)
    throws IOException {
    try (Admin admin = ctx.getEnvironment().getConnection().getAdmin()) {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public Result preIncrement(final ObserverContext<RegionCoprocessorEnvironment> c,
    final Increment increment) throws IOException {
    User user = getActiveUser(c);
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
  }

  private void internalPreRead(final ObserverContext<RegionCoprocessorEnvironment> c,
    final Query query, OpType opType) throws IOException {
    Filter filter = query.getFilter();
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postTruncateTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
    final TableName tableName) throws IOException {
    final Configuration conf = ctx.getEnvironment().getConfiguration();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
   */
  @Override
  public void preCleanupBulkLoad(ObserverContext<RegionCoprocessorEnvironment> ctx)
    throws IOException {
    requireAccess(ctx, "preCleanupBulkLoad",
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public boolean preCheckAndDelete(final ObserverContext<RegionCoprocessorEnvironment> c,
    final byte[] row, final byte[] family, final byte[] qualifier, final CompareOperator op,
    final ByteArrayComparable comparator, final Delete delete, final boolean result)
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postCompletedCreateTableAction(final ObserverContext<MasterCoprocessorEnvironment> c,
    final TableDescriptor desc, final RegionInfo[] regions) throws IOException {
    // When AC is used, it should be configured as the 1st CP.
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> c,
    MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
    if (cellFeaturesEnabled && !compatibleEarlyTermination) {
```

### BoundedWildcard
Can generalize to `? extends Mutation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
  @Override
  public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> c,
    MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
    if (cellFeaturesEnabled && !compatibleEarlyTermination) {
      TableName table = c.getEnvironment().getRegion().getRegionInfo().getTable();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
   */
  @Override
  public void prePrepareBulkLoad(ObserverContext<RegionCoprocessorEnvironment> ctx)
    throws IOException {
    requireAccess(ctx, "prePrepareBulkLoad",
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void preFlush(ObserverContext<RegionCoprocessorEnvironment> c,
    FlushLifeCycleTracker tracker) throws IOException {
    requirePermission(c, "flush", getTableName(c.getEnvironment()), null, null, Action.ADMIN,
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postPut(final ObserverContext<RegionCoprocessorEnvironment> c, final Put put,
    final WALEdit edit, final Durability durability) {
    if (aclRegion) {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postModifyTable(ObserverContext<MasterCoprocessorEnvironment> c, TableName tableName,
    TableDescriptor oldDesc, TableDescriptor currentDesc) throws IOException {
    final Configuration conf = c.getEnvironment().getConfiguration();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void preDelete(final ObserverContext<RegionCoprocessorEnvironment> c, final Delete delete,
    final WALEdit edit, final Durability durability) throws IOException {
    // An ACL on a delete is useless, we shouldn't allow it
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postOpen(ObserverContext<RegionCoprocessorEnvironment> c) {
    RegionCoprocessorEnvironment env = c.getEnvironment();
    final Region region = env.getRegion();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void prePut(final ObserverContext<RegionCoprocessorEnvironment> c, final Put put,
    final WALEdit edit, final Durability durability) throws IOException {
    User user = getActiveUser(c);
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
  @Override
  public boolean preCheckAndDeleteAfterRowLock(
    final ObserverContext<RegionCoprocessorEnvironment> c, final byte[] row, final byte[] family,
    final byte[] qualifier, final CompareOperator op, final ByteArrayComparable comparator,
    final Delete delete, final boolean result) throws IOException {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public InternalScanner preCompact(ObserverContext<RegionCoprocessorEnvironment> c, Store store,
    InternalScanner scanner, ScanType scanType, CompactionLifeCycleTracker tracker,
    CompactionRequest request) throws IOException {
```

### BoundedWildcard
Can generalize to `? extends Permission`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
  }

  private void preHasUserPermissions(User caller, String userName, List<Permission> permissions)
    throws IOException {
    String request = "hasUserPermissions";
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public boolean preCheckAndPutAfterRowLock(final ObserverContext<RegionCoprocessorEnvironment> c,
    final byte[] row, final byte[] family, final byte[] qualifier, final CompareOperator opp,
    final ByteArrayComparable comparator, final Put put, final boolean result) throws IOException {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public boolean preCheckAndPut(final ObserverContext<RegionCoprocessorEnvironment> c,
    final byte[] row, final byte[] family, final byte[] qualifier, final CompareOperator op,
    final ByteArrayComparable comparator, final Put put, final boolean result) throws IOException {
```

### BoundedWildcard
Can generalize to `? extends MasterCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postDeleteTable(ObserverContext<MasterCoprocessorEnvironment> c,
    final TableName tableName) throws IOException {
    final Configuration conf = c.getEnvironment().getConfiguration();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void preOpen(ObserverContext<RegionCoprocessorEnvironment> c) throws IOException {
    RegionCoprocessorEnvironment env = c.getEnvironment();
    final Region region = env.getRegion();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  @Override
  public void postDelete(final ObserverContext<RegionCoprocessorEnvironment> c, final Delete delete,
    final WALEdit edit, final Durability durability) throws IOException {
    if (aclRegion) {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
   */
  @Override
  public void preBulkLoadHFile(ObserverContext<RegionCoprocessorEnvironment> ctx,
    List<Pair<byte[], String>> familyPaths) throws IOException {
    User user = getActiveUser(ctx);
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
  @Override
  public void preBulkLoadHFile(ObserverContext<RegionCoprocessorEnvironment> ctx,
    List<Pair<byte[], String>> familyPaths) throws IOException {
    User user = getActiveUser(ctx);
    for (Pair<byte[], String> el : familyPaths) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  default List<Pair<Cell, Cell>> postIncrementBeforeWAL(
    ObserverContext<RegionCoprocessorEnvironment> ctx, Mutation mutation,
    List<Pair<Cell, Cell>> cellPairs) throws IOException {
    List<Pair<Cell, Cell>> resultPairs = new ArrayList<>(cellPairs.size());
    for (Pair<Cell, Cell> pair : cellPairs) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  default List<Pair<Cell, Cell>> postAppendBeforeWAL(
    ObserverContext<RegionCoprocessorEnvironment> ctx, Mutation mutation,
    List<Pair<Cell, Cell>> cellPairs) throws IOException {
    List<Pair<Cell, Cell>> resultPairs = new ArrayList<>(cellPairs.size());
    for (Pair<Cell, Cell> pair : cellPairs) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  }

  static void increaseStoreFilesRefeCount(Collection<HStoreFile> storeFiles) {
    if (CollectionUtils.isEmpty(storeFiles)) {
      return;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  }

  static void decreaseStoreFilesRefeCount(Collection<HStoreFile> storeFiles) {
    if (CollectionUtils.isEmpty(storeFiles)) {
      return;
```

### BoundedWildcard
Can generalize to `? extends StoreFileInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
  }

  private List<HStoreFile> openStoreFiles(Collection<StoreFileInfo> files, boolean warmup)
    throws IOException {
    if (CollectionUtils.isEmpty(files)) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
   * @return the committed store files
   */
  public List<HStoreFile> commitStoreFiles(List<Path> files, boolean validate) throws IOException {
    List<HStoreFile> committedFiles = new ArrayList<>(files.size());
    HRegionFileSystem hfs = ctx.getRegionFileSystem();
```

### BoundedWildcard
Can generalize to `? extends StoreFileInfo`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
   * replicas to keep up to date with the primary region files.
   */
  private void refreshStoreFilesInternal(Collection<StoreFileInfo> newFiles) throws IOException {
    Collection<HStoreFile> currentFiles = storeFileManager.getStorefiles();
    Collection<HStoreFile> compactedFiles = storeFileManager.getCompactedfiles();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
   * @return The found cell. Null if there's no such a cell.
   */
  private MobCell readCell(List<Path> locations, String fileName, Cell search,
    boolean cacheMobBlocks, long readPt, boolean readEmptyValueOnMobCellMiss) throws IOException {
    FileSystem fs = getFileSystem();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
  }

  private void insertRegionFilesIntoStoreTracker(List<Path> allFiles, MasterProcedureEnv env,
    HRegionFileSystem regionFs) throws IOException {
    TableDescriptor tblDesc =
```

### BoundedWildcard
Can generalize to `? extends ImmutableSegment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java`
#### Snippet
```java
  private long keySize = 0;

  public CompositeImmutableSegment(CellComparator comparator, List<ImmutableSegment> segments) {
    super(comparator, segments);
    this.segments = segments;
```

### BoundedWildcard
Can generalize to `? extends ImmutableSegment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentFactory.java`
#### Snippet
```java
  }

  private MemStoreLAB getMergedMemStoreLAB(Configuration conf, List<ImmutableSegment> segments) {
    List<MemStoreLAB> mslabs = new ArrayList<>();
    if (!conf.getBoolean(MemStoreLAB.USEMSLAB_KEY, MemStoreLAB.USEMSLAB_DEFAULT)) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
  }

  public static long getTotalUncompressedBytes(List<HStoreFile> files) {
    return files.stream()
      .mapToLong(file -> getStorefileFieldSize(file, StoreFileReader::getTotalUncompressedBytes))
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * Gets lowest timestamp from candidate StoreFiles
   */
  public static long getLowestTimestamp(Collection<HStoreFile> candidates) throws IOException {
    long minTs = Long.MAX_VALUE;
    for (HStoreFile storeFile : candidates) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * @param files The files.
   */
  public static boolean hasReferences(Collection<HStoreFile> files) {
    // TODO: make sure that we won't pass null here in the future.
    return files != null && files.stream().anyMatch(HStoreFile::isReference);
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * put operation, and thus do not have a memstoreTS associated with them.
   */
  public static OptionalLong getMaxMemStoreTSInList(Collection<HStoreFile> sfs) {
    return sfs.stream().filter(sf -> !sf.isBulkLoadResult()).mapToLong(HStoreFile::getMaxMemStoreTS)
      .max();
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * Return the highest sequence ID found across all storefiles in the given list.
   */
  public static OptionalLong getMaxSequenceIdInList(Collection<HStoreFile> sfs) {
    return sfs.stream().mapToLong(HStoreFile::getMaxSequenceId).max();
  }
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
  }

  public static long getStorefilesSize(Collection<HStoreFile> files,
    Predicate<HStoreFile> predicate) {
    return files.stream().filter(predicate)
```

### BoundedWildcard
Can generalize to `? super HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java

  public static long getStorefilesSize(Collection<HStoreFile> files,
    Predicate<HStoreFile> predicate) {
    return files.stream().filter(predicate)
      .mapToLong(file -> getStorefileFieldSize(file, StoreFileReader::length)).sum();
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
  }

  public static List<StoreFileInfo> toStoreFileInfo(Collection<HStoreFile> storefiles) {
    return storefiles.stream().map(HStoreFile::getFileInfo).collect(Collectors.toList());
  }
```

### BoundedWildcard
Can generalize to `? super StoreFileReader`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
  }

  public static long getStorefileFieldSize(HStoreFile file, ToLongFunction<StoreFileReader> f) {
    if (file == null) {
      return 0L;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
   * Creates a deterministic hash code for store file collection.
   */
  public static OptionalInt getDeterministicRandomSeed(Collection<HStoreFile> files) {
    return files.stream().mapToInt(f -> f.getPath().getName().hashCode()).findFirst();
  }
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
   * @return list of scanners recreated on the current Scanners
   */
  public List<KeyValueScanner> recreateScanners(List<KeyValueScanner> currentFileScanners,
    boolean cacheBlocks, boolean usePread, boolean isCompaction, ScanQueryMatcher matcher,
    byte[] startRow, boolean includeStartRow, byte[] stopRow, boolean includeStopRow, long readPt,
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java

  /** Adds the files to compacting files. filesCompacting must be locked. */
  private void addToCompactingFiles(Collection<HStoreFile> filesToAdd) {
    if (CollectionUtils.isEmpty(filesToAdd)) {
      return;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
   *                       closed, false if not
   */
  private void removeCompactedfiles(Collection<HStoreFile> compactedfiles, boolean evictOnClose)
    throws IOException {
    final List<HStoreFile> filesToRemove = new ArrayList<>(compactedfiles.size());
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
   */
  void updateSpaceQuotaAfterFileReplacement(RegionSizeStore sizeStore, RegionInfo regionInfo,
    Collection<HStoreFile> oldFiles, Collection<HStoreFile> newFiles) {
    long delta = 0;
    if (oldFiles != null) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
   */
  void updateSpaceQuotaAfterFileReplacement(RegionSizeStore sizeStore, RegionInfo regionInfo,
    Collection<HStoreFile> oldFiles, Collection<HStoreFile> newFiles) {
    long delta = 0;
    if (oldFiles != null) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  // Set correct storage policy from the file name of DTCP.
  // Rename file will not change the storage policy.
  private void setStoragePolicyFromFileName(List<Path> newFiles) throws IOException {
    String prefix = HConstants.STORAGE_POLICY_PREFIX;
    for (Path newFile : newFiles) {
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  }

  private static void clearAndClose(List<KeyValueScanner> scanners) {
    if (scanners == null) {
      return;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  }

  private long getTotalSize(Collection<HStoreFile> sfs) {
    return sfs.stream().mapToLong(sf -> sf.getReader().length()).sum();
  }
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java`
#### Snippet
```java

  @Override
  public void loadFiles(List<HStoreFile> storeFiles) {
    this.storefiles = ImmutableList.sortedCopyOf(storeFileComparator, storeFiles);
  }
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java`
#### Snippet
```java

  @Override
  public void addCompactionResults(Collection<HStoreFile> newCompactedfiles,
    Collection<HStoreFile> results) {
    this.storefiles = ImmutableList.sortedCopyOf(storeFileComparator, Iterables
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java`
#### Snippet
```java
  @Override
  public void addCompactionResults(Collection<HStoreFile> newCompactedfiles,
    Collection<HStoreFile> results) {
    this.storefiles = ImmutableList.sortedCopyOf(storeFileComparator, Iterables
      .concat(Iterables.filter(storefiles, sf -> !newCompactedfiles.contains(sf)), results));
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java`
#### Snippet
```java

  @Override
  public void insertNewFiles(Collection<HStoreFile> sfs) {
    this.storefiles =
      ImmutableList.sortedCopyOf(storeFileComparator, Iterables.concat(this.storefiles, sfs));
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
   * ScanQueryMatcher for each store file scanner for further optimization
   */
  public static List<StoreFileScanner> getScannersForStoreFiles(Collection<HStoreFile> files,
    boolean cacheBlocks, boolean usePread, boolean isCompaction, boolean canUseDrop,
    ScanQueryMatcher matcher, long readPt) throws IOException {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
   * contention with normal read request.
   */
  public static List<StoreFileScanner> getScannersForCompaction(Collection<HStoreFile> files,
    boolean canUseDropBehind, long readPt) throws IOException {
    List<StoreFileScanner> scanners = new ArrayList<>(files.size());
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
  }

  private IOException handleException(List<KeyValueScanner> instantiatedScanners, Throwable t) {
    // remove scaner read point before throw the exception
    scannerReadPoints.remove(this);
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
  }

  private void initializeScanners(Scan scan, List<KeyValueScanner> additionalScanners)
    throws IOException {
    // Here we separate all scanners into two lists - scanner that provide data required
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
  }

  protected void initializeKVHeap(List<KeyValueScanner> scanners,
    List<KeyValueScanner> joinedScanners, HRegion region) throws IOException {
    this.storeHeap = new KeyValueHeap(scanners, comparator);
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java

  protected void initializeKVHeap(List<KeyValueScanner> scanners,
    List<KeyValueScanner> joinedScanners, HRegion region) throws IOException {
    this.storeHeap = new KeyValueHeap(scanners, comparator);
    if (!joinedScanners.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends RpcCall`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
  }

  private void checkClientDisconnect(Optional<RpcCall> rpcCall) throws CallerDisconnectedException {
    if (rpcCall.isPresent()) {
      // If a user specifies a too-restrictive or too-slow scanner, the
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.java`
#### Snippet
```java
   * @return the scanner
   */
  private InternalScanner createScanner(HStore store, List<KeyValueScanner> scanners)
    throws IOException {
    InternalScanner scanner = null;
```

### BoundedWildcard
Can generalize to `? extends ImmutableSegment`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.java`
#### Snippet
```java

  // C-tor
  public MemStoreCompactorSegmentsIterator(List<ImmutableSegment> segments,
    CellComparator comparator, int compactionKVMax, HStore store) throws IOException {
    super(compactionKVMax);
```

### BoundedWildcard
Can generalize to `? super Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
    }

    public Builder withWriterCreationTracker(Consumer<Path> writerCreationTracker) {
      this.writerCreationTracker = writerCreationTracker;
      return this;
```

### BoundedWildcard
Can generalize to `? extends Collection`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
  private StoreFileWriter(FileSystem fs, Path path, final Configuration conf, CacheConfig cacheConf,
    BloomType bloomType, long maxKeys, InetSocketAddress[] favoredNodes, HFileContext fileContext,
    boolean shouldDropCacheBehind, Supplier<Collection<HStoreFile>> compactedFilesSupplier)
    throws IOException {
    this.compactedFilesSupplier = compactedFilesSupplier;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
   * @return bytes of CompactionEventTracker
   */
  private byte[] toCompactionEventTrackerBytes(Collection<HStoreFile> storeFiles) {
    Set<String> notArchivedCompactedStoreFiles = this.compactedFilesSupplier.get().stream()
      .map(sf -> sf.getPath().getName()).collect(Collectors.toSet());
```

### BoundedWildcard
Can generalize to `? extends MemStoreLAB`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableMemStoreLAB.java`
#### Snippet
```java
  private final List<MemStoreLAB> mslabs;

  public ImmutableMemStoreLAB(List<MemStoreLAB> mslabs) {
    this.mslabs = mslabs;
    this.refCnt = RefCnt.create(() -> {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    }

    private void mergeResults(Collection<HStoreFile> compactedFiles,
      Collection<HStoreFile> results) {
      assert this.compactedFiles == null && this.results == null;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java

    private void mergeResults(Collection<HStoreFile> compactedFiles,
      Collection<HStoreFile> results) {
      assert this.compactedFiles == null && this.results == null;
      this.compactedFiles = compactedFiles;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    }

    private void deleteResults(Collection<HStoreFile> compactedFiles) {
      this.compactedFiles = compactedFiles;
      // Create new state and update parent.
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  // ensure to evict the blocks from block cache so that they are no longer in
  // cache
  private void markCompactedAway(Collection<HStoreFile> compactedFiles) {
    for (HStoreFile file : compactedFiles) {
      file.markCompactedAway();
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private void ensureEdgeStripeMetadata(ArrayList<HStoreFile> stripe, boolean isFirst) {
    HashMap<HStoreFile, byte[]> targetMap = isFirst ? fileStarts : fileEnds;
    for (HStoreFile sf : stripe) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private Collection<HStoreFile> findExpiredFiles(ImmutableList<HStoreFile> stripe, long maxTs,
    List<HStoreFile> filesCompacting, Collection<HStoreFile> expiredStoreFiles) {
    // Order by seqnum is reversed.
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * @param storeFiles Store files to add.
   */
  private void loadUnclassifiedStoreFiles(List<HStoreFile> storeFiles) {
    LOG.debug("Attempting to load " + storeFiles.size() + " store files.");
    TreeMap<byte[], ArrayList<HStoreFile>> candidateStripes = new TreeMap<>(MAP_COMPARATOR);
```

### BoundedWildcard
Can generalize to `? super CompletableFuture`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.java`
#### Snippet
```java
  }

  private long writeWALMetadata(Consumer<CompletableFuture<Long>> action) throws IOException {
    CompletableFuture<Long> future = new CompletableFuture<>();
    action.accept(future);
```

### BoundedWildcard
Can generalize to `? super Long`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  }

  private void addRegion(SortedMap<Long, Collection<HRegion>> sortedRegions, HRegion region,
    long size) {
    if (!sortedRegions.containsKey(size)) {
```

### BoundedWildcard
Can generalize to `? extends AsyncWriter`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/CombinedAsyncWriter.java`
#### Snippet
```java
  private final ImmutableList<AsyncWriter> writers;

  private CombinedAsyncWriter(ImmutableList<AsyncWriter> writers) {
    this.writers = writers;
  }
```

### BoundedWildcard
Can generalize to `? extends FSWALEntry`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java

  // confirm non-empty before calling
  private static long getLastTxid(Deque<FSWALEntry> queue) {
    return queue.peekLast().getTxid();
  }
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java`
#### Snippet
```java
  }

  static Set<byte[]> collectFamilies(List<Cell> cells) {
    if (CollectionUtils.isEmpty(cells)) {
      return Collections.emptySet();
```

### BoundedWildcard
Can generalize to `? extends HRegion`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java`
#### Snippet
```java

  public FlushSnapshotSubprocedure(ProcedureMember member, ForeignExceptionDispatcher errorListener,
    long wakeFrequency, long timeout, List<HRegion> regions, SnapshotDescription snapshot,
    SnapshotSubprocedurePool taskManager) {
    super(member, snapshot.getName(), errorListener, wakeFrequency, timeout);
```

### BoundedWildcard
Can generalize to `? extends KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
  }

  private static void clearAndClose(List<KeyValueScanner> scanners) {
    if (scanners == null) {
      return;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java`
#### Snippet
```java
   * @return Sum of StoreFile.getReader().length();
   */
  private long getTotalStoreSize(List<HStoreFile> potentialMatchFiles) {
    return potentialMatchFiles.stream().mapToLong(sf -> sf.getReader().length()).sum();
  }
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
   * @param user     the user
   */
  public void postCompactSelection(final HStore store, final List<HStoreFile> selected,
    final CompactionLifeCycleTracker tracker, final CompactionRequest request, final User user)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
   * @param user       the user
   */
  public boolean preCompactSelection(final HStore store, final List<HStoreFile> candidates,
    final CompactionLifeCycleTracker tracker, final User user) throws IOException {
    if (coprocEnvironments.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java`
#### Snippet
```java
  }

  private boolean hasExpiredStores(Collection<HStoreFile> files) {
    long currentTime = EnvironmentEdgeManager.currentTime();
    for (HStoreFile sf : files) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java`
#### Snippet
```java
  }

  private Collection<HStoreFile> getExpiredStores(Collection<HStoreFile> files,
    Collection<HStoreFile> filesCompacting) {
    long currentTime = EnvironmentEdgeManager.currentTime();
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java`
#### Snippet
```java
  @Override
  @InterfaceAudience.Private
  public boolean needsCompaction(Collection<HStoreFile> storeFiles,
    List<HStoreFile> filesCompacting) {
    ArrayList<HStoreFile> candidates = new ArrayList<>(storeFiles);
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java`
#### Snippet
```java
   * Return a list of boundaries for multiple compaction output in ascending order.
   */
  private List<Long> getCompactBoundariesForMajor(Collection<HStoreFile> filesToCompact, long now) {
    long minTimestamp = filesToCompact.stream()
      .mapToLong(f -> f.getMinimumTimestamp().orElse(Long.MAX_VALUE)).min().orElse(Long.MAX_VALUE);
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.java`
#### Snippet
```java
   * data into the same compaction windows, guaranteeing contiguous compaction based on sequence id.
   */
  public CompactionRequestImpl selectMinorCompaction(ArrayList<HStoreFile> candidateSelection,
    boolean mayUseOffPeak, boolean mayBeStuck) throws IOException {
    long now = EnvironmentEdgeManager.currentTime();
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java`
#### Snippet
```java
   * @param candidates pre-filtrate
   */
  protected void filterBulk(ArrayList<HStoreFile> candidates) {
    candidates.removeIf(HStoreFile::excludeFromMinorCompaction);
  }
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java`
#### Snippet
```java
  }

  public List<HStoreFile> preSelectCompactionForCoprocessor(Collection<HStoreFile> candidates,
    List<HStoreFile> filesCompacting) {
    return getCurrentEligibleFiles(new ArrayList<>(candidates), filesCompacting);
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java`
#### Snippet
```java

  protected ArrayList<HStoreFile> getCurrentEligibleFiles(ArrayList<HStoreFile> candidateFiles,
    final List<HStoreFile> filesCompacting) {
    // candidates = all storefiles not already in compaction queue
    if (!filesCompacting.isEmpty()) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.java`
#### Snippet
```java
   * @return subset copy of candidate list that meets compaction criteria
   */
  public CompactionRequestImpl selectCompaction(Collection<HStoreFile> candidateFiles,
    List<HStoreFile> filesCompacting, boolean isUserCompaction, boolean mayUseOffPeak,
    boolean forceMajor) throws IOException {
```

### BoundedWildcard
Can generalize to `? super Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALUtil.java`
#### Snippet
```java
  }

  public static void filterCells(WALEdit edit, Function<Cell, Cell> mapper) {
    ArrayList<Cell> cells = edit.getCells();
    int size = cells.size();
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALUtil.java`
#### Snippet
```java
  }

  public static void filterCells(WALEdit edit, Function<Cell, Cell> mapper) {
    ArrayList<Cell> cells = edit.getCells();
    int size = cells.size();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
  }

  protected void archive(final Pair<Path, Long> log) {
    totalLogSize.addAndGet(-log.getSecond());
    int retry = 1;
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.java`
#### Snippet
```java
  protected abstract TableDescriptor createFinishTableDescriptor(TableDescriptor current);

  protected final void finish(BiConsumer<String, String> setValue, Consumer<String> removeValue) {
    setValue.accept(StoreFileTrackerFactory.TRACKER_IMPL, dstSFT);
    removeValue.accept(MigrationStoreFileTracker.SRC_IMPL);
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.java`
#### Snippet
```java
  protected abstract TableDescriptor createFinishTableDescriptor(TableDescriptor current);

  protected final void finish(BiConsumer<String, String> setValue, Consumer<String> removeValue) {
    setValue.accept(StoreFileTrackerFactory.TRACKER_IMPL, dstSFT);
    removeValue.accept(MigrationStoreFileTracker.SRC_IMPL);
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.java`
#### Snippet
```java
  protected abstract TableDescriptor createFinishTableDescriptor(TableDescriptor current);

  protected final void finish(BiConsumer<String, String> setValue, Consumer<String> removeValue) {
    setValue.accept(StoreFileTrackerFactory.TRACKER_IMPL, dstSFT);
    removeValue.accept(MigrationStoreFileTracker.SRC_IMPL);
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.java`
#### Snippet
```java
    TableDescriptor current);

  protected final void migrate(Configuration conf, BiConsumer<String, String> setValue) {
    setValue.accept(StoreFileTrackerFactory.TRACKER_IMPL,
      StoreFileTrackerFactory.Trackers.MIGRATION.name());
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.java`
#### Snippet
```java
    TableDescriptor current);

  protected final void migrate(Configuration conf, BiConsumer<String, String> setValue) {
    setValue.accept(StoreFileTrackerFactory.TRACKER_IMPL,
      StoreFileTrackerFactory.Trackers.MIGRATION.name());
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/HBackupFileSystem.java`
#### Snippet
```java
   * @throws IOException exception
   */
  public static void checkImageManifestExist(HashMap<TableName, BackupManifest> backupManifestMap,
    TableName[] tableArray, Configuration conf, Path backupRootPath, String backupId)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? super BackupManifest`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/HBackupFileSystem.java`
#### Snippet
```java
   * @throws IOException exception
   */
  public static void checkImageManifestExist(HashMap<TableName, BackupManifest> backupManifestMap,
    TableName[] tableArray, Configuration conf, Path backupRootPath, String backupId)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
   * @return The result.
   */
  private FileDetails getFileDetails(Collection<HStoreFile> filesToCompact, boolean allFiles,
    boolean major) throws IOException {
    FileDetails fd = new FileDetails();
```

### BoundedWildcard
Can generalize to `? extends StoreFileScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
   */
  protected InternalScanner createScanner(HStore store, ScanInfo scanInfo,
    List<StoreFileScanner> scanners, long smallestReadPoint, long earliestPutTs,
    byte[] dropDeletesFromRow, byte[] dropDeletesToRow) throws IOException {
    return new StoreScanner(store, scanInfo, scanners, smallestReadPoint, earliestPutTs,
```

### BoundedWildcard
Can generalize to `? extends T`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java

  protected final List<Path> compact(final CompactionRequestImpl request,
    InternalScannerFactory scannerFactory, CellSinkFactory<T> sinkFactory,
    ThroughputController throughputController, User user) throws IOException {
    FileDetails fd = getFileDetails(request.getFiles(), request.isAllFiles(), request.isMajor());
```

### BoundedWildcard
Can generalize to `? extends StoreFileScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
   */
  protected InternalScanner createScanner(HStore store, ScanInfo scanInfo,
    List<StoreFileScanner> scanners, ScanType scanType, long smallestReadPoint, long earliestPutTs)
    throws IOException {
    return new StoreScanner(store, scanInfo, scanners, scanType, smallestReadPoint, earliestPutTs);
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  }

  public static long getTotalFileSize(final Collection<HStoreFile> candidates) {
    long totalSize = 0;
    for (HStoreFile storeFile : candidates) {
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  }

  private boolean allFilesExpired(final List<HStoreFile> storeFiles) {
    if (storeFiles == null || storeFiles.isEmpty()) {
      return false;
```

### BoundedWildcard
Can generalize to `? extends HStoreFile`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  }

  private static long getTotalKvCount(final Collection<HStoreFile> candidates) {
    long totalSize = 0;
    for (HStoreFile storeFile : candidates) {
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupObserver.java`
#### Snippet
```java

  @Override
  public void preCommitStoreFile(final ObserverContext<RegionCoprocessorEnvironment> ctx,
    final byte[] family, final List<Pair<Path, Path>> pairs) throws IOException {
    Configuration cfg = ctx.getEnvironment().getConfiguration();
```

### BoundedWildcard
Can generalize to `? extends RegionCoprocessorEnvironment`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupObserver.java`
#### Snippet
```java

  @Override
  public void postBulkLoadHFile(ObserverContext<RegionCoprocessorEnvironment> ctx,
    List<Pair<byte[], String>> stagingFamilyPaths, Map<byte[], List<Path>> finalPaths)
    throws IOException {
```

### BoundedWildcard
Can generalize to `? extends BackupManifest`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
   * @throws IOException exception
   */
  private void restore(HashMap<TableName, BackupManifest> backupManifestMap,
    TableName[] sTableArray, TableName[] tTableArray, boolean isOverwrite) throws IOException {
    TreeSet<BackupImage> restoreImageSet = new TreeSet<>();
```

### BoundedWildcard
Can generalize to `? super TableName`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
   * @throws BackupException exception
   */
  public BackupInfo createBackupInfo(String backupId, BackupType type, List<TableName> tableList,
    String targetRootDir, int workers, long bandwidth) throws BackupException {
    if (targetRootDir == null) {
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/IncrementalTableBackupClient.java`
#### Snippet
```java
  }

  private void updateFileLists(List<String> activeFiles, List<String> archiveFiles)
    throws IOException {
    List<String> newlyArchived = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends BackupImage`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java`
#### Snippet
```java
   * @return true if fullImages can cover image, otherwise false
   */
  public static boolean canCoverImage(ArrayList<BackupImage> fullImages, BackupImage image) {
    // fullImages can cover image only when the following conditions are satisfied:
    // - each image of fullImages must not be an incremental image;
```

### BoundedWildcard
Can generalize to `? extends BackupInfo`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    }

    private String[] convertToBackupIds(List<BackupInfo> history) {
      String[] ids = new String[history.size()];
      for (int i = 0; i < ids.length; i++) {
```

### BoundedWildcard
Can generalize to `? extends BackupInfo`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java`
#### Snippet
```java
  }

  private Map<Address, Long> getServersToOldestBackupMapping(List<BackupInfo> backups)
    throws IOException {
    Map<Address, Long> serverAddressToLastBackupMap = new HashMap<>();
```

### BoundedWildcard
Can generalize to `? super String`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java`
#### Snippet
```java

  @Override
  public void init(Map<String, Object> params) {
    MasterServices master = (MasterServices) MapUtils.getObject(params, HMaster.MASTER);
    if (master != null) {
```

### BoundedWildcard
Can generalize to `? extends BackupInfo`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
   * @return sorted list of BackupCompleteData
   */
  public static ArrayList<BackupInfo> sortHistoryListDesc(ArrayList<BackupInfo> historyList) {
    ArrayList<BackupInfo> list = new ArrayList<>();
    TreeMap<String, BackupInfo> map = new TreeMap<>();
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
   * @return a set of keys to store the boundaries
   */
  byte[][] generateBoundaryKeys(ArrayList<Path> regionDirList) throws IOException {
    TreeMap<byte[], Integer> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    // Build a set of keys to store the boundaries
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java
  }

  protected List<Path> toPathList(List<Pair<TableName, Path>> processedTableList) {
    ArrayList<Path> list = new ArrayList<>();
    for (Pair<TableName, Path> p : processedTableList) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java
  }

  protected List<TableName> toTableNameList(List<Pair<TableName, Path>> processedTableList) {
    ArrayList<TableName> list = new ArrayList<>();
    for (Pair<TableName, Path> p : processedTableList) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java
  }

  protected void cleanupBulkLoadDirs(FileSystem fs, List<Path> pathList) throws IOException {
    for (Path path : pathList) {
      if (!fs.delete(path, true)) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
   */
  static List<Put> createPutForPreparedBulkload(TableName table, byte[] region, final byte[] family,
    final List<Pair<Path, Path>> pairs) {
    List<Put> puts = new ArrayList<>(pairs.size());
    for (Pair<Path, Path> pair : pairs) {
```

### BoundedWildcard
Can generalize to `? extends TIncrement`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescer.java`
#### Snippet
```java
  }

  public boolean queueIncrements(List<TIncrement> incs) {
    if (!canQueue()) {
      failedIncrements.increment();
```

### BoundedWildcard
Can generalize to `? extends Get`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java`
#### Snippet
```java

  @Override
  public boolean[] exists(List<Get> gets) throws IOException {
    List<TGet> tGets = new ArrayList<>();
    for (Get get : gets) {
```

### BoundedWildcard
Can generalize to `? extends HRegionLocation`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<THRegionLocation> regionLocationsFromHBase(List<HRegionLocation> locations) {
    List<THRegionLocation> tlocations = new ArrayList<>(locations.size());
    for (HRegionLocation hrl : locations) {
```

### BoundedWildcard
Can generalize to `? extends TGet`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
   * @see #getFromThrift(TGet)
   */
  public static List<Get> getsFromThrift(List<TGet> in) throws IOException {
    List<Get> out = new ArrayList<>(in.size());
    for (TGet get : in) {
```

### BoundedWildcard
Can generalize to `? extends TTableDescriptor`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<TableDescriptor> tableDescriptorsFromThrift(List<TTableDescriptor> in) {
    List<TableDescriptor> out = new ArrayList<>();
    for (TTableDescriptor tableDescriptor : in) {
```

### BoundedWildcard
Can generalize to `? extends Get`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<TGet> getsFromHBase(List<Get> in) {
    List<TGet> out = new ArrayList<>(in.size());
    for (Get get : in) {
```

### BoundedWildcard
Can generalize to `? extends TableDescriptor`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<TTableDescriptor> tableDescriptorsFromHBase(List<TableDescriptor> in) {
    List<TTableDescriptor> out = new ArrayList<>(in.size());
    for (TableDescriptor descriptor : in) {
```

### BoundedWildcard
Can generalize to `? extends TServerName`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static Set<ServerName> getServerNamesFromThrift(Set<TServerName> tServerNames) {
    if (CollectionUtils.isEmpty(tServerNames)) {
      return Collections.emptySet();
```

### BoundedWildcard
Can generalize to `? extends Put`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<TPut> putsFromHBase(List<Put> in) {
    List<TPut> out = new ArrayList<>(in.size());
    for (Put put : in) {
```

### BoundedWildcard
Can generalize to `? extends TPut`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
   * @see #putFromThrift(TPut)
   */
  public static List<Put> putsFromThrift(List<TPut> in) {
    List<Put> out = new ArrayList<>(in.size());
    for (TPut put : in) {
```

### BoundedWildcard
Can generalize to `? extends TResult`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static Result[] resultsFromThrift(List<TResult> in) {
    Result[] out = new Result[in.size()];
    int index = 0;
```

### BoundedWildcard
Can generalize to `? extends TTableName`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static TableName[] tableNamesArrayFromThrift(List<TTableName> tableNames) {
    TableName[] out = new TableName[tableNames.size()];
    int index = 0;
```

### BoundedWildcard
Can generalize to `? extends ServerName`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static Set<TServerName> getServerNamesFromHBase(Set<ServerName> serverNames) {
    if (CollectionUtils.isEmpty(serverNames)) {
      return Collections.emptySet();
```

### BoundedWildcard
Can generalize to `? extends TTableName`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<TableName> tableNamesFromThrift(List<TTableName> tableNames) {
    List<TableName> out = new ArrayList<>(tableNames.size());
    for (TTableName tableName : tableNames) {
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static byte[][] splitKeyFromThrift(List<ByteBuffer> in) {
    if (in == null || in.size() == 0) {
      return null;
```

### BoundedWildcard
Can generalize to `? extends Delete`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  }

  public static List<TDelete> deletesFromHBase(List<Delete> in) {
    List<TDelete> out = new ArrayList<>(in.size());
    for (Delete delete : in) {
```

### BoundedWildcard
Can generalize to `? extends TOnlineLogRecord`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java

  public static List<OnlineLogRecord>
    getSlowLogRecordsFromThrift(List<TOnlineLogRecord> tOnlineLogRecords) {
    if (CollectionUtils.isEmpty(tOnlineLogRecords)) {
      return Collections.emptyList();
```

### BoundedWildcard
Can generalize to `? extends TNamespaceDescriptor`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java

  public static NamespaceDescriptor[]
    namespaceDescriptorsFromThrift(List<TNamespaceDescriptor> in) {
    NamespaceDescriptor[] out = new NamespaceDescriptor[in.size()];
    int index = 0;
```

### BoundedWildcard
Can generalize to `? extends TDelete`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
   */

  public static List<Delete> deletesFromThrift(List<TDelete> in) {
    List<Delete> out = new ArrayList<>(in.size());
    for (TDelete delete : in) {
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public int scannerOpenTs(ByteBuffer tableName, ByteBuffer startRow, List<ByteBuffer> columns,
    long timestamp, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, TException {

```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public int scannerOpen(ByteBuffer tableName, ByteBuffer startRow, List<ByteBuffer> columns,
    Map<ByteBuffer, ByteBuffer> attributes) throws IOError {

```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TRowResult> getRowWithColumnsTs(ByteBuffer tableName, ByteBuffer row,
    List<ByteBuffer> columns, long timestamp, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError {

```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public int scannerOpenWithPrefix(ByteBuffer tableName, ByteBuffer startAndPrefix,
    List<ByteBuffer> columns, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, TException {

    Table table = null;
```

### BoundedWildcard
Can generalize to `? extends BatchMutation`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void mutateRowsTs(ByteBuffer tableName, List<BatchMutation> rowBatches, long timestamp,
    Map<ByteBuffer, ByteBuffer> attributes) throws IOError, IllegalArgument, TException {
    List<Put> puts = new ArrayList<>();
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public int scannerOpenWithStop(ByteBuffer tableName, ByteBuffer startRow, ByteBuffer stopRow,
    List<ByteBuffer> columns, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, TException {

    Table table = null;
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public List<TRowResult> getRowsWithColumnsTs(ByteBuffer tableName, List<ByteBuffer> rows,
    List<ByteBuffer> columns, long timestamp, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError {
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public List<TRowResult> getRowsWithColumnsTs(ByteBuffer tableName, List<ByteBuffer> rows,
    List<ByteBuffer> columns, long timestamp, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError {

```

### BoundedWildcard
Can generalize to `? extends Mutation`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void mutateRowTs(ByteBuffer tableName, ByteBuffer row, List<Mutation> mutations,
    long timestamp, Map<ByteBuffer, ByteBuffer> attributes) throws IOError, IllegalArgument {
    Table table = null;
```

### BoundedWildcard
Can generalize to `? extends ByteBuffer`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  @Override
  public int scannerOpenWithStopTs(ByteBuffer tableName, ByteBuffer startRow, ByteBuffer stopRow,
    List<ByteBuffer> columns, long timestamp, Map<ByteBuffer, ByteBuffer> attributes)
    throws IOError, TException {

```

### BoundedWildcard
Can generalize to `? extends ColumnDescriptor`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  @Override
  public void createTable(ByteBuffer in_tableName, List<ColumnDescriptor> columnFamilies)
    throws IOError, IllegalArgument, AlreadyExists {
    TableName tableName = getTableName(in_tableName);
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * @param now current timestamp
   */
  public void checkTimestamps(final Map<byte[], List<Cell>> familyMap, long now)
    throws FailedSanityCheckException {
    if (timestampSlop == HConstants.LATEST_TIMESTAMP) {
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * provided current timestamp.
   */
  private static void updateCellTimestamps(final Iterable<List<Cell>> cellItr, final byte[] now)
    throws IOException {
    for (List<Cell> cells : cellItr) {
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  private long replayRecoveredEditsForPaths(long minSeqIdForTheRegion, FileSystem fs,
    final NavigableSet<Path> files, final CancelableProgressable reporter, final Path regionDir)
    throws IOException {
    long seqid = minSeqIdForTheRegion;
```

### BoundedWildcard
Can generalize to `? super RowLock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      @Override
      public MiniBatchOperationInProgress<Mutation>
        lockRowsAndBuildMiniBatch(List<RowLock> acquiredRowLocks) throws IOException {
        RowLock prevRowLock = null;
        for (byte[] row : rowsToLock) {
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * @param familyPaths List of (column family, hfilePath)
   */
  private static boolean hasMultipleColumnFamilies(Collection<Pair<byte[], String>> familyPaths) {
    boolean multipleFamilies = false;
    byte[] family = null;
```

### BoundedWildcard
Can generalize to `? extends RowLock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  }

  private void releaseRowLocks(List<RowLock> rowLocks) {
    if (rowLocks != null) {
      for (RowLock rowLock : rowLocks) {
```

### BoundedWildcard
Can generalize to `? extends Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
     */
    private List<Cell> reckonDeltasByStore(HStore store, Mutation mutation, long now,
      List<Cell> deltas, List<Cell> results) throws IOException {
      assert mutation instanceof Increment || mutation instanceof Append;
      byte[] columnFamily = store.getColumnFamilyDescriptor().getName();
```

### BoundedWildcard
Can generalize to `? super Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
     */
    private List<Cell> reckonDeltasByStore(HStore store, Mutation mutation, long now,
      List<Cell> deltas, List<Cell> results) throws IOException {
      assert mutation instanceof Increment || mutation instanceof Append;
      byte[] columnFamily = store.getColumnFamilyDescriptor().getName();
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  }

  private void updateSequenceId(final Iterable<List<Cell>> cellItr, final long sequenceId)
    throws IOException {
    for (List<Cell> cells : cellItr) {
```

### BoundedWildcard
Can generalize to `? super RowLock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // TODO Support Increment/Append operations
    private void checkAndMergeCPMutations(final MiniBatchOperationInProgress<Mutation> miniBatchOp,
      final List<RowLock> acquiredRowLocks, final long timestamp) throws IOException {
      visitBatchOperations(true, nextIndexToProcess + miniBatchOp.size(), (int i) -> {
        // we pass (i - firstIndex) below since the call expects a relative index
```

### BoundedWildcard
Can generalize to `? extends Path`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  }

  private void deleteRecoveredEdits(FileSystem fs, Iterable<Path> files) throws IOException {
    for (Path file : files) {
      if (!fs.delete(file, false)) {
```

### BoundedWildcard
Can generalize to `? super RowLock`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
     */
    public MiniBatchOperationInProgress<Mutation>
      lockRowsAndBuildMiniBatch(List<RowLock> acquiredRowLocks) throws IOException {
      int readyToWriteCount = 0;
      int lastIndexExclusive = 0;
```

### BoundedWildcard
Can generalize to `? extends Pair`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    private WALEdit createWALEditForReplicateSkipWAL(
      MiniBatchOperationInProgress<Mutation> miniBatchOp,
      List<Pair<NonceKey, WALEdit>> nonceKeyAndWALEdits) {
      if (nonceKeyAndWALEdits.isEmpty()) {
        return this.createWALEdit(miniBatchOp);
```

### BoundedWildcard
Can generalize to `? super Cell`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

    private static Cell reckonDelta(final Cell delta, final Cell currentCell,
      final byte[] columnFamily, final long now, Mutation mutation, Function<Cell, byte[]> supplier)
      throws IOException {
      // Forward any tags found on the delta.
```

### BoundedWildcard
Can generalize to `? extends List`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * information are exposed by the region server metrics.
   */
  private void recordMutationWithoutWal(final Map<byte[], List<Cell>> familyMap) {
    numMutationsWithoutWAL.increment();
    if (numMutationsWithoutWAL.sum() <= 1) {
```

### BoundedWildcard
Can generalize to `? extends HStore`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  }

  private long loadRecoveredHFilesIfAny(Collection<HStore> stores) throws IOException {
    Path regionDir = fs.getRegionDir();
    long maxSeqId = -1;
```

### BoundedWildcard
Can generalize to `? super T`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java
    }

    protected AbstractAggregationCallback(CompletableFuture<T> future) {
      this.future = future;
    }
```

### BoundedWildcard
Can generalize to `? super R`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java

  private static <R, S, P extends Message, Q extends Message, T extends Message> void findMedian(
    CompletableFuture<R> future, AsyncTable<AdvancedScanResultConsumer> table,
    ColumnInterpreter<R, S, P, Q, T> ci, Scan scan, NavigableMap<byte[], S> sumByRegion) {
    double halfSum = ci.divideForAvg(sumByRegion.values().stream().reduce(ci::add).get(), 2L);
```

### BoundedWildcard
Can generalize to `? super AdvancedScanResultConsumer`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java

  private static <R, S, P extends Message, Q extends Message, T extends Message> void findMedian(
    CompletableFuture<R> future, AsyncTable<AdvancedScanResultConsumer> table,
    ColumnInterpreter<R, S, P, Q, T> ci, Scan scan, NavigableMap<byte[], S> sumByRegion) {
    double halfSum = ci.divideForAvg(sumByRegion.values().stream().reduce(ci::add).get(), 2L);
```

## RuleId[id=MissortedModifiers]
### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
  public static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
public interface Tag {

  public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
  public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
  public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java`
#### Snippet
```java
    RESERVED_NAMESPACES = Collections.unmodifiableSet(set);
  }
  public final static Set<byte[]> RESERVED_NAMESPACES_BYTES;
  static {
    Set<byte[]> set = new TreeSet<>(Bytes.BYTES_RAWCOMPARATOR);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java`
#### Snippet
```java
    NamespaceDescriptor.create(SYSTEM_NAMESPACE_NAME_STR).build();

  public final static Set<String> RESERVED_NAMESPACES;
  static {
    Set<String> set = new HashSet<>();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
   */
  private final static TimeUnit DEFAULT_TIME_UNIT = TimeUnit.MILLISECONDS;
  private final static long DEFAULT_INITIAL_DELAY = 0;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
   * Default values for scheduling parameters should they be excluded during construction
   */
  private final static TimeUnit DEFAULT_TIME_UNIT = TimeUnit.MILLISECONDS;
  private final static long DEFAULT_INITIAL_DELAY = 0;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java`
#### Snippet
```java
  static class ChoreServiceThreadFactory implements ThreadFactory {
    private final String threadPrefix;
    private final static String THREAD_NAME_SUFFIX = ".Chore.";
    private AtomicInteger threadNumber = new AtomicInteger(1);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public final static int MIN_CORE_POOL_SIZE = 1;
  /**
   * The initial number of threads in the core pool for the {@link ChoreService}.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   *         cell's family is lesser than byte[] and 0 otherwise
   */
  public final static int compareFamilies(Cell left, byte[] right, int roffset, int rlength) {
    if (left instanceof ByteBufferExtendedCell) {
      return ByteBufferUtils.compareTo(((ByteBufferExtendedCell) left).getFamilyByteBuffer(),
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   *         cell's column is lesser than byte[] and 0 otherwise
   */
  public final static int compareColumns(Cell left, byte[] right, int rfoffset, int rflength,
    int rqoffset, int rqlength) {
    int diff = compareFamilies(left, right, rfoffset, rflength);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
   *         cell's qualifier is lesser than byte[] and 0 otherwise
   */
  public final static int compareQualifiers(Cell left, byte[] right, int rOffset, int rLength) {
    if (left instanceof ByteBufferExtendedCell) {
      return ByteBufferUtils.compareTo(((ByteBufferExtendedCell) left).getQualifierByteBuffer(),
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java`
#### Snippet
```java
    }

    private static abstract class Node {
      int offset;
      int length;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/FileKeyStoreLoader.java`
#### Snippet
```java
   * @param <T> the subtype of FileKeyStoreLoader created by the Builder.
   */
  static abstract class Builder<T extends FileKeyStoreLoader> {
    String keyStorePath;
    String trustStorePath;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   * that are returned back to the clients
   */
  private static abstract class EmptyByteBufferExtendedCell extends ByteBufferExtendedCell {

    @Override
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
   * that are returned back to the clients
   */
  private static abstract class EmptyCell implements ExtendedCell {

    @Override
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CompressionState.java`
#### Snippet
```java
  byte type;

  private final static int FIRST_KEY = -1;

  boolean isFirst() {
```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    LZMA("lzma", LZMA_CODEC_CLASS_KEY, LZMA_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec lzmaCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    LZO("lzo", LZO_CODEC_CLASS_KEY, LZO_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec lzoCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    SNAPPY("snappy", SNAPPY_CODEC_CLASS_KEY, SNAPPY_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec snappyCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    BROTLI("brotli", BROTLI_CODEC_CLASS_KEY, BROTLI_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec brotliCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    LZ4("lz4", LZ4_CODEC_CLASS_KEY, LZ4_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec lz4Codec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java

    GZ("gz", GZ_CODEC_CLASS_KEY, GZ_CODEC_CLASS_DEFAULT) {
      private volatile transient CompressionCodec gzCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    ZSTD("zstd", ZSTD_CODEC_CLASS_KEY, ZSTD_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec zStandardCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `volatile transient`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    BZIP2("bzip2", BZIP2_CODEC_CLASS_KEY, BZIP2_CODEC_CLASS_DEFAULT) {
      // Use base type to avoid compile-time dependencies.
      private volatile transient CompressionCodec bzipCodec;
      private final transient Object lock = new Object();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java`
#### Snippet
```java
     * The size of a (key length, value length) tuple that prefixes each entry in a data block.
     */
    public final static int KEY_VALUE_LEN_SIZE = 2 * Bytes.SIZEOF_INT;

    protected ByteBuff currentBuffer;
```

### MissortedModifiers
Missorted modifiers `abstract protected`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
    abstract protected void decodeFirst();

    abstract protected void decodeNext();
  }

```

### MissortedModifiers
Missorted modifiers `abstract protected`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
    }

    abstract protected void decodeFirst();

    abstract protected void decodeNext();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MovingAverage.java`
#### Snippet
```java
@InterfaceAudience.Private
public abstract class MovingAverage<T> {
  private final static Logger LOG = LoggerFactory.getLogger(MovingAverage.class);

  protected final String label;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static abstract class AvlNode<TNode extends AvlNode> {
    protected TNode avlLeft;
    protected TNode avlRight;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static abstract class AvlLinkedNode<TNode extends AvlLinkedNode> extends AvlNode<TNode> {
    protected TNode iterNext = null;
    protected TNode iterPrev = null;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/WindowMovingAverage.java`
#### Snippet
```java
@InterfaceAudience.Private
public class WindowMovingAverage<T> extends MovingAverage<T> {
  protected final static int DEFAULT_SIZE = 5;

  // The last n statistics.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  public final static int VALUE_MASK = 0x7f;
  public final static int NEXT_BIT_SHIFT = 7;
  public final static int NEXT_BIT_MASK = 1 << 7;
  final static boolean UNSAFE_AVAIL = HBasePlatformDependent.isUnsafeAvailable();
  public final static boolean UNSAFE_UNALIGNED = HBasePlatformDependent.unaligned();
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  }

  static abstract class Converter {
    abstract short toShort(ByteBuffer buffer, int offset);

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  }

  static abstract class Comparer {
    abstract int compareTo(byte[] buf1, int o1, int l1, ByteBuffer buf2, int o2, int l2);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  public final static int NEXT_BIT_SHIFT = 7;
  public final static int NEXT_BIT_MASK = 1 << 7;
  final static boolean UNSAFE_AVAIL = HBasePlatformDependent.isUnsafeAvailable();
  public final static boolean UNSAFE_UNALIGNED = HBasePlatformDependent.unaligned();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  public final static int NEXT_BIT_MASK = 1 << 7;
  final static boolean UNSAFE_AVAIL = HBasePlatformDependent.isUnsafeAvailable();
  public final static boolean UNSAFE_UNALIGNED = HBasePlatformDependent.unaligned();

  private ByteBufferUtils() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  // "Compressed integer" serialization helper constants.
  public final static int VALUE_MASK = 0x7f;
  public final static int NEXT_BIT_SHIFT = 7;
  public final static int NEXT_BIT_MASK = 1 << 7;
  final static boolean UNSAFE_AVAIL = HBasePlatformDependent.isUnsafeAvailable();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
public final class ByteBufferUtils {
  // "Compressed integer" serialization helper constants.
  public final static int VALUE_MASK = 0x7f;
  public final static int NEXT_BIT_SHIFT = 7;
  public final static int NEXT_BIT_MASK = 1 << 7;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java

  /** Pass this to TreeMaps where byte [] are keys. */
  public final static Comparator<byte[]> BYTES_COMPARATOR = new ByteArrayComparator();

  /** Use comparing byte arrays, byte-by-byte */
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java

  /** Use comparing byte arrays, byte-by-byte */
  public final static RawComparator<byte[]> BYTES_RAWCOMPARATOR = new ByteArrayComparator();

  /**
```

### MissortedModifiers
Missorted modifiers `final public static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
   * @return 0 if b is null, otherwise returns length
   */
  final public static int len(byte[] b) {
    return b == null ? 0 : b.length;
  }
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  }

  static abstract class Converter {
    abstract long toLong(byte[] bytes, int offset, int length);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final static class ArrayHolder<K, V> {
    private final COWEntry<K, V>[] entries;
    private final int startIndex;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final static class COWEntry<K, V> implements Map.Entry<K, V> {
    K key = null;
    V value = null;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/OperationMetrics.java`
#### Snippet
```java
  private static final String FAILED_COUNT = "FailedCount";

  final private Counter submittedCounter;
  final private Histogram timeHisto;
  final private Counter failedCounter;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/OperationMetrics.java`
#### Snippet
```java
  final private Counter submittedCounter;
  final private Histogram timeHisto;
  final private Counter failedCounter;

  public OperationMetrics(final MetricRegistry registry, final String metricNamePrefix) {
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/OperationMetrics.java`
#### Snippet
```java

  final private Counter submittedCounter;
  final private Histogram timeHisto;
  final private Counter failedCounter;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java`
#### Snippet
```java
  /** Namespace delimiter */
  // this should always be only 1 byte long
  public final static char NAMESPACE_DELIM = ':';

  // A non-capture group so that this can be embedded.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableSizeHistogram.java`
#### Snippet
```java

  private final static String RANGE_TYPE = "SizeRangeCount";
  private final static long[] RANGES =
    { 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000 };

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableSizeHistogram.java`
#### Snippet
```java
public class MutableSizeHistogram extends MutableRangeHistogram {

  private final static String RANGE_TYPE = "SizeRangeCount";
  private final static long[] RANGES =
    { 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000 };
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MetricsExecutorImpl.java`
#### Snippet
```java
  }

  private final static class ThreadPoolExecutorThreadFactory implements ThreadFactory {
    private final String name;
    private final AtomicInteger threadNumber = new AtomicInteger(1);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java

  /** Used as a magic return value while optimized index key feature enabled(HBASE-7845) */
  public final static int INDEX_KEY_MAGIC = -2;

  /*
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   * Default value of {@link #HBASE_SERVER_USEIP_ENABLED_KEY}
   */
  public final static boolean HBASE_SERVER_USEIP_ENABLED_DEFAULT = false;

  private HConstants() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   * to use ip for Master/RS service. See HBASE-27304 for details.
   */
  public final static String HBASE_SERVER_USEIP_ENABLED_KEY = "hbase.server.useip.enabled";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   * Default block size for an HFile.
   */
  public final static int DEFAULT_BLOCKSIZE = 64 * 1024;

  /** Used as a magic return value while optimized index key feature enabled(HBASE-7845) */
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableTimeHistogram.java`
#### Snippet
```java
public class MutableTimeHistogram extends MutableRangeHistogram {
  private final static String RANGE_TYPE = "TimeRangeCount";
  private final static long[] RANGES =
    { 1, 3, 10, 30, 100, 300, 1000, 3000, 10000, 30000, 60000, 120000, 300000, 600000 };

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableTimeHistogram.java`
#### Snippet
```java
@InterfaceAudience.Private
public class MutableTimeHistogram extends MutableRangeHistogram {
  private final static String RANGE_TYPE = "TimeRangeCount";
  private final static long[] RANGES =
    { 1, 3, 10, 30, 100, 300, 1000, 3000, 10000, 30000, 60000, 120000, 300000, 600000 };
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java`
#### Snippet
```java
  }

  final static class JmxCacheBusterRunnable implements Runnable {
    @Override
    public void run() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/DynamicMetricsRegistry.java`
#### Snippet
```java
  private final MetricsInfo metricsInfo;
  private final DefaultMetricsSystemHelper helper = new DefaultMetricsSystemHelper();
  private final static String[] histogramSuffixes = new String[] { "_num_ops", "_min", "_max",
    "_median", "_75th_percentile", "_90th_percentile", "_95th_percentile", "_99th_percentile" };

```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   * @return count current number of samples
   */
  synchronized public int getSampleCount() {
    return samples.size();
  }
```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   * @return snapshot of the tracked quantiles if no items have been added to the estimator
   */
  synchronized public Map<MetricQuantile, Long> snapshot() throws IOException {
    // flush the buffer first for best results
    insertBatch();
```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   * @param v the value to insert
   */
  synchronized public void insert(long v) {
    buffer[bufferCount] = v;
    bufferCount++;
```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   * Resets the estimator, clearing out all previously inserted items
   */
  synchronized public void clear() {
    count = 0;
    bufferCount = 0;
```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   * @return count total number of items processed
   */
  synchronized public long getCount() {
    return count;
  }
```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HttpDoAsClient.class);

  static protected int port;
  static protected String host;
  private static boolean secure = false;
```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  static protected String doAsUser = null;
  static protected String principal = null;
  static protected String keyTab = null;

  public static void main(String[] args) throws Exception {
```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  static protected String host;
  private static boolean secure = false;
  static protected String doAsUser = null;
  static protected String principal = null;
  static protected String keyTab = null;
```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  private static boolean secure = false;
  static protected String doAsUser = null;
  static protected String principal = null;
  static protected String keyTab = null;

```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java

  static protected int port;
  static protected String host;
  private static boolean secure = false;
  static protected String doAsUser = null;
```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
public class DemoClient {

  static protected int port;
  static protected String host;

```

### MissortedModifiers
Missorted modifiers `static protected`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

  static protected int port;
  static protected String host;

  private static boolean secure = false;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/Record.java`
#### Snippet
```java
  }

  public final static class Builder {

    private final ImmutableMap.Builder<Field, FieldValue> builder;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/Record.java`
#### Snippet
```java
  private final ImmutableMap<Field, FieldValue> values;

  public final static class Entry extends AbstractMap.SimpleImmutableEntry<Field, FieldValue> {
    private Entry(Field key, FieldValue value) {
      super(key, value);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RemoteWithExtrasException.java`
#### Snippet
```java
   * Dynamic class loader to load filter/comparators
   */
  private final static class ClassLoaderHolder {
    private final static ClassLoader CLASS_LOADER;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RemoteWithExtrasException.java`
#### Snippet
```java
   */
  private final static class ClassLoaderHolder {
    private final static ClassLoader CLASS_LOADER;

    static {
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java`
#### Snippet
```java
   * A {@link Visitor} that collects content out of passed {@link Result}.
   */
  private static abstract class CollectingVisitor<T> implements Visitor {
    final List<T> results = new ArrayList<>();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfoDisplay.java`
#### Snippet
```java
@InterfaceAudience.Private
public class RegionInfoDisplay {
  public final static String DISPLAY_KEYS_KEY = "hbase.display.keys";
  public final static byte[] HIDDEN_END_KEY = Bytes.toBytes("hidden-end-key");
  public final static byte[] HIDDEN_START_KEY = Bytes.toBytes("hidden-start-key");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfoDisplay.java`
#### Snippet
```java
  public final static String DISPLAY_KEYS_KEY = "hbase.display.keys";
  public final static byte[] HIDDEN_END_KEY = Bytes.toBytes("hidden-end-key");
  public final static byte[] HIDDEN_START_KEY = Bytes.toBytes("hidden-start-key");

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfoDisplay.java`
#### Snippet
```java
public class RegionInfoDisplay {
  public final static String DISPLAY_KEYS_KEY = "hbase.display.keys";
  public final static byte[] HIDDEN_END_KEY = Bytes.toBytes("hidden-end-key");
  public final static byte[] HIDDEN_START_KEY = Bytes.toBytes("hidden-start-key");

```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalancerDecision.java`
#### Snippet
```java
@InterfaceAudience.Public
@InterfaceStability.Evolving
final public class BalancerDecision extends LogEntry {

  private final String initialFunctionCosts;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalanceResponse.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public final static class Builder {
    private boolean balancerRan;
    private int movesCalculated;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalanceRequest.java`
#### Snippet
```java
   */
  @InterfaceAudience.Public
  public final static class Builder {
    private boolean dryRun = false;
    private boolean ignoreRegionsInTransition = false;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorUtils.java`
#### Snippet
```java
@InterfaceAudience.Public
public final class TableDescriptorUtils {
  public final static class TableDescriptorDelta {
    private final Set<byte[]> columnsAdded;
    private final Set<byte[]> columnsDeleted;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
  }

  private final static Set<Bytes> RESERVED_KEYWORDS = new HashSet<>();

  static {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
  public static final boolean DEFAULT_PREFETCH_BLOCKS_ON_OPEN = false;

  private final static Map<String, String> DEFAULT_VALUES = new HashMap<>();

  private static Map<Bytes, Bytes> getDefaultValuesBytes() {
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  // define this attribute with the appropriate table name by calling
  // scan.setAttribute(Scan.SCAN_ATTRIBUTES_TABLE_NAME, Bytes.toBytes(tableName))
  static public final String SCAN_ATTRIBUTES_TABLE_NAME = "scan.attributes.table.name";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @Deprecated
  public final static String NAMESPACE_FAMILY_INFO = "info";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java

  private final static Map<String, String> DEFAULT_VALUES = new HashMap<>();
  private final static Set<Bytes> RESERVED_KEYWORDS = new HashSet<>();

  static {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @Deprecated
  public final static byte[] NAMESPACE_FAMILY_INFO_BYTES = Bytes.toBytes(NAMESPACE_FAMILY_INFO);

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  public static final boolean DEFAULT_REGION_MEMSTORE_REPLICATION = true;

  private final static Map<String, String> DEFAULT_VALUES = new HashMap<>();
  private final static Set<Bytes> RESERVED_KEYWORDS = new HashSet<>();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @Deprecated
  public final static byte[] NAMESPACE_COL_DESC_BYTES = Bytes.toBytes("d");

  /**
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalancerRejection.java`
#### Snippet
```java
@InterfaceAudience.Public
@InterfaceStability.Evolving
final public class BalancerRejection extends LogEntry {
  // The reason why balancer was rejected
  private final String reason;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
  public static final Result EMPTY_RESULT = new Result(true);

  private final static int INITIAL_CELLSCANNER_INDEX = -1;

  /**
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/OnlineLogRecord.java`
#### Snippet
```java
@InterfaceAudience.Public
@InterfaceStability.Evolving
final public class OnlineLogRecord extends LogEntry {

  // used to convert object to pretty printed format
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
final public class FilterList extends FilterBase {

  /** set operator */
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public boolean filterRowKey(Cell firstRowCell) throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public boolean filterRow() throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public void filterRowCells(List<Cell> kvs) throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @return True if this filter actively uses filterRowCells(List) or filterRow().
   */
  abstract public boolean hasFilterRow();

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public boolean filterAllRemaining() throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public byte[] toByteArray() throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public boolean isFamilyEssential(byte[] name) throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public Cell getNextCellHint(final Cell currentCell) throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public Cell transformCell(final Cell v) throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @throws IOException in case an I/O or an filter specific failure needs to be signaled.
   */
  abstract public void reset() throws IOException;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java
  protected byte[] hint = null;
  protected TreeSet<byte[]> sortedPrefixes = createTreeSet();
  private final static int MAX_LOG_PREFIXES = 5;

  public MultipleColumnPrefixFilter(final byte[][] prefixes) {
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
  }

  private static abstract class BasicRowRange implements Comparable<BasicRowRange> {
    protected byte[] startRow;
    protected boolean startRowInclusive = true;
```

### MissortedModifiers
Missorted modifiers `static private`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/EncryptionUtil.java`
#### Snippet
```java
@InterfaceStability.Evolving
public final class EncryptionUtil {
  static private final Logger LOG = LoggerFactory.getLogger(EncryptionUtil.class);

  /**
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java`
#### Snippet
```java
  private final int keepAliveTimeMs;

  private static abstract class Task implements Delayed {

    protected long time = System.nanoTime();
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
   * Data structure with reference to remote operation.
   */
  public static abstract class RemoteOperation {
    private final RemoteProcedure remoteProcedure;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/util/DelayedUtil.java`
#### Snippet
```java
  }

  public static abstract class DelayedContainer<T> extends DelayedObject {
    private final T object;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/util/DelayedUtil.java`
#### Snippet
```java
  }

  public static abstract class DelayedObject implements DelayedWithTimeout {
    @Override
    public long getDelay(final TimeUnit unit) {
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/VersionResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/VersionResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `synchronized static`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "MS_EXPOSE_REP",
      justification = "singleton pattern")
  public synchronized static RESTServlet getInstance() {
    assert (INSTANCE != null);
    return INSTANCE;
```

### MissortedModifiers
Missorted modifiers `synchronized static`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "MS_EXPOSE_REP",
      justification = "singleton pattern")
  public synchronized static RESTServlet getInstance(Configuration conf, UserProvider userProvider)
    throws IOException {
    if (INSTANCE == null) {
```

### MissortedModifiers
Missorted modifiers `synchronized static`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java`
#### Snippet
```java
  }

  public synchronized static void stop() {
    if (INSTANCE != null) {
      INSTANCE.shutdown();
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableScanResource.java`
#### Snippet
```java
  @GET
  @Produces({ Constants.MIMETYPE_PROTOBUF, Constants.MIMETYPE_PROTOBUF_IETF })
  public Response getProtobuf(final @Context UriInfo uriInfo,
    final @HeaderParam("Accept") String contentType) {
    if (LOG.isTraceEnabled()) {
```

### MissortedModifiers
Missorted modifiers `final @HeaderParam("Accept")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableScanResource.java`
#### Snippet
```java
  @Produces({ Constants.MIMETYPE_PROTOBUF, Constants.MIMETYPE_PROTOBUF_IETF })
  public Response getProtobuf(final @Context UriInfo uriInfo,
    final @HeaderParam("Accept") String contentType) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath() + " as " + MIMETYPE_BINARY);
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableScanResource.java`
#### Snippet
```java
  @GET
  @Produces({ Constants.MIMETYPE_XML, Constants.MIMETYPE_JSON })
  public CellSetModelStream get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java
  @GET
  @Produces({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo, @QueryParam("n") int maxRows,
    final @QueryParam("c") int maxValues) {
    if (LOG.isTraceEnabled()) {
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("c")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo, @QueryParam("n") int maxRows,
    final @QueryParam("c") int maxValues) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java
  @GET
  @Produces(MIMETYPE_BINARY)
  public Response getBinary(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath() + " as " + MIMETYPE_BINARY);
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java

  @DELETE
  public Response delete(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("DELETE " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java`
#### Snippet
```java
  @GET
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON })
  public Response get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java
  @GET
  @Produces({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo) {
    MultivaluedMap<String, String> params = uriInfo.getQueryParameters();

```

### MissortedModifiers
Missorted modifiers `final @QueryParam("v")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java

  @Path("{multiget: multiget.*}")
  public MultiRowResource getMultipleRowResource(final @QueryParam("v") String versions,
    @PathParam("multiget") String path) throws IOException {
    return new MultiRowResource(this, versions, path.replace("multiget", "").replace("/", ""));
```

### MissortedModifiers
Missorted modifiers `final @PathParam("rowspec") @Encoded`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // We need the @Encoded decorator so Jersey won't urldecode before
    // the RowSpec constructor has a chance to parse
    final @PathParam("rowspec") @Encoded String rowspec, final @QueryParam("v") String versions,
    final @QueryParam("check") String check, final @QueryParam("rr") String returnResult)
    throws IOException {
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("v")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // We need the @Encoded decorator so Jersey won't urldecode before
    // the RowSpec constructor has a chance to parse
    final @PathParam("rowspec") @Encoded String rowspec, final @QueryParam("v") String versions,
    final @QueryParam("check") String check, final @QueryParam("rr") String returnResult)
    throws IOException {
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("check")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // the RowSpec constructor has a chance to parse
    final @PathParam("rowspec") @Encoded String rowspec, final @QueryParam("v") String versions,
    final @QueryParam("check") String check, final @QueryParam("rr") String returnResult)
    throws IOException {
    return new RowResource(this, rowspec, versions, check, returnResult);
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("rr")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // the RowSpec constructor has a chance to parse
    final @PathParam("rowspec") @Encoded String rowspec, final @QueryParam("v") String versions,
    final @QueryParam("check") String check, final @QueryParam("rr") String returnResult)
    throws IOException {
    return new RowResource(this, rowspec, versions, check, returnResult);
```

### MissortedModifiers
Missorted modifiers `final @PathParam("scanspec")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java

  @Path("{scanspec: .*[*]$}")
  public TableScanResource getScanResource(final @PathParam("scanspec") String scanSpec,
    @DefaultValue(Integer.MAX_VALUE + "") @QueryParam(Constants.SCAN_LIMIT) int userRequestedLimit,
    @DefaultValue("") @QueryParam(Constants.SCAN_START_ROW) String startRow,
```

### MissortedModifiers
Missorted modifiers `final @PathParam("suffixglobbingspec") @Encoded`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // We need the @Encoded decorator so Jersey won't urldecode before
    // the RowSpec constructor has a chance to parse
    final @PathParam("suffixglobbingspec") @Encoded String suffixglobbingspec,
    final @QueryParam("v") String versions, final @QueryParam("check") String check,
    final @QueryParam("rr") String returnResult) throws IOException {
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("v")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // the RowSpec constructor has a chance to parse
    final @PathParam("suffixglobbingspec") @Encoded String suffixglobbingspec,
    final @QueryParam("v") String versions, final @QueryParam("check") String check,
    final @QueryParam("rr") String returnResult) throws IOException {
    return new RowResource(this, suffixglobbingspec, versions, check, returnResult);
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("check")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    // the RowSpec constructor has a chance to parse
    final @PathParam("suffixglobbingspec") @Encoded String suffixglobbingspec,
    final @QueryParam("v") String versions, final @QueryParam("check") String check,
    final @QueryParam("rr") String returnResult) throws IOException {
    return new RowResource(this, suffixglobbingspec, versions, check, returnResult);
```

### MissortedModifiers
Missorted modifiers `final @QueryParam("rr")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/TableResource.java`
#### Snippet
```java
    final @PathParam("suffixglobbingspec") @Encoded String suffixglobbingspec,
    final @QueryParam("v") String versions, final @QueryParam("check") String check,
    final @QueryParam("rr") String returnResult) throws IOException {
    return new RowResource(this, suffixglobbingspec, versions, check, returnResult);
  }
```

### MissortedModifiers
Missorted modifiers `final @PathParam("namespace")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesResource.java`
#### Snippet
```java
  @Path("{namespace}")
  public NamespacesInstanceResource getNamespaceInstanceResource(
    final @PathParam("namespace") String namespace) throws IOException {
    return new NamespacesInstanceResource(namespace);
  }
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
  @POST
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response post(final ScannerModel model, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("POST " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
  @PUT
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response put(final ScannerModel model, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("PUT " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @PathParam("scanner")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java

  @Path("{scanner: .+}")
  public ScannerInstanceResource getScannerInstanceResource(final @PathParam("scanner") String id)
    throws IOException {
    ScannerInstanceResource instance = scanners.get(id);
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
   */
  @DELETE
  public Response deleteNoBody(final byte[] message, final @Context UriInfo uriInfo,
    final @Context HttpHeaders headers) {
    if (LOG.isTraceEnabled()) {
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  @DELETE
  public Response deleteNoBody(final byte[] message, final @Context UriInfo uriInfo,
    final @Context HttpHeaders headers) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("DELETE " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  @PUT
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response put(final NamespacesInstanceModel model, final @Context UriInfo uriInfo) {
    return processUpdate(model, true, uriInfo);
  }
```

### MissortedModifiers
Missorted modifiers `final @PathParam("tables")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  @Path("tables")
  public NamespacesInstanceResource
    getNamespaceInstanceResource(final @PathParam("tables") String namespace) throws IOException {
    return new NamespacesInstanceResource(this.namespace, true);
  }
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context ServletContext context, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  @POST
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response post(final NamespacesInstanceModel model, final @Context UriInfo uriInfo) {
    return processUpdate(model, false, uriInfo);
  }
```

### MissortedModifiers
Missorted modifiers `final @PathParam("table")`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RootResource.java`
#### Snippet
```java

  @Path("{table}")
  public TableResource getTableResource(final @PathParam("table") String table) throws IOException {
    return new TableResource(table);
  }
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RootResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF,
    MIMETYPE_BINARY })
  public Response get(final @Context UriInfo uriInfo) {
    try {
      if (!tableResource.exists()) {
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java`
#### Snippet
```java
      justification = "Expected")
  @DELETE
  public Response delete(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("DELETE " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java`
#### Snippet
```java
  @POST
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response post(final TableSchemaModel model, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("PUT " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java`
#### Snippet
```java
  @PUT
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response put(final TableSchemaModel model, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("PUT " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java`
#### Snippet
```java
  @Produces({ MIMETYPE_TEXT, MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF,
    MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @POST
  @Consumes(MIMETYPE_BINARY)
  public Response postBinary(final byte[] message, final @Context UriInfo uriInfo,
    final @Context HttpHeaders headers) {
    if (LOG.isTraceEnabled()) {
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @Consumes(MIMETYPE_BINARY)
  public Response postBinary(final byte[] message, final @Context UriInfo uriInfo,
    final @Context HttpHeaders headers) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("POST " + uriInfo.getAbsolutePath() + " as " + MIMETYPE_BINARY);
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @GET
  @Produces(MIMETYPE_BINARY)
  public Response getBinary(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath() + " as " + MIMETYPE_BINARY);
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @POST
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response post(final CellSetModel model, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("POST " + uriInfo.getAbsolutePath() + " " + uriInfo.getQueryParameters());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @GET
  @Produces({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response get(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("GET " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java

  @DELETE
  public Response delete(final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("DELETE " + uriInfo.getAbsolutePath());
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @PUT
  @Consumes(MIMETYPE_BINARY)
  public Response putBinary(final byte[] message, final @Context UriInfo uriInfo,
    final @Context HttpHeaders headers) {
    if (LOG.isTraceEnabled()) {
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @Consumes(MIMETYPE_BINARY)
  public Response putBinary(final byte[] message, final @Context UriInfo uriInfo,
    final @Context HttpHeaders headers) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("PUT " + uriInfo.getAbsolutePath() + " as " + MIMETYPE_BINARY);
```

### MissortedModifiers
Missorted modifiers `final @Context`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  @PUT
  @Consumes({ MIMETYPE_XML, MIMETYPE_JSON, MIMETYPE_PROTOBUF, MIMETYPE_PROTOBUF_IETF })
  public Response put(final CellSetModel model, final @Context UriInfo uriInfo) {
    if (LOG.isTraceEnabled()) {
      LOG.trace("PUT " + uriInfo.getAbsolutePath() + " " + uriInfo.getQueryParameters());
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  private final static Cell[] EMPTY_CELL_ARRAY = new Cell[] {};
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  final static Result EMPTY_RESULT_EXISTS_TRUE = Result.create(null, true);
  final static Result EMPTY_RESULT_EXISTS_FALSE = Result.create(null, false);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static ClientProtos.Result EMPTY_RESULT_PB;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE_STALE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static ClientProtos.Result EMPTY_RESULT_PB_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE_STALE;

  static {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   */
  private final static class ClassLoaderHolder {
    private final static ClassLoader CLASS_LOADER;

    static {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static Cell[] EMPTY_CELL_ARRAY = new Cell[] {};
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  final static Result EMPTY_RESULT_EXISTS_TRUE = Result.create(null, true);
  final static Result EMPTY_RESULT_EXISTS_FALSE = Result.create(null, false);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   * Dynamic class loader to load filter/comparators
   */
  private final static class ClassLoaderHolder {
    private final static ClassLoader CLASS_LOADER;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
   * them across calls.
   */
  private final static Cell[] EMPTY_CELL_ARRAY = new Cell[] {};
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  final static Result EMPTY_RESULT_EXISTS_TRUE = Result.create(null, true);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    Result.create((Cell[]) null, false, true);

  private final static ClientProtos.Result EMPTY_RESULT_PB;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static Result EMPTY_RESULT_EXISTS_TRUE_STALE =
    Result.create((Cell[]) null, true, true);
  private final static Result EMPTY_RESULT_EXISTS_FALSE_STALE =
    Result.create((Cell[]) null, false, true);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  final static Result EMPTY_RESULT_EXISTS_FALSE = Result.create(null, false);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);
  private final static Result EMPTY_RESULT_EXISTS_TRUE_STALE =
    Result.create((Cell[]) null, true, true);
  private final static Result EMPTY_RESULT_EXISTS_FALSE_STALE =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE_STALE;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java

  private final static ClientProtos.Result EMPTY_RESULT_PB;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_STALE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_TRUE_STALE;
  private final static ClientProtos.Result EMPTY_RESULT_PB_EXISTS_FALSE_STALE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  final static Result EMPTY_RESULT_EXISTS_TRUE = Result.create(null, true);
  final static Result EMPTY_RESULT_EXISTS_FALSE = Result.create(null, false);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);
  private final static Result EMPTY_RESULT_EXISTS_TRUE_STALE =
    Result.create((Cell[]) null, true, true);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  final static Result EMPTY_RESULT_EXISTS_TRUE = Result.create(null, true);
  final static Result EMPTY_RESULT_EXISTS_FALSE = Result.create(null, false);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);
  private final static Result EMPTY_RESULT_EXISTS_TRUE_STALE =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
  static final String NAME = "CellCounter";

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  protected static final String tableSeparator = ";";

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

  public WALPlayer() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  final static String NAME = "WALPlayer";
  public final static String BULK_OUTPUT_CONF_KEY = "wal.bulk.output";
  public final static String TABLES_KEY = "wal.input.tables";
  public final static String TABLE_MAP_KEY = "wal.input.tablesmap";
  public final static String INPUT_FILES_SEPARATOR_KEY = "wal.input.separator";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  public final static String TABLES_KEY = "wal.input.tables";
  public final static String TABLE_MAP_KEY = "wal.input.tablesmap";
  public final static String INPUT_FILES_SEPARATOR_KEY = "wal.input.separator";
  public final static String IGNORE_MISSING_FILES = "wal.input.ignore.missing.files";
  public final static String MULTI_TABLES_SUPPORT = "wal.multi.tables.support";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  public final static String BULK_OUTPUT_CONF_KEY = "wal.bulk.output";
  public final static String TABLES_KEY = "wal.input.tables";
  public final static String TABLE_MAP_KEY = "wal.input.tablesmap";
  public final static String INPUT_FILES_SEPARATOR_KEY = "wal.input.separator";
  public final static String IGNORE_MISSING_FILES = "wal.input.ignore.missing.files";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
public class WALPlayer extends Configured implements Tool {
  private static final Logger LOG = LoggerFactory.getLogger(WALPlayer.class);
  final static String NAME = "WALPlayer";
  public final static String BULK_OUTPUT_CONF_KEY = "wal.bulk.output";
  public final static String TABLES_KEY = "wal.input.tables";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  public final static String TABLE_MAP_KEY = "wal.input.tablesmap";
  public final static String INPUT_FILES_SEPARATOR_KEY = "wal.input.separator";
  public final static String IGNORE_MISSING_FILES = "wal.input.ignore.missing.files";
  public final static String MULTI_TABLES_SUPPORT = "wal.multi.tables.support";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(WALPlayer.class);
  final static String NAME = "WALPlayer";
  public final static String BULK_OUTPUT_CONF_KEY = "wal.bulk.output";
  public final static String TABLES_KEY = "wal.input.tables";
  public final static String TABLE_MAP_KEY = "wal.input.tablesmap";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  public final static String INPUT_FILES_SEPARATOR_KEY = "wal.input.separator";
  public final static String IGNORE_MISSING_FILES = "wal.input.ignore.missing.files";
  public final static String MULTI_TABLES_SUPPORT = "wal.multi.tables.support";

  protected static final String tableSeparator = ";";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String snapshot = null;

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

  private Path generateUniqTempDir(boolean withDirCreated) throws IOException {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(CopyTable.class);

  final static String NAME = "copytable";
  long startTime = 0;
  long endTime = HConstants.LATEST_TIMESTAMP;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
  private final static String OPT_END_TIME = "endtime";
  private final static String OPT_RANGE = "range";
  private final static String OPT_EXPECTED_COUNT = "expectedCount";

  private String tableName;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
  static final String NAME = "rowcounter";

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
  private final static String EXPECTED_COUNT_KEY = RowCounter.class.getName() + ".expected_count";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java

  private final static String OPT_START_TIME = "starttime";
  private final static String OPT_END_TIME = "endtime";
  private final static String OPT_RANGE = "range";
  private final static String OPT_EXPECTED_COUNT = "expectedCount";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
  private final static String EXPECTED_COUNT_KEY = RowCounter.class.getName() + ".expected_count";

  private final static String OPT_START_TIME = "starttime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
  private final static String OPT_START_TIME = "starttime";
  private final static String OPT_END_TIME = "endtime";
  private final static String OPT_RANGE = "range";
  private final static String OPT_EXPECTED_COUNT = "expectedCount";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java`
#### Snippet
```java
  private final static String EXPECTED_COUNT_KEY = RowCounter.class.getName() + ".expected_count";

  private final static String OPT_START_TIME = "starttime";
  private final static String OPT_END_TIME = "endtime";
  private final static String OPT_RANGE = "range";
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java`
#### Snippet
```java
   * HLogInputFormat.
   */
  static abstract class WALRecordReader<K extends WALKey> extends RecordReader<K, WALEdit> {
    private Reader reader = null;
    // visible until we can remove the deprecated HLogInputFormat
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  // This config is used to propagate credentials from parent MR jobs which launch
  // ImportTSV jobs. SEE IntegrationTestImportTsv.
  public final static String CREDENTIALS_LOCATION = "credentials_location";
  final static String DEFAULT_SEPARATOR = "\t";
  final static String DEFAULT_ATTRIBUTES_SEPERATOR = "=>";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  // ImportTSV jobs. SEE IntegrationTestImportTsv.
  public final static String CREDENTIALS_LOCATION = "credentials_location";
  final static String DEFAULT_SEPARATOR = "\t";
  final static String DEFAULT_ATTRIBUTES_SEPERATOR = "=>";
  final static String DEFAULT_MULTIPLE_ATTRIBUTES_SEPERATOR = ",";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String CREDENTIALS_LOCATION = "credentials_location";
  final static String DEFAULT_SEPARATOR = "\t";
  final static String DEFAULT_ATTRIBUTES_SEPERATOR = "=>";
  final static String DEFAULT_MULTIPLE_ATTRIBUTES_SEPERATOR = ",";
  final static Class DEFAULT_MAPPER = TsvImporterMapper.class;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String LOG_BAD_LINES_CONF_KEY = "importtsv.log.bad.lines";
  public final static String SKIP_LINES_CONF_KEY = "importtsv.skip.bad.lines";
  public final static String SKIP_EMPTY_COLUMNS = "importtsv.skip.empty.columns";
  public final static String COLUMNS_CONF_KEY = "importtsv.columns";
  public final static String SEPARATOR_CONF_KEY = "importtsv.separator";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  final static Class DEFAULT_MAPPER = TsvImporterMapper.class;
  public final static String CREATE_TABLE_CONF_KEY = "create.table";
  public final static String NO_STRICT_COL_FAMILY = "no.strict";
  /**
   * If table didn't exist and was created in dry-run mode, this flag is flipped to delete it when
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String MAPPER_CONF_KEY = "importtsv.mapper.class";
  public final static String BULK_OUTPUT_CONF_KEY = "importtsv.bulk.output";
  public final static String TIMESTAMP_CONF_KEY = "importtsv.timestamp";
  public final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
  // TODO: the rest of these configs are used exclusively by TsvImporterMapper.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  final static String DEFAULT_SEPARATOR = "\t";
  final static String DEFAULT_ATTRIBUTES_SEPERATOR = "=>";
  final static String DEFAULT_MULTIPLE_ATTRIBUTES_SEPERATOR = ",";
  final static Class DEFAULT_MAPPER = TsvImporterMapper.class;
  public final static String CREATE_TABLE_CONF_KEY = "create.table";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  final static String DEFAULT_ATTRIBUTES_SEPERATOR = "=>";
  final static String DEFAULT_MULTIPLE_ATTRIBUTES_SEPERATOR = ",";
  final static Class DEFAULT_MAPPER = TsvImporterMapper.class;
  public final static String CREATE_TABLE_CONF_KEY = "create.table";
  public final static String NO_STRICT_COL_FAMILY = "no.strict";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String COLUMNS_CONF_KEY = "importtsv.columns";
  public final static String SEPARATOR_CONF_KEY = "importtsv.separator";
  public final static String ATTRIBUTE_SEPERATOR_CONF_KEY = "attributes.seperator";
  // This config is used to propagate credentials from parent MR jobs which launch
  // ImportTSV jobs. SEE IntegrationTestImportTsv.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  final static String DEFAULT_MULTIPLE_ATTRIBUTES_SEPERATOR = ",";
  final static Class DEFAULT_MAPPER = TsvImporterMapper.class;
  public final static String CREATE_TABLE_CONF_KEY = "create.table";
  public final static String NO_STRICT_COL_FAMILY = "no.strict";
  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String SKIP_LINES_CONF_KEY = "importtsv.skip.bad.lines";
  public final static String SKIP_EMPTY_COLUMNS = "importtsv.skip.empty.columns";
  public final static String COLUMNS_CONF_KEY = "importtsv.columns";
  public final static String SEPARATOR_CONF_KEY = "importtsv.separator";
  public final static String ATTRIBUTE_SEPERATOR_CONF_KEY = "attributes.seperator";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  final static String NAME = "importtsv";

  public final static String MAPPER_CONF_KEY = "importtsv.mapper.class";
  public final static String BULK_OUTPUT_CONF_KEY = "importtsv.bulk.output";
  public final static String TIMESTAMP_CONF_KEY = "importtsv.timestamp";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  protected static final Logger LOG = LoggerFactory.getLogger(ImportTsv.class);

  final static String NAME = "importtsv";

  public final static String MAPPER_CONF_KEY = "importtsv.mapper.class";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String BULK_OUTPUT_CONF_KEY = "importtsv.bulk.output";
  public final static String TIMESTAMP_CONF_KEY = "importtsv.timestamp";
  public final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
  // TODO: the rest of these configs are used exclusively by TsvImporterMapper.
  // Move them out of the tool and let the mapper handle its own validation.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  // If true, bad lines are logged to stderr. Default: false.
  public final static String LOG_BAD_LINES_CONF_KEY = "importtsv.log.bad.lines";
  public final static String SKIP_LINES_CONF_KEY = "importtsv.skip.bad.lines";
  public final static String SKIP_EMPTY_COLUMNS = "importtsv.skip.empty.columns";
  public final static String COLUMNS_CONF_KEY = "importtsv.columns";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String SKIP_EMPTY_COLUMNS = "importtsv.skip.empty.columns";
  public final static String COLUMNS_CONF_KEY = "importtsv.columns";
  public final static String SEPARATOR_CONF_KEY = "importtsv.separator";
  public final static String ATTRIBUTE_SEPERATOR_CONF_KEY = "attributes.seperator";
  // This config is used to propagate credentials from parent MR jobs which launch
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  // TODO: the rest of these configs are used exclusively by TsvImporterMapper.
  // Move them out of the tool and let the mapper handle its own validation.
  public final static String DRY_RUN_CONF_KEY = "importtsv.dry.run";
  // If true, bad lines are logged to stderr. Default: false.
  public final static String LOG_BAD_LINES_CONF_KEY = "importtsv.log.bad.lines";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java

  public final static String MAPPER_CONF_KEY = "importtsv.mapper.class";
  public final static String BULK_OUTPUT_CONF_KEY = "importtsv.bulk.output";
  public final static String TIMESTAMP_CONF_KEY = "importtsv.timestamp";
  public final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
  public final static String DRY_RUN_CONF_KEY = "importtsv.dry.run";
  // If true, bad lines are logged to stderr. Default: false.
  public final static String LOG_BAD_LINES_CONF_KEY = "importtsv.log.bad.lines";
  public final static String SKIP_LINES_CONF_KEY = "importtsv.skip.bad.lines";
  public final static String SKIP_EMPTY_COLUMNS = "importtsv.skip.empty.columns";
```

### MissortedModifiers
Missorted modifiers `final private static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java`
#### Snippet
```java
  }

  final private static int validateCompositeKey(byte[] keyBytes) {

    int separatorIdx = Bytes.indexOf(keyBytes, tableSeparator);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  final static String HASH_DATA_DIR = "hashes";
  final static String OUTPUT_DATA_FILE_PREFIX = "part-r-";
  final static String IGNORE_TIMESTAMPS = "ignoreTimestamps";
  private final static String TMP_MANIFEST_FILE_NAME = "manifest.tmp";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  private static final int DEFAULT_BATCH_SIZE = 8000;

  private final static String HASH_BATCH_SIZE_CONF_KEY = "hash.batch.size";
  final static String PARTITIONS_FILE_NAME = "partitions";
  final static String MANIFEST_FILE_NAME = "manifest";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java

  private final static String HASH_BATCH_SIZE_CONF_KEY = "hash.batch.size";
  final static String PARTITIONS_FILE_NAME = "partitions";
  final static String MANIFEST_FILE_NAME = "manifest";
  final static String HASH_DATA_DIR = "hashes";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  final static String MANIFEST_FILE_NAME = "manifest";
  final static String HASH_DATA_DIR = "hashes";
  final static String OUTPUT_DATA_FILE_PREFIX = "part-r-";
  final static String IGNORE_TIMESTAMPS = "ignoreTimestamps";
  private final static String TMP_MANIFEST_FILE_NAME = "manifest.tmp";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  final static String PARTITIONS_FILE_NAME = "partitions";
  final static String MANIFEST_FILE_NAME = "manifest";
  final static String HASH_DATA_DIR = "hashes";
  final static String OUTPUT_DATA_FILE_PREFIX = "part-r-";
  final static String IGNORE_TIMESTAMPS = "ignoreTimestamps";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  private final static String HASH_BATCH_SIZE_CONF_KEY = "hash.batch.size";
  final static String PARTITIONS_FILE_NAME = "partitions";
  final static String MANIFEST_FILE_NAME = "manifest";
  final static String HASH_DATA_DIR = "hashes";
  final static String OUTPUT_DATA_FILE_PREFIX = "part-r-";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
  final static String OUTPUT_DATA_FILE_PREFIX = "part-r-";
  final static String IGNORE_TIMESTAMPS = "ignoreTimestamps";
  private final static String TMP_MANIFEST_FILE_NAME = "manifest.tmp";

  TableHash tableHash = new TableHash();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java
public class SimpleTotalOrderPartitioner<VALUE> extends Partitioner<ImmutableBytesWritable, VALUE>
  implements Configurable {
  private final static Logger LOG = LoggerFactory.getLogger(SimpleTotalOrderPartitioner.class);

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    extends Mapper<BytesWritable, NullWritable, NullWritable, NullWritable> {
    private static final Logger LOG = LoggerFactory.getLogger(ExportMapper.class);
    final static int REPORT_SIZE = 1 * 1024 * 1024;
    final static int BUFFER_SIZE = 64 * 1024;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    private static final Logger LOG = LoggerFactory.getLogger(ExportMapper.class);
    final static int REPORT_SIZE = 1 * 1024 * 1024;
    final static int BUFFER_SIZE = 64 * 1024;

    private boolean verifyChecksum;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  final static String NAME = "import";
  public final static String CF_RENAME_PROP = "HBASE_IMPORTER_RENAME_CFS";
  public final static String BULK_OUTPUT_CONF_KEY = "import.bulk.output";
  public final static String FILTER_CLASS_CONF_KEY = "import.filter.class";
  public final static String FILTER_ARGS_CONF_KEY = "import.filter.args";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public final static String BULK_OUTPUT_CONF_KEY = "import.bulk.output";
  public final static String FILTER_CLASS_CONF_KEY = "import.filter.class";
  public final static String FILTER_ARGS_CONF_KEY = "import.filter.args";
  public final static String TABLE_NAME = "import.table.name";
  public final static String WAL_DURABILITY = "import.wal.durability";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public final static String CF_RENAME_PROP = "HBASE_IMPORTER_RENAME_CFS";
  public final static String BULK_OUTPUT_CONF_KEY = "import.bulk.output";
  public final static String FILTER_CLASS_CONF_KEY = "import.filter.class";
  public final static String FILTER_ARGS_CONF_KEY = "import.filter.args";
  public final static String TABLE_NAME = "import.table.name";
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
   * @param renameMap a mapping from source CF names to destination CF names
   */
  static public void configureCfRenaming(Configuration conf, Map<String, String> renameMap) {
    StringBuilder sb = new StringBuilder();
    for (Map.Entry<String, String> entry : renameMap.entrySet()) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
public class Import extends Configured implements Tool {
  private static final Logger LOG = LoggerFactory.getLogger(Import.class);
  final static String NAME = "import";
  public final static String CF_RENAME_PROP = "HBASE_IMPORTER_RENAME_CFS";
  public final static String BULK_OUTPUT_CONF_KEY = "import.bulk.output";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public final static String HAS_LARGE_RESULT = "import.bulk.hasLargeResult";

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

  public static class CellWritableComparablePartitioner
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(Import.class);
  final static String NAME = "import";
  public final static String CF_RENAME_PROP = "HBASE_IMPORTER_RENAME_CFS";
  public final static String BULK_OUTPUT_CONF_KEY = "import.bulk.output";
  public final static String FILTER_CLASS_CONF_KEY = "import.filter.class";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public final static String FILTER_ARGS_CONF_KEY = "import.filter.args";
  public final static String TABLE_NAME = "import.table.name";
  public final static String WAL_DURABILITY = "import.wal.durability";
  public final static String HAS_LARGE_RESULT = "import.bulk.hasLargeResult";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public final static String FILTER_CLASS_CONF_KEY = "import.filter.class";
  public final static String FILTER_ARGS_CONF_KEY = "import.filter.args";
  public final static String TABLE_NAME = "import.table.name";
  public final static String WAL_DURABILITY = "import.wal.durability";
  public final static String HAS_LARGE_RESULT = "import.bulk.hasLargeResult";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public final static String TABLE_NAME = "import.table.name";
  public final static String WAL_DURABILITY = "import.wal.durability";
  public final static String HAS_LARGE_RESULT = "import.bulk.hasLargeResult";

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java

  public final static String NAME = "verifyrep";
  private final static String PEER_CONFIG_PREFIX = NAME + ".peer.";
  long startTime = 0;
  long endTime = Long.MAX_VALUE;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerTableName = null;

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(VerifyReplication.class);

  public final static String NAME = "verifyrep";
  private final static String PEER_CONFIG_PREFIX = NAME + ".peer.";
  long startTime = 0;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
  private final static String CONF_COMPACT_ONCE = "hbase.compactiontool.compact.once";
  private final static String CONF_COMPACT_MAJOR = "hbase.compactiontool.compact.major";
  private final static String CONF_DELETE_COMPACTED = "hbase.compactiontool.delete";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java

  private final static String CONF_COMPACT_ONCE = "hbase.compactiontool.compact.once";
  private final static String CONF_COMPACT_MAJOR = "hbase.compactiontool.compact.major";
  private final static String CONF_DELETE_COMPACTED = "hbase.compactiontool.delete";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(CompactionTool.class);

  private final static String CONF_COMPACT_ONCE = "hbase.compactiontool.compact.once";
  private final static String CONF_COMPACT_MAJOR = "hbase.compactiontool.compact.major";
  private final static String CONF_DELETE_COMPACTED = "hbase.compactiontool.delete";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_resubmit = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_failed = new LongAdder();
  public final static LongAdder tot_mgr_null_data = new LongAdder();
  public final static LongAdder tot_mgr_orphan_task_acquired = new LongAdder();
  public final static LongAdder tot_mgr_wait_for_zk_delete = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_create_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_retry = new LongAdder();
  public final static LongAdder tot_mgr_get_data_queued = new LongAdder();
  public final static LongAdder tot_mgr_get_data_result = new LongAdder();
  public final static LongAdder tot_mgr_get_data_nonode = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_log_split_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_create_result = new LongAdder();
  public final static LongAdder tot_mgr_node_already_exists = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_get_data_result = new LongAdder();
  public final static LongAdder tot_wkr_get_data_retry = new LongAdder();
  public final static LongAdder tot_wkr_preempt_task = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat_failed = new LongAdder();
  public final static LongAdder tot_wkr_final_transition_failed = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_create_result = new LongAdder();
  public final static LongAdder tot_mgr_node_already_exists = new LongAdder();
  public final static LongAdder tot_mgr_node_create_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_retry = new LongAdder();
  public final static LongAdder tot_mgr_get_data_queued = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_delete_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_result = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_err = new LongAdder();
  public final static LongAdder tot_mgr_resubmit = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_failed = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_rescan = new LongAdder();
  public final static LongAdder tot_mgr_rescan_deleted = new LongAdder();
  public final static LongAdder tot_mgr_task_deleted = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_unassigned = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_dead_server_task = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_get_data_result = new LongAdder();
  public final static LongAdder tot_mgr_get_data_nonode = new LongAdder();
  public final static LongAdder tot_mgr_get_data_err = new LongAdder();
  public final static LongAdder tot_mgr_get_data_retry = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_queued = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_wait_for_zk_delete = new LongAdder();
  public final static LongAdder tot_mgr_unacquired_orphan_done = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_threshold_reached = new LongAdder();
  public final static LongAdder tot_mgr_missing_state_in_delete = new LongAdder();
  public final static LongAdder tot_mgr_heartbeat = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_acquired = new LongAdder();
  public final static LongAdder tot_wkr_task_resigned = new LongAdder();
  public final static LongAdder tot_wkr_task_done = new LongAdder();
  public final static LongAdder tot_wkr_task_err = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_done = new LongAdder();
  public final static LongAdder tot_wkr_task_err = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired_rescan = new LongAdder();
  public final static LongAdder tot_wkr_get_data_queued = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java

  // SplitLogWorker counters
  public final static LongAdder tot_wkr_failed_to_grab_task_no_data = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_exception = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_owned = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_get_data_queued = new LongAdder();
  public final static LongAdder tot_mgr_get_data_result = new LongAdder();
  public final static LongAdder tot_mgr_get_data_nonode = new LongAdder();
  public final static LongAdder tot_mgr_get_data_err = new LongAdder();
  public final static LongAdder tot_mgr_get_data_retry = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  // Spnager counters
  public final static LongAdder tot_mgr_log_split_batch_start = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_err = new LongAdder();
  public final static LongAdder tot_mgr_log_split_success = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_get_data_retry = new LongAdder();
  public final static LongAdder tot_wkr_preempt_task = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat_failed = new LongAdder();
  public final static LongAdder tot_wkr_final_transition_failed = new LongAdder();
  public final static LongAdder tot_wkr_task_grabing = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_resubmit_unassigned = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_dead_server_task = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_force = new LongAdder();

  // SplitLogWorker counters
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_failed_to_grab_task_owned = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_lost_race = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired = new LongAdder();
  public final static LongAdder tot_wkr_task_resigned = new LongAdder();
  public final static LongAdder tot_wkr_task_done = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_delete_err = new LongAdder();
  public final static LongAdder tot_mgr_resubmit = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_failed = new LongAdder();
  public final static LongAdder tot_mgr_null_data = new LongAdder();
  public final static LongAdder tot_mgr_orphan_task_acquired = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_log_split_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_create_result = new LongAdder();
  public final static LongAdder tot_mgr_node_already_exists = new LongAdder();
  public final static LongAdder tot_mgr_node_create_err = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_heartbeat = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired_rescan = new LongAdder();
  public final static LongAdder tot_wkr_get_data_queued = new LongAdder();
  public final static LongAdder tot_wkr_get_data_result = new LongAdder();
  public final static LongAdder tot_wkr_get_data_retry = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_get_data_queued = new LongAdder();
  public final static LongAdder tot_wkr_get_data_result = new LongAdder();
  public final static LongAdder tot_wkr_get_data_retry = new LongAdder();
  public final static LongAdder tot_wkr_preempt_task = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat_failed = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_heartbeat_failed = new LongAdder();
  public final static LongAdder tot_wkr_final_transition_failed = new LongAdder();
  public final static LongAdder tot_wkr_task_grabing = new LongAdder();

  public static void resetCounters() throws Exception {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_preempt_task = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat_failed = new LongAdder();
  public final static LongAdder tot_wkr_final_transition_failed = new LongAdder();
  public final static LongAdder tot_wkr_task_grabing = new LongAdder();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_failed_to_grab_task_lost_race = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired = new LongAdder();
  public final static LongAdder tot_wkr_task_resigned = new LongAdder();
  public final static LongAdder tot_wkr_task_done = new LongAdder();
  public final static LongAdder tot_wkr_task_err = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_err = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired_rescan = new LongAdder();
  public final static LongAdder tot_wkr_get_data_queued = new LongAdder();
  public final static LongAdder tot_wkr_get_data_result = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_get_data_retry = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_result = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_err = new LongAdder();
  public final static LongAdder tot_mgr_resubmit = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_acquired_rescan = new LongAdder();
  public final static LongAdder tot_wkr_get_data_queued = new LongAdder();
  public final static LongAdder tot_wkr_get_data_result = new LongAdder();
  public final static LongAdder tot_wkr_get_data_retry = new LongAdder();
  public final static LongAdder tot_wkr_preempt_task = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_log_split_batch_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_err = new LongAdder();
  public final static LongAdder tot_mgr_log_split_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_queued = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_orphan_task_acquired = new LongAdder();
  public final static LongAdder tot_mgr_wait_for_zk_delete = new LongAdder();
  public final static LongAdder tot_mgr_unacquired_orphan_done = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_threshold_reached = new LongAdder();
  public final static LongAdder tot_mgr_missing_state_in_delete = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_failed_to_grab_task_exception = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_owned = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_lost_race = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired = new LongAdder();
  public final static LongAdder tot_wkr_task_resigned = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_resubmit_threshold_reached = new LongAdder();
  public final static LongAdder tot_mgr_missing_state_in_delete = new LongAdder();
  public final static LongAdder tot_mgr_heartbeat = new LongAdder();
  public final static LongAdder tot_mgr_rescan = new LongAdder();
  public final static LongAdder tot_mgr_rescan_deleted = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_unacquired_orphan_done = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_threshold_reached = new LongAdder();
  public final static LongAdder tot_mgr_missing_state_in_delete = new LongAdder();
  public final static LongAdder tot_mgr_heartbeat = new LongAdder();
  public final static LongAdder tot_mgr_rescan = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_null_data = new LongAdder();
  public final static LongAdder tot_mgr_orphan_task_acquired = new LongAdder();
  public final static LongAdder tot_mgr_wait_for_zk_delete = new LongAdder();
  public final static LongAdder tot_mgr_unacquired_orphan_done = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_threshold_reached = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java

  // Spnager counters
  public final static LongAdder tot_mgr_log_split_batch_start = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_err = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_rescan_deleted = new LongAdder();
  public final static LongAdder tot_mgr_task_deleted = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_unassigned = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_dead_server_task = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_force = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_task_deleted = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_unassigned = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_dead_server_task = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_force = new LongAdder();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_get_data_nonode = new LongAdder();
  public final static LongAdder tot_mgr_get_data_err = new LongAdder();
  public final static LongAdder tot_mgr_get_data_retry = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_result = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_resubmit_failed = new LongAdder();
  public final static LongAdder tot_mgr_null_data = new LongAdder();
  public final static LongAdder tot_mgr_orphan_task_acquired = new LongAdder();
  public final static LongAdder tot_mgr_wait_for_zk_delete = new LongAdder();
  public final static LongAdder tot_mgr_unacquired_orphan_done = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_already_exists = new LongAdder();
  public final static LongAdder tot_mgr_node_create_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_retry = new LongAdder();
  public final static LongAdder tot_mgr_get_data_queued = new LongAdder();
  public final static LongAdder tot_mgr_get_data_result = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_failed_to_grab_task_no_data = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_exception = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_owned = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_lost_race = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_missing_state_in_delete = new LongAdder();
  public final static LongAdder tot_mgr_heartbeat = new LongAdder();
  public final static LongAdder tot_mgr_rescan = new LongAdder();
  public final static LongAdder tot_mgr_rescan_deleted = new LongAdder();
  public final static LongAdder tot_mgr_task_deleted = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_create_retry = new LongAdder();
  public final static LongAdder tot_mgr_get_data_queued = new LongAdder();
  public final static LongAdder tot_mgr_get_data_result = new LongAdder();
  public final static LongAdder tot_mgr_get_data_nonode = new LongAdder();
  public final static LongAdder tot_mgr_get_data_err = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_wkr_task_resigned = new LongAdder();
  public final static LongAdder tot_wkr_task_done = new LongAdder();
  public final static LongAdder tot_wkr_task_err = new LongAdder();
  public final static LongAdder tot_wkr_task_heartbeat = new LongAdder();
  public final static LongAdder tot_wkr_task_acquired_rescan = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_heartbeat = new LongAdder();
  public final static LongAdder tot_mgr_rescan = new LongAdder();
  public final static LongAdder tot_mgr_rescan_deleted = new LongAdder();
  public final static LongAdder tot_mgr_task_deleted = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_unassigned = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_log_split_batch_start = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_batch_err = new LongAdder();
  public final static LongAdder tot_mgr_log_split_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_err = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_delete_result = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_err = new LongAdder();
  public final static LongAdder tot_mgr_resubmit = new LongAdder();
  public final static LongAdder tot_mgr_resubmit_failed = new LongAdder();
  public final static LongAdder tot_mgr_null_data = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_get_data_err = new LongAdder();
  public final static LongAdder tot_mgr_get_data_retry = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_result = new LongAdder();
  public final static LongAdder tot_mgr_node_delete_err = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  // SplitLogWorker counters
  public final static LongAdder tot_wkr_failed_to_grab_task_no_data = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_exception = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_owned = new LongAdder();
  public final static LongAdder tot_wkr_failed_to_grab_task_lost_race = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_node_create_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_create_result = new LongAdder();
  public final static LongAdder tot_mgr_node_already_exists = new LongAdder();
  public final static LongAdder tot_mgr_node_create_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_retry = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
  public final static LongAdder tot_mgr_log_split_batch_err = new LongAdder();
  public final static LongAdder tot_mgr_log_split_success = new LongAdder();
  public final static LongAdder tot_mgr_log_split_err = new LongAdder();
  public final static LongAdder tot_mgr_node_create_queued = new LongAdder();
  public final static LongAdder tot_mgr_node_create_result = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java`
#### Snippet
```java
  private final List<JVMClusterUtil.RegionServerThread> regionThreads =
    new CopyOnWriteArrayList<>();
  private final static int DEFAULT_NO = 1;
  /** local mode */
  public static final String LOCAL = "local";
```

### MissortedModifiers
Missorted modifiers `static private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java`
#### Snippet
```java

  /** Pattern used for searching in the output of the node health script */
  static private final String ERROR_PATTERN = "ERROR";

  private String healthCheckScript;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java`
#### Snippet
```java
   * @param hFileLinkPattern The path of the HFile Link.
   */
  public final static HFileLink buildFromHFileLinkPattern(final Path rootDir, final Path archiveDir,
    final Path hFileLinkPattern) {
    Path hfilePath = getHFileLinkPatternRelativePath(hFileLinkPattern);
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
   * checksum verification is done by HBase.
   */
  static public FileSystem get(Configuration conf) throws IOException {
    return new HFileSystem(conf, true);
  }
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
   * Wrap a LocalFileSystem within a HFileSystem.
   */
  static public FileSystem getLocalFs(Configuration conf) throws IOException {
    return new HFileSystem(FileSystem.getLocal(conf));
  }
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
  private AtomicInteger hbaseChecksumOffCount = new AtomicInteger(-1);

  private final static ReadStatistics readStatistics = new ReadStatistics();

  private static class ReadStatistics {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * HFile path.
   */
  public final static int MIN_NUM_HFILE_PATH_LEVELS = 5;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * Maximum length of key in HFile.
   */
  public final static int MAXIMUM_KEY_LENGTH = Integer.MAX_VALUE;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java

  /** Default compression name: none. */
  public final static String DEFAULT_COMPRESSION = DEFAULT_COMPRESSION_ALGORITHM.getName();

  /** Meta data block name for bloom filter bits. */
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
   * Default compression: none.
   */
  public final static Compression.Algorithm DEFAULT_COMPRESSION_ALGORITHM =
    Compression.Algorithm.NONE;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
   * entry corresponding to the given key without having to deserialize the block.
   */
  static abstract class BlockIndexReader implements HeapSize {

    protected long[] blockOffsets;
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/AgeSnapshot.java`
#### Snippet
```java
public class AgeSnapshot {

  private transient final FastLongHistogram ageHistogram;
  private transient final long[] quantiles;

```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/AgeSnapshot.java`
#### Snippet
```java

  private transient final FastLongHistogram ageHistogram;
  private transient final long[] quantiles;

  AgeSnapshot(final FastLongHistogram ageHistogram) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
   * The size of a (key length, value length) tuple that prefixes each entry in a data block.
   */
  public final static int KEY_VALUE_LEN_SIZE = 2 * Bytes.SIZEOF_INT;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruCachedBlock.java`
#### Snippet
```java
public class LruCachedBlock implements HeapSize, Comparable<LruCachedBlock> {

  public final static long PER_BLOCK_OVERHEAD =
    ClassSize.align(ClassSize.OBJECT + (3 * ClassSize.REFERENCE) + (3 * Bytes.SIZEOF_LONG)
      + ClassSize.STRING + ClassSize.BYTE_BUFFER);
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
  private transient BlockCache victimCache;

  transient final Cache<BlockCacheKey, Cacheable> cache;

  /**
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
  private static final int STAT_THREAD_PERIOD_SECONDS = 5 * 60;

  private transient final Eviction<BlockCacheKey, Cacheable> policy;
  private transient final ScheduledExecutorService statsThreadPool;
  private final long maxBlockSize;
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java

  private transient final Eviction<BlockCacheKey, Cacheable> policy;
  private transient final ScheduledExecutorService statsThreadPool;
  private final long maxBlockSize;
  private final CacheStats stats;
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java

  /** Eviction lock (locked when eviction in process) */
  private transient final ReentrantLock evictionLock = new ReentrantLock(true);

  private final long maxBlockSize;
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
   * {@link java.util.concurrent.ConcurrentSkipListMap} can not guarantee that.
   */
  private transient final ConcurrentHashMap<BlockCacheKey, LruCachedBlock> map;

  /** Eviction lock (locked when eviction in process) */
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java

  /** Statistics thread schedule pool (for heavy debugging, could remove) */
  private transient final ScheduledExecutorService scheduleThreadPool =
    Executors.newScheduledThreadPool(1, new ThreadFactoryBuilder()
      .setNameFormat("LruBlockCacheStatsExecutor").setDaemon(true).build());
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java

  /** Eviction thread */
  private transient final EvictionThread evictionThread;

  /** Statistics thread schedule pool (for heavy debugging, could remove) */
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
  }

  public final static long CACHE_FIXED_OVERHEAD =
    ClassSize.estimateBase(LruBlockCache.class, false);

```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
   * Notice that the {@link java.util.concurrent.ConcurrentSkipListMap} can not guarantee that.
   */
  private transient final ConcurrentHashMap<BlockCacheKey, LruCachedBlock> map;

  /** Eviction lock (locked when eviction in process) */
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
  }

  public final static long CACHE_FIXED_OVERHEAD =
    ClassSize.estimateBase(LruAdaptiveBlockCache.class, false);

```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java

  /** Eviction lock (locked when eviction in process) */
  private transient final ReentrantLock evictionLock = new ReentrantLock(true);

  private final long maxBlockSize;
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java

  /** Statistics thread schedule pool (for heavy debugging, could remove) */
  private transient final ScheduledExecutorService scheduleThreadPool =
    Executors.newScheduledThreadPool(1, new ThreadFactoryBuilder()
      .setNameFormat("LruAdaptiveBlockCacheStatsExecutor").setDaemon(true).build());
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java

  /** Eviction thread */
  private transient final EvictionThread evictionThread;

  /** Statistics thread schedule pool (for heavy debugging, could remove) */
```

### MissortedModifiers
Missorted modifiers `static private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
   * Return the appropriate DUMMY_HEADER for the minor version
   */
  static private byte[] getDummyHeaderForVersion(boolean usesHBaseChecksum) {
    return usesHBaseChecksum ? HConstants.HFILEBLOCK_DUMMY_HEADER : DUMMY_HEADER_NO_CHECKSUM;
  }
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcScheduler.java`
#### Snippet
```java

  /** Exposes runtime information of a {@code RpcServer} that a {@code RpcScheduler} may need. */
  public static abstract class Context {
    public abstract InetSocketAddress getListenerAddress();
  }
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  public static final byte[] FAVOREDNODES_QUALIFIER = Bytes.toBytes("fn");
  public final static short FAVORED_NODES_NUM = 3;
  public final static short MAX_ATTEMPTS_FN_GENERATION = 10;

  public FavoredNodeAssignmentHelper(final List<ServerName> servers, Configuration conf) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
  private List<ServerName> servers;
  public static final byte[] FAVOREDNODES_QUALIFIER = Bytes.toBytes("fn");
  public final static short FAVORED_NODES_NUM = 3;
  public final static short MAX_ATTEMPTS_FN_GENERATION = 10;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcExecutor.java`
#### Snippet
```java
   */
  private static class CallPriorityComparator implements Comparator<CallRunner> {
    private final static int DEFAULT_MAX_CALL_DELAY = 5000;

    private final PriorityFunction priority;
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
  /** Volatile boolean to track if free space is in process or not */
  private volatile boolean freeInProgress = false;
  private transient final Lock freeSpaceLock = new ReentrantLock();

  private final LongAdder realCacheSize = new LongAdder();
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
   * then updates the ramCache and backingMap accordingly.
   */
  transient final ArrayList<BlockingQueue<RAMQueueEntry>> writerQueues = new ArrayList<>();
  transient final WriterThread[] writerThreads;

```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

  // Store the block in this map before writing it to cache
  transient final RAMCache ramCache;
  // In this map, store the block's meta data like offset, length
  transient ConcurrentHashMap<BlockCacheKey, BucketEntry> backingMap;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

  final static int DEFAULT_WRITER_THREADS = 3;
  final static int DEFAULT_WRITER_QUEUE_ITEMS = 64;

  // Store/read block data
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

  // Store/read block data
  transient final IOEngine ioEngine;

  // Store the block in this map before writing it to cache
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

  /** Statistics thread schedule pool (for heavy debugging, could remove) */
  private transient final ScheduledExecutorService scheduleThreadPool =
    Executors.newScheduledThreadPool(1,
      new ThreadFactoryBuilder().setNameFormat("BucketCacheStatsExecutor").setDaemon(true).build());
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
  private static final int statThreadPeriod = 5 * 60;

  final static int DEFAULT_WRITER_THREADS = 3;
  final static int DEFAULT_WRITER_QUEUE_ITEMS = 64;

```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
   * <p>
   */
  transient final IdReadWriteLock<Long> offsetLock;

  private final NavigableSet<BlockCacheKey> blocksByHFile = new ConcurrentSkipListSet<>((a, b) -> {
```

### MissortedModifiers
Missorted modifiers `transient final`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
   */
  transient final ArrayList<BlockingQueue<RAMQueueEntry>> writerQueues = new ArrayList<>();
  transient final WriterThread[] writerThreads;

  /** Volatile boolean to track if free space is in process or not */
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
   * So, what is the minimum amount of items we'll tolerate in a single bucket?
   */
  static public final int FEWEST_ITEMS_IN_BUCKET = 4;

  private final int[] bucketSizes;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(BucketAllocator.class);

  public final static class Bucket {
    private long baseOffset;
    private int itemAllocationSize, sizeIndex;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  }

  private static abstract class NamespaceProcedureBiConsumer extends ProcedureBiConsumer {
    protected final String namespaceName;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  }

  private static abstract class TableProcedureBiConsumer extends ProcedureBiConsumer {
    protected final TableName tableName;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
  }

  private static abstract class ProcedureBiConsumer implements BiConsumer<Void, Throwable> {

    abstract void onFinished();
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

  private class ConnectionManager {
    final private AtomicInteger count = new AtomicInteger();
    final private Set<SimpleServerRpcConnection> connections;

```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    final private Set<SimpleServerRpcConnection> connections;

    final private Timer idleScanTimer;
    final private int idleScanThreshold;
    final private int idleScanInterval;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  private class ConnectionManager {
    final private AtomicInteger count = new AtomicInteger();
    final private Set<SimpleServerRpcConnection> connections;

    final private Timer idleScanTimer;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    final private Timer idleScanTimer;
    final private int idleScanThreshold;
    final private int idleScanInterval;
    final private int maxIdleTime;
    final private int maxIdleToClose;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    final private int idleScanInterval;
    final private int maxIdleTime;
    final private int maxIdleToClose;

    ConnectionManager() {
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    final private int idleScanThreshold;
    final private int idleScanInterval;
    final private int maxIdleTime;
    final private int maxIdleToClose;

```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

    final private Timer idleScanTimer;
    final private int idleScanThreshold;
    final private int idleScanInterval;
    final private int maxIdleTime;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

    private class Reader implements Runnable {
      final private LinkedBlockingQueue<SimpleServerRpcConnection> pendingConnections;
      private final Selector readSelector;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobStoreEngine.java`
#### Snippet
```java
@InterfaceAudience.Private
public class MobStoreEngine extends DefaultStoreEngine {
  public final static String MOB_COMPACTOR_CLASS_KEY = "hbase.hstore.mobengine.compactor.class";

  @Override
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
  public final static String MOB_COMPACTION_TYPE_KEY = "hbase.mob.compaction.type";

  public final static String DEFAULT_MOB_COMPACTION_TYPE = FULL_MOB_COMPACTION_TYPE;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java

  public final static String BULKLOAD_DIR_NAME = ".bulkload";
  public final static byte[] MOB_TABLE_LOCK_SUFFIX = Bytes.toBytes(".mobLock");
  public final static String EMPTY_STRING = "";
  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
   */

  public final static String BULKLOAD_DIR_NAME = ".bulkload";
  public final static byte[] MOB_TABLE_LOCK_SUFFIX = Bytes.toBytes(".mobLock");
  public final static String EMPTY_STRING = "";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
  public final static String OPTIMIZED_MOB_COMPACTION_TYPE = "optimized";

  public final static String FULL_MOB_COMPACTION_TYPE = "full";

  public final static String MOB_COMPACTION_TYPE_KEY = "hbase.mob.compaction.type";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
   */

  public final static String OPTIMIZED_MOB_COMPACTION_TYPE = "optimized";

  public final static String FULL_MOB_COMPACTION_TYPE = "full";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
  public final static String BULKLOAD_DIR_NAME = ".bulkload";
  public final static byte[] MOB_TABLE_LOCK_SUFFIX = Bytes.toBytes(".mobLock");
  public final static String EMPTY_STRING = "";
  /**
   * If the size of a mob file is less than this value, it's regarded as a small file and needs to
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
  public static final long DEFAULT_MOB_CACHE_EVICT_PERIOD = 3600L;

  public final static String TEMP_DIR_NAME = ".tmp";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java`
#### Snippet
```java
  public final static String FULL_MOB_COMPACTION_TYPE = "full";

  public final static String MOB_COMPACTION_TYPE_KEY = "hbase.mob.compaction.type";

  public final static String DEFAULT_MOB_COMPACTION_TYPE = FULL_MOB_COMPACTION_TYPE;
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @return the number of open rpc connections
   */
  abstract public int getNumOpenConnections();

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
  public final static String SPLIT_WRITER_CREATION_BOUNDED = "hbase.split.writer.creation.bounded";

  public final static String SPLIT_WAL_BUFFER_SIZE = "hbase.regionserver.hlog.splitlog.buffersize";
  public final static String SPLIT_WAL_WRITER_THREADS =
    "hbase.regionserver.hlog.splitlog.writer.threads";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java

  public final static String SPLIT_WAL_BUFFER_SIZE = "hbase.regionserver.hlog.splitlog.buffersize";
  public final static String SPLIT_WAL_WRITER_THREADS =
    "hbase.regionserver.hlog.splitlog.writer.threads";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
   * Bounded writing tends to have higher throughput.
   */
  public final static String SPLIT_WRITER_CREATION_BOUNDED = "hbase.split.writer.creation.bounded";

  public final static String SPLIT_WAL_BUFFER_SIZE = "hbase.regionserver.hlog.splitlog.buffersize";
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
   * A Monitor super-class can be extended by users
   */
  public static abstract class Monitor implements Runnable, Closeable {
    protected Connection connection;
    protected Admin admin;
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CollectionBackedScanner.java`
#### Snippet
```java
@InterfaceAudience.Private
public class CollectionBackedScanner extends NonReversedNonLazyKeyValueScanner {
  final private Iterable<Cell> data;
  final CellComparator comparator;
  private Iterator<Cell> iter;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
   */
  public static class HexStringSplit extends NumberStringSplit {
    final static String DEFAULT_MIN_HEX = "00000000";
    final static String DEFAULT_MAX_HEX = "FFFFFFFF";
    final static int RADIX_HEX = 16;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
   */
  public static class DecimalStringSplit extends NumberStringSplit {
    final static String DEFAULT_MIN_DEC = "00000000";
    final static String DEFAULT_MAX_DEC = "99999999";
    final static int RADIX_DEC = 10;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
  public static class DecimalStringSplit extends NumberStringSplit {
    final static String DEFAULT_MIN_DEC = "00000000";
    final static String DEFAULT_MAX_DEC = "99999999";
    final static int RADIX_DEC = 10;

```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ShutdownHookManager.java`
#### Snippet
```java
  abstract public void addShutdownHook(Thread shutdownHook, int priority);

  abstract public boolean removeShutdownHook(Runnable shutdownHook);

  public static void affixShutdownHook(Thread shutdownHook, int priority) {
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ShutdownHookManager.java`
#### Snippet
```java
  }

  abstract public void addShutdownHook(Thread shutdownHook, int priority);

  abstract public boolean removeShutdownHook(Runnable shutdownHook);
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ShutdownHookManager.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
abstract public class ShutdownHookManager {
  private static ShutdownHookManager instance;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
  public static class HexStringSplit extends NumberStringSplit {
    final static String DEFAULT_MIN_HEX = "00000000";
    final static String DEFAULT_MAX_HEX = "FFFFFFFF";
    final static int RADIX_HEX = 16;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
    final static String DEFAULT_MIN_HEX = "00000000";
    final static String DEFAULT_MAX_HEX = "FFFFFFFF";
    final static int RADIX_HEX = 16;

    public HexStringSplit() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
    final static String DEFAULT_MIN_DEC = "00000000";
    final static String DEFAULT_MAX_DEC = "99999999";
    final static int RADIX_DEC = 10;

    public DecimalStringSplit() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  }

  final static Comparator<HbckRegionInfo> COMPARATOR = new Comparator<HbckRegionInfo>() {
    @Override
    public int compare(HbckRegionInfo l, HbckRegionInfo r) {
```

### MissortedModifiers
Missorted modifiers `static private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSRegionScanner.java`
#### Snippet
```java
@InterfaceAudience.Private
class FSRegionScanner implements Runnable {
  static private final Logger LOG = LoggerFactory.getLogger(FSRegionScanner.class);

  private Path regionPath;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java
   * SPECIAL CASE
   */
  private final static byte[] ENDKEY = null;

  public RegionSplitCalculator(Comparator<R> cmp) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java
  }

  public final static Comparator<byte[]> BYTES_COMPARATOR = new ByteArrayComparator() {
    @Override
    public int compare(byte[] l, byte[] r) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
  }

  private final static Boolean[] compressionTestResults =
    new Boolean[Compression.Algorithm.values().length];
  static {
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
abstract public class TableIntegrityErrorHandlerImpl implements TableIntegrityErrorHandler {
  HbckTableInfo ti;

```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * State
   *********/
  final private HbckErrorReporter errors;
  int fixes = 0;

```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
final public class FilterWrapper extends Filter {
  Filter filter = null;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
   * Wrapper to handle file operations uniformly
   */
  private static abstract class File {
    protected final FileSystem fs;

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
   * @param <T> type to adapt to the {@link File} interface
   */
  private static abstract class FileConverter<T> implements Function<T, File> {
    protected final FileSystem fs;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
   * message.
   */
  public final static int NB_SEND = 5;

  public ClusterStatusPublisher(HMaster master, Configuration conf,
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
   * reasonable size for ip / ethernet is less than 1Kb.
   */
  public final static int MAX_SERVER_PER_MESSAGE = 10;

  /**
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
    private static final class ClusterMetricsEncoder
      extends MessageToMessageEncoder<ClusterMetrics> {
      final private InetSocketAddress isa;

      private ClusterMetricsEncoder(InetSocketAddress isa) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
   * Filter *in* WAL files that are for the hbase:meta Region.
   */
  final static PathFilter META_FILTER = new PathFilter() {
    @Override
    public boolean accept(Path p) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
   * Filter *out* WAL files that are for the hbase:meta Region; i.e. return user-space WALs only.
   */
  public final static PathFilter NON_META_FILTER = new PathFilter() {
    @Override
    public boolean accept(Path p) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/migrate/RollingUpgradeChore.java`
#### Snippet
```java
  static final int CONCURRENT_PROCEDURES_COUNT = 5;

  private final static Logger LOG = LoggerFactory.getLogger(RollingUpgradeChore.class);
  ProcedureExecutor<MasterProcedureEnv> procedureExecutor;
  private TableDescriptors tableDescriptors;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java
   * nonce. See submitProcedure() for an example.
   */
  public static abstract class NonceProcedureRunnable {
    private final MasterServices master;
    private final NonceKey nonceKey;
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
   * @return The HDFS blocks distribution
   */
  static public HDFSBlocksDistribution computeHDFSBlocksDistribution(final FileSystem fs,
    FileStatus status, long start, long length) throws IOException {
    HDFSBlocksDistribution blocksDistribution = new HDFSBlocksDistribution();
```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
   * themselves.
   */
  static public HDFSBlocksDistribution
    computeHDFSBlocksDistribution(HdfsDataInputStream inputStream) throws IOException {
    List<LocatedBlock> blocks = inputStream.getAllBlocks();
```

### MissortedModifiers
Missorted modifiers `final public static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
  public static class RegionDirFilter extends AbstractFileStatusFilter {
    // This pattern will accept 0.90+ style hex region dirs and older numeric region dir names.
    final public static Pattern regionDirPattern = Pattern.compile("^[0-9a-f]*$");
    final FileSystem fs;

```

### MissortedModifiers
Missorted modifiers `static public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
   * @param blockLocations     an array containing block location
   */
  static public void addToHDFSBlocksDistribution(HDFSBlocksDistribution blocksDistribution,
    BlockLocation[] blockLocations) throws IOException {
    for (BlockLocation bl : blockLocations) {
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
  }

  public static abstract class RegionOperation extends RemoteOperation {
    protected final RegionInfo regionInfo;
    protected final long procId;
```

### MissortedModifiers
Missorted modifiers `final @MetaMutationAnnotation`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java`
#### Snippet
```java
   */
  public void preMergeRegionsCommit(final RegionInfo[] regionsToMerge,
    final @MetaMutationAnnotation List<Mutation> metaEntries, final User user) throws IOException {
    execOperation(coprocEnvironments.isEmpty() ? null : new MasterObserverOperation(user) {
      @Override
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  public final static String SMALL_HFILE_QUEUE_INIT_SIZE =
    "hbase.regionserver.hfilecleaner.small.queue.size";
  public final static int DEFAULT_SMALL_HFILE_QUEUE_INIT_SIZE = 10240;

  // Configuration key for large file delete thread number
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java

  // Configuration key for large/small throttle point
  public final static String HFILE_DELETE_THROTTLE_THRESHOLD =
    "hbase.regionserver.thread.hfilecleaner.throttle";
  public final static int DEFAULT_HFILE_DELETE_THROTTLE_THRESHOLD = 64 * 1024 * 1024;// 64M
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java

  // Configuration key for large queue initial size
  public final static String LARGE_HFILE_QUEUE_INIT_SIZE =
    "hbase.regionserver.hfilecleaner.large.queue.size";
  public final static int DEFAULT_LARGE_HFILE_QUEUE_INIT_SIZE = 10240;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  public final static String HFILE_DELETE_THROTTLE_THRESHOLD =
    "hbase.regionserver.thread.hfilecleaner.throttle";
  public final static int DEFAULT_HFILE_DELETE_THROTTLE_THRESHOLD = 64 * 1024 * 1024;// 64M

  // Configuration key for large queue initial size
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java

  // Configuration key for large file delete thread number
  public final static String LARGE_HFILE_DELETE_THREAD_NUMBER =
    "hbase.regionserver.hfilecleaner.large.thread.count";
  public final static int DEFAULT_LARGE_HFILE_DELETE_THREAD_NUMBER = 1;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  public final static String LARGE_HFILE_QUEUE_INIT_SIZE =
    "hbase.regionserver.hfilecleaner.large.queue.size";
  public final static int DEFAULT_LARGE_HFILE_QUEUE_INIT_SIZE = 10240;

  // Configuration key for small queue initial size
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java

  // Configuration key for small file delete thread number
  public final static String SMALL_HFILE_DELETE_THREAD_NUMBER =
    "hbase.regionserver.hfilecleaner.small.thread.count";
  public final static int DEFAULT_SMALL_HFILE_DELETE_THREAD_NUMBER = 1;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  public final static String SMALL_HFILE_DELETE_THREAD_NUMBER =
    "hbase.regionserver.hfilecleaner.small.thread.count";
  public final static int DEFAULT_SMALL_HFILE_DELETE_THREAD_NUMBER = 1;

  public static final String HFILE_DELETE_THREAD_TIMEOUT_MSEC =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  public final static String LARGE_HFILE_DELETE_THREAD_NUMBER =
    "hbase.regionserver.hfilecleaner.large.thread.count";
  public final static int DEFAULT_LARGE_HFILE_DELETE_THREAD_NUMBER = 1;

  // Configuration key for small file delete thread number
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java

  // Configuration key for small queue initial size
  public final static String SMALL_HFILE_QUEUE_INIT_SIZE =
    "hbase.regionserver.hfilecleaner.small.queue.size";
  public final static int DEFAULT_SMALL_HFILE_QUEUE_INIT_SIZE = 10240;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
  private final static AvlKeyComparator<PeerQueue> PEER_QUEUE_KEY_COMPARATOR =
    (n, k) -> n.compareKey((String) k);
  private final static AvlKeyComparator<MetaQueue> META_QUEUE_KEY_COMPARATOR =
    (n, k) -> n.compareKey((TableName) k);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
  private final static AvlKeyComparator<TableQueue> TABLE_QUEUE_KEY_COMPARATOR =
    (n, k) -> n.compareKey((TableName) k);
  private final static AvlKeyComparator<PeerQueue> PEER_QUEUE_KEY_COMPARATOR =
    (n, k) -> n.compareKey((String) k);
  private final static AvlKeyComparator<MetaQueue> META_QUEUE_KEY_COMPARATOR =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
  private static final AvlKeyComparator<ServerQueue> SERVER_QUEUE_KEY_COMPARATOR =
    (n, k) -> n.compareKey((ServerName) k);
  private final static AvlKeyComparator<TableQueue> TABLE_QUEUE_KEY_COMPARATOR =
    (n, k) -> n.compareKey((TableName) k);
  private final static AvlKeyComparator<PeerQueue> PEER_QUEUE_KEY_COMPARATOR =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  }

  public final static RegionStateStampComparator REGION_STATE_STAMP_COMPARATOR =
    new RegionStateStampComparator();

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/HBasePolicyProvider.java`
#### Snippet
```java
@InterfaceAudience.Private
public class HBasePolicyProvider extends PolicyProvider {
  protected final static Service[] services =
    { new Service("security.client.protocol.acl", ClientService.BlockingInterface.class),
      new Service("security.client.protocol.acl", AdminService.BlockingInterface.class),
```

### MissortedModifiers
Missorted modifiers `static private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
  }

  static private void removeTablePermissions(TableName tableName, byte[] column, Table table,
    boolean closeTable) throws IOException {
    Scan scan = new Scan();
```

### MissortedModifiers
Missorted modifiers `synchronized static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "MS_EXPOSE_REP",
      justification = "singleton pattern")
  public synchronized static VisibilityLabelsCache createAndGet(ZKWatcher watcher,
    Configuration conf) throws IOException {
    // VisibilityLabelService#init() for different regions (in same RS) passes same instance of
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ProcedureMember.class);

  final static long KEEP_ALIVE_MILLIS_DEFAULT = 5000;

  private final SubprocedureFactory builder;
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
   * created.
   */
  abstract public void cleanup(Exception e);

  /**
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
   * @return the data the subprocedure wants to return to coordinator side.
   */
  abstract public byte[] insideBarrier() throws ForeignException;

  /**
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java

  // Name of the procedure
  final private String barrierName;

  //
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
  @SuppressWarnings("finally")
  @Override
  final public Void call() {
    LOG.debug("Starting subprocedure '" + barrierName + "' with timeout "
      + executionTimeoutTimer.getMaxTime() + "ms");
```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
abstract public class Subprocedure implements Callable<Void> {
  private static final Logger LOG = LoggerFactory.getLogger(Subprocedure.class);

```

### MissortedModifiers
Missorted modifiers `abstract public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
   * but can still be used to execute a procedure on all members and to propagate any exceptions.
   */
  abstract public void acquireBarrier() throws ForeignException;

  /**
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java`
#### Snippet
```java

  // Name of the procedure
  final private String procName;
  // Arguments for this procedure execution
  final private byte[] args;
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java`
#### Snippet
```java
  @Override
  @SuppressWarnings("finally")
  final public Void call() {
    LOG.info("Starting procedure '" + procName + "'");
    // start the timer
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java`
#### Snippet
```java
  final private String procName;
  // Arguments for this procedure execution
  final private byte[] args;

  //
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java`
#### Snippet
```java
  final static long KEEP_ALIVE_MILLIS_DEFAULT = 5000;
  final static long TIMEOUT_MILLIS_DEFAULT = 60000;
  final static long WAKE_MILLIS_DEFAULT = 500;

  private final ProcedureCoordinatorRpcs rpcs;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ProcedureCoordinator.class);

  final static long KEEP_ALIVE_MILLIS_DEFAULT = 5000;
  final static long TIMEOUT_MILLIS_DEFAULT = 60000;
  final static long WAKE_MILLIS_DEFAULT = 500;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java`
#### Snippet
```java

  final static long KEEP_ALIVE_MILLIS_DEFAULT = 5000;
  final static long TIMEOUT_MILLIS_DEFAULT = 60000;
  final static long WAKE_MILLIS_DEFAULT = 500;

```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
   */
  @Override
  final public void sendGlobalBarrierAcquire(Procedure proc, byte[] info, List<String> nodeNames)
    throws IOException, IllegalArgumentException {
    String procName = proc.getName();
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
   */
  @Override
  final public boolean start(final ProcedureCoordinator coordinator) {
    if (this.coordinator != null) {
      throw new IllegalStateException(
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java

  @Override
  final public void close() throws IOException {
    zkProc.close();
  }
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
   */
  @Override
  final public void resetMembers(Procedure proc) throws IOException {
    String procName = proc.getName();
    boolean stillGettingNotifications = false;
```

### MissortedModifiers
Missorted modifiers `final public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
   */
  @Override
  final public void sendAbortToMembers(Procedure proc, ForeignException ee) {
    String procName = proc.getName();
    LOG.debug("Aborting procedure '" + procName + "' in zk");
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination.java`
#### Snippet
```java
  class SplitLogManagerDetails {
    final private ConcurrentMap<String, Task> tasks;
    final private MasterServices master;
    final private Set<String> failedDeletions;

```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination.java`
#### Snippet
```java
    final private ConcurrentMap<String, Task> tasks;
    final private MasterServices master;
    final private Set<String> failedDeletions;

    public SplitLogManagerDetails(ConcurrentMap<String, Task> tasks, MasterServices master,
```

### MissortedModifiers
Missorted modifiers `final private`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination.java`
#### Snippet
```java
   */
  class SplitLogManagerDetails {
    final private ConcurrentMap<String, Task> tasks;
    final private MasterServices master;
    final private Set<String> failedDeletions;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java`
#### Snippet
```java
   * regionserver references in values of the map.
   */
  private final static Map<Runnable, Integer> fsShutdownHooks = new HashMap<>();

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HFileReplicator.class);
  private static final String UNDERSCORE = "_";
  private final static FsPermission PERM_ALL_ACCESS = FsPermission.valueOf("-rwxrwxrwx");

  private Configuration sourceClusterConf;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java
  // @formatter:on

  public final static long DEEP_OVERHEAD = FIXED_OVERHEAD;

  public static void addToScanners(List<? extends Segment> segments, long readPt,
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AbstractMemStore.java`
#### Snippet
```java

  // @formatter:off
  public final static long FIXED_OVERHEAD = (long) ClassSize.OBJECT
    + (5 * ClassSize.REFERENCE)
    + (2 * Bytes.SIZEOF_LONG); // snapshotId, timeOfOldestEdit
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MutableSegment.java`
#### Snippet
```java
  private final AtomicBoolean flushed = new AtomicBoolean(false);

  public final static long DEEP_OVERHEAD =
    ClassSize.align(Segment.DEEP_OVERHEAD + ClassSize.CONCURRENT_SKIPLISTMAP
      + ClassSize.SYNC_TIMERANGE_TRACKER + ClassSize.REFERENCE + ClassSize.ATOMIC_BOOLEAN);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java`
#### Snippet
```java
public abstract class Segment implements MemStoreSizing {

  public final static long FIXED_OVERHEAD =
    ClassSize.align(ClassSize.OBJECT + 6 * ClassSize.REFERENCE // cellSet, comparator, updatesLock,
                                                               // memStoreLAB, memStoreSizing,
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Segment.java`
#### Snippet
```java
      + Bytes.SIZEOF_LONG // minSequenceId
      + Bytes.SIZEOF_BOOLEAN); // tagsPresent
  public final static long DEEP_OVERHEAD = FIXED_OVERHEAD + ClassSize.ATOMIC_REFERENCE
    + ClassSize.CELL_SET + 2 * ClassSize.ATOMIC_LONG + ClassSize.REENTRANT_LOCK;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java

  /** Name of the region info file that resides just under the region directory. */
  public final static String REGION_INFO_FILE = ".regioninfo";

  /** Temporary subdirectory of the region directory used for merges. */
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java

  // Configuration key for split threads
  public final static String SPLIT_THREADS = "hbase.regionserver.thread.split";
  public final static int SPLIT_THREADS_DEFAULT = 1;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java

  // Configuration key for the large compaction threads.
  public final static String LARGE_COMPACTION_THREADS =
    "hbase.regionserver.thread.compaction.large";
  public final static int LARGE_COMPACTION_THREADS_DEFAULT = 1;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
  public final static String SMALL_COMPACTION_THREADS =
    "hbase.regionserver.thread.compaction.small";
  public final static int SMALL_COMPACTION_THREADS_DEFAULT = 1;

  // Configuration key for split threads
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java

  // Configuration key for the small compaction threads.
  public final static String SMALL_COMPACTION_THREADS =
    "hbase.regionserver.thread.compaction.small";
  public final static int SMALL_COMPACTION_THREADS_DEFAULT = 1;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
  public final static String LARGE_COMPACTION_THREADS =
    "hbase.regionserver.thread.compaction.large";
  public final static int LARGE_COMPACTION_THREADS_DEFAULT = 1;

  // Configuration key for the small compaction threads.
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
  // Configuration key for split threads
  public final static String SPLIT_THREADS = "hbase.regionserver.thread.split";
  public final static int SPLIT_THREADS_DEFAULT = 1;

  public static final String REGION_SERVER_REGION_SPLIT_LIMIT =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java`
#### Snippet
```java

  public final static long DEEP_OVERHEAD = ClassSize.align(AbstractMemStore.DEEP_OVERHEAD);
  public final static long FIXED_OVERHEAD = ClassSize.align(AbstractMemStore.FIXED_OVERHEAD);

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(DefaultMemStore.class);

  public final static long DEEP_OVERHEAD = ClassSize.align(AbstractMemStore.DEEP_OVERHEAD);
  public final static long FIXED_OVERHEAD = ClassSize.align(AbstractMemStore.FIXED_OVERHEAD);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SecureBulkLoadManager.class);

  private final static FsPermission PERM_ALL_ACCESS = FsPermission.valueOf("-rwxrwxrwx");
  private final static FsPermission PERM_HIDDEN = FsPermission.valueOf("-rwx--x--x");
  private SecureRandom random;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java

  private final static FsPermission PERM_ALL_ACCESS = FsPermission.valueOf("-rwxrwxrwx");
  private final static FsPermission PERM_HIDDEN = FsPermission.valueOf("-rwx--x--x");
  private SecureRandom random;
  private FileSystem fs;
```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java

    @Override
    synchronized public void run() {
      try {
        HDFSBlocksDistribution hdfsBlocksDistribution = new HDFSBlocksDistribution();
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java`
#### Snippet
```java
  public final static long FIXED_OVERHEAD =
    ClassSize.align(ClassSize.OBJECT + (3 * ClassSize.REFERENCE) + Bytes.SIZEOF_LONG);
  public final static long DEEP_OVERHEAD = FIXED_OVERHEAD + (2 * ClassSize.LINKEDLIST);

  private final RegionServicesForStores region;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(CompactionPipeline.class);

  public final static long FIXED_OVERHEAD =
    ClassSize.align(ClassSize.OBJECT + (3 * ClassSize.REFERENCE) + Bytes.SIZEOF_LONG);
  public final static long DEEP_OVERHEAD = FIXED_OVERHEAD + (2 * ClassSize.LINKEDLIST);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   * The key value used for range boundary, indicating that the boundary is open (i.e. +-inf).
   */
  public final static byte[] OPEN_KEY = HConstants.EMPTY_BYTE_ARRAY;
  final static byte[] INVALID_KEY = null;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  public static final byte[] STRIPE_END_KEY = Bytes.toBytes("STRIPE_END_KEY");

  private final static Bytes.RowEndKeyComparator MAP_COMPARATOR = new Bytes.RowEndKeyComparator();

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   */
  public final static byte[] OPEN_KEY = HConstants.EMPTY_BYTE_ARRAY;
  final static byte[] INVALID_KEY = null;

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    private final HRegionServer instance;
    private final int majorCompactPriority;
    private final static int DEFAULT_PRIORITY = Integer.MAX_VALUE;
    // Iteration is 1-based rather than 0-based so we don't check for compaction
    // immediately upon region server startup
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  @Deprecated
  @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
  final static String RS_HOSTNAME_DISABLE_MASTER_REVERSEDNS_KEY =
    "hbase.regionserver.hostname.disable.master.reversedns";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
   */
  @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
  final static String UNSAFE_RS_HOSTNAME_DISABLE_MASTER_REVERSEDNS_KEY =
    "hbase.unsafe.regionserver.hostname.disable.master.reversedns";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  private static class PeriodicMemStoreFlusher extends ScheduledChore {
    private final HRegionServer server;
    private final static int RANGE_OF_DELAY = 5 * 60; // 5 min in seconds
    private final static int MIN_DELAY_TIME = 0; // millisec
    private final long rangeOfDelayMs;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    private final HRegionServer server;
    private final static int RANGE_OF_DELAY = 5 * 60; // 5 min in seconds
    private final static int MIN_DELAY_TIME = 0; // millisec
    private final long rangeOfDelayMs;

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java`
#### Snippet
```java

  // the maximum number of attempts we flush
  final static int MAX_RETRIES = 3;

  public FlushSnapshotSubprocedure(ProcedureMember member, ForeignExceptionDispatcher errorListener,
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String PUT_BATCH_TIME = "putBatchTime";
  private final static String DELETE_TIME = "deleteTime";
  private final static String DELETE_BATCH_TIME = "deleteBatchTime";
  private final static String INCREMENT_TIME = "incrementTime";
  private final static String APPEND_TIME = "appendTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String DELETE_TIME = "deleteTime";
  private final static String DELETE_BATCH_TIME = "deleteBatchTime";
  private final static String INCREMENT_TIME = "incrementTime";
  private final static String APPEND_TIME = "appendTime";
  private final static String CHECK_AND_DELETE_TIME = "checkAndDeleteTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
   * Description
   */
  private final static String METRICS_DESCRIPTION =
    "Metrics about Tables on a single HBase RegionServer";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
   * The name of the metrics context that metrics will be under.
   */
  private final static String METRICS_CONTEXT = "regionserver";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String CHECK_AND_MUTATE_TIME = "checkAndMutateTime";
  private final static String TABLE_READ_QUERY_PER_SECOND = "tableReadQueryPerSecond";
  private final static String TABLE_WRITE_QUERY_PER_SECOND = "tableWriteQueryPerSecond";

  private Histogram getTimeHistogram;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String CHECK_AND_PUT_TIME = "checkAndPutTime";
  private final static String CHECK_AND_MUTATE_TIME = "checkAndMutateTime";
  private final static String TABLE_READ_QUERY_PER_SECOND = "tableReadQueryPerSecond";
  private final static String TABLE_WRITE_QUERY_PER_SECOND = "tableWriteQueryPerSecond";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String GET_TIME = "getTime";
  private final static String SCAN_TIME = "scanTime";
  private final static String SCAN_SIZE = "scanSize";
  private final static String PUT_TIME = "putTime";
  private final static String PUT_BATCH_TIME = "putBatchTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
   * The name of the metrics
   */
  private final static String METRICS_NAME = "TableRequests";

  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String DELETE_BATCH_TIME = "deleteBatchTime";
  private final static String INCREMENT_TIME = "incrementTime";
  private final static String APPEND_TIME = "appendTime";
  private final static String CHECK_AND_DELETE_TIME = "checkAndDeleteTime";
  private final static String CHECK_AND_PUT_TIME = "checkAndPutTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String SCAN_SIZE = "scanSize";
  private final static String PUT_TIME = "putTime";
  private final static String PUT_BATCH_TIME = "putBatchTime";
  private final static String DELETE_TIME = "deleteTime";
  private final static String DELETE_BATCH_TIME = "deleteBatchTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java

  private final static String GET_TIME = "getTime";
  private final static String SCAN_TIME = "scanTime";
  private final static String SCAN_SIZE = "scanSize";
  private final static String PUT_TIME = "putTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String APPEND_TIME = "appendTime";
  private final static String CHECK_AND_DELETE_TIME = "checkAndDeleteTime";
  private final static String CHECK_AND_PUT_TIME = "checkAndPutTime";
  private final static String CHECK_AND_MUTATE_TIME = "checkAndMutateTime";
  private final static String TABLE_READ_QUERY_PER_SECOND = "tableReadQueryPerSecond";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String SCAN_TIME = "scanTime";
  private final static String SCAN_SIZE = "scanSize";
  private final static String PUT_TIME = "putTime";
  private final static String PUT_BATCH_TIME = "putBatchTime";
  private final static String DELETE_TIME = "deleteTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String METRICS_JMX_CONTEXT = "RegionServer,sub=" + METRICS_NAME;

  private final static String GET_TIME = "getTime";
  private final static String SCAN_TIME = "scanTime";
  private final static String SCAN_SIZE = "scanSize";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String PUT_TIME = "putTime";
  private final static String PUT_BATCH_TIME = "putBatchTime";
  private final static String DELETE_TIME = "deleteTime";
  private final static String DELETE_BATCH_TIME = "deleteBatchTime";
  private final static String INCREMENT_TIME = "incrementTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String INCREMENT_TIME = "incrementTime";
  private final static String APPEND_TIME = "appendTime";
  private final static String CHECK_AND_DELETE_TIME = "checkAndDeleteTime";
  private final static String CHECK_AND_PUT_TIME = "checkAndPutTime";
  private final static String CHECK_AND_MUTATE_TIME = "checkAndMutateTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
   * The name of the metrics context that metrics will be under in jmx
   */
  private final static String METRICS_JMX_CONTEXT = "RegionServer,sub=" + METRICS_NAME;

  private final static String GET_TIME = "getTime";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/MetricsTableRequests.java`
#### Snippet
```java
  private final static String CHECK_AND_DELETE_TIME = "checkAndDeleteTime";
  private final static String CHECK_AND_PUT_TIME = "checkAndPutTime";
  private final static String CHECK_AND_MUTATE_TIME = "checkAndMutateTime";
  private final static String TABLE_READ_QUERY_PER_SECOND = "tableReadQueryPerSecond";
  private final static String TABLE_WRITE_QUERY_PER_SECOND = "tableWriteQueryPerSecond";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareCompactionThroughputController.java`
#### Snippet
```java
public class PressureAwareCompactionThroughputController extends PressureAwareThroughputController {

  private final static Logger LOG =
    LoggerFactory.getLogger(PressureAwareCompactionThroughputController.class);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
  public final static String PARALLEL_PUT_STORE_THREADS_LIMIT =
    "hbase.region.store.parallel.put.limit";
  public final static String PARALLEL_PREPARE_PUT_STORE_MULTIPLIER =
    "hbase.region.store.parallel.prepare.put.multiplier";
  private final static int DEFAULT_PARALLEL_PUT_STORE_THREADS_LIMIT = 0;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
  private final static int DEFAULT_PARALLEL_PUT_STORE_THREADS_LIMIT = 0;
  private volatile int parallelPutToStoreThreadLimitCheckMinColumnCount;
  public final static String PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_COUNT =
    "hbase.region.store.parallel.put.limit.min.column.count";
  private final static int DEFAULT_PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_NUM = 100;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java

  private volatile int parallelPreparePutToStoreThreadLimit;
  public final static String PARALLEL_PUT_STORE_THREADS_LIMIT =
    "hbase.region.store.parallel.put.limit";
  public final static String PARALLEL_PREPARE_PUT_STORE_MULTIPLIER =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
    "hbase.region.store.parallel.put.limit.min.column.count";
  private final static int DEFAULT_PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_NUM = 100;
  private final static int DEFAULT_PARALLEL_PREPARE_PUT_STORE_MULTIPLIER = 2;

  private final ConcurrentMap<byte[], AtomicInteger> preparePutToStoreMap =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
  public final static String PARALLEL_PREPARE_PUT_STORE_MULTIPLIER =
    "hbase.region.store.parallel.prepare.put.multiplier";
  private final static int DEFAULT_PARALLEL_PUT_STORE_THREADS_LIMIT = 0;
  private volatile int parallelPutToStoreThreadLimitCheckMinColumnCount;
  public final static String PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_COUNT =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java`
#### Snippet
```java
  public final static String PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_COUNT =
    "hbase.region.store.parallel.put.limit.min.column.count";
  private final static int DEFAULT_PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_NUM = 100;
  private final static int DEFAULT_PARALLEL_PREPARE_PUT_STORE_MULTIPLIER = 2;

```

### MissortedModifiers
Missorted modifiers `synchronized public`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java`
#### Snippet
```java

  @Override
  synchronized public ForeignException getException() {
    return exception;
  }
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupRestoreFactory.java`
#### Snippet
```java
  public final static String HBASE_INCR_RESTORE_IMPL_CLASS = "hbase.incremental.restore.class";
  public final static String HBASE_BACKUP_COPY_IMPL_CLASS = "hbase.backup.copy.class";
  public final static String HBASE_BACKUP_MERGE_IMPL_CLASS = "hbase.backup.merge.class";

  private BackupRestoreFactory() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupRestoreFactory.java`
#### Snippet
```java
public final class BackupRestoreFactory {
  public final static String HBASE_INCR_RESTORE_IMPL_CLASS = "hbase.incremental.restore.class";
  public final static String HBASE_BACKUP_COPY_IMPL_CLASS = "hbase.backup.copy.class";
  public final static String HBASE_BACKUP_MERGE_IMPL_CLASS = "hbase.backup.merge.class";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupRestoreFactory.java`
#### Snippet
```java
@InterfaceAudience.Private
public final class BackupRestoreFactory {
  public final static String HBASE_INCR_RESTORE_IMPL_CLASS = "hbase.incremental.restore.class";
  public final static String HBASE_BACKUP_COPY_IMPL_CLASS = "hbase.backup.copy.class";
  public final static String HBASE_BACKUP_MERGE_IMPL_CLASS = "hbase.backup.merge.class";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
@InterfaceAudience.Private
public class StripeCompactionPolicy extends CompactionPolicy {
  private final static Logger LOG = LoggerFactory.getLogger(StripeCompactionPolicy.class);
  // Policy used to compact individual stripes.
  private ExploringCompactionPolicy stripePolicy = null;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
public class BackupManager implements Closeable {
  // in seconds
  public final static String BACKUP_EXCLUSIVE_OPERATION_TIMEOUT_SECONDS_KEY =
    "hbase.backup.exclusive.op.timeout.seconds";
  // In seconds
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
    "hbase.backup.exclusive.op.timeout.seconds";
  // In seconds
  private final static int DEFAULT_BACKUP_EXCLUSIVE_OPERATION_TIMEOUT = 3600;
  private static final Logger LOG = LoggerFactory.getLogger(BackupManager.class);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

  public static class BackupSetCommand extends Command {
    private final static String SET_ADD_CMD = "add";
    private final static String SET_REMOVE_CMD = "remove";
    private final static String SET_DELETE_CMD = "delete";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    private final static String SET_REMOVE_CMD = "remove";
    private final static String SET_DELETE_CMD = "delete";
    private final static String SET_DESCRIBE_CMD = "describe";
    private final static String SET_LIST_CMD = "list";

```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
  public static final String USAGE_FOOTER = "";

  public static abstract class Command extends Configured {
    CommandLine cmdline;
    Connection conn;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    private final static String SET_ADD_CMD = "add";
    private final static String SET_REMOVE_CMD = "remove";
    private final static String SET_DELETE_CMD = "delete";
    private final static String SET_DESCRIBE_CMD = "describe";
    private final static String SET_LIST_CMD = "list";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
  public final static String INCORRECT_USAGE = "Incorrect usage";

  public final static String TOP_LEVEL_NOT_ALLOWED =
    "Top level (root) folder is not allowed to be a backup destination";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
  public static class BackupSetCommand extends Command {
    private final static String SET_ADD_CMD = "add";
    private final static String SET_REMOVE_CMD = "remove";
    private final static String SET_DELETE_CMD = "delete";
    private final static String SET_DESCRIBE_CMD = "describe";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java

  public static class HistoryCommand extends Command {
    private final static int DEFAULT_HISTORY_LENGTH = 10;

    HistoryCommand(Configuration conf, CommandLine cmdline) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
    private final static String SET_DELETE_CMD = "delete";
    private final static String SET_DESCRIBE_CMD = "describe";
    private final static String SET_LIST_CMD = "list";

    BackupSetCommand(Configuration conf, CommandLine cmdline) {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
@InterfaceAudience.Private
public final class BackupCommands {
  public final static String INCORRECT_USAGE = "Incorrect usage";

  public final static String TOP_LEVEL_NOT_ALLOWED =
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
  final static String NAME = "HFileSplitterJob";
  public final static String BULK_OUTPUT_CONF_KEY = "hfile.bulk.output";
  public final static String TABLES_KEY = "hfile.input.tables";
  public final static String TABLE_MAP_KEY = "hfile.input.tablesmap";
  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
public class MapReduceHFileSplitterJob extends Configured implements Tool {
  private static final Logger LOG = LoggerFactory.getLogger(MapReduceHFileSplitterJob.class);
  final static String NAME = "HFileSplitterJob";
  public final static String BULK_OUTPUT_CONF_KEY = "hfile.bulk.output";
  public final static String TABLES_KEY = "hfile.input.tables";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
  public final static String BULK_OUTPUT_CONF_KEY = "hfile.bulk.output";
  public final static String TABLES_KEY = "hfile.input.tables";
  public final static String TABLE_MAP_KEY = "hfile.input.tablesmap";
  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
  public final static String TABLES_KEY = "hfile.input.tables";
  public final static String TABLE_MAP_KEY = "hfile.input.tablesmap";
  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";

  public MapReduceHFileSplitterJob() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MapReduceHFileSplitterJob.class);
  final static String NAME = "HFileSplitterJob";
  public final static String BULK_OUTPUT_CONF_KEY = "hfile.bulk.output";
  public final static String TABLES_KEY = "hfile.input.tables";
  public final static String TABLE_MAP_KEY = "hfile.input.tablesmap";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
@InterfaceAudience.Private
public class BackupAdminImpl implements BackupAdmin {
  public final static String CHECK_OK = "Checking backup images: OK";
  public final static String CHECK_FAILED =
    "Checking backup images: Failed. Some dependencies are missing for restore";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
public class BackupAdminImpl implements BackupAdmin {
  public final static String CHECK_OK = "Checking backup images: OK";
  public final static String CHECK_FAILED =
    "Checking backup images: Failed. Some dependencies are missing for restore";
  private static final Logger LOG = LoggerFactory.getLogger(BackupAdminImpl.class);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
public class RestoreTool {
  public static final Logger LOG = LoggerFactory.getLogger(BackupUtils.class);
  private final static long TABLE_AVAILABILITY_WAIT_TIME = 180000;

  private final String[] ignoreDirs = { HConstants.RECOVERED_EDITS_DIR };
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static byte[] MERGE_OP_ROW = Bytes.toBytes("merge_op_row");

  final static byte[] TBL_COL = Bytes.toBytes("tbl");
  final static byte[] FAM_COL = Bytes.toBytes("fam");
  final static byte[] PATH_COL = Bytes.toBytes("path");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static String BULK_LOAD_PREFIX = "bulk:";
  private final static byte[] BULK_LOAD_PREFIX_BYTES = Bytes.toBytes(BULK_LOAD_PREFIX);
  private final static byte[] DELETE_OP_ROW = Bytes.toBytes("delete_op_row");
  private final static byte[] MERGE_OP_ROW = Bytes.toBytes("merge_op_row");

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static byte[] ACTIVE_SESSION_COL = Bytes.toBytes("c");

  private final static byte[] ACTIVE_SESSION_YES = Bytes.toBytes("yes");
  private final static byte[] ACTIVE_SESSION_NO = Bytes.toBytes("no");

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static String START_CODE_ROW = "startcode:";
  private final static byte[] ACTIVE_SESSION_ROW = Bytes.toBytes("activesession:");
  private final static byte[] ACTIVE_SESSION_COL = Bytes.toBytes("c");

  private final static byte[] ACTIVE_SESSION_YES = Bytes.toBytes("yes");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  private final static String BULK_LOAD_PREFIX = "bulk:";
  private final static byte[] BULK_LOAD_PREFIX_BYTES = Bytes.toBytes(BULK_LOAD_PREFIX);
  private final static byte[] DELETE_OP_ROW = Bytes.toBytes("delete_op_row");
  private final static byte[] MERGE_OP_ROW = Bytes.toBytes("merge_op_row");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  private final static String BACKUP_INFO_PREFIX = "session:";
  private final static String START_CODE_ROW = "startcode:";
  private final static byte[] ACTIVE_SESSION_ROW = Bytes.toBytes("activesession:");
  private final static byte[] ACTIVE_SESSION_COL = Bytes.toBytes("c");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  final static byte[] TBL_COL = Bytes.toBytes("tbl");
  final static byte[] FAM_COL = Bytes.toBytes("fam");
  final static byte[] PATH_COL = Bytes.toBytes("path");
  final static byte[] STATE_COL = Bytes.toBytes("state");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static String INCR_BACKUP_SET = "incrbackupset:";
  private final static String TABLE_RS_LOG_MAP_PREFIX = "trslm:";
  private final static String RS_LOG_TS_PREFIX = "rslogts:";

  private final static String BULK_LOAD_PREFIX = "bulk:";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  // Safe delimiter in a string
  private final static String NULL = "\u0000";

  public BackupSystemTable(Connection conn) throws IOException {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  private final static byte[] ACTIVE_SESSION_YES = Bytes.toBytes("yes");
  private final static byte[] ACTIVE_SESSION_NO = Bytes.toBytes("no");

  private final static String INCR_BACKUP_SET = "incrbackupset:";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  final static byte[] BL_COMMIT = Bytes.toBytes("D");

  private final static String SET_KEY_PREFIX = "backupset:";

  // separator between BULK_LOAD_PREFIX and ordinals
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
   */
  final static byte[] META_FAMILY = Bytes.toBytes("meta");
  final static byte[] BULK_LOAD_FAMILY = Bytes.toBytes("bulk");
  /**
   * Connection to HBase cluster, shared among all instances
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  final static byte[] FAM_COL = Bytes.toBytes("fam");
  final static byte[] PATH_COL = Bytes.toBytes("path");
  final static byte[] STATE_COL = Bytes.toBytes("state");
  // the two states a bulk loaded file can be
  final static byte[] BL_PREPARE = Bytes.toBytes("R");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  final static byte[] STATE_COL = Bytes.toBytes("state");
  // the two states a bulk loaded file can be
  final static byte[] BL_PREPARE = Bytes.toBytes("R");
  final static byte[] BL_COMMIT = Bytes.toBytes("D");

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  final static byte[] TBL_COL = Bytes.toBytes("tbl");
  final static byte[] FAM_COL = Bytes.toBytes("fam");
  final static byte[] PATH_COL = Bytes.toBytes("path");
  final static byte[] STATE_COL = Bytes.toBytes("state");
  // the two states a bulk loaded file can be
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  // separator between BULK_LOAD_PREFIX and ordinals
  private final static String BLK_LD_DELIM = ":";
  private final static byte[] EMPTY_VALUE = new byte[] {};

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  // the two states a bulk loaded file can be
  final static byte[] BL_PREPARE = Bytes.toBytes("R");
  final static byte[] BL_COMMIT = Bytes.toBytes("D");

  private final static String SET_KEY_PREFIX = "backupset:";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
   * Stores backup sessions (contexts)
   */
  final static byte[] SESSIONS_FAMILY = Bytes.toBytes("session");
  /**
   * Stores other meta
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final Connection connection;

  private final static String BACKUP_INFO_PREFIX = "session:";
  private final static String START_CODE_ROW = "startcode:";
  private final static byte[] ACTIVE_SESSION_ROW = Bytes.toBytes("activesession:");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static String RS_LOG_TS_PREFIX = "rslogts:";

  private final static String BULK_LOAD_PREFIX = "bulk:";
  private final static byte[] BULK_LOAD_PREFIX_BYTES = Bytes.toBytes(BULK_LOAD_PREFIX);
  private final static byte[] DELETE_OP_ROW = Bytes.toBytes("delete_op_row");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static String BACKUP_INFO_PREFIX = "session:";
  private final static String START_CODE_ROW = "startcode:";
  private final static byte[] ACTIVE_SESSION_ROW = Bytes.toBytes("activesession:");
  private final static byte[] ACTIVE_SESSION_COL = Bytes.toBytes("c");

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
   * Stores other meta
   */
  final static byte[] META_FAMILY = Bytes.toBytes("meta");
  final static byte[] BULK_LOAD_FAMILY = Bytes.toBytes("bulk");
  /**
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  // separator between BULK_LOAD_PREFIX and ordinals
  private final static String BLK_LD_DELIM = ":";
  private final static byte[] EMPTY_VALUE = new byte[] {};

  // Safe delimiter in a string
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java

  private final static String INCR_BACKUP_SET = "incrbackupset:";
  private final static String TABLE_RS_LOG_MAP_PREFIX = "trslm:";
  private final static String RS_LOG_TS_PREFIX = "rslogts:";

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static byte[] ACTIVE_SESSION_NO = Bytes.toBytes("no");

  private final static String INCR_BACKUP_SET = "incrbackupset:";
  private final static String TABLE_RS_LOG_MAP_PREFIX = "trslm:";
  private final static String RS_LOG_TS_PREFIX = "rslogts:";
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private final static byte[] BULK_LOAD_PREFIX_BYTES = Bytes.toBytes(BULK_LOAD_PREFIX);
  private final static byte[] DELETE_OP_ROW = Bytes.toBytes("delete_op_row");
  private final static byte[] MERGE_OP_ROW = Bytes.toBytes("merge_op_row");

  final static byte[] TBL_COL = Bytes.toBytes("tbl");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java

  private final static Cell[] EMPTY_CELL_ARRAY = new Cell[] {};
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);

```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
public final class ThriftUtilities {

  private final static Cell[] EMPTY_CELL_ARRAY = new Cell[] {};
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
  private final static Cell[] EMPTY_CELL_ARRAY = new Cell[] {};
  private final static Result EMPTY_RESULT = Result.create(EMPTY_CELL_ARRAY);
  private final static Result EMPTY_RESULT_STALE = Result.create(EMPTY_CELL_ARRAY, null, true);

  private ThriftUtilities() {
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java
  }

  public final static class Response {
    private final long rowCount;
    private final long cellCount;
```

### MissortedModifiers
Missorted modifiers `static abstract`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java
  }

  private static abstract class AbstractAggregationCallback<T>
    implements CoprocessorCallback<AggregateResponse> {
    private final CompletableFuture<T> future;
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  // Canned table and table descriptor creation

  public final static byte[] fam1 = Bytes.toBytes("colfamily11");
  public final static byte[] fam2 = Bytes.toBytes("colfamily21");
  public final static byte[] fam3 = Bytes.toBytes("colfamily31");
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java

  public final static byte[] fam1 = Bytes.toBytes("colfamily11");
  public final static byte[] fam2 = Bytes.toBytes("colfamily21");
  public final static byte[] fam3 = Bytes.toBytes("colfamily31");
  public static final byte[][] COLUMNS = { fam1, fam2, fam3 };
```

### MissortedModifiers
Missorted modifiers `final static`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  public final static byte[] fam1 = Bytes.toBytes("colfamily11");
  public final static byte[] fam2 = Bytes.toBytes("colfamily21");
  public final static byte[] fam3 = Bytes.toBytes("colfamily31");
  public static final byte[][] COLUMNS = { fam1, fam2, fam3 };
  private static final int MAXVERSIONS = 3;
```

## RuleId[id=AnonymousHasLambdaAlternative]
### AnonymousHasLambdaAlternative
Anonymous new ThreadLocal\>() can be replaced with ThreadLocal.withInitial()
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java`
#### Snippet
```java
  private HMobStore mobStore;
  // MOB file reference set
  static ThreadLocal<Set<String>> mobRefSet = new ThreadLocal<Set<String>>() {
    @Override
    protected Set<String> initialValue() {
```

### AnonymousHasLambdaAlternative
Anonymous new ThreadLocal() can be replaced with ThreadLocal.withInitial()
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java`
#### Snippet
```java

  private static final ThreadLocal<SimpleDateFormat> LOCAL_FORMAT =
    new ThreadLocal<SimpleDateFormat>() {
      @Override
      protected SimpleDateFormat initialValue() {
```

### AnonymousHasLambdaAlternative
Anonymous new ThreadLocal\>() can be replaced with ThreadLocal.withInitial()
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
   */
  static ThreadLocal<HashMap<String, Long>> mobLengthMap =
    new ThreadLocal<HashMap<String, Long>>() {
      @Override
      protected HashMap<String, Long> initialValue() {
```

### AnonymousHasLambdaAlternative
Anonymous new ThreadLocal() can be replaced with ThreadLocal.withInitial()
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
   */

  static ThreadLocal<Boolean> disableIO = new ThreadLocal<Boolean>() {

    @Override
```

### AnonymousHasLambdaAlternative
Anonymous new ThreadLocal() can be replaced with ThreadLocal.withInitial()
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
   */

  static ThreadLocal<Boolean> userRequest = new ThreadLocal<Boolean>() {
    @Override
    protected Boolean initialValue() {
```

### AnonymousHasLambdaAlternative
Anonymous new Thread() can be replaced with new Thread(() -\> {...})
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    // kill the hbck with a ctrl-c, we want to cleanup the lock so that
    // it is available for further calls
    Runtime.getRuntime().addShutdownHook(new Thread() {
      @Override
      public void run() {
```

### AnonymousHasLambdaAlternative
Anonymous new Thread() can be replaced with new Thread(() -\> {...})
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
    // start thread for large file deletion
    for (int i = 0; i < largeFileDeleteThreadNumber; i++) {
      Thread large = new Thread() {
        @Override
        public void run() {
```

### AnonymousHasLambdaAlternative
Anonymous new Thread() can be replaced with new Thread(() -\> {...})
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
    // start thread for small file deletion
    for (int i = 0; i < smallFileDeleteThreadNumber; i++) {
      Thread small = new Thread() {
        @Override
        public void run() {
```

## RuleId[id=NegativeIntConstantInLongContext]
### NegativeIntConstantInLongContext
Negative int hexadecimal constant in long context
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
    long h = procId;
    h ^= h >> 16;
    h *= 0x85ebca6b;
    h ^= h >> 13;
    h *= 0xc2b2ae35;
```

### NegativeIntConstantInLongContext
Negative int hexadecimal constant in long context
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
    h *= 0x85ebca6b;
    h ^= h >> 13;
    h *= 0xc2b2ae35;
    h ^= h >> 16;
    return h;
```

## RuleId[id=IfStatementMissingBreakInLoop]
### IfStatementMissingBreakInLoop
Loop can be terminated after condition is met
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
    // If all of the exceptions are DNRIOE not exception
    for (Throwable t : exceptions) {
      if (!(t instanceof DoNotRetryIOException)) {
        res = true;
      }
```

## RuleId[id=FunctionalExpressionCanBeFolded]
### FunctionalExpressionCanBeFolded
Method reference can be replaced with qualifier
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java`
#### Snippet
```java
    Threads.setDaemonThreadRunning(this,
      name + ".replicationSource.shipper" + walGroupId + "," + source.getQueueId(),
      handler::uncaughtException);
  }

```

### FunctionalExpressionCanBeFolded
Lambda can be replaced with call qualifier
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      // order is preserved as its expected from the client
      if (!atomic) {
        Arrays.sort(mArray, (v1, v2) -> Row.COMPARATOR.compare(v1, v2));
      }

```

## RuleId[id=WhileLoopSpinsOnField]
### WhileLoopSpinsOnField
`while` loop spins on field
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcHandler.java`
#### Snippet
```java
    running = true;
    try {
      while (running) {
        try {
          run(getCallRunner());
```

### WhileLoopSpinsOnField
`while` loop spins on field
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
      isMaster = true;

      while (!stopped) {
        long now = EnvironmentEdgeManager.currentTime();

```

### WhileLoopSpinsOnField
`while` loop spins on field
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
      readyForRolling = false;
      consumeExecutor.execute(consumer);
      while (!readyForRolling) {
        readyForRollingCond.awaitUninterruptibly();
      }
```

## RuleId[id=DefaultAnnotationParam]
### DefaultAnnotationParam
Redundant default parameter value assignment
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java`
#### Snippet
```java

  @Override
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "EQ_UNUSUAL", justification = "")
  public boolean equals(Object obj) {
    return obj == null;
```

## RuleId[id=ThrowFromFinallyBlock]
### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
      lock.unlock();
      if (isSyncAborted()) {
        throw new RuntimeException("sync aborted", syncException.get());
      }
    }
```

### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          // Just throw the first exception as an indication something bad happened
          // Don't need to propagate all the exceptions, we already logged them all anyway
          throw new ExecutionException("First exception in WorkItemHdfsDir",
            exceptions.firstElement());
        }
```

### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
        // Don't need to propagate all the exceptions, we already logged them all anyway
        Throwables.propagateIfPossible(exceptions.firstElement(), IOException.class);
        throw new IOException(exceptions.firstElement());
      }
    }
```

### ThrowFromFinallyBlock
`throw` inside 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
        throw (InterruptedIOException) new InterruptedIOException().initCause(e);
      }
    }
```

## RuleId[id=IfStatementWithIdenticalBranches]
### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java`
#### Snippet
```java
      byte status = src.get();
      int tagLen;
      if (status == Dictionary.NOT_IN_DICTIONARY) {
        tagLen = StreamUtils.readRawVarint32(src);
        offset = Bytes.putAsShort(dest, offset, tagLen);
```

### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java`
#### Snippet
```java
      }

      if (plaintextLength > 0) {

        // Set up the cipher
```

### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
        return src.getPosition() - start;
      case BLOB_COPY:
        if (Order.DESCENDING == ord) {
          // if descending, read to termination byte.
          do {
```

### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/screen/field/FieldScreenView.java`
#### Snippet
```java
    if (selected) {
      String prefix = display ? "* " : "  ";
      if (moveMode) {
        printer.print(prefix);

```

### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterChunk.java`
#### Snippet
```java
    int hash2;
    HashKey<Cell> hashKey;
    if (this.bloomType == BloomType.ROWCOL) {
      hashKey = new RowColBloomHashKey(cell);
      hash1 = this.hash.hash(hashKey, 0);
```

### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.java`
#### Snippet
```java
    int numOfCellsInChunk = calcNumOfCellsInChunk(dataChunkSize);
    int cellsInLastChunk = numOfCells % numOfCellsInChunk;
    if (cellsInLastChunk == 0) { // There is no free space in the last chunk and thus,
      return ChunkCreator.ChunkType.DATA_CHUNK; // no need to use index chunks.
    } else {
```

### IfStatementWithIdenticalBranches
Common part can be extracted from 'if'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java`
#### Snippet
```java
      pos = Bytes.putByte(backingArray, pos, (byte) in.read());
      int valLen = typeValLen - 1;
      if (hasValueCompression) {
        readCompressedValue(in, backingArray, pos, valLen);
        pos += valLen;
```

## RuleId[id=UnnecessarySuperQualifier]
### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimplePositionedByteRange.java`
#### Snippet
```java
  @Override
  public PositionedByteRange set(int capacity) {
    if (super.bytes != null) {
      throw new ReadOnlyByteRangeException();
    }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleByteRange.java`
#### Snippet
```java
  @Override
  public ByteRange set(int capacity) {
    if (super.bytes != null) {
      throw new ReadOnlyByteRangeException();
    }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RemoteWithExtrasException.java`
#### Snippet
```java
      try {
        // cause could be a hadoop exception, try to load from hadoop classpath
        realClass = Class.forName(getClassName(), false, super.getClass().getClassLoader());
      } catch (ClassNotFoundException e) {
        return new DoNotRetryIOException(
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java`
#### Snippet
```java
   */
  public Map<byte[], NavigableMap<byte[], Long>> getFamilyMapOfLongs() {
    NavigableMap<byte[], List<Cell>> map = super.getFamilyCellMap();
    Map<byte[], NavigableMap<byte[], Long>> results = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    for (Map.Entry<byte[], List<Cell>> entry : map.entrySet()) {
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java`
#### Snippet
```java
  public byte[] toByteArray() {
    FilterProtos.ValueFilter.Builder builder = FilterProtos.ValueFilter.newBuilder();
    builder.setCompareFilter(super.convert());
    return builder.build().toByteArray();
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java`
#### Snippet
```java
    FilterProtos.DependentColumnFilter.Builder builder =
      FilterProtos.DependentColumnFilter.newBuilder();
    builder.setCompareFilter(super.convert());
    if (this.columnFamily != null) {
      builder.setColumnFamily(UnsafeByteOperations.unsafeWrap(this.columnFamily));
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java`
#### Snippet
```java
  public byte[] toByteArray() {
    FilterProtos.QualifierFilter.Builder builder = FilterProtos.QualifierFilter.newBuilder();
    builder.setCompareFilter(super.convert());
    return builder.build().toByteArray();
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java`
#### Snippet
```java
  public byte[] toByteArray() {
    FilterProtos.RowFilter.Builder builder = FilterProtos.RowFilter.newBuilder();
    builder.setCompareFilter(super.convert());
    return builder.build().toByteArray();
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java
  public byte[] toByteArray() {
    FilterProtos.FamilyFilter.Builder builder = FilterProtos.FamilyFilter.newBuilder();
    builder.setCompareFilter(super.convert());
    return builder.build().toByteArray();
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java
    FilterProtos.SingleColumnValueExcludeFilter.Builder builder =
      FilterProtos.SingleColumnValueExcludeFilter.newBuilder();
    builder.setSingleColumnValueFilter(super.convert());
    return builder.build().toByteArray();
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java`
#### Snippet
```java

    // check actions
    return super.implies(action);
  }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ClusterStatusTracker.java`
#### Snippet
```java
   */
  public boolean isClusterUp() {
    return super.getData(false) != null;
  }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java`
#### Snippet
```java
   */
  public boolean hasMaster() {
    return super.getData(false) != null;
  }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MasterAddressTracker.java`
#### Snippet
```java
  public ServerName getMasterAddress(final boolean refresh) {
    try {
      return ProtobufUtil.parseServerNameFrom(super.getData(refresh));
    } catch (DeserializationException e) {
      LOG.warn("Failed parse", e);
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      @Override
      public void println(int x) {
        super.print(x);
        super.print(", ");
      }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      public void println(int x) {
        super.print(x);
        super.print(", ");
      }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      @Override
      public void println(String x) {
        super.print(x);
        super.print(", ");
      }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
      public void println(String x) {
        super.print(x);
        super.print(", ");
      }
    }) {
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.java`
#### Snippet
```java

  protected void setFailure(Throwable cause) {
    super.setFailure(getClass().getSimpleName(), cause);
  }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/AverageIntervalRateLimiter.java`
#### Snippet
```java
    }

    double r = ((double) (amount - available)) * super.getTimeUnitInMillis() / limit;
    return (long) r;
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/AverageIntervalRateLimiter.java`
#### Snippet
```java
    long timeInterval = now - nextRefillTime;
    long delta = 0;
    long timeUnitInMillis = super.getTimeUnitInMillis();
    if (timeInterval >= timeUnitInMillis) {
      delta = limit;
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FixedIntervalRateLimiter.java`
#### Snippet
```java
      return 0;
    }
    nextRefillTime = now + super.getTimeUnitInMillis();
    return limit;
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
  @InterfaceAudience.Private
  public void setConf(Configuration conf, ZKWatcher zk) {
    super.setConf(conf);
    try {
      this.zkw = zk;
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
  public void setConf(Configuration conf, ZKWatcher zk,
    ReplicationQueueStorage replicationQueueStorage) {
    super.setConf(conf);
    this.zkw = zk;
    this.queueStorage = replicationQueueStorage;
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.java`
#### Snippet
```java
     */
    public int compareRows(Cell left, Cell right) {
      return super.kvComparator.compareRows(left, right);
    }
  }
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedRegionScannerImpl.java`
#### Snippet
```java
  @Override
  protected boolean nextRow(ScannerContext scannerContext, Cell curRowCell) throws IOException {
    assert super.joinedContinuationRow == null : "Trying to go to next row during joinedHeap read.";
    this.storeHeap.seekToPreviousRow(PrivateCellUtil.createFirstOnRow(curRowCell));
    resetFilters();
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureAsyncProtobufLogWriter.java`
#### Snippet
```java
  protected WALHeader buildWALHeader(Configuration conf, WALHeader.Builder builder)
    throws IOException {
    return super.buildSecureWALHeader(conf, builder);
  }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureAsyncProtobufLogWriter.java`
#### Snippet
```java
  @Override
  protected void initAfterHeader(boolean doCompress) throws IOException {
    super.secureInitAfterHeader(doCompress, encryptor);
  }
}
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogWriter.java`
#### Snippet
```java
  @Override
  protected void initAfterHeader(boolean doCompress) throws IOException {
    super.secureInitAfterHeader(doCompress, encryptor);
  }
}
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogWriter.java`
#### Snippet
```java
  protected WALHeader buildWALHeader(Configuration conf, WALHeader.Builder builder)
    throws IOException {
    return super.buildSecureWALHeader(conf, builder);
  }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
        long totalSrcLgth = 0;
        for (Path aSrc : srcs) {
          totalSrcLgth += BackupUtils.getFilesLength(aSrc.getFileSystem(super.getConf()), aSrc);
        }

```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/HThreadedSelectorServerArgs.java`
#### Snippet
```java
      conf.get(ACCEPT_POLICY_CONF_KEY, getAcceptPolicy().toString()).toUpperCase(Locale.ROOT));

    super.selectorThreads(selectorThreads).workerThreads(workerThreads)
      .stopTimeoutVal(stopTimeoutVal).acceptQueueSizePerThread(acceptQueueSizePerThread)
      .acceptPolicy(acceptPolicy);
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        newWriteEntry = true;
      }
      super.writeMiniBatchOperationsToMemStore(miniBatchOp, writeEntry.getWriteNumber());
      if (newWriteEntry) {
        /**
```

### UnnecessarySuperQualifier
Qualifier `super` is unnecessary in this context
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      final MiniBatchOperationInProgress<Mutation> miniBatchOp, final WriteEntry writeEntry,
      long now) throws IOException {
      super.writeMiniBatchOperationsToMemStore(miniBatchOp, getOrigLogSeqNum());
      return writeEntry;
    }
```

## RuleId[id=SlowListContainsAll]
### SlowListContainsAll
Call to 'list.containsAll(collection)' may have poor performance
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java`
#### Snippet
```java
  @Override
  public boolean containsAll(Collection<?> c) {
    return list.containsAll(c);
  }

```

### SlowListContainsAll
Call to 'list.containsAll(collection)' may have poor performance
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
        // sanity check: we're compacting files that this store knows about
        // TODO: change this to LOG.error() after more debugging
        Preconditions.checkArgument(filesCompacting.containsAll(filesToCompact));
      }

```

## RuleId[id=InfiniteLoopStatement]
### InfiniteLoopStatement
`while` statement cannot complete without throwing an exception
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmPauseMonitor.java`
#### Snippet
```java
    List<String> list = Lists.newArrayList();
    int i = 0;
    while (true) {
      list.add(String.valueOf(i++));
    }
```

## RuleId[id=CodeBlock2Expr]
### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/X509Util.java`
#### Snippet
```java
      throw new IOException("Key/trust store path does not have a parent: " + filePath);
    }
    FileChangeWatcher fileChangeWatcher = new FileChangeWatcher(parentPath, watchEvent -> {
      handleWatchEvent(filePath, watchEvent, resetContext);
    });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java

  static final ChannelReader CHANNEL_READER = (channel, buf, offset) -> {
    return channel.read(buf);
  };

```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java

  static final ChannelReader FILE_READER = (channel, buf, offset) -> {
    return ((FileChannel) channel).read(buf, offset);
  };

```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java`
#### Snippet
```java
        String encodedRegionNameStr = Bytes.toString(encodedRegionName);
        results.stream().filter(result -> !result.isEmpty())
          .filter(result -> CatalogFamilyFormat.getRegionInfo(result) != null).forEach(result -> {
            getRegionLocations(result).ifPresent(locations -> {
              for (HRegionLocation location : locations.getRegionLocations()) {
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcConnection.java`
#### Snippet
```java
        LOG.warn("Relogin failed", e);
      }
      eventLoop.execute(() -> {
        reloginInProgress = false;
      });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
      if (!tableCache.allRequests.isEmpty()) {
        IOException error = new IOException("Cache cleared");
        tableCache.allRequests.values().forEach(f -> {
          futureResultList.add(new RegionLocationsFutureResult(f, null, error));
        });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncTableImpl.java`
#### Snippet
```java
    Converter<MutateRequest, byte[], REQ> reqConvert) {
    return mutate(controller, loc, stub, req, reqConvert, (c, resp) -> {
      return null;
    });
  }
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    }
    scanBuilder.setMaxVersions(scan.getMaxVersions());
    scan.getColumnFamilyTimeRange().forEach((cf, timeRange) -> {
      scanBuilder.addCfTimeRange(HBaseProtos.ColumnFamilyTimeRange.newBuilder()
        .setColumnFamily(UnsafeByteOperations.unsafeWrap(cf)).setTimeRange(toTimeRange(timeRange))
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      builder.setFilter(ProtobufUtil.toFilter(get.getFilter()));
    }
    get.getColumnFamilyTimeRange().forEach((cf, timeRange) -> {
      builder.addCfTimeRange(HBaseProtos.ColumnFamilyTimeRange.newBuilder()
        .setColumnFamily(UnsafeByteOperations.unsafeWrap(cf)).setTimeRange(toTimeRange(timeRange))
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
    return (instance, src, masked, clientName, flag, createParent, replication, blockSize,
      supportedVersions) -> {
      return (HdfsFileStatus) createMethod.invoke(instance, src, masked, clientName, flag,
        createParent, replication, blockSize, supportedVersions, null, null);
    };
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
    return (instance, src, masked, clientName, flag, createParent, replication, blockSize,
      supportedVersions) -> {
      return (HdfsFileStatus) createMethod.invoke(instance, src, masked, clientName, flag,
        createParent, replication, blockSize, supportedVersions, null);
    };
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
    updateBalancerLoadInfo(Map<TableName, Map<ServerName, List<RegionInfo>>> loadOfAllTable) {
    if (isByTable) {
      loadOfAllTable.forEach((tableName, loadOfOneTable) -> {
        updateBalancerTableLoadInfo(tableName, loadOfOneTable);
      });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
    loads = new HashMap<>();

    clusterStatus.getLiveServerMetrics().forEach((ServerName sn, ServerMetrics sm) -> {
      sm.getRegionMetrics().forEach((byte[] regionName, RegionMetrics rm) -> {
        String regionNameAsString = RegionInfo.getRegionNameAsString(regionName);
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      }
    }
    server2LoadMap.forEach((k, v) -> {
      serverLoadList.add(new ServerAndLoad(k, v));
    });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
              if (cachedBlock.getDataBlockEncoding() != dataBlockEncoder.getDataBlockEncoding()) {
                // Remember to release the block when in exceptional path.
                cacheConf.getBlockCache().ifPresent(cache -> {
                  returnAndEvictBlock(cache, cacheKey, cachedBlock);
                });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      List<CompletableFuture<Void>> futures = new ArrayList<>();
      peers.stream().filter(peer -> peer.getPeerConfig().needToReplicate(tableName))
        .forEach(peer -> {
          futures.add(trySyncTableToPeerCluster(tableName, splits, peer));
        });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
    CompletableFuture<Boolean> future = new CompletableFuture<>();
    addListener(ClientMetaTableAccessor.getTableState(metaTable, tableName),
      (tableState, error) -> {
        completeCheckTableState(future, tableState.isPresent() ? tableState.get() : null, error,
          TableState.State.ENABLED, tableName);
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          Stream.of(table.getColumnFamilies())
            .filter(column -> column.getScope() != HConstants.REPLICATION_SCOPE_LOCAL)
            .forEach(column -> {
              cfs.put(column.getNameAsString(), column.getScope());
            });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          locations.stream().filter(loc -> loc.getServerName() != null)
            .filter(loc -> loc.getRegion() != null).filter(loc -> !loc.getRegion().isOffline())
            .map(loc -> loc.getRegion().getRegionName()).forEach(region -> {
              futures.add(getCompactionStateForRegion(region).whenComplete((regionState, err2) -> {
                // If any region compaction state is MAJOR_AND_MINOR
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      Map<ServerName, Boolean> serverStates = new ConcurrentHashMap<>(serverNames.size());
      List<CompletableFuture<Boolean>> futures = new ArrayList<>(serverNames.size());
      serverNames.stream().forEach(serverName -> {
        futures.add(switchCompact(serverName, switchState).whenComplete((serverState, err2) -> {
          if (err2 != null) {
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
    CompletableFuture<Boolean> future = new CompletableFuture<>();
    addListener(ClientMetaTableAccessor.getTableState(metaTable, tableName),
      (tableState, error) -> {
        completeCheckTableState(future, tableState.isPresent() ? tableState.get() : null, error,
          TableState.State.DISABLED, tableName);
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java`
#### Snippet
```java
  public void checkUnDeletedQueues() throws ReplicationException {
    undeletedQueueIds = getUnDeletedQueues();
    undeletedQueueIds.forEach((replicator, queueIds) -> {
      queueIds.forEach(queueId -> {
        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
          targetRSGInfo == null || !targetRSGInfo.containsServer(currentHostServer.getAddress())
        ) {
          regionInfoList.forEach(regionInfo -> {
            regionPlansForMisplacedRegions.add(new RegionPlan(regionInfo, currentHostServer, null));
          });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
        this.memstore.clearSnapshot(snapshotId);
        HStoreFile.increaseStoreFilesRefeCount(sfs);
      } : () -> {
        HStoreFile.increaseStoreFilesRefeCount(sfs);
      });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java
      MAX_ALLOC_KEY + " must be less than " + CHUNK_SIZE_KEY);

    this.refCnt = RefCnt.create(() -> {
      recycleChunks();
    });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableMemStoreLAB.java`
#### Snippet
```java
  public ImmutableMemStoreLAB(List<MemStoreLAB> mslabs) {
    this.mslabs = mslabs;
    this.refCnt = RefCnt.create(() -> {
      closeMSLABs();
    });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
      // make it async
      for (Pair<Path, Long> log : localLogsToArchive) {
        logArchiveExecutor.execute(() -> {
          archive(log);
        });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    }
    MutableLong txidHolder = new MutableLong();
    MultiVersionConcurrencyControl.WriteEntry we = key.getMvcc().begin(() -> {
      txidHolder.setValue(ringBuffer.next());
    });
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFile.java`
#### Snippet
```java
    NavigableMap<Long, List<Path>> toDelete =
      loadedSeqId >= 0 ? seqId2TrackFiles.tailMap(loadedSeqId, false) : seqId2TrackFiles;
    toDelete.values().stream().flatMap(l -> l.stream()).forEach(file -> {
      ForkJoinPool.commonPool().execute(() -> {
        LOG.info("Deleting track file {}", file);
```

### CodeBlock2Expr
Statement lambda can be replaced with expression lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    }
    final ServerCall<?> rpcCall = RpcServer.getCurrentServerCallWithCellScanner().orElse(null);
    regionReplicationSink.ifPresent(sink -> writeEntry.attachCompletionAction(() -> {
      sink.add(walKey, walEdit, rpcCall);
    }));
```

## RuleId[id=UseOfPropertiesAsHashtable]
### UseOfPropertiesAsHashtable
Call to `Hashtable.put()` on properties object
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
    // If clientPort is not set, assign the default.
    if (zkProperties.getProperty(HConstants.CLIENT_PORT_STR) == null) {
      zkProperties.put(HConstants.CLIENT_PORT_STR, HConstants.DEFAULT_ZOOKEEPER_CLIENT_PORT);
    }

```

### UseOfPropertiesAsHashtable
Call to `Hashtable.put()` on properties object
in `hbase-common/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java`
#### Snippet
```java
      address = serverHost + ":" + peerPort + ":" + leaderPort;
      key = "server." + i;
      zkProperties.put(key, address);
    }

```

### UseOfPropertiesAsHashtable
Call to `Hashtable.get()` on properties object
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java`
#### Snippet
```java
    }

    String dataDirStr = properties.get("dataDir").toString().trim();
    File dataDir = new File(dataDirStr);
    if (!dataDir.isDirectory()) {
```

### UseOfPropertiesAsHashtable
Call to `Hashtable.put()` on properties object
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  public MiniKdc setupMiniKdc(File keytabFile) throws Exception {
    Properties conf = MiniKdc.createConf();
    conf.put(MiniKdc.DEBUG, true);
    MiniKdc kdc = null;
    File dir = null;
```

## RuleId[id=EmptyMethod]
### EmptyMethod
The method is empty
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollBackupSubprocedure.java`
#### Snippet
```java
   * Hooray!
   */
  public void releaseBarrier() {
    // NO OP
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java`
#### Snippet
```java
   * Marks the region as online at balancer.
   */
  void regionOnline(RegionInfo regionInfo, ServerName sn);

  /**
```

### EmptyMethod
All implementations of this method are empty
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java`
#### Snippet
```java
   * Marks the region as offline at balancer.
   */
  void regionOffline(RegionInfo regionInfo);

  /**
```

### EmptyMethod
All implementations of this method are empty
in `hbase-client/src/main/java/org/apache/hadoop/hbase/coprocessor/ColumnInterpreter.java`
#### Snippet
```java
   * passed message bytes (used on the server side).
   */
  public abstract void initialize(P msg);

  /**
```

### EmptyMethod
Method only calls its super
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/GlobalPermission.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

    @Override
    public int hashCode() {
      return super.hashCode();
    }
```

### EmptyMethod
Method only calls its super
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedByteBufferKeyValue.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-common/src/main/java/org/apache/hadoop/hbase/SizeCachedNoTagsByteBufferKeyValue.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterProcSourceImpl.java`
#### Snippet
```java

  @Override
  public void init() {
    super.init();
  }
```

### EmptyMethod
Method only calls its super
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java

    @Override
    public void execute() throws IOException {
      super.execute();
    }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    void procedureLoaded(long procId);

    void procedureAdded(long procId);

    void procedureFinished(long procId);
```

### EmptyMethod
All implementations of this method are empty
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    void procedureAdded(long procId);

    void procedureFinished(long procId);
  }

```

### EmptyMethod
The method is empty
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
   * Shut down the client. Close any open persistent connections.
   */
  public void shutdown() {
  }

```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/EndpointObserver.java`
#### Snippet
```java
   *                        Service's method merged into it.
   */
  default void postEndpointInvocation(ObserverContext<RegionCoprocessorEnvironment> ctx,
    Service service, String methodName, Message request, Message.Builder responseBuilder)
    throws IOException {
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param switchType type of switch
   */
  default void postSetSplitOrMergeEnabled(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final boolean newValue, final MasterSwitchType switchType) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param splitRow  split point
   */
  default void preSplitRegionAction(final ObserverContext<MasterCoprocessorEnvironment> c,
    final TableName tableName, final byte[] splitRow) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param regionInfoB the right daughter region
   */
  default void postCompletedSplitRegionAction(final ObserverContext<MasterCoprocessorEnvironment> c,
    final RegionInfo regionInfoA, final RegionInfo regionInfoB) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param ctx the environment to interact with the framework and master
   */
  default void postRollBackSplitRegionAction(
    final ObserverContext<MasterCoprocessorEnvironment> ctx) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param ctx the environment to interact with the framework and master
   */
  default void postCompletedMergeRegionsAction(
    final ObserverContext<MasterCoprocessorEnvironment> ctx, final RegionInfo[] regionsToMerge,
    final RegionInfo mergedRegion) throws IOException {
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param ctx the environment to interact with the framework and master
   */
  default void postRollBackMergeRegionsAction(
    final ObserverContext<MasterCoprocessorEnvironment> ctx, final RegionInfo[] regionsToMerge)
    throws IOException {
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param tableName the name of the table
   */
  default void postTableFlush(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final TableName tableName) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param quotas   the resulting quota for the user
   */
  default void postSetUserQuota(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final String userName, final GlobalQuotaSettings quotas) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param quotas    the resulting quota for the user on the table
   */
  default void postSetUserQuota(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final String userName, final TableName tableName, final GlobalQuotaSettings quotas)
    throws IOException {
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param quotas    the resulting quota for the user on the namespace
   */
  default void postSetUserQuota(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final String userName, final String namespace, final GlobalQuotaSettings quotas)
    throws IOException {
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param quotas    the resulting quota for the table
   */
  default void postSetTableQuota(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final TableName tableName, final GlobalQuotaSettings quotas) throws IOException {
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java`
#### Snippet
```java
   * @param quotas    the resulting quota for the namespace
   */
  default void postSetNamespaceQuota(final ObserverContext<MasterCoprocessorEnvironment> ctx,
    final String namespace, final GlobalQuotaSettings quotas) throws IOException {
  }
```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java`
#### Snippet
```java
  }

  public void stop() {
  }

```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  }

  public void onlineRegion(final RegionInfo regionInfo, final ServerName serverName) {
    // TODO used by TestSplitTransactionOnCluster.java
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.java`
#### Snippet
```java
    throws IOException, ProcedureSuspendedException;

  protected abstract void finishTransition(MasterProcedureEnv env, RegionStateNode regionNode)
    throws IOException, ProcedureSuspendedException;

```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.java`
#### Snippet
```java
    throws IOException, ProcedureSuspendedException;

  protected abstract void reportTransition(MasterProcedureEnv env, RegionStateNode regionNode,
    TransitionCode code, long seqId) throws UnexpectedStateException;

```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/MasterProcedureManager.java`
#### Snippet
```java
   * @throws IOException if permissions requirements are not met.
   */
  public abstract void checkPermissions(ProcedureDescription desc, AccessChecker accessChecker,
    User user) throws IOException;

```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java`
#### Snippet
```java
  }

  public void releaseBarrier() {
    // NO OP
  }
```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
  }

  public void stop() {
  }

```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/DisableTableViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void enable() throws IOException {
    // do nothing
  }
```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/DisableTableViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void disable() throws IOException {
    // do nothing
  }
```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/NoInsertsViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void enable() {
  }

```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/NoInsertsViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void disable() {
  }

```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/NoWritesViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void enable() {
  }

```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/policies/NoWritesViolationPolicyEnforcement.java`
#### Snippet
```java

  @Override
  public void disable() {
  }

```

### EmptyMethod
Method only calls its super
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java`
#### Snippet
```java

  @Override
  protected void postUpdate(MutableSegment currentActive) {
    return;
  }
```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  }

  private void replayWALFlushAbortMarker(FlushDescriptor flush) {
    // nothing to do for now. A flush abort will cause a RS abort which means that the region
    // will be opened somewhere else later. We will see the region open event soon, and replaying
```

### EmptyMethod
Method only calls its super
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java`
#### Snippet
```java
   * the replaying edits from WAL
   */
  default void startReplayingFromWAL() {
    return;
  }
```

### EmptyMethod
Method only calls its super
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java`
#### Snippet
```java
   * This message intends to inform the MemStore that the replaying edits from WAL are done
   */
  default void stopReplayingFromWAL() {
    return;
  }
```

### EmptyMethod
Method only calls its super
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobReferenceOnlyFilter.java`
#### Snippet
```java

  @Override
  public int hashCode() {
    return super.hashCode();
  }
```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
  }

  public void stop() throws IOException {
  }

```

### EmptyMethod
Method only calls its super
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java`
#### Snippet
```java

    @Override
    public void run() {
      super.run();
    }
```

### EmptyMethod
The method is empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java`
#### Snippet
```java
   * Hooray!
   */
  public void releaseBarrier() {
    // NO OP
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntrySinkFilter.java`
#### Snippet
```java
   * Called after Construction. Use passed Connection to keep any context the filter might need.
   */
  void init(AsyncConnection conn);

  /**
```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ZKVisibilityLabelWatcher.java`
#### Snippet
```java

  @Override
  public void nodeDeleted(String path) {
    // There is no case of visibility labels path to get deleted.
  }
```

### EmptyMethod
Empty method overrides empty method
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ZKVisibilityLabelWatcher.java`
#### Snippet
```java

  @Override
  public void nodeChildrenChanged(String path) {
    // We are not dealing with child nodes under the label znode or userauths znode.
  }
```

### EmptyMethod
All implementations of this method are empty
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterWriter.java`
#### Snippet
```java

  /** Compact the Bloom filter before writing metadata &amp; data to disk. */
  void compactBloom();

  /**
```

### EmptyMethod
Method only calls its super
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/DisabledWALProvider.java`
#### Snippet
```java

    @Override
    public void updateStore(byte[] encodedRegionName, byte[] familyName, Long sequenceid,
      boolean onlyIfGreater) {
      return;
```

### EmptyMethod
The method is empty
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCluster.java`
#### Snippet
```java
   * Wait for the namenode.
   */
  public void waitForNamenodeAvailable() throws InterruptedException {
  }

```

### EmptyMethod
The method is empty
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCluster.java`
#### Snippet
```java
  }

  public void waitForDatanodesRegistered(int nbDN) throws Exception {
  }
}
```

### EmptyMethod
Method only calls its super
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
   */
  @Override
  public Configuration getConfiguration() {
    return super.getConfiguration();
  }
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TAppend.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TIncrement.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TAuthorization.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TCellVisibility.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // alas, we cannot check 'scannerId' because it's a primitive and you chose the non-beans generator.
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // alas, we cannot check 'scannerId' because it's a primitive and you chose the non-beans generator.
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // alas, we cannot check 'includeSysTables' because it's a primitive and you chose the non-beans generator.
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // alas, we cannot check 'includeSysTables' because it's a primitive and you chose the non-beans generator.
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java`
#### Snippet
```java
    }

    public void validate() throws org.apache.thrift.TException {
      // check for required fields
      // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIOError.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIllegalArgument.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TLogQueryFilter.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
```

### EmptyMethod
The method is empty
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/TTimeRange.java`
#### Snippet
```java
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // alas, we cannot check 'minStamp' because it's a primitive and you chose the non-beans generator.
```

## RuleId[id=RedundantFieldInitialization]
### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ByteBufferKeyOnlyKeyValue.java`
#### Snippet
```java
  private ByteBuffer buf;
  private int offset = 0; // offset into buffer where key starts at
  private int length = 0; // length of this.
  private short rowLen;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ByteBufferKeyOnlyKeyValue.java`
#### Snippet
```java
    ClassSize.OBJECT + ClassSize.REFERENCE + (2 * Bytes.SIZEOF_INT) + Bytes.SIZEOF_SHORT;
  private ByteBuffer buf;
  private int offset = 0; // offset into buffer where key starts at
  private int length = 0; // length of this.
  private short rowLen;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected long seqId = 0;
  protected byte[] tags = null;
  protected int tagsOffset = 0;
  protected int tagsLength = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected byte[] tags = null;
  protected int tagsOffset = 0;
  protected int tagsLength = 0;

  @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int rLength = 0;
  protected byte[] family = null;
  protected int fOffset = 0;
  protected int fLength = 0;
  protected byte[] qualifier = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
public abstract class ExtendedCellBuilderImpl implements ExtendedCellBuilder {
  protected byte[] row = null;
  protected int rOffset = 0;
  protected int rLength = 0;
  protected byte[] family = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int fOffset = 0;
  protected int fLength = 0;
  protected byte[] qualifier = null;
  protected int qOffset = 0;
  protected int qLength = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int qLength = 0;
  protected long timestamp = HConstants.LATEST_TIMESTAMP;
  protected KeyValue.Type type = null;
  protected byte[] value = null;
  protected int vOffset = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected long timestamp = HConstants.LATEST_TIMESTAMP;
  protected KeyValue.Type type = null;
  protected byte[] value = null;
  protected int vOffset = 0;
  protected int vLength = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected byte[] qualifier = null;
  protected int qOffset = 0;
  protected int qLength = 0;
  protected long timestamp = HConstants.LATEST_TIMESTAMP;
  protected KeyValue.Type type = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected KeyValue.Type type = null;
  protected byte[] value = null;
  protected int vOffset = 0;
  protected int vLength = 0;
  protected long seqId = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected byte[] family = null;
  protected int fOffset = 0;
  protected int fLength = 0;
  protected byte[] qualifier = null;
  protected int qOffset = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int rOffset = 0;
  protected int rLength = 0;
  protected byte[] family = null;
  protected int fOffset = 0;
  protected int fLength = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
@InterfaceAudience.Private
public abstract class ExtendedCellBuilderImpl implements ExtendedCellBuilder {
  protected byte[] row = null;
  protected int rOffset = 0;
  protected int rLength = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int vLength = 0;
  protected long seqId = 0;
  protected byte[] tags = null;
  protected int tagsOffset = 0;
  protected int tagsLength = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected byte[] row = null;
  protected int rOffset = 0;
  protected int rLength = 0;
  protected byte[] family = null;
  protected int fOffset = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected byte[] value = null;
  protected int vOffset = 0;
  protected int vLength = 0;
  protected long seqId = 0;
  protected byte[] tags = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int fLength = 0;
  protected byte[] qualifier = null;
  protected int qOffset = 0;
  protected int qLength = 0;
  protected long timestamp = HConstants.LATEST_TIMESTAMP;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCellBuilderImpl.java`
#### Snippet
```java
  protected int vOffset = 0;
  protected int vLength = 0;
  protected long seqId = 0;
  protected byte[] tags = null;
  protected int tagsOffset = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ArrayBackedTag.java`
#### Snippet
```java
  private final byte type;// TODO extra type state needed?
  private final byte[] bytes;
  private int offset = 0;
  private int length = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ArrayBackedTag.java`
#### Snippet
```java
  private final byte[] bytes;
  private int offset = 0;
  private int length = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TagBuilderFactory.java`
#### Snippet
```java
  // This assumes that we never create tag with value less than 0.
  private byte tagType = (byte) -1;
  private byte[] tagBytes = null;
  public static final String TAG_TYPE_NOT_SET_EXCEPTION = "Need to set type of the tag.";
  public static final String TAG_VALUE_NULL_EXCEPTION = "TagBytes can't be null";
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
public class CompoundConfiguration extends Configuration {

  private Configuration mutableConf = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ScheduledChore.java`
#### Snippet
```java
  private long timeOfLastRun = -1; // system time millis
  private long timeOfThisRun = -1; // system time millis
  private boolean initialChoreComplete = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java`
#### Snippet
```java
@InterfaceAudience.Private
public class HDFSBlocksDistribution {
  private Map<String, HostAndWeight> hostAndWeights = null;
  private long uniqueBlocksTotalWeight = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java`
#### Snippet
```java
public class HDFSBlocksDistribution {
  private Map<String, HostAndWeight> hostAndWeights = null;
  private long uniqueBlocksTotalWeight = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ByteBufferKeyValue.java`
#### Snippet
```java
  protected final int offset;
  protected final int length;
  private long seqId = 0;

  public static final int FIXED_OVERHEAD =
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteArrayOutputStream.java`
#### Snippet
```java

  private byte[] buf;
  private int pos = 0;

  public ByteArrayOutputStream() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferOutputStream.java`
#### Snippet
```java
  private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

  protected ByteBuffer curBuf = null;

  ByteBufferOutputStream() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
  private final LongAdder heapAllocationBytes = new LongAdder();
  private long lastPoolAllocationBytes = 0;
  private long lastHeapAllocationBytes = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
  private final AtomicInteger usedBufCount = new AtomicInteger(0);

  private boolean maxPoolSizeInfoLevelLogged = false;

  // If the desired size is at least this size, it'll allocated from ByteBufferPool, otherwise it'll
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
  private final LongAdder poolAllocationBytes = new LongAdder();
  private final LongAdder heapAllocationBytes = new LongAdder();
  private long lastPoolAllocationBytes = 0;
  private long lastHeapAllocationBytes = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferListOutputStream.java`
#### Snippet
```java
  protected List<SingleByteBuff> allBufs = new ArrayList<>();

  private boolean lastBufFlipped = false;// Indicate whether the curBuf/lastBuf is flipped already

  public ByteBufferListOutputStream(ByteBuffAllocator allocator) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
    return new CellScanner() {
      private final Iterator<? extends CellScannable> iterator = cellScannerables.iterator();
      private CellScanner cellScanner = null;

      @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
    return new CellScanner() {
      private final Iterator<Entry<byte[], List<Cell>>> entries = map.entrySet().iterator();
      private Iterator<Cell> currentIterator = null;
      private Cell currentCell;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java`
#### Snippet
```java
    return new CellScanner() {
      private final Iterator<Cell> iterator = cells;
      private Cell current = null;

      @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java`
#### Snippet
```java
   */
  static class BidirectionalLRUMap {
    private int currSize = 0;

    // Head and tail of the LRU list.
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/LRUDictionary.java`
#### Snippet
```java
    private HashMap<Node, Short> nodeToIndex = new HashMap<>();
    private Node[] indexToNode;
    private int initSize = 0;

    public BidirectionalLRUMap(int initialSize) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java`
#### Snippet
```java

  private String hfileName = null;
  private byte[] columnFamily = null;
  private byte[] tableName = null;
  private CellComparator cellComparator;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java`
#### Snippet
```java
  /** Crypto context */
  private Encryption.Context cryptoContext = Encryption.Context.NONE;
  private long fileCreateTime = 0;

  private String hfileName = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java`
#### Snippet
```java
  private String hfileName = null;
  private byte[] columnFamily = null;
  private byte[] tableName = null;
  private CellComparator cellComparator;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java`
#### Snippet
```java
  private long fileCreateTime = 0;

  private String hfileName = null;
  private byte[] columnFamily = null;
  private byte[] tableName = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java`
#### Snippet
```java
  private boolean includesMvcc = true;
  /** Whether tags are to be included in the Read/Write **/
  private boolean includesTags = false;
  /** Compression algorithm used **/
  private Algorithm compression = Algorithm.NONE;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContextBuilder.java`
#### Snippet
```java
  private Algorithm compression = Algorithm.NONE;
  /** Whether tags to be compressed or not **/
  private boolean compressTags = false;
  /** the checksum type **/
  private ChecksumType checkSumType = ChecksumType.getDefaultChecksumType();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/trace/HFileContextAttributesBuilderConsumer.java`
#### Snippet
```java

  private boolean skipChecksum = false;
  private ReadType readType = null;

  public HFileContextAttributesBuilderConsumer(final HFileContext hFileContext) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/trace/HFileContextAttributesBuilderConsumer.java`
#### Snippet
```java
  private final HFileContext hFileContext;

  private boolean skipChecksum = false;
  private ReadType readType = null;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/CryptoAES.java`
#### Snippet
```java

    private int mySeqNum = 0;
    private int peerSeqNum = 0;
    private byte[] seqNum = new byte[4];

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/CryptoAES.java`
#### Snippet
```java
  private static class Integrity {

    private int mySeqNum = 0;
    private int peerSeqNum = 0;
    private byte[] seqNum = new byte[4];
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/CommonsCryptoAESEncryptor.java`
#### Snippet
```java
  private Key key;
  private byte[] iv;
  private boolean initialized = false;
  private SecureRandom rng;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESEncryptor.java`
#### Snippet
```java
  private Key key;
  private byte[] iv;
  private boolean initialized = false;

  public AESEncryptor(javax.crypto.Cipher cipher, SecureRandom rng) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java`
#### Snippet
```java
  private Key key;
  private byte[] iv;
  private boolean initialized = false;

  public AESDecryptor(javax.crypto.Cipher cipher) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java`
#### Snippet
```java
  // Size of actual data being written. Not considering the block encoding/compression. This
  // includes the header size also.
  protected int unencodedDataSizeWritten = 0;

  // Size of actual data being written. considering the block encoding. This
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java`
#### Snippet
```java
   * The previous Cell the encoder encoded.
   */
  protected Cell prevCell = null;

  // Size of actual data being written. Not considering the block encoding/compression. This
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java`
#### Snippet
```java
  // Size of actual data being written. considering the block encoding. This
  // includes the header size also.
  protected int encodedDataSizeWritten = 0;

  public void beforeShipped() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexEncoderV1.java`
#### Snippet
```java

  /** The Cell previously appended. */
  private Cell lastCell = null;

  private DataOutputStream out;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java`
#### Snippet
```java
    protected int tagsOffset = -1;

    protected ByteBuffer keyBuffer = null;
    protected long memstoreTS;
    protected int nextKvOffset;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java`
#### Snippet
```java
    protected int keyLength;
    protected int valueLength;
    protected int tagsLength = 0;
    protected int tagsOffset = -1;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexSeekerV1.java`
#### Snippet
```java

  private int rowNumber;
  private ByteBuff rowOffsets = null;
  private final CellComparator cellComparator;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/RowIndexCodecV1.java`
#### Snippet
```java

  private static class RowIndexEncodingState extends EncodingState {
    RowIndexEncoderV1 encoder = null;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java`
#### Snippet
```java

  private static class CopyKeyEncodingState extends EncodingState {
    NoneEncoder encoder = null;
  }

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hadoopbackport/ThrottledInputStream.java`
#### Snippet
```java

  private long bytesRead = 0;
  private long totalSleepTime = 0;

  public ThrottledInputStream(InputStream rawStream) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/hadoopbackport/ThrottledInputStream.java`
#### Snippet
```java
  private final long startTime = EnvironmentEdgeManager.currentTime();

  private long bytesRead = 0;
  private long totalSleepTime = 0;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java`
#### Snippet
```java

    return new Iterator<Cell>() {
      private ByteBuffer decompressedData = null;
      private Iterator<Boolean> it = isTagsLenZero.iterator();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
    protected int valueLength;
    protected int lastCommonPrefix;
    protected int tagsLength = 0;
    protected int tagsOffset = -1;
    protected int tagsCompressedLength = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
    extends AbstractEncodedSeeker {
    protected ByteBuff currentBuffer;
    protected TagCompressionContext tagCompressionContext = null;
    protected KeyValue.KeyOnlyKeyValue keyOnlyKV = new KeyValue.KeyOnlyKeyValue();
    // A temp pair object which will be reused by ByteBuff#asSubByteBuffer calls. This avoids too
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
    protected int tagsLength = 0;
    protected int tagsOffset = -1;
    protected int tagsCompressedLength = 0;
    protected boolean uncompressTags = true;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/SingleByteBuff.java`
#### Snippet
```java
  // To access primitive values from underlying ByteBuffer using Unsafe
  private long unsafeOffset;
  private Object unsafeRef = null;

  public SingleByteBuff(ByteBuffer buf) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Sleeper.java`
#### Snippet
```java

  private final Object sleepLock = new Object();
  private boolean triggerWake = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java`
#### Snippet
```java
  public Iterator<T> iterator() {
    return new Iterator<T>() {
      private int returned = 0;

      @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
  private int curItemIndex = 0;

  private int limit = 0;
  private int limitedItemIndex;
  private int markedItemIndex = -1;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
  private ByteBuffer curItem = null;
  // Index of the current item in the MBB
  private int curItemIndex = 0;

  private int limit = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/MultiByteBuff.java`
#### Snippet
```java
  private final ByteBuffer[] items;
  // Pointer to the current item in the MBB
  private ByteBuffer curItem = null;
  // Index of the current item in the MBB
  private int curItemIndex = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferArray.java`
#### Snippet
```java
    private final int len;
    private int startBuffer, startOffset, endBuffer, endOffset;
    private int curIndex, sum = 0;

    private int index(long pos) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
  @InterfaceAudience.Private
  public static abstract class AvlLinkedNode<TNode extends AvlLinkedNode> extends AvlNode<TNode> {
    protected TNode iterNext = null;
    protected TNode iterPrev = null;
  }
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
    private final Object[] stack = new Object[64];

    private TNode current = null;
    private int height = 0;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
  public static abstract class AvlLinkedNode<TNode extends AvlLinkedNode> extends AvlNode<TNode> {
    protected TNode iterNext = null;
    protected TNode iterPrev = null;
  }

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java

    private TNode current = null;
    private int height = 0;

    public AvlTreeIterator() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractPositionedByteRange.java`
#### Snippet
```java
  protected int position = 0;

  protected int limit = 0;

  @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractPositionedByteRange.java`
#### Snippet
```java
   * </p>
   */
  protected int position = 0;

  protected int limit = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMovingAverage.java`
#### Snippet
```java
public class SimpleMovingAverage<T> extends MovingAverage<T> {
  private double averageTime = 0.0;
  protected long count = 0;

  public SimpleMovingAverage(String label) {
```

### RedundantFieldInitialization
Field initialization to `0.0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/SimpleMovingAverage.java`
#### Snippet
```java
@InterfaceAudience.Private
public class SimpleMovingAverage<T> extends MovingAverage<T> {
  private double averageTime = 0.0;
  protected long count = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java`
#### Snippet
```java
    protected int currentComponent = 0;
    protected int indexWithinComponent = -1;
    protected boolean nextWasCalled = false;

    @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java`
#### Snippet
```java
public class ConcatenatedLists<T> extends AbstractCollection<T> {
  protected final ArrayList<List<T>> components = new ArrayList<>();
  protected int size = 0;

  public void addAllSublists(List<? extends List<T>> items) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java`
#### Snippet
```java
      justification = "nextWasCalled is using by StripeStoreFileManager")
  public class Iterator implements java.util.Iterator<T> {
    protected int currentComponent = 0;
    protected int indexWithinComponent = -1;
    protected boolean nextWasCalled = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ReflectionUtils.java`
#### Snippet
```java

  /* synchronized on ReflectionUtils.class */
  private static long previousLogTime = 0;
  private static final ThreadMXBean threadBean = ManagementFactory.getThreadMXBean();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java`
#### Snippet
```java
  private static final long serialVersionUID = -3986244606585552569L;
  protected T1 first = null;
  protected T2 second = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Pair.java`
#### Snippet
```java
public class Pair<T1, T2> implements Serializable {
  private static final long serialVersionUID = -3986244606585552569L;
  protected T1 first = null;
  protected T2 second = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java`
#### Snippet
```java
  protected final Options options = new Options();

  protected Configuration conf = null;

  protected String[] cmdLineArgs = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java`
#### Snippet
```java
  protected Configuration conf = null;

  protected String[] cmdLineArgs = null;

  // To print options in order they were added in help text.
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java`
#### Snippet
```java
  // To print options in order they were added in help text.
  private HashMap<Option, Integer> optionsOrder = new HashMap<>();
  private int optionsCount = 0;

  public class OptionsOrderComparator implements Comparator<Option> {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/KeyValueCodec.java`
#### Snippet
```java

    protected final ByteBuff buf;
    protected Cell current = null;

    public ByteBuffKeyValueDecoder(ByteBuff buf) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java`
#### Snippet
```java

  protected final InputStream in;
  private Cell current = null;

  protected static class PBIS extends PushbackInputStream {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseEncoder.java`
#### Snippet
```java
  protected final OutputStream out;
  // This encoder is 'done' once flush has been called.
  protected boolean flushed = false;

  public BaseEncoder(final OutputStream out) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/StructIterator.java`
#### Snippet
```java

  protected final PositionedByteRange src;
  protected int idx = 0;
  @SuppressWarnings("rawtypes")
  protected final DataType[] types;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
      new ThreadFactoryBuilder().setDaemon(true).setNameFormat("group-cache-%d").build()));

  private LoadingCache<String, String[]> groupCache = null;

  static Groups groups = Groups.getUserToGroupsMappingService();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  private final static class COWEntry<K, V> implements Map.Entry<K, V> {
    K key = null;
    V value = null;

    COWEntry(K key, V value) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

  private final static class COWEntry<K, V> implements Map.Entry<K, V> {
    K key = null;
    V value = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/BaseSourceImpl.java`
#### Snippet
```java
    INSTANCE;

    private boolean inited = false;

    synchronized void init(String name) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/AuthUtil.java`
#### Snippet
```java
  private static Stoppable createDummyStoppable() {
    return new Stoppable() {
      private volatile boolean isStopped = false;

      @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
  protected byte[] bytes = null; // an immutable byte array that contains the KV
  protected int offset = 0; // offset into bytes buffer KV starts at
  protected int length = 0; // length of the KV starting from offset.

  /** Here be dragons **/
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
  ////
  // KeyValue core instance fields.
  protected byte[] bytes = null; // an immutable byte array that contains the KV
  protected int offset = 0; // offset into bytes buffer KV starts at
  protected int length = 0; // length of the KV starting from offset.
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
  // KeyValue core instance fields.
  protected byte[] bytes = null; // an immutable byte array that contains the KV
  protected int offset = 0; // offset into bytes buffer KV starts at
  protected int length = 0; // length of the KV starting from offset.

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java

  // multi-version concurrency control version. default value is 0, aka do not care.
  private long seqId = 0;

  /** Dragon time over, return to normal business */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   */
  private long[] buffer = new long[500];
  private int bufferCount = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/util/MetricSampleQuantiles.java`
#### Snippet
```java
   * Total number of items in stream
   */
  private long count = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/mapreduce/SampleUploader.java`
#### Snippet
```java
  static class Uploader extends Mapper<LongWritable, Text, ImmutableBytesWritable, Put> {
    private long checkpoint = 100;
    private long count = 0;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
  private static int port = 9090;
  private static boolean secure = false;
  private static String user = null;

  public static void main(String[] args) throws Exception {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
  private static String host = "localhost";
  private static int port = 9090;
  private static boolean secure = false;
  private static String user = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  static protected String doAsUser = null;
  static protected String principal = null;
  static protected String keyTab = null;

  public static void main(String[] args) throws Exception {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  static protected String host;
  private static boolean secure = false;
  static protected String doAsUser = null;
  static protected String principal = null;
  static protected String keyTab = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  static protected int port;
  static protected String host;
  private static boolean secure = false;
  static protected String doAsUser = null;
  static protected String principal = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
  private static boolean secure = false;
  static protected String doAsUser = null;
  static protected String principal = null;
  static protected String keyTab = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ScanModifyingObserver.java`
#### Snippet
```java

  private byte[] FAMILY_TO_ADD = null;
  private byte[] QUALIFIER_TO_ADD = null;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ScanModifyingObserver.java`
#### Snippet
```java
    "hbase.examples.coprocessor.scanmodifying.qualifier";

  private byte[] FAMILY_TO_ADD = null;
  private byte[] QUALIFIER_TO_ADD = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
  static protected String host;

  private static boolean secure = false;
  private static String serverPrincipal = "hbase";

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ValueRewritingObserver.java`
#### Snippet
```java
    "hbase.examples.coprocessor.value.rewrite.replaced";

  private byte[] sourceValue = null;
  private byte[] replacedValue = null;
  private Bytes.ByteArrayComparator comparator;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ValueRewritingObserver.java`
#### Snippet
```java

  private byte[] sourceValue = null;
  private byte[] replacedValue = null;
  private Bytes.ByteArrayComparator comparator;
  private CellBuilder cellBuilder;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/CacheEvictionStatsBuilder.java`
#### Snippet
```java
public final class CacheEvictionStatsBuilder {
  long evictedBlocks = 0;
  long maxCacheSize = 0;
  Map<byte[], Throwable> exceptions = new HashMap<>();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/CacheEvictionStatsBuilder.java`
#### Snippet
```java
@InterfaceAudience.Private
public final class CacheEvictionStatsBuilder {
  long evictedBlocks = 0;
  long maxCacheSize = 0;
  Map<byte[], Throwable> exceptions = new HashMap<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/FailedServers.java`
#### Snippet
```java
public class FailedServers {
  private final Map<Address, Long> failedServers = new HashMap<Address, Long>();
  private long latestExpiry = 0;
  private final int recheckServersTimeout;
  private static final Logger LOG = LoggerFactory.getLogger(FailedServers.class);
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcConnection.java`
#### Snippet
```java
  protected static String CRYPTO_AES_ENABLED_KEY = "hbase.rpc.crypto.encryption.aes.enabled";

  protected static boolean CRYPTO_AES_ENABLED_DEFAULT = false;

  // the last time we were picked up from connection pool.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
  public static class BlockingRpcCallback<R> implements RpcCallback<R> {
    private R result;
    private boolean resultSet = false;

    /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcCallback.java`
#### Snippet
```java
public class BlockingRpcCallback<R> implements RpcCallback<R> {
  private R result;
  private boolean resultSet = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClientMetaTableAccessor.java`
#### Snippet
```java
    private final boolean excludeOfflinedSplitParents;

    private RegionLocations current = null;

    CollectRegionLocationsVisitor(boolean excludeOfflinedSplitParents) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
  private List<ReplicationLoadSource> sources = Collections.emptyList();
  @Nullable
  private ReplicationLoadSink sink = null;
  private final Map<byte[], RegionMetrics> regionStatus = new TreeMap<>(Bytes.BYTES_COMPARATOR);
  private final Map<byte[], UserMetrics> userMetrics = new TreeMap<>(Bytes.BYTES_COMPARATOR);
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
  private final Set<String> coprocessorNames = new TreeSet<>();
  private long reportTimestamp = EnvironmentEdgeManager.currentTime();
  private long lastReportTimestamp = 0;
  private final List<ServerTask> tasks = new ArrayList<>();

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  private Integer callTimeout;

  private boolean done = false;

  private boolean cancelled = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcControllerImpl.java`
#### Snippet
```java
  private boolean done = false;

  private boolean cancelled = false;

  private final List<RpcCallback<Object>> cancellationCbs = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java`
#### Snippet
```java
  private final TableName tableName;
  private final IntSupplier getNumOfReplicas;
  private volatile boolean isStopped = false;

  CatalogReplicaLoadBalanceSimpleSelector(TableName tableName, AsyncConnectionImpl conn,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
  private final CallSender callSender;

  private boolean closed = false;

  private byte[] connectionHeaderPreamble;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
  private byte[] connectionHeaderWithLength;

  private boolean waitingConnectionHeaderResponse = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java

  // connected socket. protected for writing UT.
  protected Socket socket = null;
  private DataInputStream in;
  private DataOutputStream out;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(Get.class);

  private byte[] row = null;
  private int maxVersions = 1;
  private boolean cacheBlocks = true;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
  private int storeOffset = 0;
  private TimeRange tr = TimeRange.allTime();
  private boolean checkExistenceOnly = false;
  private Map<byte[], NavigableSet<byte[]>> familyMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
  private boolean cacheBlocks = true;
  private int storeLimit = -1;
  private int storeOffset = 0;
  private TimeRange tr = TimeRange.allTime();
  private boolean checkExistenceOnly = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
    private long responseSizeBytes = 0;
    private long startTime = 0;
    private long callTimeMs = 0;
    private int concurrentCallsPerServer = 0;
    private int numActionsPerServer = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
    private long startTime = 0;
    private long callTimeMs = 0;
    private int concurrentCallsPerServer = 0;
    private int numActionsPerServer = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
    private long callTimeMs = 0;
    private int concurrentCallsPerServer = 0;
    private int numActionsPerServer = 0;

    public long getRequestSizeBytes() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
  /** A container class for collecting details about the RPC call as it percolates. */
  public static class CallStats {
    private long requestSizeBytes = 0;
    private long responseSizeBytes = 0;
    private long startTime = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
    private long requestSizeBytes = 0;
    private long responseSizeBytes = 0;
    private long startTime = 0;
    private long callTimeMs = 0;
    private int concurrentCallsPerServer = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java`
#### Snippet
```java
  public static class CallStats {
    private long requestSizeBytes = 0;
    private long responseSizeBytes = 0;
    private long startTime = 0;
    private long callTimeMs = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(AdminOverAsyncAdmin.class);

  private volatile boolean aborted = false;

  private final Connection conn;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfoBuilder.java`
#### Snippet
```java
  private int replicaId = RegionInfo.DEFAULT_REPLICA_ID;
  private boolean offLine = false;
  private boolean split = false;

  public static RegionInfoBuilder newBuilder(TableName tableName) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfoBuilder.java`
#### Snippet
```java
  private long regionId = EnvironmentEdgeManager.currentTime();
  private int replicaId = RegionInfo.DEFAULT_REPLICA_ID;
  private boolean offLine = false;
  private boolean split = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalanceRequest.java`
#### Snippet
```java
  public final static class Builder {
    private boolean dryRun = false;
    private boolean ignoreRegionsInTransition = false;

    private Builder() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BalanceRequest.java`
#### Snippet
```java
  @InterfaceAudience.Public
  public final static class Builder {
    private boolean dryRun = false;
    private boolean ignoreRegionsInTransition = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
  private long writeBufferPeriodicFlushTimerTickMs = UNSET;
  private int maxKeyValueSize = UNSET;
  private ExecutorService pool = null;
  private String implementationClassName = null;
  private int rpcTimeout = UNSET;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
  private int maxKeyValueSize = UNSET;
  private ExecutorService pool = null;
  private String implementationClassName = null;
  private int rpcTimeout = UNSET;
  private int operationTimeout = UNSET;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java

  // Used to pass the span instance to the `AsyncTableImpl` from its underlying `rawAsyncTable`.
  private Span span = null;

  public AsyncTableResultScanner(TableName tableName, Scan scan, long maxCacheSize) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTableResultScanner.java`
#### Snippet
```java
  private long cacheSize;

  private boolean closed = false;

  private Throwable error;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java`
#### Snippet
```java
  // Will be removed in 4.0.0 along with the deprecated coprocessor methods in Table and Admin
  // interface.
  private volatile ExecutorService batchPool = null;

  private final AsyncConnectionImpl conn;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ConnectionOverAsyncConnection.class);

  private volatile boolean aborted = false;

  // only used for executing coprocessor calls, as users may reference the methods in the
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultScanner.java`
#### Snippet
```java
    return new Iterator<Result>() {
      // The next RowResult, possibly pre-read
      Result next = null;

      // return true if there is another item pending, false if there isn't.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CoprocessorBlockingRpcCallback.java`
#### Snippet
```java
class CoprocessorBlockingRpcCallback<R> implements RpcCallback<R> {
  private R result;
  private boolean resultSet = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  private long maxResultSize = -1;
  private boolean cacheBlocks = true;
  private boolean reversed = false;
  private TimeRange tr = TimeRange.allTime();
  private Map<byte[], NavigableSet<byte[]>> familyMap =
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java

  private int storeLimit = -1;
  private int storeOffset = 0;

  private static final String SCAN_ATTRIBUTES_METRICS_ENABLE = "scan.attributes.metrics.enable";
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
   * from the server.
   */
  private boolean allowPartialResults = false;

  private int storeLimit = -1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  private Map<byte[], NavigableSet<byte[]>> familyMap =
    new TreeMap<byte[], NavigableSet<byte[]>>(Bytes.BYTES_COMPARATOR);
  private Boolean asyncPrefetch = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  private boolean includeStartRow = true;
  private byte[] stopRow = HConstants.EMPTY_END_ROW;
  private boolean includeStopRow = false;
  private int maxVersions = 1;
  private int batch = -1;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  private ReadType readType = ReadType.DEFAULT;

  private boolean needCursorResult = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SingleResponse.java`
#### Snippet
```java
  public static class Entry {
    private Result result = null;
    private boolean processed = false;

    public Result getResult() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SingleResponse.java`
#### Snippet
```java
@InterfaceAudience.Private
public class SingleResponse extends AbstractResponse {
  private Entry entry = null;

  @InterfaceAudience.Private
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SingleResponse.java`
#### Snippet
```java
  @InterfaceAudience.Private
  public static class Entry {
    private Result result = null;
    private boolean processed = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
  static Checker newChecker(List<RowChecker> checkers) {
    return new Checker() {
      private boolean isEnd = false;

      @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java

    private final long maxHeapSizeSubmit;
    private long heapSize = 0;

    SubmittedSizeChecker(final long maxHeapSizeSubmit) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
  private final long minTimeBetweenRefreshesMs;

  private boolean refreshNow = false;
  private boolean stopped = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java

  private boolean refreshNow = false;
  private boolean stopped = false;

  synchronized void stop() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
    conn.getTable(META_TABLE_NAME).scan(scan, new AdvancedScanResultConsumer() {

      private boolean completeNormally = false;

      private boolean tableNotFound = true;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java

  // TODO: row should be final
  protected byte[] row = null;
  protected long ts = HConstants.LATEST_TIMESTAMP;
  protected Durability durability = Durability.USE_DEFAULT;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Query.java`
#### Snippet
```java
  protected Consistency consistency = Consistency.STRONG;
  protected Map<byte[], TimeRange> colFamTimeRangeMap = Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  protected Boolean loadColumnFamiliesOnDemand = null;

  public Filter getFilter() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Query.java`
#### Snippet
```java
public abstract class Query extends OperationWithAttributes {
  private static final String ISOLATION_LEVEL = "_isolationlevel_";
  protected Filter filter = null;
  protected int targetReplicaId = -1;
  protected Consistency consistency = Consistency.STRONG;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCallerFactory.java`
#### Snippet
```java
  public class ScanSingleRegionCallerBuilder extends BuilderBase {

    private Long scannerId = null;

    private Scan scan;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
  private final boolean readonly;

  private Cursor cursor = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
   * See {@link #mayHaveMoreCellsInRow()}.
   */
  private boolean mayHaveMoreCellsInRow = false;
  // We're not using java serialization. Transient here is just a marker to say
  // that this is where we cache row if we're ever asked for it.
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
  // Ditto for familyMap. It can be composed on fly from passed in kvs.
  private transient NavigableMap<byte[],
    NavigableMap<byte[], NavigableMap<Long, byte[]>>> familyMap = null;

  private static ThreadLocal<byte[]> localBuffer = new ThreadLocal<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
  // We're not using java serialization. Transient here is just a marker to say
  // that this is where we cache row if we're ever asked for it.
  private transient byte[] row = null;
  // Ditto for familyMap. It can be composed on fly from passed in kvs.
  private transient NavigableMap<byte[],
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
  private Cell[] cells;
  private Boolean exists; // if the query was just to check existence.
  private boolean stale = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java`
#### Snippet
```java
    private int memstoreLoad = 0;
    private int heapOccupancy = 0;
    private int compactionPressure = 0;

    public void update(RegionLoadStats currentStats) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java`
#### Snippet
```java

  public static class RegionStatistics {
    private int memstoreLoad = 0;
    private int heapOccupancy = 0;
    private int compactionPressure = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/backoff/ServerStatistics.java`
#### Snippet
```java
  public static class RegionStatistics {
    private int memstoreLoad = 0;
    private int heapOccupancy = 0;
    private int compactionPressure = 0;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class ColumnPrefixFilter extends FilterBase {
  protected byte[] prefix = null;

  public ColumnPrefixFilter(final byte[] prefix) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
  protected boolean maxColumnInclusive = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class ColumnRangeFilter extends FilterBase {
  protected byte[] minColumn = null;
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
  protected byte[] minColumn = null;
  protected boolean minColumnInclusive = true;
  protected byte[] maxColumn = null;
  protected boolean maxColumnInclusive = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class SkipFilter extends FilterBase {
  private boolean filterRow = false;
  private Filter filter;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/InclusiveStopFilter.java`
#### Snippet
```java
public class InclusiveStopFilter extends FilterBase {
  private byte[] stopRowKey;
  private boolean done = false;

  public InclusiveStopFilter(final byte[] stopRowKey) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java`
#### Snippet
```java
public class ColumnCountGetFilter extends FilterBase {
  private int limit = 0;
  private int count = 0;

  public ColumnCountGetFilter(final int n) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class ColumnCountGetFilter extends FilterBase {
  private int limit = 0;
  private int count = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  protected boolean foundColumn = false;
  protected boolean matchedColumn = false;
  protected boolean filterIfMissing = false;
  protected boolean latestVersionOnly = true;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  protected CompareOperator op;
  protected org.apache.hadoop.hbase.filter.ByteArrayComparable comparator;
  protected boolean foundColumn = false;
  protected boolean matchedColumn = false;
  protected boolean filterIfMissing = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  protected org.apache.hadoop.hbase.filter.ByteArrayComparable comparator;
  protected boolean foundColumn = false;
  protected boolean matchedColumn = false;
  protected boolean filterIfMissing = false;
  protected boolean latestVersionOnly = true;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class FirstKeyOnlyFilter extends FilterBase {
  private boolean foundKV = false;

  public FirstKeyOnlyFilter() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java`
#### Snippet
```java
  private int limit = 0;
  private int offset = -1;
  private byte[] columnOffset = null;
  private int count = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java`
#### Snippet
```java
  private int offset = -1;
  private byte[] columnOffset = null;
  private int count = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java`
#### Snippet
```java
public class ColumnPaginationFilter extends FilterBase {

  private int limit = 0;
  private int offset = -1;
  private byte[] columnOffset = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java`
#### Snippet
```java
  // This flag is used to speed up seeking cells when matched column is found, such that following
  // columns in the same row can be skipped faster by NEXT_ROW instead of NEXT_COL.
  private boolean columnFound = false;

  public ColumnValueFilter(final byte[] family, final byte[] qualifier, final CompareOperator op,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java`
#### Snippet
```java
public class PrefixFilter extends FilterBase {
  protected byte[] prefix = null;
  protected boolean passedPrefix = false;
  protected boolean filterRow = true;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class PrefixFilter extends FilterBase {
  protected byte[] prefix = null;
  protected boolean passedPrefix = false;
  protected boolean filterRow = true;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java
public class MultipleColumnPrefixFilter extends FilterBase {
  private static final Logger LOG = LoggerFactory.getLogger(MultipleColumnPrefixFilter.class);
  protected byte[] hint = null;
  protected TreeSet<byte[]> sortedPrefixes = createTreeSet();
  private final static int MAX_LOG_PREFIXES = 5;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    private boolean initialized = false;
    private boolean foundFirstRange = false;
    private boolean reversed = false;
    private final List<RowRange> sortedAndMergedRanges;
    private List<? extends BasicRowRange> ranges;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    private boolean exclusive = false;
    private boolean initialized = false;
    private boolean foundFirstRange = false;
    private boolean reversed = false;
    private final List<RowRange> sortedAndMergedRanges;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    protected boolean startRowInclusive = true;
    protected byte[] stopRow;
    protected boolean stopRowInclusive = false;

    public BasicRowRange() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
  private static class RangeIteration {
    private boolean exclusive = false;
    private boolean initialized = false;
    private boolean foundFirstRange = false;
    private boolean reversed = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
  @InterfaceAudience.Private
  private static class RangeIteration {
    private boolean exclusive = false;
    private boolean initialized = false;
    private boolean foundFirstRange = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
  private final RangeIteration ranges;

  private boolean done = false;
  private int index;
  private BasicRowRange range;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java`
#### Snippet
```java
public class PageFilter extends FilterBase {
  private long pageSize = Long.MAX_VALUE;
  private int rowsAccepted = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class WhileMatchFilter extends FilterBase {
  private boolean filterAllRemaining = false;
  private Filter filter;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class RowFilter extends CompareFilter {
  private boolean filterOutRow = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
public class QuotaFilter {
  private Set<QuotaType> types = new HashSet<>();
  private boolean hasFilters = false;
  private String namespaceRegex;
  private String tableRegex;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
   * the filter. For MUST_PASS_ALL, the two list are meaningless.
   */
  private List<ReturnCode> prevFilterRCList = null;
  private List<Cell> prevCellList = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
   */
  private List<ReturnCode> prevFilterRCList = null;
  private List<Cell> prevCellList = null;

  public FilterListWithOR(List<Filter> filters) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaRetriever.java`
#### Snippet
```java
   * Should QutoaRetriever manage the state of the connection, or leave it be.
   */
  private boolean isManagedConnection = false;

  QuotaRetriever() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
  private class RowTracker {
    private final PriorityQueue<Pair<byte[], Pair<byte[], byte[]>>> nextRows;
    private boolean initialized = false;

    RowTracker() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
  // details.
  private boolean filterRow;
  private boolean done = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/NettyHBaseSaslRpcClientHandler.java`
#### Snippet
```java

  // flag to mark if Crypto AES encryption is enable
  private boolean needProcessConnectionHeader = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/BuiltInProviderSelector.java`
#### Snippet
```java
  GssSaslClientAuthenticationProvider krbAuth = null;
  DigestSaslClientAuthenticationProvider digestAuth = null;
  Text digestAuthTokenKind = null;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/BuiltInProviderSelector.java`
#### Snippet
```java
  SimpleSaslClientAuthenticationProvider simpleAuth = null;
  GssSaslClientAuthenticationProvider krbAuth = null;
  DigestSaslClientAuthenticationProvider digestAuth = null;
  Text digestAuthTokenKind = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/BuiltInProviderSelector.java`
#### Snippet
```java
  Configuration conf;
  SimpleSaslClientAuthenticationProvider simpleAuth = null;
  GssSaslClientAuthenticationProvider krbAuth = null;
  DigestSaslClientAuthenticationProvider digestAuth = null;
  Text digestAuthTokenKind = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/BuiltInProviderSelector.java`
#### Snippet
```java

  Configuration conf;
  SimpleSaslClientAuthenticationProvider simpleAuth = null;
  GssSaslClientAuthenticationProvider krbAuth = null;
  DigestSaslClientAuthenticationProvider digestAuth = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java`
#### Snippet
```java
  ZooKeeper zookeeper;

  private int pendingRequests = 0;

  private String getId() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private final Map<byte[], byte[]> peerData;
  private final Map<String, String> configuration;
  private Map<TableName, ? extends Collection<String>> tableCFsMap = null;
  private Set<String> namespaces = null;
  // Default value is true, means replicate all user tables to peer cluster.
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  // Default value is true, means replicate all user tables to peer cluster.
  private boolean replicateAllUserTables = true;
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
  private long bandwidth = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private long bandwidth = 0;

    private boolean serial = false;

    private String remoteWALDir = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private Map<TableName, List<String>> tableCFsMap = null;

    private Set<String> namespaces = null;

    // Default value is true, means replicate all user tables to peer cluster.
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private final Map<String, String> configuration;
  private Map<TableName, ? extends Collection<String>> tableCFsMap = null;
  private Set<String> namespaces = null;
  // Default value is true, means replicate all user tables to peer cluster.
  private boolean replicateAllUserTables = true;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private boolean serial = false;

    private String remoteWALDir = null;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private Map<TableName, List<String>> excludeTableCFsMap = null;

    private Set<String> excludeNamespaces = null;

    private long bandwidth = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private Set<String> excludeNamespaces = null;

    private long bandwidth = 0;

    private boolean serial = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private Map<String, String> configuration = new HashMap<>();

    private Map<TableName, List<String>> tableCFsMap = null;

    private Set<String> namespaces = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private boolean replicateAllUserTables = true;
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
  private long bandwidth = 0;
  private final boolean serial;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
  private Map<TableName, ? extends Collection<String>> excludeTableCFsMap = null;
  private Set<String> excludeNamespaces = null;
  private long bandwidth = 0;
  private final boolean serial;
  // Used by synchronous replication
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    private boolean replicateAllUserTables = true;

    private Map<TableName, List<String>> excludeTableCFsMap = null;

    private Set<String> excludeNamespaces = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SequentialProcedure.java`
#### Snippet
```java
@InterfaceStability.Evolving
public abstract class SequentialProcedure<TEnvironment> extends Procedure<TEnvironment> {
  private boolean executed = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockAndQueue.java`
#### Snippet
```java
  private final Function<Long, Procedure<?>> procedureRetriever;
  private final ProcedureDeque queue = new ProcedureDeque();
  private Procedure<?> exclusiveLockOwnerProcedure = null;
  private int sharedLock = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockAndQueue.java`
#### Snippet
```java
  private final ProcedureDeque queue = new ProcedureDeque();
  private Procedure<?> exclusiveLockOwnerProcedure = null;
  private int sharedLock = 0;

  // ======================================================================
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java
  private Flow stateFlow = Flow.HAS_MORE_STATE;
  protected int stateCount = 0;
  private int[] states = null;

  private List<Procedure<TEnvironment>> subProcList = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java

  private Flow stateFlow = Flow.HAS_MORE_STATE;
  protected int stateCount = 0;
  private int[] states = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java
  private int[] states = null;

  private List<Procedure<TEnvironment>> subProcList = null;

  protected final int getCycles() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java`
#### Snippet
```java
   * Cycles on same state. Good for figuring if we are stuck.
   */
  private int cycles = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java`
#### Snippet
```java

  private Set<Procedure<TEnvironment>> subprocs = null;
  private ArrayList<Procedure<TEnvironment>> subprocStack = null;
  private State state = State.RUNNING;
  private int running = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java`
#### Snippet
```java
  private ArrayList<Procedure<TEnvironment>> subprocStack = null;
  private State state = State.RUNNING;
  private int running = 0;

  public synchronized boolean isFailed() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RootProcedureState.java`
#### Snippet
```java
  }

  private Set<Procedure<TEnvironment>> subprocs = null;
  private ArrayList<Procedure<TEnvironment>> subprocStack = null;
  private State state = State.RUNNING;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java

  // Unchanged after initialization
  private NonceKey nonceKey = null;
  private String owner = null;
  private long parentProcId = NO_PROC_ID;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  // Unchanged after initialization
  private NonceKey nonceKey = null;
  private String owner = null;
  private long parentProcId = NO_PROC_ID;
  private long rootProcId = NO_PROC_ID;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
   * ignore the aborting.
   */
  private volatile boolean bypass = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  private volatile long lastUpdate;

  private volatile byte[] result = null;

  private volatile boolean locked = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  private RemoteProcedureException exception = null;
  private int[] stackIndexes = null;
  private int childrenLatch = 0;

  private volatile int timeout = NO_TIMEOUT;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  private volatile boolean locked = false;

  private boolean lockedWhenLoading = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  private ProcedureState state = ProcedureState.INITIALIZING;
  private RemoteProcedureException exception = null;
  private int[] stackIndexes = null;
  private int childrenLatch = 0;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  // Runtime state, updated every operation
  private ProcedureState state = ProcedureState.INITIALIZING;
  private RemoteProcedureException exception = null;
  private int[] stackIndexes = null;
  private int childrenLatch = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
  private volatile byte[] result = null;

  private volatile boolean locked = false;

  private boolean lockedWhenLoading = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureEvent.java`
#### Snippet
```java

  private final T object;
  private boolean ready = false;
  private ProcedureDeque suspendedProcedures = new ProcedureDeque();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java

  // TODO: metrics
  private long pollCalls = 0;
  private long nullPollCalls = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java
  private final ReentrantLock schedulerLock = new ReentrantLock();
  private final Condition schedWaitCond = schedulerLock.newCondition();
  private boolean running = false;

  // TODO: metrics
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/AbstractProcedureScheduler.java`
#### Snippet
```java
  // TODO: metrics
  private long pollCalls = 0;
  private long nullPollCalls = 0;

  @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALFormatReader.java`
#### Snippet
```java
  private ProcedureStoreTracker localTracker;

  private long maxProcId = 0;

  public ProcedureWALFormatReader(final ProcedureStoreTracker tracker,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-metrics/src/main/java/org/apache/hadoop/hbase/metrics/impl/FastLongHistogram.java`
#### Snippet
```java
    // set to true when any of data has been inserted to the Bins. It is set after the counts are
    // updated.
    private volatile boolean hasData = false;

    /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
  private LinkedTransferQueue<ByteSlot> slotsCache = null;
  private Set<ProcedureWALFile> corruptedLogs = null;
  private FSDataOutputStream stream = null;
  private int runningProcCount = 1;
  private long flushLogId = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
  private long flushLogId = 0;
  private int syncMaxSlot = 1;
  private int slotIndex = 0;
  private Thread syncThread;
  private ByteSlot[] slots;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java

  private LinkedTransferQueue<ByteSlot> slotsCache = null;
  private Set<ProcedureWALFile> corruptedLogs = null;
  private FSDataOutputStream stream = null;
  private int runningProcCount = 1;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
  private FSDataOutputStream stream = null;
  private int runningProcCount = 1;
  private long flushLogId = 0;
  private int syncMaxSlot = 1;
  private int slotIndex = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java
  private final AtomicLong syncId = new AtomicLong(0);

  private LinkedTransferQueue<ByteSlot> slotsCache = null;
  private Set<ProcedureWALFile> corruptedLogs = null;
  private FSDataOutputStream stream = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java
  }

  ResultGenerator generator = null;
  String id = null;
  int batch = 1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java`
#### Snippet
```java

  ResultGenerator generator = null;
  String id = null;
  int batch = 1;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java

  TableResource tableResource;
  Integer versions = null;
  String[] columns = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java
  TableResource tableResource;
  Integer versions = null;
  String[] columns = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
      value = { "ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD", "MS_CANNOT_BE_FINAL" },
      justification = "For testing")
  public static Configuration conf = null;
  private final UserProvider userProvider;
  private Server server;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RESTServer.java`
#### Snippet
```java
  static final String REST_CSRF_ENABLED_KEY = "hbase.rest.csrf.enabled";
  static final boolean REST_CSRF_ENABLED_DEFAULT = false;
  boolean restCSRFEnabled = false;
  static final String REST_CSRF_CUSTOM_HEADER_KEY = "hbase.rest.csrf.custom.header";
  static final String REST_CSRF_CUSTOM_HEADER_DEFAULT = "X-XSRF-HEADER";
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/NamespacesInstanceResource.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(NamespacesInstanceResource.class);
  String namespace;
  boolean queryTables = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
  public static class Testing {
    protected volatile boolean killIfHasParent = true;
    protected volatile boolean killIfSuspended = false;

    /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
     * persisting all the state it needs to recover after a crash.
     */
    protected volatile boolean killBeforeStoreUpdate = false;
    protected volatile boolean toggleKillBeforeStoreUpdate = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
     * us to get stuck. This flag allows killing at what was a vulnerable time.
     */
    protected volatile boolean killAfterStoreUpdate = false;
    protected volatile boolean toggleKillAfterStoreUpdate = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
     */
    protected volatile boolean killAfterStoreUpdate = false;
    protected volatile boolean toggleKillAfterStoreUpdate = false;

    protected boolean shouldKillBeforeStoreUpdate() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
   * internal {@link Testing} class with flags set for the particular test.
   */
  volatile Testing testing = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
     */
    protected volatile boolean killBeforeStoreUpdate = false;
    protected volatile boolean toggleKillBeforeStoreUpdate = false;

    /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java`
#### Snippet
```java

  private byte[] row = HConstants.EMPTY_START_ROW;
  private byte[] endRow = null;
  private TreeSet<byte[]> columns = new TreeSet<>(Bytes.BYTES_COMPARATOR);
  private List<String> labels = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/NamespacesInstanceModel.java`
#### Snippet
```java

  // JAX-RS automatically converts Map to XMLAnyElement.
  private Map<String, String> properties = null;

  @XmlTransient
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  private RowSpec rowspec;
  private String check = null;
  private boolean returnResult = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
  private TableResource tableResource;
  private RowSpec rowspec;
  private String check = null;
  private boolean returnResult = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/filter/RestCsrfPreventionFilter.java`
#### Snippet
```java
  static final String METHODS_TO_IGNORE_DEFAULT = "GET,OPTIONS,HEAD,TRACE";
  private String headerName = HEADER_DEFAULT;
  private Set<String> methodsToIgnore = null;
  private Set<Pattern> browserUserAgents;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java`
#### Snippet
```java
  private long startTime = 0;
  private long endTime = Long.MAX_VALUE;
  private String filter = null;
  private int maxVersions = Integer.MAX_VALUE;
  private int caching = -1;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java`
#### Snippet
```java
  private List<byte[]> columns = new ArrayList<>();
  private int batch = Integer.MAX_VALUE;
  private long startTime = 0;
  private long endTime = Long.MAX_VALUE;
  private String filter = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  }

  private static volatile boolean classLoaderLoaded = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/client/Client.java`
#### Snippet
```java
  private boolean sslEnabled;
  private HttpResponse resp;
  private HttpGet httpGet = null;

  private Map<String, String> extraHeaders;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKNodeTracker.java`
#### Snippet
```java
  protected final Abortable abortable;

  private boolean stopped = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java

  private int activeZKServerIndex;
  private int tickTime = 0;

  private final Configuration configuration;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java`
#### Snippet
```java
   * The default port. If zero, we use a random port.
   */
  private int defaultClientPort = 0;

  private final List<NIOServerCnxnFactory> standaloneServerFactoryList;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java`
#### Snippet
```java
  protected Abortable abortable;
  // Used if abortable is null
  private boolean aborted = false;

  private final ZNodePaths znodePaths;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
   * old update entries can be skipped.
   */
  private boolean keepDeletes = false;
  /**
   * If true, it means tracker has incomplete information about the active/deleted procedures. It's
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
   * it's real use.
   */
  boolean partial = false;

  private long minModifiedProcId = Long.MAX_VALUE;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/WrapperAsyncFSOutput.java`
#### Snippet
```java
  private final ExecutorService executor;

  private volatile long syncedLength = 0;

  public WrapperAsyncFSOutput(Path file, FSDataOutputStream out) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java

  private static Class<?> protobufMessageLiteClass = null;
  private static Class<?> protobufMessageLiteBuilderClass = null;

  private static final boolean HAS_PARSER;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ProtobufDecoder.class);

  private static Class<?> protobufMessageLiteClass = null;
  private static Class<?> protobufMessageLiteBuilderClass = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java`
#### Snippet
```java
  private long timestamp;
  private int rowcount;
  private boolean logScannerActivity = false;
  private int logPerRowCount = 100;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java`
#### Snippet
```java
  // the length of the trailing partial chunk, this is because the packet start offset must be
  // aligned with the length of checksum chunk, so we need to resend the same data.
  private int trailingPartialChunkLength = 0;

  private long nextPacketSeqno = 0L;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java`
#### Snippet
```java
  private int trailingPartialChunkLength = 0;

  private long nextPacketSeqno = 0L;

  private ByteBuf buf;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java`
#### Snippet
```java
  // this could be different from acked block length because a packet can not start at the middle of
  // a chunk.
  private long nextPacketOffsetInBlock = 0L;

  // the length of the trailing partial chunk, this is because the packet start offset must be
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java`
#### Snippet
```java
  private final ConcurrentLinkedDeque<Callback> waitingAckQueue = new ConcurrentLinkedDeque<>();

  private volatile long ackedBlockLength = 0L;

  // this could be different from acked block length because a packet can not start at the middle of
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/RoundRobinTableInputFormat.java`
#### Snippet
```java
@InterfaceAudience.Public
public class RoundRobinTableInputFormat extends TableInputFormat {
  private Boolean hbaseRegionsizecalculatorEnableOriginalValue = null;
  /**
   * Boolean config for whether superclass should produce InputSplits with 'lengths'. If true, TIF
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    String currentQualifierName = null;
    // rowkey + family + qualifier
    String currentRowQualifierName = null;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    byte[] currentFamily = null;
    String currentFamilyName = null;
    byte[] currentQualifier = null;
    // family + qualifier
    String currentQualifierName = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    byte[] currentQualifier = null;
    // family + qualifier
    String currentQualifierName = null;
    // rowkey + family + qualifier
    String currentRowQualifierName = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    private byte[] lastRow;
    private String currentRowKey;
    byte[] currentFamily = null;
    String currentFamilyName = null;
    byte[] currentQualifier = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java`
#### Snippet
```java
    private String currentRowKey;
    byte[] currentFamily = null;
    String currentFamilyName = null;
    byte[] currentQualifier = null;
    // family + qualifier
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java`
#### Snippet
```java

  /** The configuration. */
  private Configuration conf = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java`
#### Snippet
```java
    private Cell value = null;
    private long count;
    private boolean seeked = false;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileInputFormat.java`
#### Snippet
```java
     * A private cache of the key value so it doesn't need to be loaded twice from the scanner.
     */
    private Cell value = null;
    private long count;
    private boolean seeked = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @see Scan
   */
  private Scan scan = null;
  /** The {@link Admin}. */
  private Admin admin;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
  private RegionLocator regionLocator;
  /** The reader scanning the table, can be a custom one. */
  private TableRecordReader tableRecordReader = null;
  /** The underlying {@link Connection} of the table. */
  private Connection connection;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
    private final DFSClient dfsClient;

    private int step = 0;

    public SaslNegotiateHandler(Configuration conf, String username, char[] password,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
  static class WALKeyValueMapper extends Mapper<WALKey, WALEdit, ImmutableBytesWritable, Cell> {
    private Set<String> tableSet = new HashSet<String>();
    private boolean multiTableSupport = false;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java`
#### Snippet
```java

  /** The configuration. */
  private Configuration conf = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String families = null;
  boolean allCells = false;
  static boolean shuffle = false;

  boolean bulkload = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  static boolean shuffle = false;

  boolean bulkload = false;
  Path bulkloadDir = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String peerAddress = null;
  String families = null;
  boolean allCells = false;
  static boolean shuffle = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  int versions = -1;
  String tableName = null;
  String startRow = null;
  String stopRow = null;
  String dstTableName = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  int cacheRow = -1;
  int versions = -1;
  String tableName = null;
  String startRow = null;
  String stopRow = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java

  boolean bulkload = false;
  Path bulkloadDir = null;

  boolean readingSnapshot = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  Path bulkloadDir = null;

  boolean readingSnapshot = false;
  String snapshot = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String stopRow = null;
  String dstTableName = null;
  String peerAddress = null;
  String families = null;
  boolean allCells = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String startRow = null;
  String stopRow = null;
  String dstTableName = null;
  String peerAddress = null;
  String families = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java

  final static String NAME = "copytable";
  long startTime = 0;
  long endTime = HConstants.LATEST_TIMESTAMP;
  int batch = Integer.MAX_VALUE;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java

  boolean readingSnapshot = false;
  String snapshot = null;

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String tableName = null;
  String startRow = null;
  String stopRow = null;
  String dstTableName = null;
  String peerAddress = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
  String dstTableName = null;
  String peerAddress = null;
  String families = null;
  boolean allCells = false;
  static boolean shuffle = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java`
#### Snippet
```java
   */
  static abstract class WALRecordReader<K extends WALKey> extends RecordReader<K, WALEdit> {
    private Reader reader = null;
    // visible until we can remove the deprecated HLogInputFormat
    Entry currentEntry = new Entry();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java`
#### Snippet
```java

  /** The reader scanning the table, can be a custom one. */
  private TableRecordReader tableRecordReader = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(HRegionPartitioner.class);
  private Configuration conf = null;
  // Connection and locator are not cleaned up; they just die when partitioner is done.
  private Connection connection;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java`
#### Snippet
```java
  protected byte[][] columns;
  /** The current configuration. */
  private Configuration conf = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    private MessageDigest digest;

    private boolean batchStarted = false;
    private ImmutableBytesWritable batchStartKey;
    private ImmutableBytesWritable batchHash;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    int versions = -1;
    long startTime = 0;
    long endTime = 0;
    boolean ignoreTimestamps;
    boolean rawScan;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    int scanBatch = 0;
    int versions = -1;
    long startTime = 0;
    long endTime = 0;
    boolean ignoreTimestamps;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java

    String tableName;
    String families = null;
    long batchSize = DEFAULT_BATCH_SIZE;
    int numHashFiles = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    private ImmutableBytesWritable batchStartKey;
    private ImmutableBytesWritable batchHash;
    private long batchSize = 0;
    boolean ignoreTimestamps;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    byte[] startRow = HConstants.EMPTY_START_ROW;
    byte[] stopRow = HConstants.EMPTY_END_ROW;
    int scanBatch = 0;
    int versions = -1;
    long startTime = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    String families = null;
    long batchSize = DEFAULT_BATCH_SIZE;
    int numHashFiles = 0;
    byte[] startRow = HConstants.EMPTY_START_ROW;
    byte[] stopRow = HConstants.EMPTY_END_ROW;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java`
#### Snippet
```java

  /** The configuration. */
  private Configuration conf = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      private final List<Pair<BytesWritable, Long>> files;
      private long totalSize = 0;
      private long procSize = 0;
      private int index = -1;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private int filesMode = 0;
  private int mappers = 0;
  private boolean resetTtl = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private String filesGroup = null;
  private String filesUser = null;
  private Path outputRoot = null;
  private Path inputRoot = null;
  private int bandwidthMB = Integer.MAX_VALUE;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private String snapshotName = null;
  private String targetName = null;
  private boolean overwrite = false;
  private String filesGroup = null;
  private String filesUser = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private int bandwidthMB = Integer.MAX_VALUE;
  private int filesMode = 0;
  private int mappers = 0;
  private boolean resetTtl = false;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    static final String CONF_TEST_FAILURE_COUNT = "test.snapshot.export.failure.count";
    int failuresCountToInject = 0;
    int injectedFailureCount = 0;
  }

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private boolean verifySource = true;
  private boolean verifyChecksum = true;
  private String snapshotName = null;
  private String targetName = null;
  private boolean overwrite = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private Path inputRoot = null;
  private int bandwidthMB = Integer.MAX_VALUE;
  private int filesMode = 0;
  private int mappers = 0;
  private boolean resetTtl = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private boolean verifyChecksum = true;
  private String snapshotName = null;
  private String targetName = null;
  private boolean overwrite = false;
  private String filesGroup = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private boolean overwrite = false;
  private String filesGroup = null;
  private String filesUser = null;
  private Path outputRoot = null;
  private Path inputRoot = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      extends RecordReader<BytesWritable, NullWritable> {
      private final List<Pair<BytesWritable, Long>> files;
      private long totalSize = 0;
      private long procSize = 0;
      private int index = -1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private String filesUser = null;
  private Path outputRoot = null;
  private Path inputRoot = null;
  private int bandwidthMB = Integer.MAX_VALUE;
  private int filesMode = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
  private String targetName = null;
  private boolean overwrite = false;
  private String filesGroup = null;
  private String filesUser = null;
  private Path outputRoot = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    static final String CONF_TEST_FAILURE = "test.snapshot.export.failure";
    static final String CONF_TEST_FAILURE_COUNT = "test.snapshot.export.failure.count";
    int failuresCountToInject = 0;
    int injectedFailureCount = 0;
  }
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private TaskAttemptContext context = null;
  private long numRestarts = 0;
  private long numStale = 0;
  private long timestamp;
  private int rowcount;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private Table htable = null;
  private byte[] lastSuccessfulRow = null;
  private ImmutableBytesWritable key = null;
  private Result value = null;
  private TaskAttemptContext context = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private Scan currentScan = null;
  private Table htable = null;
  private byte[] lastSuccessfulRow = null;
  private ImmutableBytesWritable key = null;
  private Result value = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java

  private ResultScanner scanner = null;
  private Scan scan = null;
  private Scan currentScan = null;
  private Table htable = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private Result value = null;
  private TaskAttemptContext context = null;
  private long numRestarts = 0;
  private long numStale = 0;
  private long timestamp;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  static final String HBASE_COUNTER_GROUP_NAME = "HBaseCounters";

  private ResultScanner scanner = null;
  private Scan scan = null;
  private Scan currentScan = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private ResultScanner scanner = null;
  private Scan scan = null;
  private Scan currentScan = null;
  private Table htable = null;
  private byte[] lastSuccessfulRow = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private ImmutableBytesWritable key = null;
  private Result value = null;
  private TaskAttemptContext context = null;
  private long numRestarts = 0;
  private long numStale = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private byte[] lastSuccessfulRow = null;
  private ImmutableBytesWritable key = null;
  private Result value = null;
  private TaskAttemptContext context = null;
  private long numRestarts = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private long timestamp;
  private int rowcount;
  private boolean logScannerActivity = false;
  private int logPerRowCount = 100;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
  private Scan scan = null;
  private Scan currentScan = null;
  private Table htable = null;
  private byte[] lastSuccessfulRow = null;
  private ImmutableBytesWritable key = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
    private Scan scan;
    private Result result = null;
    private ImmutableBytesWritable row = null;
    private ClientSideRegionScanner scanner;
    private int numOfCompleteRows = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
    private ImmutableBytesWritable row = null;
    private ClientSideRegionScanner scanner;
    private int numOfCompleteRows = 0;
    private int rowLimitPerSplit;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
    private InputSplit split;
    private Scan scan;
    private Result result = null;
    private ImmutableBytesWritable row = null;
    private ClientSideRegionScanner scanner;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
   */
  static class WriterLength {
    long written = 0;
    StoreFileWriter writer = null;
  }
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
  static class WriterLength {
    long written = 0;
    StoreFileWriter writer = null;
  }

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java
  private Configuration conf;
  private RetryCounterFactory retryCounterFactory;
  private volatile boolean connected = false;

  public ChaosAgent(Configuration conf, String quorum, String agentName) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public static class CellWritableComparablePartitioner
    extends Partitioner<CellWritableComparable, Cell> {
    private static CellWritableComparable[] START_KEYS = null;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
  public static class CellWritableComparable implements WritableComparable<CellWritableComparable> {

    private Cell kv = null;

    static {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  int sleepMsBeforeReCompare = 0;
  boolean verbose = false;
  boolean includeDeletedCells = false;
  // Source table snapshot name
  String sourceSnapshotName = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerFSAddress = null;
  // Peer cluster HBase root dir location
  String peerHBaseRootAddress = null;
  // Peer Table Name
  String peerTableName = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerSnapshotTmpDir = null;
  // Peer cluster Hadoop FS address
  String peerFSAddress = null;
  // Peer cluster HBase root dir location
  String peerHBaseRootAddress = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  boolean includeDeletedCells = false;
  // Source table snapshot name
  String sourceSnapshotName = null;
  // Temp location in source cluster to restore source snapshot
  String sourceSnapshotTmpDir = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerId = null;
  String peerQuorumAddress = null;
  String rowPrefixes = null;
  int sleepMsBeforeReCompare = 0;
  boolean verbose = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  public final static String NAME = "verifyrep";
  private final static String PEER_CONFIG_PREFIX = NAME + ".peer.";
  long startTime = 0;
  long endTime = Long.MAX_VALUE;
  int batch = -1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String sourceSnapshotTmpDir = null;
  // Peer table snapshot name
  String peerSnapshotName = null;
  // Temp location in peer cluster to restore peer snapshot
  String peerSnapshotTmpDir = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  int batch = -1;
  int versions = -1;
  String tableName = null;
  String families = null;
  String delimiter = "";
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String rowPrefixes = null;
  int sleepMsBeforeReCompare = 0;
  boolean verbose = false;
  boolean includeDeletedCells = false;
  // Source table snapshot name
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  int versions = -1;
  String tableName = null;
  String families = null;
  String delimiter = "";
  String peerId = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
    private int sleepMsBeforeReCompare;
    private String delimiter = "";
    private boolean verbose = false;
    private int batch = -1;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerSnapshotName = null;
  // Temp location in peer cluster to restore peer snapshot
  String peerSnapshotTmpDir = null;
  // Peer cluster Hadoop FS address
  String peerFSAddress = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerHBaseRootAddress = null;
  // Peer Table Name
  String peerTableName = null;

  private final static String JOB_NAME_CONF_KEY = "mapreduce.job.name";
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String families = null;
  String delimiter = "";
  String peerId = null;
  String peerQuorumAddress = null;
  String rowPrefixes = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String sourceSnapshotName = null;
  // Temp location in source cluster to restore source snapshot
  String sourceSnapshotTmpDir = null;
  // Peer table snapshot name
  String peerSnapshotName = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String peerQuorumAddress = null;
  String rowPrefixes = null;
  int sleepMsBeforeReCompare = 0;
  boolean verbose = false;
  boolean includeDeletedCells = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
  String delimiter = "";
  String peerId = null;
  String peerQuorumAddress = null;
  String rowPrefixes = null;
  int sleepMsBeforeReCompare = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    private CompactionWorker compactor = null;
    private boolean compactOnce = false;
    private boolean major = false;

    @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
    extends Mapper<LongWritable, Text, NullWritable, NullWritable> {
    private CompactionWorker compactor = null;
    private boolean compactOnce = false;
    private boolean major = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
  private static class CompactionMapper
    extends Mapper<LongWritable, Text, NullWritable, NullWritable> {
    private CompactionWorker compactor = null;
    private boolean compactOnce = false;
    private boolean major = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java

    return new HttpServletRequestWrapper(request) {
      private Map<String, String[]> parameters = null;

      @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFunction.java`
#### Snippet
```java
  public static final double COST_EPSILON = 0.0001;

  private float multiplier = 0;

  protected BalancerClusterState cluster;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java`
#### Snippet
```java
  private boolean emptyRegionServerPresent = false;
  private int numRegions = 0;
  private int numServers = 0;

  public ClusterLoadState(Map<ServerName, List<RegionInfo>> clusterState) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java`
#### Snippet
```java
  private final NavigableMap<ServerAndLoad, List<RegionInfo>> serversByLoad;
  private boolean emptyRegionServerPresent = false;
  private int numRegions = 0;
  private int numServers = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java`
#### Snippet
```java
  private final Map<ServerName, List<RegionInfo>> clusterState;
  private final NavigableMap<ServerAndLoad, List<RegionInfo>> serversByLoad;
  private boolean emptyRegionServerPresent = false;
  private int numRegions = 0;
  private int numServers = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/HeterogeneousRegionCountCostFunction.java`
#### Snippet
```java
   * Total capacity of regions for the cluster, based on the online RS and their associated rules
   */
  private int totalCapacity = 0;
  double overallUsage;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsStochasticBalancer.java`
#### Snippet
```java
   * Use the stochastic source instead of the default source.
   */
  private MetricsStochasticBalancerSource stochasticSource = null;

  public MetricsStochasticBalancer() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionReplicaGroupingCostFunction.java`
#### Snippet
```java
@InterfaceAudience.Private
abstract class RegionReplicaGroupingCostFunction extends CostFunction {
  protected long maxCost = 0;
  protected long[] costsPerGroup; // group is either server, host or rack

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
    private String[] pathSpecs;
    private AccessControlList adminsAcl;
    private boolean securityEnabled = false;
    private String usernameConfKey;
    private String keytabConfKey;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/MetricsBalancer.java`
#### Snippet
```java
public class MetricsBalancer {

  private MetricsBalancerSource source = null;

  public MetricsBalancer() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java

  // The cache for where regions are located.
  private LoadingCache<RegionInfo, HDFSBlocksDistribution> cache = null;

  RegionHDFSBlockLocationFinder() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  // For region balancing information
  private float avgRegionsOnRS = 0;
  private int maxRegionsOnRS = 0;
  private int minRegionsOnRS = Integer.MAX_VALUE;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  private Set<ServerName> minDispersionScoreServerSet = new HashSet<>();

  private float avgDispersionNum = 0;
  private float maxDispersionNum = 0;
  private Set<ServerName> maxDispersionNumServerSet = new HashSet<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  // For regions running on the favored nodes
  private int totalFavoredAssignments = 0;
  private int[] favoredNodes = new int[FavoredNodeAssignmentHelper.FAVORED_NODES_NUM];
  private float[] favoredNodesLocalitySummary =
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  // For region balancing information
  private float avgRegionsOnRS = 0;
  private int maxRegionsOnRS = 0;
  private int minRegionsOnRS = Integer.MAX_VALUE;
  private Set<ServerName> mostLoadedRSSet = new HashSet<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
    LoggerFactory.getLogger(AssignmentVerificationReport.class.getName());

  private TableName tableName = null;
  private boolean enforceLocality = false;
  private boolean isFilledUp = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  private TableName tableName = null;
  private boolean enforceLocality = false;
  private boolean isFilledUp = false;

  private int totalRegions = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  private int totalRegions = 0;
  private int totalRegionServers = 0;
  // for unassigned regions
  private List<RegionInfo> unAssignedRegionsList = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  private boolean isFilledUp = false;

  private int totalRegions = 0;
  private int totalRegionServers = 0;
  // for unassigned regions
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  private float avgDispersionScore = 0;
  private float maxDispersionScore = 0;
  private Set<ServerName> maxDispersionScoreServerSet = new HashSet<>();
  private float minDispersionScore = Float.MAX_VALUE;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  private float avgDispersionNum = 0;
  private float maxDispersionNum = 0;
  private Set<ServerName> maxDispersionNumServerSet = new HashSet<>();
  private float minDispersionNum = Float.MAX_VALUE;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  private Set<ServerName> leastLoadedRSSet = new HashSet<>();

  private float avgDispersionScore = 0;
  private float maxDispersionScore = 0;
  private Set<ServerName> maxDispersionScoreServerSet = new HashSet<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
  private float[] favoredNodesLocalitySummary =
    new float[FavoredNodeAssignmentHelper.FAVORED_NODES_NUM];
  private float actualLocalitySummary = 0;

  // For region balancing information
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java

  private TableName tableName = null;
  private boolean enforceLocality = false;
  private boolean isFilledUp = false;

```

### RedundantFieldInitialization
Field initialization to `0d` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
  private float sumMultiplier;
  // to save and report costs to JMX
  private double curOverallCost = 0d;
  private double[] tempFunctionCosts;
  private double[] curFunctionCosts;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
   */
  private static JMXConnectorServer JMX_CS = null;
  private Registry rmiRegistry = null;

  public static JMXServiceURL buildJMXServiceURL(int rmiRegistryPort, int rmiConnectorPort)
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/JMXListener.java`
#### Snippet
```java
   * coprocessor on master
   */
  private static JMXConnectorServer JMX_CS = null;
  private Registry rmiRegistry = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java

  protected static final int MIN_SERVER_BALANCE = 2;
  private volatile boolean stopped = false;

  protected volatile RegionHDFSBlockLocationFinder regionFinder;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
  protected volatile RackManager rackManager;
  protected MetricsBalancer metricsBalancer = null;
  protected ClusterMetrics clusterStatus = null;
  protected ServerName masterServerName;
  protected ClusterInfoProvider provider;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
  protected float slop;
  protected volatile RackManager rackManager;
  protected MetricsBalancer metricsBalancer = null;
  protected ClusterMetrics clusterStatus = null;
  protected ServerName masterServerName;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthCheckChore.java`
#### Snippet
```java
  private Configuration config;
  private int threshold;
  private int numTimesUnhealthy = 0;
  private long failureWindow;
  private long startWindow;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
  // shutdown. Also set by call to stop when debugging or running unit tests
  // of HRegionServer in isolation.
  protected volatile boolean stopped = false;

  // Only for testing
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java

  // Only for testing
  private boolean isShutdownHookInstalled = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(HealthChecker.class);
  private ShellCommandExecutor shexec = null;
  private String exceptionStackTrace;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
  private Optional<Cell> firstKey = Optional.empty();

  private boolean firstKeySeeked = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
    return new HFileScanner() {
      final HFileScanner delegate = s;
      public boolean atEnd = false;

      @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
  }

  private Boolean instanceOfCanUnbuffer = null;
  private CanUnbuffer unbuffer = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
   */
  private volatile FSDataInputStream stream = null;
  private volatile FSDataInputStream streamNoFsChecksum = null;
  private final Object streamNoFsChecksumFirstCreateLock = new Object();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
   * have 2 handles; however we presume checksums fail so rarely that we don't care.
   */
  private volatile FSDataInputStream stream = null;
  private volatile FSDataInputStream streamNoFsChecksum = null;
  private final Object streamNoFsChecksumFirstCreateLock = new Object();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java

  private Boolean instanceOfCanUnbuffer = null;
  private CanUnbuffer unbuffer = null;

  public FSDataInputStreamWrapper(FileSystem fs, Path path) throws IOException {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
  private static class FileLinkInputStream extends InputStream
    implements Seekable, PositionedReadable, CanSetDropBehind, CanSetReadahead, CanUnbuffer {
    private FSDataInputStream in = null;
    private Path currentPath = null;
    private long pos = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
  }

  private Path[] locations = null;

  protected FileLink() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
    private FSDataInputStream in = null;
    private Path currentPath = null;
    private long pos = 0;

    private final FileLink fileLink;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
    implements Seekable, PositionedReadable, CanSetDropBehind, CanSetReadahead, CanUnbuffer {
    private FSDataInputStream in = null;
    private Path currentPath = null;
    private long pos = 0;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java`
#### Snippet
```java

  /** The first key in the current Bloom filter chunk. */
  private byte[] firstKeyInChunk = null;

  private HFileBlockIndex.BlockIndexWriter bloomBlockIndexWriter =
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
    protected InetSocketAddress[] favoredNodes;
    private HFileContext fileContext;
    protected boolean shouldDropBehind = false;

    WriterFactory(Configuration conf, CacheConfig cacheConf) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockBuilder.java`
#### Snippet
```java
  private long prevBlockOffset;
  private ByteBuff buf;
  private boolean fillHeader = false;
  private long offset = UNSET;
  private int nextBlockOnDiskSize = UNSET;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCachesIterator.java`
#### Snippet
```java
@InterfaceAudience.Private
class BlockCachesIterator implements Iterator<CachedBlock> {
  int index = 0;
  final BlockCache[] bcs;
  Iterator<CachedBlock> current;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
  double[] meanRegionsPerTable; // mean region count per table
  int[] regionIndexToPrimaryIndex; // regionIndex -> regionIndex of the primary
  boolean hasRegionReplicas = false; // whether there is regions with replicas

  Integer[] serverIndicesSortedByRegionCount;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
  int numRegions;

  int numMovedRegions = 0; // num moved regions from the initial configuration
  Map<ServerName, List<RegionInfo>> clusterState;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
  String[] hosts;
  String[] racks;
  boolean multiServersPerHost = false; // whether or not any host has more than one server

  ArrayList<String> tables;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
     * the secondary index.
     */
    private int curTotalNonRootEntrySize = 0;

    /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
     * The accumulated size of this chunk if stored in the root index format.
     */
    private int curTotalRootSize = 0;

    /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java

    private HFileBlock.Writer blockWriter;
    private byte[] firstKey = null;

    /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
    protected long[] blockOffsets;
    protected int[] blockDataSizes;
    protected int rootCount = 0;

    // Mid-key metadata.
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java`
#### Snippet
```java

  private static class NoneEncodingState extends EncodingState {
    NoneEncoder encoder = null;
  }

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
    // Whether we returned a result for curBlock's size in recordBlockSize().
    // gets reset whenever curBlock is changed.
    private boolean providedCurrentBlockSize = false;
    // Previous blocks that were used in the course of the read
    protected final ArrayList<HFileBlock> prevBlocks = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
    protected long[] blockOffsets;
    protected int[] blockDataSizes;
    protected int rootCount = 0;

    // Mid-key metadata.
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java

  /** Total # of key/value entries, i.e. how many times add() was called. */
  protected long entryCount = 0;

  /** Used for calculating the average key length. */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
  protected final HFileContext hFileContext;

  private int maxTagsLength = 0;

  /** KeyValue version in FileInfo */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
  private List<BlockWritable> additionalLoadOnOpenData = new ArrayList<>();

  protected long maxMemstoreTS = 0;

  public HFileWriterImpl(final Configuration conf, CacheConfig cacheConf, Path path,
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
   * First cell in a block. This reference should be short-lived since we write hfiles in a burst.
   */
  protected Cell firstCellInBlock = null;

  /** May be null if we were passed a stream. */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
   * write hfiles in a burst.
   */
  private Cell lastCellOfPreviousBlock = null;

  /** Additional data items to be written to the "load-on-open" section. */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java

  /** Total uncompressed bytes, maybe calculate a compression ratio later. */
  protected long totalUncompressedBytes = 0;

  /** Meta block names. */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java

  /** Used for calculating the average key length. */
  protected long totalKeyLength = 0;

  /** Used for calculating the average value length. */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java

  /** Used for calculating the average value length. */
  protected long totalValueLength = 0;

  /** Total uncompressed bytes, maybe calculate a compression ratio later. */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java

  /** The Cell previously appended. Becomes the last cell in the file. */
  protected Cell lastCell = null;

  /** FileSystem stream to write into. */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java`
#### Snippet
```java
    private int count = 0;
    private long size = 0;
    private int countData = 0;
    private long sizeData = 0;
    private final String filename;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheStats.java`
#### Snippet
```java
  private long lastHitCachingCount = 0;
  /** Last request count read */
  private long lastRequestCount = 0;
  /** Last request caching count read */
  private long lastRequestCachingCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheStats.java`
#### Snippet
```java
  private long lastHitCount = 0;
  /** Last hit caching count read */
  private long lastHitCachingCount = 0;
  /** Last request count read */
  private long lastRequestCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheStats.java`
#### Snippet
```java
  private long lastRequestCount = 0;
  /** Last request caching count read */
  private long lastRequestCachingCount = 0;
  /** Current window index (next to be updated) */
  private int windowIndex = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheStats.java`
#### Snippet
```java
  private final long[] requestCachingCounts;
  /** Last hit count read */
  private long lastHitCount = 0;
  /** Last hit caching count read */
  private long lastHitCachingCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheStats.java`
#### Snippet
```java
  private long lastRequestCachingCount = 0;
  /** Current window index (next to be updated) */
  private int windowIndex = 0;
  /**
   * Keep running age at eviction time
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java`
#### Snippet
```java
   */
  static class CachedBlockCountsPerFile {
    private int count = 0;
    private long size = 0;
    private int countData = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java`
#### Snippet
```java
    private long size = 0;
    private int countData = 0;
    private long sizeData = 0;
    private final String filename;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java`
#### Snippet
```java
  static class CachedBlockCountsPerFile {
    private int count = 0;
    private long size = 0;
    private int countData = 0;
    private long sizeData = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java
  private int avgValueLen = -1;
  private boolean includesMemstoreTS = false;
  private boolean decodeMemstoreTS = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java

  /** Last key in the file. Filled in when we read in the file info */
  private Cell lastKeyCell = null;
  /** Average key length read from file info */
  private int avgKeyLen = -1;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java
  /** Average value length read from file info */
  private int avgValueLen = -1;
  private boolean includesMemstoreTS = false;
  private boolean decodeMemstoreTS = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java`
#### Snippet
```java
   * unit test to fail.
   */
  private static boolean generateExceptions = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    private volatile boolean go = true;
    // flag set after enter the run method, used for test
    private boolean enteringRun = false;

    public EvictionThread(LruBlockCache cache) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java

  /** Volatile boolean to track if we are in an eviction process or not */
  private volatile boolean evictionInProgress = false;

  /** Eviction thread */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    private final String name;
    private LruCachedBlockQueue queue;
    private long totalSize = 0;
    private long bucketSize;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
   * external cache as L2. Note: See org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache
   */
  private transient BlockCache victimHandler = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
   * external cache as L2. Note: See org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache
   */
  private transient BlockCache victimHandler = null;

  /** Percent of cached data blocks */
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    private volatile boolean go = true;
    // flag set after enter the run method, used for test
    private boolean enteringRun = false;

    public EvictionThread(LruAdaptiveBlockCache cache) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    private final String name;
    private final LruCachedBlockQueue queue;
    private long totalSize = 0;
    private final long bucketSize;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java

  /** Volatile boolean to track if we are in an eviction process or not */
  private volatile boolean evictionInProgress = false;

  /** Eviction thread */
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      final FSReader owner = this; // handle for inner class
      return new BlockIterator() {
        private volatile boolean freed = false;
        // Tracking all read blocks until we call freeBlocks.
        private List<HFileBlock> blockTracker = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
    // 4 byte int - bytes per checksum
    // 4 byte int - onDiskDataSizeWithHeader
    static int BLOCK_MAGIC_INDEX = 0;
    static int ON_DISK_SIZE_WITHOUT_HEADER_INDEX = 8;
    static int UNCOMPRESSED_SIZE_WITHOUT_HEADER_INDEX = 12;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.java`
#### Snippet
```java
  protected ByteBufferArray bufferArray;
  private final FileChannel fileChannel;
  private RandomAccessFile raf = null;

  public FileMmapIOEngine(String filePath, long capacity) throws IOException {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java

    private Cell prevCell = null;
    private long maxRowBytes = 0;
    private long curRowKeyLength;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    private long max = Long.MIN_VALUE;
    private long min = Long.MAX_VALUE;
    private boolean collectRanges = false;
    private final LongAdder[] rangeCounts;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
   * The row which the user wants to specify and print all the KeyValues for.
   */
  private byte[] row = null;

  private List<Path> files = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java

    long curRowBytes = 0;
    long curRowCols = 0;

    byte[] biggestRow = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
  private boolean checkRow;
  private boolean checkFamily;
  private boolean isSeekToRow = false;
  private boolean checkMobIntegrity = false;
  private Map<String, List<Path>> mobFileLocations;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    byte[] biggestRow = null;

    private Cell prevCell = null;
    private long maxRowBytes = 0;
    private long curRowKeyLength;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
        .addStats(valLen).addStats(rowSizeBytes).addStats(rowSizeCols).build();

    long curRowBytes = 0;
    long curRowCols = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
  private boolean checkFamily;
  private boolean isSeekToRow = false;
  private boolean checkMobIntegrity = false;
  private Map<String, List<Path>> mobFileLocations;
  private static final int FOUND_MOB_FILES_CACHE_CAPACITY = 50;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
    long curRowCols = 0;

    byte[] biggestRow = null;

    private Cell prevCell = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java

  private long responseCellSize = 0;
  private long responseBlockSize = 0;
  // cumulative size of serialized exceptions
  private long exceptionSize = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  protected boolean isError;
  protected ByteBufferListOutputStream cellBlockStream = null;
  protected CallCleanup reqCleanup = null;

  protected final User user;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  protected final long size; // size of current call
  protected boolean isError;
  protected ByteBufferListOutputStream cellBlockStream = null;
  protected CallCleanup reqCleanup = null;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  private long responseBlockSize = 0;
  // cumulative size of serialized exceptions
  private long exceptionSize = 0;
  private final boolean retryImmediatelySupported;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerCall.java`
#### Snippet
```java
  protected RpcCallback rpcCallback;

  private long responseCellSize = 0;
  private long responseBlockSize = 0;
  // cumulative size of serialized exceptions
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

  /** Volatile boolean to track if free space is in process or not */
  private volatile boolean freeInProgress = false;
  private transient final Lock freeSpaceLock = new ReentrantLock();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java

    private CachedEntryQueue queue;
    private long totalSize = 0;
    private long bucketSize;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
  private BucketSizeInfo[] bucketSizeInfos;
  private final long totalSize;
  private transient long usedSize = 0;

  BucketAllocator(long availableSpace, int[] bucketSizes) throws BucketAllocatorException {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/BufferChain.java`
#### Snippet
```java
class BufferChain {
  private final ByteBuffer[] buffers;
  private int remaining = 0;
  private int size;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.java`
#### Snippet
```java
  // When is this set? FindBugs wants to know! Says NP
  private ByteBuffer unwrappedDataLengthBuffer = ByteBuffer.allocate(4);
  boolean useWrap = false;

  final ConcurrentLinkedDeque<RpcResponse> responseQueue = new ConcurrentLinkedDeque<>();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.java`
#### Snippet
```java

  // If initial preamble with version and magic has been read or not.
  private boolean connectionPreambleRead = false;
  private boolean saslContextEstablished;
  private ByteBuffer unwrappedData;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected boolean retryImmediatelySupported = false;

  protected User user = null;
  protected UserGroupInformation ugi = null;
  protected SaslServerAuthenticationProviders saslProviders = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected User user = null;
  protected UserGroupInformation ugi = null;
  protected SaslServerAuthenticationProviders saslProviders = null;

  public ServerRpcConnection(RpcServer rpcServer) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected final RpcServer rpcServer;
  // If the connection header has been read or not.
  protected boolean connectionHeaderRead = false;

  protected CallCleanup callCleanup;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java

  protected User user = null;
  protected UserGroupInformation ugi = null;
  protected SaslServerAuthenticationProviders saslProviders = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
  protected boolean authenticatedWithFallback;

  protected boolean retryImmediatelySupported = false;

  protected User user = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        }
        TimerTask pollingTask = new TimerTask() {
          int tries = 0;
          long startTime = EnvironmentEdgeManager.currentTime();
          long endTime = startTime + expectedTimeout;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      long expectedTimeout = resp.getExpectedTimeout();
      TimerTask pollingTask = new TimerTask() {
        int tries = 0;
        long startTime = EnvironmentEdgeManager.currentTime();
        long endTime = startTime + expectedTimeout;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.java`
#### Snippet
```java
  private final int highPriorityLevel;

  private Abortable abortable = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  // maintains the set of client connections and handles idle timeouts
  private ConnectionManager connectionManager;
  private Listener listener = null;
  protected SimpleRpcServerResponder responder = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  private ConnectionManager connectionManager;
  private Listener listener = null;
  protected SimpleRpcServerResponder responder = null;

  /** Listens on the socket. Creates jobs for the handler threads */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
    private int currentReader = 0;
    private final int readerPendingConnectionQueueLength;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
  private class Listener extends Thread {

    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
    private int currentReader = 0;
    private final int readerPendingConnectionQueueLength;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

    private ServerSocketChannel acceptChannel = null; // the accept channel
    private Selector selector = null; // the selector that we use for the server
    private Reader[] readers = null;
    private int currentReader = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCompactionChore.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MobFileCompactionChore.class);
  private HMaster master;
  private int regionBatchSize = 0;// not set - compact all

  public MobFileCompactionChore(HMaster master) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java`
#### Snippet
```java
  // caches access count
  private final AtomicLong count = new AtomicLong(0);
  private long lastAccess = 0;
  private final LongAdder miss = new LongAdder();
  private long lastMiss = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java`
#### Snippet
```java
  private long lastMiss = 0;
  private final LongAdder evictedFileCount = new LongAdder();
  private long lastEvictedFileCount = 0;

  // a lock to sync the evict to guarantee the eviction occurs in sequence.
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java`
#### Snippet
```java
  private long lastAccess = 0;
  private final LongAdder miss = new LongAdder();
  private long lastMiss = 0;
  private final LongAdder evictedFileCount = new LongAdder();
  private long lastEvictedFileCount = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCache.java`
#### Snippet
```java

  // a ConcurrentHashMap, accesses to this map are synchronized.
  private Map<String, CachedMobFile> map = null;
  // caches access count
  private final AtomicLong count = new AtomicLong(0);
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
  volatile boolean started = false;

  protected AuthenticationTokenSecretManager authTokenSecretMgr = null;

  protected HBaseRPCErrorHandler errorHandler = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
  protected AuthenticationTokenSecretManager authTokenSecretMgr = null;

  protected HBaseRPCErrorHandler errorHandler = null;

  public static final String MAX_REQUEST_SIZE = "hbase.ipc.max.request.size";
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * for business by the call to {@link #start()}.
   */
  volatile boolean started = false;

  protected AuthenticationTokenSecretManager authTokenSecretMgr = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(DefaultMobStoreFlusher.class);
  private final Object flushLock = new Object();
  private long mobCellValueSizeThreshold = 0;
  private Path targetPath;
  private HMobStore mobStore;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
    long editsWritten = 0;
    /* Count of edits skipped to this path */
    long editsSkipped = 0;
    /* Number of nanos spent writing to this log */
    long nanosSpent = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
    long editsSkipped = 0;
    /* Number of nanos spent writing to this log */
    long nanosSpent = 0;

    final byte[] encodedRegionName;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.java`
#### Snippet
```java
  final class RecoveredEditsWriter {
    /* Count of edits written to this path */
    long editsWritten = 0;
    /* Count of edits skipped to this path */
    long editsSkipped = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java
   * Set of families in a transaction; makes for a bunch of CPU savings.
   */
  private Set<byte[]> families = null;

  public WALEdit() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
   */
  static class RegionEntryBuffer implements HeapSize {
    private long heapInBuffer = 0;
    final List<WAL.Entry> entryBuffer;
    final TableName tableName;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/EntryBuffers.java`
#### Snippet
```java
  private final Set<byte[]> currentlyWriting = new TreeSet<>(Bytes.BYTES_COMPARATOR);

  protected long totalBuffered = 0;
  protected final long maxHeapUsage;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
    private WALSplitter.PipelineController controller;
    private EntryBuffers entryBuffers;
    private OutputSink outputSink = null;

    WriterThread(WALSplitter.PipelineController controller, EntryBuffers entryBuffers,
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
  protected final List<Path> splits = new ArrayList<>();

  protected MonitoredTask status = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java

  public static class WriterThread extends Thread {
    private volatile boolean shouldStop = false;
    private WALSplitter.PipelineController controller;
    private EntryBuffers entryBuffers;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
  protected final int numThreads;

  protected CancelableProgressable reporter = null;

  protected final AtomicLong totalSkippedEdits = new AtomicLong();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConnectionCache.java`
#### Snippet
```java
    final int cleanInterval, final int maxIdleTime) throws IOException {
    Stoppable stoppable = new Stoppable() {
      private volatile boolean isStopped = false;

      @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
  protected long mobSizeThreshold;
  protected HMobStore mobStore;
  protected boolean ioOptimizedMode = false;

  /*
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    private AtomicLong writeLatency = null;
    private boolean readSuccess = false;
    private boolean writeSuccess = false;

    public RegionTaskResult(RegionInfo region, TableName tableName, ServerName serverName,
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
  private Configuration conf = null;
  private long interval = 0;
  private Sink sink = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

    protected boolean done = false;
    protected int errorCode = 0;
    protected long allowedFailures = 0;
    protected Sink sink;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    protected boolean useRegExp;
    protected boolean treatFailureAsError;
    protected boolean initialized = false;

    protected boolean done = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    private ServerName serverName;
    private ColumnFamilyDescriptor column;
    private AtomicLong readLatency = null;
    private AtomicLong writeLatency = null;
    private boolean readSuccess = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java

  private Configuration conf = null;
  private long interval = 0;
  private Sink sink = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    protected boolean initialized = false;

    protected boolean done = false;
    protected int errorCode = 0;
    protected long allowedFailures = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    private ColumnFamilyDescriptor column;
    private AtomicLong readLatency = null;
    private AtomicLong writeLatency = null;
    private boolean readSuccess = false;
    private boolean writeSuccess = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
   * True if we are to run in zookeeper 'mode'.
   */
  private boolean zookeeperMode = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
   * True if we are to run in 'regionServer' mode.
   */
  private boolean regionServerMode = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
  private static final String CANARY_TABLE_FAMILY_NAME = "Test";

  private Configuration conf = null;
  private long interval = 0;
  private Sink sink = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    protected boolean done = false;
    protected int errorCode = 0;
    protected long allowedFailures = 0;
    protected Sink sink;
    protected ExecutorService executor;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
    private AtomicLong readLatency = null;
    private AtomicLong writeLatency = null;
    private boolean readSuccess = false;
    private boolean writeSuccess = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
  private final boolean usecache;
  private volatile boolean fsvisited;
  private boolean tableDescriptorParallelLoadEnable = false;
  private ThreadPoolExecutor executor;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java

  long cachehits = 0;
  long invocations = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
  private ThreadPoolExecutor executor;

  long cachehits = 0;
  long invocations = 0;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ShutdownHookManager.java`
#### Snippet
```java
  private static ShutdownHookManager instance;

  static Class shutdownHookManagerClass = null;
  static {
    try {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  public static class HdfsEntry {
    RegionInfo hri;
    Path regionDir = null;
    long regionDirModTime = 0;
    boolean hdfsRegioninfoFilePresent = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private boolean isMerged = false;// whether this region has already been merged into another one
  private int deployedReplicaId = RegionInfo.DEFAULT_REPLICA_ID;
  private RegionInfo primaryHRIForDeployedReplica = null;

  public HbckRegionInfo(MetaEntry metaEntry) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    Path regionDir = null;
    long regionDirModTime = 0;
    boolean hdfsRegioninfoFilePresent = false;
    boolean hdfsOnlyEdits = false;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    RegionInfo hri;
    Path regionDir = null;
    long regionDirModTime = 0;
    boolean hdfsRegioninfoFilePresent = false;
    boolean hdfsOnlyEdits = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private List<ServerName> deployedOn = Lists.newArrayList(); // info on RS's
  private boolean skipChecks = false; // whether to skip further checks to this region info.
  private boolean isMerged = false;// whether this region has already been merged into another one
  private int deployedReplicaId = RegionInfo.DEFAULT_REPLICA_ID;
  private RegionInfo primaryHRIForDeployedReplica = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HbckRegionInfo.class.getName());

  private MetaEntry metaEntry = null; // info in META
  private HdfsEntry hdfsEntry = null; // info in HDFS
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java

  private MetaEntry metaEntry = null; // info in META
  private HdfsEntry hdfsEntry = null; // info in HDFS
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
  private List<ServerName> deployedOn = Lists.newArrayList(); // info on RS's
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  private List<OnlineEntry> deployedEntries = Lists.newArrayList(); // on Region Server
  private List<ServerName> deployedOn = Lists.newArrayList(); // info on RS's
  private boolean skipChecks = false; // whether to skip further checks to this region info.
  private boolean isMerged = false;// whether this region has already been merged into another one
  private int deployedReplicaId = RegionInfo.DEFAULT_REPLICA_ID;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    long regionDirModTime = 0;
    boolean hdfsRegioninfoFilePresent = false;
    boolean hdfsOnlyEdits = false;

    HdfsEntry() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java`
#### Snippet
```java
    private final E[] objects;

    private int head = 0;
    private int tail = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java`
#### Snippet
```java

    private int head = 0;
    private int tail = 0;

    @SuppressWarnings("unchecked")
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
    private String hostname;
    private String filename;
    private String excludeFile = null;
    private String designatedFile = null;
    private String defaultDir = System.getProperty("java.io.tmpdir");
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
    private String filename;
    private String excludeFile = null;
    private String designatedFile = null;
    private String defaultDir = System.getProperty("java.io.tmpdir");
    @InterfaceAudience.Private
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java

  // list of regions derived from meta entries.
  private ImmutableList<RegionInfo> regionsFromMeta = null;

  HBaseFsck hbck;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixHFileLinks = false; // fix lingering HFileLinks
  private boolean fixEmptyMetaCells = false; // fix (remove) empty REGIONINFO_QUALIFIER rows
  private boolean fixReplication = false; // fix undeleted replication queues for removed peer
  private boolean cleanReplicationBarrier = false; // clean replication barriers of a table
  private boolean fixAny = false; // Set to true if any of the fix is required.
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

  static class PrintingErrorReporter implements HbckErrorReporter {
    public int errorCount = 0;
    private int showProgress;
    // How frequently calls to progress() will create output
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixHdfsOrphans = false; // fix fs holes (missing .regioninfo)
  private boolean fixTableOrphans = false; // fix fs holes (missing .tableinfo)
  private boolean fixVersionFile = false; // fix missing hbase.version file in hdfs
  private boolean fixSplitParents = false; // fix lingering split parents
  private boolean removeParents = false; // remove split parents
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private Map<TableName, Set<String>> skippedRegions = new HashMap<>();

  private ZKWatcher zkw = null;
  private String hbckEphemeralNodePath = null;
  private boolean hbckZodeCreated = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

  private boolean rerun = false; // if we tried to fix something, rerun hbck
  private static boolean summary = false; // if we want to print less output
  private boolean checkMetaOnly = false;
  private boolean checkRegionBoundaries = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private long startMillis = EnvironmentEdgeManager.currentTime();
  private HFileCorruptionChecker hfcc;
  private int retcode = 0;
  private Path HBCK_LOCK_PATH;
  private FSDataOutputStream hbckOutFd;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixVersionFile = false; // fix missing hbase.version file in hdfs
  private boolean fixSplitParents = false; // fix lingering split parents
  private boolean removeParents = false; // remove split parents
  private boolean fixReferenceFiles = false; // fix lingering reference store file
  private boolean fixHFileLinks = false; // fix lingering HFileLinks
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean removeParents = false; // remove split parents
  private boolean fixReferenceFiles = false; // fix lingering reference store file
  private boolean fixHFileLinks = false; // fix lingering HFileLinks
  private boolean fixEmptyMetaCells = false; // fix (remove) empty REGIONINFO_QUALIFIER rows
  private boolean fixReplication = false; // fix undeleted replication queues for removed peer
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixReplication = false; // fix undeleted replication queues for removed peer
  private boolean cleanReplicationBarrier = false; // clean replication barriers of a table
  private boolean fixAny = false; // Set to true if any of the fix is required.

  // limit checking/fixes to listed tables, if empty attempt to check/fix all
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixHdfsOverlaps = false; // fix fs overlaps (risky)
  private boolean fixHdfsOrphans = false; // fix fs holes (missing .regioninfo)
  private boolean fixTableOrphans = false; // fix fs holes (missing .tableinfo)
  private boolean fixVersionFile = false; // fix missing hbase.version file in hdfs
  private boolean fixSplitParents = false; // fix lingering split parents
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixReferenceFiles = false; // fix lingering reference store file
  private boolean fixHFileLinks = false; // fix lingering HFileLinks
  private boolean fixEmptyMetaCells = false; // fix (remove) empty REGIONINFO_QUALIFIER rows
  private boolean fixReplication = false; // fix undeleted replication queues for removed peer
  private boolean cleanReplicationBarrier = false; // clean replication barriers of a table
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixSplitParents = false; // fix lingering split parents
  private boolean removeParents = false; // remove split parents
  private boolean fixReferenceFiles = false; // fix lingering reference store file
  private boolean fixHFileLinks = false; // fix lingering HFileLinks
  private boolean fixEmptyMetaCells = false; // fix (remove) empty REGIONINFO_QUALIFIER rows
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

  private ZKWatcher zkw = null;
  private String hbckEphemeralNodePath = null;
  private boolean hbckZodeCreated = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixHdfsHoles = false; // fix fs holes?
  private boolean fixHdfsOverlaps = false; // fix fs overlaps (risky)
  private boolean fixHdfsOrphans = false; // fix fs holes (missing .regioninfo)
  private boolean fixTableOrphans = false; // fix fs holes (missing .tableinfo)
  private boolean fixVersionFile = false; // fix missing hbase.version file in hdfs
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixTableOrphans = false; // fix fs holes (missing .tableinfo)
  private boolean fixVersionFile = false; // fix missing hbase.version file in hdfs
  private boolean fixSplitParents = false; // fix lingering split parents
  private boolean removeParents = false; // remove split parents
  private boolean fixReferenceFiles = false; // fix lingering reference store file
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private static boolean summary = false; // if we want to print less output
  private boolean checkMetaOnly = false;
  private boolean checkRegionBoundaries = false;
  private boolean ignorePreCheckPermission = false; // if pre-check permission

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private Path sidelineDir = null;

  private boolean rerun = false; // if we tried to fix something, rerun hbck
  private static boolean summary = false; // if we want to print less output
  private boolean checkMetaOnly = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private long timelag = DEFAULT_TIME_LAG; // tables whose modtime is older
  private static boolean forceExclusive = false; // only this hbck can modify HBase
  private boolean fixAssignments = false; // fix assignment errors?
  private boolean fixMeta = false; // fix meta errors?
  private boolean checkHdfs = true; // load and check fs consistency?
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private static boolean details = false; // do we display the full report
  private long timelag = DEFAULT_TIME_LAG; // tables whose modtime is older
  private static boolean forceExclusive = false; // only this hbck can modify HBase
  private boolean fixAssignments = false; // fix assignment errors?
  private boolean fixMeta = false; // fix meta errors?
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private ZKWatcher zkw = null;
  private String hbckEphemeralNodePath = null;
  private boolean hbckZodeCreated = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean checkHdfs = true; // load and check fs consistency?
  private boolean fixHdfsHoles = false; // fix fs holes?
  private boolean fixHdfsOverlaps = false; // fix fs overlaps (risky)
  private boolean fixHdfsOrphans = false; // fix fs holes (missing .regioninfo)
  private boolean fixTableOrphans = false; // fix fs holes (missing .tableinfo)
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean rerun = false; // if we tried to fix something, rerun hbck
  private static boolean summary = false; // if we want to print less output
  private boolean checkMetaOnly = false;
  private boolean checkRegionBoundaries = false;
  private boolean ignorePreCheckPermission = false; // if pre-check permission
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private static boolean forceExclusive = false; // only this hbck can modify HBase
  private boolean fixAssignments = false; // fix assignment errors?
  private boolean fixMeta = false; // fix meta errors?
  private boolean checkHdfs = true; // load and check fs consistency?
  private boolean fixHdfsHoles = false; // fix fs holes?
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private int maxOverlapsToSideline = DEFAULT_OVERLAPS_TO_SIDELINE;
  private boolean sidelineBigOverlaps = false; // sideline overlaps with >maxMerge regions
  private Path sidelineDir = null;

  private boolean rerun = false; // if we tried to fix something, rerun hbck
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  // maximum number of overlapping regions to sideline
  private int maxOverlapsToSideline = DEFAULT_OVERLAPS_TO_SIDELINE;
  private boolean sidelineBigOverlaps = false; // sideline overlaps with >maxMerge regions
  private Path sidelineDir = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   * Options
   ***********/
  private static boolean details = false; // do we display the full report
  private long timelag = DEFAULT_TIME_LAG; // tables whose modtime is older
  private static boolean forceExclusive = false; // only this hbck can modify HBase
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   *********/
  final private HbckErrorReporter errors;
  int fixes = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean checkMetaOnly = false;
  private boolean checkRegionBoundaries = false;
  private boolean ignorePreCheckPermission = false; // if pre-check permission

  /*********
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    RetryCounter retryCounter;
    private final Configuration conf;
    private Path hbckLockPath = null;

    public FileLockCallable(Configuration conf, RetryCounter retryCounter) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixEmptyMetaCells = false; // fix (remove) empty REGIONINFO_QUALIFIER rows
  private boolean fixReplication = false; // fix undeleted replication queues for removed peer
  private boolean cleanReplicationBarrier = false; // clean replication barriers of a table
  private boolean fixAny = false; // Set to true if any of the fix is required.

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  private boolean fixMeta = false; // fix meta errors?
  private boolean checkHdfs = true; // load and check fs consistency?
  private boolean fixHdfsHoles = false; // fix fs holes?
  private boolean fixHdfsOverlaps = false; // fix fs overlaps (risky)
  private boolean fixHdfsOrphans = false; // fix fs holes (missing .regioninfo)
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactor.java`
#### Snippet
```java
  private int numServers = -1;
  private int numRegions = -1;
  private boolean skipWait = false;

  MajorCompactor() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HFileArchiveManager.class);
  private final ZKWatcher zooKeeper;
  private volatile boolean stopped = false;

  public HFileArchiveManager(Connection connection, Configuration conf)
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.java`
#### Snippet
```java
  private HFileArchiveTableMonitor monitor;
  private String archiveHFileZNode;
  private boolean stopped = false;

  private TableHFileArchiveTracker(ZKWatcher watcher, HFileArchiveTableMonitor monitor) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java`
#### Snippet
```java
  private final boolean snapshotAlreadyRestored;

  private ClientSideRegionScanner currentRegionScanner = null;
  private int currentRegion = -1;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/TableSnapshotScanner.java`
#### Snippet
```java
  private int currentRegion = -1;

  private int numOfCompleteRows = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java

  protected class LockHeartbeatWorker extends Thread {
    private volatile boolean shutdown = false;

    public LockHeartbeatWorker(final String desc) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java
  // set to a non-zero value for tweaking sleep time during testing so that worker doesn't wait
  // for long time periods between heartbeats.
  private long testingSleepTime = 0;

  private Long procId = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java
  private long testingSleepTime = 0;

  private Long procId = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java`
#### Snippet
```java
@InterfaceAudience.Private
final public class FilterWrapper extends Filter {
  Filter filter = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java`
#### Snippet
```java
   */
  public static class LocalHMaster extends HMaster {
    private MiniZooKeeperCluster zkcluster = null;

    public LocalHMaster(Configuration conf) throws IOException {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
  public static final int DEFAULT_STATUS_PUBLISH_PERIOD = 10000;

  private long lastMessageTime = 0;
  private final HMaster master;
  private final int messagePeriod; // time between two message
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
  private final ConcurrentMap<ServerName, Integer> lastSent = new ConcurrentHashMap<>();
  private Publisher publisher;
  private boolean connected = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
  private List<String> clusterIds = new ArrayList<>();
  private boolean replicate = true;
  private boolean failIfNeedSplitHFile = false;

  public BulkLoadHFilesTool(Configuration conf) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java

  private boolean persistFlushedSequenceId = true;
  private volatile boolean isFlushSeqIdPersistInProgress = false;
  /** File on hdfs to store last flushed sequence id of regions */
  private static final String LAST_FLUSHED_SEQ_ID_FILE = ".lastflushedseqids";
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
  private static final float NOT_CURRENT_HOST_PENALTY = 0.1f;

  private static boolean USE_MUNKRES_FOR_PLACING_SECONDARY_AND_TERTIARY = false;

  private Configuration conf;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckReport.java`
#### Snippet
```java
    new HashMap<>();

  private Instant checkingStartTimestamp = null;
  private Instant checkingEndTimestamp = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckReport.java`
#### Snippet
```java

  private Instant checkingStartTimestamp = null;
  private Instant checkingEndTimestamp = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckChore.java`
#### Snippet
```java
   * Saved report from last time this chore ran. Check its date.
   */
  private volatile HbckReport lastReport = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckChore.java`
#### Snippet
```java
  private volatile boolean running = false;

  private boolean disabled = false;

  public HbckChore(MasterServices master) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckChore.java`
#### Snippet
```java
   * When running, the "snapshot" may be changed when this round's checking finish.
   */
  private volatile boolean running = false;

  private boolean disabled = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
  private TimeoutMonitor timeoutMonitor;

  private volatile Set<ServerName> deadWorkers = null;
  private final Object deadWorkersLock = new Object();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
    public int installed = 0;
    public int done = 0;
    public int error = 0;
    public volatile boolean isDead = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
    public int done = 0;
    public int error = 0;
    public volatile boolean isDead = false;

    @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
  @InterfaceAudience.Private
  public static class TaskBatch {
    public int installed = 0;
    public int done = 0;
    public int error = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
  public static class TaskBatch {
    public int installed = 0;
    public int done = 0;
    public int error = 0;
    public volatile boolean isDead = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java`
#### Snippet
```java
   */
  private class TimeoutMonitor extends ScheduledChore {
    private long lastLog = 0;

    public TimeoutMonitor(final int period, Stoppable stopper) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/MasterRegionFlusherAndCompactor.java`
#### Snippet
```java
  private boolean compactRequest = false;

  private volatile boolean closed = false;

  MasterRegionFlusherAndCompactor(Configuration conf, Abortable abortable, HRegion region,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/MasterRegionFlusherAndCompactor.java`
#### Snippet
```java
  private final Lock compactLock = new ReentrantLock();

  private boolean compactRequest = false;

  private volatile boolean closed = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/MasterRegionFlusherAndCompactor.java`
#### Snippet
```java
  private final Condition flushCond = flushLock.newCondition();

  private boolean flushRequest = false;

  private long lastFlushTime;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/BaseTimeToLiveFileCleaner.java`
#### Snippet
```java
  private long ttlMs;

  private volatile boolean stopped = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HFileLinkCleaner.class);

  private FileSystem fs = null;
  private ReentrantReadWriteLock lock = new ReentrantReadWriteLock();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKeyImpl.java`
#### Snippet
```java
   * Used during WAL replay; the sequenceId of the edit when it came into the system.
   */
  private long origLogSeqNum = 0;

  /** Time at which this edit was written. */
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/BaseHFileCleanerDelegate.java`
#### Snippet
```java
public abstract class BaseHFileCleanerDelegate extends BaseFileCleanerDelegate {

  private boolean stopped = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/CatalogJanitorReport.java`
#### Snippet
```java
  final Map<RegionInfo, Result> splitParents = new TreeMap<>(new SplitParentFirstComparator());
  final Map<RegionInfo, Result> mergedRegions = new TreeMap<>(RegionInfo.COMPARATOR);
  int count = 0;

  final List<Pair<RegionInfo, RegionInfo>> holes = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/ReportMakingVisitor.java`
#### Snippet
```java
   * overlap unless we keep a running 'highest-endpoint-seen'.
   */
  private RegionInfo highestEndKeyRegionInfo = null;

  ReportMakingVisitor(MasterServices services) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/ReportMakingVisitor.java`
#### Snippet
```java
   * RegionInfo from previous row.
   */
  private RegionInfo previous = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockManager.java`
#### Snippet
```java
    private final String description;

    private LockProcedure proc = null;

    public MasterLock(final String namespace, final LockType type, final String description) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
  private final CountDownLatch lockAcquireLatch;

  private volatile boolean suspended = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(SnapshotFileCache.class);
  private volatile boolean stop = false;
  private final FileSystem fs, workingFs;
  private final SnapshotFileInspector fileInspector;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java`
#### Snippet
```java
    byte[] data;

    boolean delete = false;

    synchronized void set(byte[] data) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/MaintenanceLoadBalancer.java`
#### Snippet
```java
public class MaintenanceLoadBalancer implements LoadBalancer {

  private volatile boolean stopped = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private MasterProcedureManagerHost mpmHost;

  private RegionsRecoveryChore regionsRecoveryChore = null;

  private RegionsRecoveryConfigManager regionsRecoveryConfigManager = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  // flag set after master services are started,
  // initialization may have not completed yet.
  volatile boolean serviceStarted = false;

  // Maximum time we should run balancer for
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private ClusterStatusChore clusterStatusChore;
  private ClusterStatusPublisher clusterStatusPublisherChore = null;
  private SnapshotCleanerChore snapshotCleanerChore = null;

  private HbckChore hbckChore;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private RegionsRecoveryChore regionsRecoveryChore = null;

  private RegionsRecoveryConfigManager regionsRecoveryConfigManager = null;
  // it is assigned after 'initialized' guard set to true, so should be volatile
  private volatile MasterQuotaManager quotaManager;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  // flag set after we become the active master (used for testing)
  private volatile boolean activeMaster = false;

  // flag set after we complete initialization once active
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private RSGroupBasedLoadBalancer balancer;
  private BalancerChore balancerChore;
  private static boolean disableBalancerChoreForTest = false;
  private RegionNormalizerManager regionNormalizerManager;
  private ClusterStatusChore clusterStatusChore;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  private RegionNormalizerManager regionNormalizerManager;
  private ClusterStatusChore clusterStatusChore;
  private ClusterStatusPublisher clusterStatusPublisherChore = null;
  private SnapshotCleanerChore snapshotCleanerChore = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ModifyTableProcedure.class);

  private TableDescriptor unmodifiedTableDescriptor = null;
  private TableDescriptor modifiedTableDescriptor;
  private boolean deleteColumnFamilyInModify;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
    private final Set<RemoteProcedure> remoteProcedures;

    private int numberOfAttemptsSoFar = 0;
    private long maxWaitTime = -1;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
    private static final int DEFAULT_RS_RPC_RETRY_INTERVAL = 100;

    private ExecuteProceduresRequest.Builder request = null;

    public ExecuteProceduresRemoteCall(final ServerName serverName,
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java

    private boolean hasResult = false;
    private byte[] result = null;

    public ProcedureFuture(ProcedureExecutor<MasterProcedureEnv> procExec, Procedure<?> proc) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.java`
#### Snippet
```java
    private final Procedure<?> proc;

    private boolean hasResult = false;
    private byte[] result = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java

  // Is snapshot feature enabled?
  private boolean isSnapshotSupported = false;

  // Snapshot handlers map, with table name as key.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  private static final class HFileDeleteTask {

    boolean done = false;
    boolean result;
    final Path filePath;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedurePrepareLatch.java`
#### Snippet
```java
    private final CountDownLatch latch = new CountDownLatch(1);

    private IOException exception = null;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/FairQueue.java`
#### Snippet
```java
public class FairQueue<T extends Comparable<T>> {

  private Queue<T> queueHead = null;
  private int size = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/FairQueue.java`
#### Snippet
```java

  private Queue<T> queueHead = null;
  private int size = 0;

  public boolean hasRunnables() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java`
#### Snippet
```java
   * Whether DeadServer knows that we are processing it.
   */
  private boolean notifiedDeadServer = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java`
#### Snippet
```java
  private List<RegionInfo> regionsOnCrashedServer;

  private boolean carryingMeta = false;
  private boolean shouldSplitWal;
  private MonitoredTask status;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java

  // Monitor
  private MonitoredTask monitorStatus = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java
  private List<RegionInfo> regionsToRestore = null;
  private List<RegionInfo> regionsToRemove = null;
  private List<RegionInfo> regionsToAdd = null;
  private Map<String, Pair<String, String>> parentsToChildrenPairMap = new HashMap<>();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java

  private TableDescriptor modifiedTableDescriptor;
  private List<RegionInfo> regionsToRestore = null;
  private List<RegionInfo> regionsToRemove = null;
  private List<RegionInfo> regionsToAdd = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java
  private TableDescriptor modifiedTableDescriptor;
  private List<RegionInfo> regionsToRestore = null;
  private List<RegionInfo> regionsToRemove = null;
  private List<RegionInfo> regionsToAdd = null;
  private Map<String, Pair<String, String>> parentsToChildrenPairMap = new HashMap<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java`
#### Snippet
```java
  private boolean restoreAcl;
  private String customSFT;
  private List<RegionInfo> newRegions = null;
  private Map<String, Pair<String, String>> parentsToChildrenPairMap = new HashMap<>();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java`
#### Snippet
```java

  // Monitor
  private MonitoredTask monitorStatus = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignProcedure.java`
#### Snippet
```java
public class AssignProcedure extends RegionTransitionProcedure {

  private boolean forceNewPlan = false;

  protected volatile ServerName targetServer;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java`
#### Snippet
```java
  // UnassignProcedure, so do not use it for critical condition as the data maybe stale and unsync
  // with the data in meta.
  private volatile ServerName lastHost = null;
  /**
   * A Region-in-Transition (RIT) moves through states. See {@link State} for complete list. A
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java`
#### Snippet
```java
   * {@link #setState(RegionState.State, RegionState.State...)}.
   */
  private volatile long lastUpdate = 0;

  private volatile long openSeqNum = HConstants.NO_SEQNUM;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java`
#### Snippet
```java
  // volatile only for getLastUpdate and test usage, the upper layer should sync on the
  // RegionStateNode before accessing usually.
  private volatile TransitRegionStateProcedure procedure = null;
  private volatile ServerName regionLocation = null;
  // notice that, the lastHost will only be updated when a region is successfully CLOSED through
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java`
#### Snippet
```java
  // RegionStateNode before accessing usually.
  private volatile TransitRegionStateProcedure procedure = null;
  private volatile ServerName regionLocation = null;
  // notice that, the lastHost will only be updated when a region is successfully CLOSED through
  // UnassignProcedure, so do not use it for critical condition as the data maybe stale and unsync
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
  private final ServerQueue[] serverBuckets = new ServerQueue[128];
  private TableQueue tableMap = null;
  private PeerQueue peerMap = null;
  private MetaQueue metaMap = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
  private TableQueue tableMap = null;
  private PeerQueue peerMap = null;
  private MetaQueue metaMap = null;

  private final SchemaLocking locking;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java

  private final ServerQueue[] serverBuckets = new ServerQueue[128];
  private TableQueue tableMap = null;
  private PeerQueue peerMap = null;
  private MetaQueue metaMap = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java`
#### Snippet
```java
  private final Object startStopLock = new Object();
  private boolean started = false;
  private boolean stopped = false;

  RegionNormalizerManager(@NonNull final RegionNormalizerStateStore regionNormalizerStateStore,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java`
#### Snippet
```java

  private final Object startStopLock = new Object();
  private boolean started = false;
  private boolean stopped = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    private final RegionStateNode regionNode;

    private volatile Exception exception = null;
    private AtomicInteger retries = new AtomicInteger();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java`
#### Snippet
```java
    LoggerFactory.getLogger(SyncReplicationReplayWALProcedure.class);

  private ServerName worker = null;

  private List<String> wals;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
public class QuotaState {
  protected long lastUpdate = 0;
  protected long lastQuery = 0;

  protected QuotaLimiter globalLimiter = NoopQuotaLimiter.get();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      + "are mostly synchronized...but to me it looks like they are totally synchronized")
public class QuotaState {
  protected long lastUpdate = 0;
  protected long lastQuery = 0;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotasObserver.java`
#### Snippet
```java
  private CoprocessorEnvironment cpEnv;
  private Configuration conf;
  private boolean quotasEnabled = false;
  private MasterServices masterServices;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerSpaceQuotaManager.java`
#### Snippet
```java
  private SpaceQuotaRefresherChore spaceQuotaRefresher;
  private AtomicReference<Map<TableName, SpaceQuotaSnapshot>> currentQuotaSnapshots;
  private boolean started = false;
  private final ConcurrentHashMap<TableName, SpaceViolationPolicyEnforcement> enforcedPolicies;
  private SpaceViolationPolicyEnforcementFactory factory;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java`
#### Snippet
```java
  private final RegionServerServices rsServices;

  private QuotaCache quotaCache = null;
  private volatile boolean rpcThrottleEnabled;
  // Storage for quota rpc throttle
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaRefresherChore.java`
#### Snippet
```java
  private final RegionServerSpaceQuotaManager manager;
  private final Connection conn;
  private boolean quotaTablePresent = false;

  public SpaceQuotaRefresherChore(RegionServerSpaceQuotaManager manager, Connection conn) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
      + "but to me it looks like they are totally synchronized")
public class UserQuotaState extends QuotaState {
  private Map<String, QuotaLimiter> namespaceLimiters = null;
  private Map<TableName, QuotaLimiter> tableLimiters = null;
  private boolean bypassGlobals = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
  private Map<String, QuotaLimiter> namespaceLimiters = null;
  private Map<TableName, QuotaLimiter> tableLimiters = null;
  private boolean bypassGlobals = false;

  public UserQuotaState() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
public class UserQuotaState extends QuotaState {
  private Map<String, QuotaLimiter> namespaceLimiters = null;
  private Map<TableName, QuotaLimiter> tableLimiters = null;
  private boolean bypassGlobals = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/MasterQuotaManager.java`
#### Snippet
```java
  private NamedLock<String> userLocks;
  private NamedLock<String> regionServerLocks;
  private boolean initialized = false;
  private NamespaceAuditor namespaceQuotaManager;
  private ConcurrentHashMap<RegionInfo, SizeSnapshotWithTimestamp> regionSizes;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  protected long readAvailable = 0;
  // estimated quota
  protected long writeConsumed = 0;
  protected long readConsumed = 0;
  protected long writeCapacityUnitConsumed = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  // estimated quota
  protected long writeConsumed = 0;
  protected long readConsumed = 0;
  protected long writeCapacityUnitConsumed = 0;
  protected long readCapacityUnitConsumed = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  // to adjust quota amount. Also used by ExceedOperationQuota which is a subclass
  // of DefaultOperationQuota
  protected long writeDiff = 0;
  protected long readDiff = 0;
  protected long writeCapacityUnitDiff = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  protected long readConsumed = 0;
  protected long writeCapacityUnitConsumed = 0;
  protected long readCapacityUnitConsumed = 0;
  // real consumed quota
  private final long[] operationSize;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java

  // the available read/write quota size in bytes
  protected long readAvailable = 0;
  // estimated quota
  protected long writeConsumed = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  protected long writeDiff = 0;
  protected long readDiff = 0;
  protected long writeCapacityUnitDiff = 0;
  protected long readCapacityUnitDiff = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  protected long writeConsumed = 0;
  protected long readConsumed = 0;
  protected long writeCapacityUnitConsumed = 0;
  protected long readCapacityUnitConsumed = 0;
  // real consumed quota
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  protected long readDiff = 0;
  protected long writeCapacityUnitDiff = 0;
  protected long readCapacityUnitDiff = 0;

  public DefaultOperationQuota(final Configuration conf, final QuotaLimiter... limiters) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/DefaultOperationQuota.java`
#### Snippet
```java
  // of DefaultOperationQuota
  protected long writeDiff = 0;
  protected long readDiff = 0;
  protected long writeCapacityUnitDiff = 0;
  protected long readCapacityUnitDiff = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
  private RateLimiter readCapacityUnitLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
  private RateLimiter readCapacityUnitLimiter = null;

  private TimeBasedLimiter() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
  private RateLimiter writeCapacityUnitLimiter = null;
  private RateLimiter readCapacityUnitLimiter = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
  private RateLimiter readSizeLimiter = null;
  private RateLimiter reqCapacityUnitLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private RateLimiter reqsLimiter = null;
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
  private RateLimiter readReqsLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
public class TimeBasedLimiter implements QuotaLimiter {
  private static final Configuration conf = HBaseConfiguration.create();
  private RateLimiter reqsLimiter = null;
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
  private static final Configuration conf = HBaseConfiguration.create();
  private RateLimiter reqsLimiter = null;
  private RateLimiter reqSizeLimiter = null;
  private RateLimiter writeReqsLimiter = null;
  private RateLimiter writeSizeLimiter = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
  private final ConcurrentMap<String, QuotaState> regionServerQuotaCache =
    new ConcurrentHashMap<>();
  private volatile boolean exceedThrottleQuotaEnabled = false;
  // factors used to divide cluster scope quota into machine scope quota
  private volatile double machineQuotaFactor = 1;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java
  // TODO: Remove this once we have the notification bus
  private class QuotaRefresherChore extends ScheduledChore {
    private long lastUpdate = 0;

    public QuotaRefresherChore(final int period, final Stoppable stoppable) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaCache.java`
#### Snippet
```java

  // for testing purpose only, enforce the cache to be always refreshed
  static boolean TEST_FORCE_REFRESH = false;

  private final ConcurrentMap<String, QuotaState> namespaceQuotaCache = new ConcurrentHashMap<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.java`
#### Snippet
```java
    return new SizedCellScanner() {
      private final Iterator<List<? extends Cell>> entries = cells.iterator();
      private Iterator<? extends Cell> currentIterator = null;
      private Cell currentCell;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java`
#### Snippet
```java
    public static final long KEEP_ALIVE_TIME_MILLIS_DEFAULT = 1000;
    private int corePoolSize = -1;
    private boolean allowCoreThreadTimeout = false;
    private long keepAliveTimeMillis = KEEP_ALIVE_TIME_MILLIS_DEFAULT;
    private ExecutorType executorType;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java`
#### Snippet
```java

  private boolean hasForwardedToken = false;
  private Token<?> userToken = null;
  private FileSystem fs = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java`
#### Snippet
```java
  private final String renewer;

  private boolean hasForwardedToken = false;
  private Token<?> userToken = null;
  private FileSystem fs = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/FsDelegationToken.java`
#### Snippet
```java
  private boolean hasForwardedToken = false;
  private Token<?> userToken = null;
  private FileSystem fs = null;

  /*
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java

  private class LeaderElector extends Thread implements Stoppable {
    private boolean stopped = false;
    /** Flag indicating whether we're in charge of rolling/expiring keys */
    private boolean isMaster = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
    private boolean stopped = false;
    /** Flag indicating whether we're in charge of rolling/expiring keys */
    private boolean isMaster = false;
    private ZKLeaderManager zkLeader;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclCleaner.java`
#### Snippet
```java

  private HMaster master;
  private boolean userScanSnapshotEnabled = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java

  public static class Params {
    private String namespace = null;
    private TableName tableName = null;
    private Map<byte[], ? extends Collection<?>> families = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
    private Map<byte[], ? extends Collection<?>> families = null;
    byte[] family = null;
    byte[] qualifier = null;
    // For extra parameters to be shown in audit log
    private final Map<String, String> extraParams = new HashMap<String, String>(2);
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
    private String namespace = null;
    private TableName tableName = null;
    private Map<byte[], ? extends Collection<?>> families = null;
    byte[] family = null;
    byte[] qualifier = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
    private TableName tableName = null;
    private Map<byte[], ? extends Collection<?>> families = null;
    byte[] family = null;
    byte[] qualifier = null;
    // For extra parameters to be shown in audit log
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
  public static class Params {
    private String namespace = null;
    private TableName tableName = null;
    private Map<byte[], ? extends Collection<?>> families = null;
    byte[] family = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
  public static class InputUser extends User {
    private String name;
    private String shortName = null;
    private String[] groups;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
  public static final String FALLBACK_GROUP_ENABLE_KEY = "hbase.rsgroup.fallback.enable";

  private volatile boolean fallbackEnabled = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java

  private SnapshotScannerHDFSAclHelper hdfsAclHelper = null;
  private PathHelper pathHelper = null;
  private MasterServices masterServices = null;
  private volatile boolean initialized = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SnapshotScannerHDFSAclController.class);

  private SnapshotScannerHDFSAclHelper hdfsAclHelper = null;
  private PathHelper pathHelper = null;
  private MasterServices masterServices = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
  private MasterServices masterServices = null;
  private volatile boolean initialized = false;
  private volatile boolean aclTableInitialized = false;
  /** Provider for mapping principal names to Users */
  private UserProvider userProvider;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
  private SnapshotScannerHDFSAclHelper hdfsAclHelper = null;
  private PathHelper pathHelper = null;
  private MasterServices masterServices = null;
  private volatile boolean initialized = false;
  private volatile boolean aclTableInitialized = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
  private PathHelper pathHelper = null;
  private MasterServices masterServices = null;
  private volatile boolean initialized = false;
  private volatile boolean aclTableInitialized = false;
  /** Provider for mapping principal names to Users */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    private long statTimestamp;
    private long oldestRITTime = 0;
    private int totalRITsTwiceThreshold = 0;
    private int totalRITs = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    private long oldestRITTime = 0;
    private int totalRITsTwiceThreshold = 0;
    private int totalRITs = 0;

    public RegionInTransitionStat(final Configuration conf) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    private final int ritThreshold;

    private HashMap<String, RegionState> ritsOverThreshold = null;
    private long statTimestamp;
    private long oldestRITTime = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    private HashMap<String, RegionState> ritsOverThreshold = null;
    private long statTimestamp;
    private long oldestRITTime = 0;
    private int totalRITsTwiceThreshold = 0;
    private int totalRITs = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelServiceManager.java`
#### Snippet
```java

  private volatile VisibilityLabelService visibilityLabelService = null;
  private String vlsClazzName = null;

  private VisibilityLabelServiceManager() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelServiceManager.java`
#### Snippet
```java
  private static final VisibilityLabelServiceManager INSTANCE = new VisibilityLabelServiceManager();

  private volatile VisibilityLabelService visibilityLabelService = null;
  private String vlsClazzName = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
  private class RSGroupStartupWorker extends Thread {
    private final Logger LOG = LoggerFactory.getLogger(RSGroupStartupWorker.class);
    private volatile boolean online = false;

    RSGroupStartupWorker() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/CreateSnapshot.java`
#### Snippet
```java
public class CreateSnapshot extends AbstractHBaseTool {
  private SnapshotType snapshotType = SnapshotType.FLUSH;
  private TableName tableName = null;
  private String snapshotName = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/CreateSnapshot.java`
#### Snippet
```java
  private SnapshotType snapshotType = SnapshotType.FLUSH;
  private TableName tableName = null;
  private String snapshotName = null;

  public static void main(String[] args) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceStateManager.java`
#### Snippet
```java
  private ConcurrentMap<String, NamespaceTableAndRegionInfo> nsStateCache;
  private MasterServices master;
  private volatile boolean initialized = false;

  public NamespaceStateManager(MasterServices masterServices) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  private Configuration conf;
  private volatile boolean initialized = false;
  private boolean checkAuths = false;
  /** Mapping of scanner instances to the user who created them */
  private Map<InternalScanner, String> scannerOwners = new MapMaker().weakKeys().makeMap();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    LoggerFactory.getLogger("SecurityLogger." + VisibilityController.class.getName());
  // flags if we are running on a region of the 'labels' table
  private boolean labelsRegion = false;
  // Flag denoting whether AcessController is available or not.
  private boolean accessControllerAvailable = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  private boolean labelsRegion = false;
  // Flag denoting whether AcessController is available or not.
  private boolean accessControllerAvailable = false;
  private Configuration conf;
  private volatile boolean initialized = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
  private boolean accessControllerAvailable = false;
  private Configuration conf;
  private volatile boolean initialized = false;
  private boolean checkAuths = false;
  /** Mapping of scanner instances to the user who created them */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java

    private List<RegionInfo> regionsToRestore = null;
    private List<RegionInfo> regionsToRemove = null;
    private List<RegionInfo> regionsToAdd = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    private final TableDescriptor htd;

    private List<RegionInfo> regionsToRestore = null;
    private List<RegionInfo> regionsToRemove = null;
    private List<RegionInfo> regionsToAdd = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java`
#### Snippet
```java
    private List<RegionInfo> regionsToRestore = null;
    private List<RegionInfo> regionsToRemove = null;
    private List<RegionInfo> regionsToAdd = null;

    public RestoreMetaChanges(TableDescriptor htd, Map<String, Pair<String, String>> parentsMap) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
  protected final ProcedureMemberRpcs rpcs;

  private volatile boolean complete = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private boolean showFiles = false;
  private boolean showStats = false;
  private boolean printSizeInBytes = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private Path remoteDir;
  private boolean showSchema = false;
  private boolean showFiles = false;
  private boolean showStats = false;
  private boolean printSizeInBytes = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private String snapshotName;
  private Path remoteDir;
  private boolean showSchema = false;
  private boolean showFiles = false;
  private boolean showStats = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private SnapshotManifest snapshotManifest;

  private boolean listSnapshots = false;
  private String snapshotName;
  private Path remoteDir;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private boolean showSchema = false;
  private boolean showFiles = false;
  private boolean showStats = false;
  private boolean printSizeInBytes = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ZKProcedureCoordinator.class);
  private ZKProcedureUtil zkProc = null;
  protected ProcedureCoordinator coordinator = null; // if started this should be non-null

  ZKWatcher watcher;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
public class ZKProcedureCoordinator implements ProcedureCoordinatorRpcs {
  private static final Logger LOG = LoggerFactory.getLogger(ZKProcedureCoordinator.class);
  private ZKProcedureUtil zkProc = null;
  protected ProcedureCoordinator coordinator = null; // if started this should be non-null

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MemoryBoundedLogMessageBuffer.java`
#### Snippet
```java
public class MemoryBoundedLogMessageBuffer {
  private final long maxSizeBytes;
  private long usage = 0;
  private LinkedList<LogMessage> messages;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
  private Object[] params = {};
  private Message packet;
  private boolean snapshot = false;
  private Map<String, Object> callInfoMap = new HashMap<>();

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/NamedQueueRecorder.java`
#### Snippet
```java

  private static volatile NamedQueueRecorder namedQueueRecorder;
  private static boolean isInit = false;
  private static final Object LOCK = new Object();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/WALEventTrackerQueueService.java`
#### Snippet
```java
  private final boolean walEventTrackerEnabled;
  private int queueSize;
  private MetricsWALEventTrackerSource source = null;

  private static final Logger LOG = LoggerFactory.getLogger(WALEventTrackerQueueService.class);
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
  private MetricRegistry registry;
  private LossyCounting<String> clientMetricsLossyCounting, regionMetricsLossyCounting;
  private boolean active = false;
  private Set<String> metrics = ConcurrentHashMap.newKeySet();

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(HBaseReplicationEndpoint.class);

  private ZKWatcher zkw = null;
  private final Object zkwLock = new Object();

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ChainWALEmptyEntryFilter.java`
#### Snippet
```java
public class ChainWALEmptyEntryFilter extends ChainWALEntryFilter {

  private boolean filterEmptyEntry = false;

  public ChainWALEmptyEntryFilter(final WALEntryFilter... filters) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationHFileCleaner.java`
#### Snippet
```java
  private ZKWatcher zkw;
  private ReplicationQueueStorage rqs;
  private boolean stopped = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
  private boolean shareZK = false;
  private ReplicationQueueStorage queueStorage;
  private boolean stopped = false;
  private Set<String> wals;
  private long readZKTimestamp = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
  private boolean stopped = false;
  private Set<String> wals;
  private long readZKTimestamp = 0;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
public class ReplicationLogCleaner extends BaseLogCleanerDelegate {
  private static final Logger LOG = LoggerFactory.getLogger(ReplicationLogCleaner.class);
  private ZKWatcher zkw = null;
  private boolean shareZK = false;
  private ReplicationQueueStorage queueStorage;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ReplicationLogCleaner.class);
  private ZKWatcher zkw = null;
  private boolean shareZK = false;
  private ReplicationQueueStorage queueStorage;
  private boolean stopped = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java`
#### Snippet
```java
  private long currentPositionOfEntry = 0;
  // position after reading current entry
  private long currentPositionOfReader = 0;
  private final ReplicationSourceLogQueue logQueue;
  private final String walGroupId;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.java`
#### Snippet
```java
  // choose to return before reading the current entry, so it is not safe to return the value below
  // in getPosition.
  private long currentPositionOfEntry = 0;
  // position after reading current entry
  private long currentPositionOfReader = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryBatch.java`
#### Snippet
```java
  private long lastWalPosition = 0;
  // number of distinct row keys in this batch
  private int nbRowKeys = 0;
  // number of HFiles
  private int nbHFiles = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryBatch.java`
#### Snippet
```java
  private int nbRowKeys = 0;
  // number of HFiles
  private int nbHFiles = 0;
  // heap size of data we need to replicate
  private long heapSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryBatch.java`
#### Snippet
```java
  private Path lastWalPath;
  // position in WAL of last entry in this batch
  private long lastWalPosition = 0;
  // number of distinct row keys in this batch
  private int nbRowKeys = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/WALEntryBatch.java`
#### Snippet
```java
  private int nbHFiles = 0;
  // heap size of data we need to replicate
  private long heapSize = 0;
  // save the last sequenceid for each region if the table has serial-replication scope
  private Map<String, Long> lastSeqIds = new HashMap<>();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java`
#### Snippet
```java
  private int maxRetriesMultiplier;
  // Indicates if this particular source is running
  volatile boolean sourceRunning = false;
  // Metrics for this source
  private MetricsSource metrics;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  private boolean isSerial = false;
  // Initialising as 0 to guarantee at least one logging message
  private long lastSinkFetchTime = 0;
  private volatile boolean stopping = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  // Metrics for this source
  private MetricsSource metrics;
  private boolean peersSelected = false;
  private String replicationClusterId = "";
  private int maxThreads;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  // Initialising as 0 to guarantee at least one logging message
  private long lastSinkFetchTime = 0;
  private volatile boolean stopping = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
  private boolean dropOnDeletedTables;
  private boolean dropOnDeletedColumnFamilies;
  private boolean isSerial = false;
  // Initialising as 0 to guarantee at least one logging message
  private long lastSinkFetchTime = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
  private final Object sharedConnLock = new Object();
  // Number of hfiles that we successfully replicated
  private long hfilesReplicated = 0;
  private SourceFSConfigurationProvider provider;
  private WALEntrySinkFilter walEntrySinkFilter;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java`
#### Snippet
```java
  private Map<String, Long> lastShippedTimeStamps = new HashMap<String, Long>();
  private Map<String, Long> ageOfLastShippedOp = new HashMap<>();
  private long lastHFileRefsQueueSize = 0;
  private String id;
  private long timeStampNextToReplicate;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
  static class DumpOptions {
    boolean hdfs = false;
    boolean distributed = false;

    public DumpOptions() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java

  static class DumpOptions {
    boolean hdfs = false;
    boolean distributed = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
  SplitLogManagerDetails details;

  public boolean ignoreZKDeleteForTesting = false;

  public ZKSplitLogManagerCoordination(Configuration conf, ZKWatcher watcher) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
  private volatile boolean shouldStop = false;
  private final Object grabTaskLock = new Object();
  private boolean workerInGrabTask = false;
  private int reportPeriod;
  private RegionServerServices server = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java

    CancelableProgressable reporter = new CancelableProgressable() {
      private long last_report_at = 0;

      @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
  private volatile String currentTask = null;
  private int currentVersion;
  private volatile boolean shouldStop = false;
  private final Object grabTaskLock = new Object();
  private boolean workerInGrabTask = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java

  private final AtomicInteger taskReadySeq = new AtomicInteger(0);
  private volatile String currentTask = null;
  private int currentVersion;
  private volatile boolean shouldStop = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
  private RegionServerServices server = null;
  protected final AtomicInteger tasksInProgress = new AtomicInteger(0);
  private int maxConcurrentTasks = 0;

  private final ServerName serverName;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
  private boolean workerInGrabTask = false;
  private int reportPeriod;
  private RegionServerServices server = null;
  protected final AtomicInteger tasksInProgress = new AtomicInteger(0);
  private int maxConcurrentTasks = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java

  // HDFS blocks distribution information
  private HDFSBlocksDistribution hdfsBlocksDistribution = null;

  private HFileInfo hfileInfo;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
  // It is not just arithmetic sum of past tuner periods. More priority is given to recent
  // tuning steps.
  private double decayingTunerStepSizeSum = 0;

  @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
  // Counter to ignore few initial periods while cache is still warming up
  // Memory tuner will do no operation for the first "tunerLookupPeriods"
  private int ignoreInitialPeriods = 0;

  private float globalMemStorePercentMinRange;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreMergerSegmentsIterator.java`
#### Snippet
```java

  // heap of scanners, lazily initialized
  private KeyValueHeap heap = null;
  // remember the initial version of the scanners list
  List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreMergerSegmentsIterator.java`
#### Snippet
```java
  List<KeyValueScanner> scanners = new ArrayList<KeyValueScanner>();

  private boolean closed = false;

  // C-tor
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  private Cell stopSkippingKVsRow;
  // last iterated KVs by seek (to restore the iterator state after reseek)
  private Cell last = null;

  // flag to indicate if this scanner is closed
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  protected Iterator<Cell> iter;
  // the pre-calculated cell to be returned by peek()
  protected Cell current = null;
  // or next()
  // A flag represents whether could stop skipping KeyValues for MVCC
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java

  // flag to indicate if this scanner is closed
  protected boolean closed = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
  // A flag represents whether could stop skipping KeyValues for MVCC
  // if have encountered the next row. Only used for reversed scan
  private boolean stopSkippingKVsIfNextRow = false;
  // Stop skipping KeyValues for MVCC if finish this row. Only used for reversed scan
  private Cell stopSkippingKVsRow;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitWALCallable.java`
#### Snippet
```java
  private String walPath;
  private final KeyLocker<String> splitWALLocks = new KeyLocker<>();
  private volatile Lock splitWALLock = null;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AdaptiveMemStoreCompactionStrategy.java`
#### Snippet
```java
  private double compactionProbability;
  private double numCellsInVersionedList = 0;
  private boolean compacted = false;

  public AdaptiveMemStoreCompactionStrategy(Configuration conf, String cfName) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AdaptiveMemStoreCompactionStrategy.java`
#### Snippet
```java
  private double initialCompactionProbability;
  private double compactionProbability;
  private double numCellsInVersionedList = 0;
  private boolean compacted = false;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.java`
#### Snippet
```java
  public static final String PREFIX_LENGTH_KEY = "KeyPrefixRegionSplitPolicy.prefix_length";

  private int prefixLength = 0;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.java`
#### Snippet
```java

  private static final Logger LOG = LoggerFactory.getLogger(ReversedMobStoreScanner.class);
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(ReversedMobStoreScanner.class);
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.java`
#### Snippet
```java
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
  private final List<MobCell> referencedMobCells;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LeaseManager.java`
#### Snippet
```java
  private final Map<String, Lease> leases = new ConcurrentHashMap<>();
  private final int leaseCheckFrequency;
  private volatile boolean stopRequested = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  /** if we have been successfully initialized */
  private volatile boolean initialized = false;

  /** if the ACL table is available, only relevant in the master */
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  /** flags if we are running on a region of the _acl_ table */
  private boolean aclRegion = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java

  /** if the ACL table is available, only relevant in the master */
  private volatile boolean aclTabAvailable = false;

  public static boolean isCellAuthorizationSupported(Configuration conf) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CustomizedScanInfoBuilder.java`
#### Snippet
```java
  private Long ttl;

  private KeepDeletedCells keepDeletedCells = null;

  private Integer minVersions;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitPolicy.java`
#### Snippet
```java
  public static final String DELIMITER_KEY = "DelimitedKeyPrefixRegionSplitPolicy.delimiter";

  private byte[] delimiter = null;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // StoreFile.Reader
  private volatile StoreFileReader initialReader;
  private volatile InputStreamBlockDistribution initialReaderBlockDistribution = null;

  // Block cache configuration and reference.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // If true, this file should not be included in minor compactions.
  // It's set whenever you get a Reader.
  private boolean excludeFromMinorCompaction = false;

  // This file was product of these compacted store files
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java
  // If true, this file was product of a major compaction. Its then set
  // whenever you get a Reader.
  private AtomicBoolean majorCompaction = null;

  // If true, this file should not be included in minor compactions.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java`
#### Snippet
```java

  // Indicates if the file got compacted
  private volatile boolean compactedAway = false;

  // Keys for metadata stored in backing HFile.
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
   * {@link Durability#SKIP_WAL} to region replica.
   */
  private WALEdit walEditForReplicateIfExistsSkipWAL = null;

  public MiniBatchOperationInProgress(T[] operations, OperationStatus[] retCodeDetails,
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  private final int lastIndexExclusive;

  private int readyToWriteCount = 0;
  private int cellCount = 0;
  private int numOfPuts = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  private int cellCount = 0;
  private int numOfPuts = 0;
  private int numOfDeletes = 0;
  private int numOfIncrements = 0;
  private int numOfAppends = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java

  private int readyToWriteCount = 0;
  private int cellCount = 0;
  private int numOfPuts = 0;
  private int numOfDeletes = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  private int readyToWriteCount = 0;
  private int cellCount = 0;
  private int numOfPuts = 0;
  private int numOfDeletes = 0;
  private int numOfIncrements = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  private int numOfDeletes = 0;
  private int numOfIncrements = 0;
  private int numOfAppends = 0;
  /**
   * Here is for HBASE-26993,saving the all the {@link Mutation}s if there is
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java`
#### Snippet
```java
  private int numOfPuts = 0;
  private int numOfDeletes = 0;
  private int numOfIncrements = 0;
  private int numOfAppends = 0;
  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
  private byte[] lastBloomKey;
  private long deleteFamilyCnt = -1;
  private boolean bulkLoadResult = false;
  private KeyValue.KeyOnlyKeyValue lastBloomKeyOnlyKV = null;
  private boolean skipResetSeqId = true;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(StoreFileReader.class.getName());

  protected BloomFilter generalBloomFilter = null;
  protected BloomFilter deleteFamilyBloomFilter = null;
  private BloomFilterMetrics bloomFilterMetrics = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
  protected BloomFilter generalBloomFilter = null;
  protected BloomFilter deleteFamilyBloomFilter = null;
  private BloomFilterMetrics bloomFilterMetrics = null;
  protected BloomType bloomFilterType;
  private final HFile.Reader reader;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
  private final HFile.Reader reader;
  protected long sequenceID = -1;
  protected TimeRange timeRange = null;
  private byte[] lastBloomKey;
  private long deleteFamilyCnt = -1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java
  private long deleteFamilyCnt = -1;
  private boolean bulkLoadResult = false;
  private KeyValue.KeyOnlyKeyValue lastBloomKeyOnlyKV = null;
  private boolean skipResetSeqId = true;
  private int prefixLength = -1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileReader.java`
#### Snippet
```java

  protected BloomFilter generalBloomFilter = null;
  protected BloomFilter deleteFamilyBloomFilter = null;
  private BloomFilterMetrics bloomFilterMetrics = null;
  protected BloomType bloomFilterType;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitRestriction.java`
#### Snippet
```java
    "hbase.regionserver.region.split_restriction.delimiter";

  private byte[] delimiter = null;

  @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
    private Cell lastCell;
    private StoreFileWriter currentWriter;
    protected byte[] lastRowInCurrentWriter = null;
    private long cellsInCurrentWriter = 0;
    private long cellsSeen = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java

    private Cell lastCell;
    private long cellsInCurrentWriter = 0;
    private int majorRangeFromIndex = -1, majorRangeToIndex = -1;
    private boolean hasAnyWriter = false;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
    private long cellsInCurrentWriter = 0;
    private long cellsSeen = 0;
    private long cellsSeenInPrevious = 0;

    /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
    protected byte[] lastRowInCurrentWriter = null;
    private long cellsInCurrentWriter = 0;
    private long cellsSeen = 0;
    private long cellsSeenInPrevious = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
    private StoreFileWriter currentWriter;
    protected byte[] lastRowInCurrentWriter = null;
    private long cellsInCurrentWriter = 0;
    private long cellsSeen = 0;
    private long cellsSeenInPrevious = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java`
#### Snippet
```java
    private long cellsInCurrentWriter = 0;
    private int majorRangeFromIndex = -1, majorRangeToIndex = -1;
    private boolean hasAnyWriter = false;

    /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java`
#### Snippet
```java

  // inWalReplay is true while we are synchronously replaying the edits from WAL
  private boolean inWalReplay = false;

  protected final AtomicBoolean allowCompaction = new AtomicBoolean(true);
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.java`
#### Snippet
```java

  private final List<ImmutableSegment> segments;
  private long keySize = 0;

  public CompositeImmutableSegment(CellComparator comparator, List<ImmutableSegment> segments) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NonThreadSafeMemStoreSizing.java`
#### Snippet
```java
class NonThreadSafeMemStoreSizing implements MemStoreSizing {
  private long dataSize = 0;
  private long heapSize = 0;
  private long offHeapSize = 0;
  private int cellsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NonThreadSafeMemStoreSizing.java`
#### Snippet
```java
  private long dataSize = 0;
  private long heapSize = 0;
  private long offHeapSize = 0;
  private int cellsCount = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NonThreadSafeMemStoreSizing.java`
#### Snippet
```java
@InterfaceAudience.Private
class NonThreadSafeMemStoreSizing implements MemStoreSizing {
  private long dataSize = 0;
  private long heapSize = 0;
  private long offHeapSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NonThreadSafeMemStoreSizing.java`
#### Snippet
```java
  private long heapSize = 0;
  private long offHeapSize = 0;
  private int cellsCount = 0;

  NonThreadSafeMemStoreSizing() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
  implements KeyValueScanner, InternalScanner {
  private static final Logger LOG = LoggerFactory.getLogger(KeyValueHeap.class);
  protected PriorityQueue<KeyValueScanner> heap = null;
  // Holds the scanners when a ever a eager close() happens. All such eagerly closed
  // scans are collected and when the final scanner.close() happens will perform the
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
  // scans are collected and when the final scanner.close() happens will perform the
  // actual close.
  protected List<KeyValueScanner> scannersForDelayedClose = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
   * update current.
   */
  protected KeyValueScanner current = null;

  protected KVScannerComparator comparator;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  protected Configuration conf;
  private long lastCompactSize = 0;
  volatile boolean forceMajor = false;
  private AtomicLong storeSize = new AtomicLong();
  private AtomicLong totalUncompressedBytes = new AtomicLong();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
  private final HRegion region;
  protected Configuration conf;
  private long lastCompactSize = 0;
  volatile boolean forceMajor = false;
  private AtomicLong storeSize = new AtomicLong();
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java`
#### Snippet
```java
    private AtomicLong unblockedFlushCount = new AtomicLong();
    private long evictCount = 0L;
    private long cacheMissCount = 0L;
    private TunerContext tunerContext = new TunerContext();
    private boolean alarming = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java`
#### Snippet
```java
  private final RegionServerAccounting regionServerAccounting;

  private HeapMemoryTunerChore heapMemTunerChore = null;
  private final boolean tunerOn;
  private final int defaultChorePeriod;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java`
#### Snippet
```java
    private AtomicLong blockedFlushCount = new AtomicLong();
    private AtomicLong unblockedFlushCount = new AtomicLong();
    private long evictCount = 0L;
    private long cacheMissCount = 0L;
    private TunerContext tunerContext = new TunerContext();
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HeapMemoryManager.java`
#### Snippet
```java
    private long cacheMissCount = 0L;
    private TunerContext tunerContext = new TunerContext();
    private boolean alarming = false;

    public HeapMemoryTunerChore() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
   * towards progress.
   */
  boolean skippingRow = false;

  private Cell lastPeekedCell = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  boolean skippingRow = false;

  private Cell lastPeekedCell = null;

  // Set this to true will have the same behavior with reaching the time limit.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
   */
  boolean keepProgress;
  private static boolean DEFAULT_KEEP_PROGRESS = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  public static final class Builder {
    boolean keepProgress = DEFAULT_KEEP_PROGRESS;
    boolean trackMetrics = false;
    LimitFields limits = new LimitFields();

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ServerNonceManager.java`
#### Snippet
```java

    // 0..1 - state, 2..2 - whether anyone is waiting, 3.. - ts of last activity
    private long data = 0;
    private static final long STATE_BITS = 3;
    private static final long WAITING_BIT = 4;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
    private final long createTime;
    private long whenToExpire;
    private int requeueCount = 0;

    private final List<byte[]> families;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
  // A flag represents whether could stop skipping KeyValues for MVCC
  // if have encountered the next row. Only used for reversed scan
  private boolean stopSkippingKVsIfNextRow = false;

  private static LongAdder seekCount;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
  private final HFileScanner hfs;
  private Cell cur = null;
  private boolean closed = false;

  private boolean realSeekDone;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
  private final StoreFileReader reader;
  private final HFileScanner hfs;
  private Cell cur = null;
  private boolean closed = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
   * for which we are populating the values.
   */
  protected Cell joinedContinuationRow = null;
  private boolean filterClosed = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
   */
  protected Cell joinedContinuationRow = null;
  private boolean filterClosed = false;

  protected final byte[] stopRow;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java
   * if on-demand column family loading is enabled.
   */
  KeyValueHeap joinedHeap = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionScannerImpl.java`
#### Snippet
```java

  // Package local for testability
  KeyValueHeap storeHeap = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    // Source filesystem
    private FileSystem srcFs = null;
    private Map<String, FsPermission> origPermissions = null;
    private Map<String, String> origSources = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    private final Configuration conf;
    // Source filesystem
    private FileSystem srcFs = null;
    private Map<String, FsPermission> origPermissions = null;
    private Map<String, String> origSources = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
    private FileSystem srcFs = null;
    private Map<String, FsPermission> origPermissions = null;
    private Map<String, String> origSources = null;

    public SecureBulkLoadListener(FileSystem fs, String stagingDir, Configuration conf) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java`
#### Snippet
```java
  public static final class WriteEntry {
    private final long writeNumber;
    private boolean completed = false;
    /**
     * Will be called after completion, i.e, when being removed from the
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
  private byte[] bloomParam = null;
  private long earliestPutTs = HConstants.LATEST_TIMESTAMP;
  private long deleteFamilyCnt = 0;
  private BloomContext bloomContext = null;
  private BloomContext deleteFamilyBloomContext = null;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java

    private BloomType bloomType = BloomType.NONE;
    private long maxKeyCount = 0;
    private Path dir;
    private Path filePath;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
  private final BloomFilterWriter deleteFamilyBloomFilterWriter;
  private final BloomType bloomType;
  private byte[] bloomParam = null;
  private long earliestPutTs = HConstants.LATEST_TIMESTAMP;
  private long deleteFamilyCnt = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
  private long deleteFamilyCnt = 0;
  private BloomContext bloomContext = null;
  private BloomContext deleteFamilyBloomContext = null;
  private final TimeRangeTracker timeRangeTracker;
  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
  private long earliestPutTs = HConstants.LATEST_TIMESTAMP;
  private long deleteFamilyCnt = 0;
  private BloomContext bloomContext = null;
  private BloomContext deleteFamilyBloomContext = null;
  private final TimeRangeTracker timeRangeTracker;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java

  private volatile long numStores = 0;
  private volatile long numWALFiles = 0;
  private volatile long walFileSize = 0;
  private volatile long numStoreFiles = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private BlockCache blockCache;
  private BlockCache l1Cache = null;
  private BlockCache l2Cache = null;
  private MobFileCache mobFileCache;
  private CacheStats cacheStats;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long bloomFilterNegativeResultsCount = 0;
  private volatile long bloomFilterEligibleRequestsCount = 0;
  private volatile long numMutationsWithoutWAL = 0;
  private volatile long dataInMemoryWithoutWAL = 0;
  private volatile double percentFileLocal = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private CacheStats l2Stats = null;

  private volatile long numStores = 0;
  private volatile long numWALFiles = 0;
  private volatile long walFileSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long totalStaticIndexSize = 0;
  private volatile long totalStaticBloomSize = 0;
  private volatile long bloomFilterRequestsCount = 0;
  private volatile long bloomFilterNegativeResultsCount = 0;
  private volatile long bloomFilterEligibleRequestsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFileCacheMissCount = 0;
  private volatile double mobFileCacheHitRatio = 0;
  private volatile long mobFileCacheEvictedCount = 0;
  private volatile long mobFileCacheCount = 0;
  private volatile long blockedRequestsCount = 0L;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long cellsCountCompactedToMob = 0;
  private volatile long cellsCountCompactedFromMob = 0;
  private volatile long cellsSizeCompactedToMob = 0;
  private volatile long cellsSizeCompactedFromMob = 0;
  private volatile long mobFlushCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long storefileIndexSize = 0;
  private volatile long totalStaticIndexSize = 0;
  private volatile long totalStaticBloomSize = 0;
  private volatile long bloomFilterRequestsCount = 0;
  private volatile long bloomFilterNegativeResultsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long totalStaticBloomSize = 0;
  private volatile long bloomFilterRequestsCount = 0;
  private volatile long bloomFilterNegativeResultsCount = 0;
  private volatile long bloomFilterEligibleRequestsCount = 0;
  private volatile long numMutationsWithoutWAL = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private CacheStats cacheStats;
  private CacheStats l1Stats = null;
  private CacheStats l2Stats = null;

  private volatile long numStores = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long compactedCellsSize = 0;
  private volatile long majorCompactedCellsSize = 0;
  private volatile long cellsCountCompactedToMob = 0;
  private volatile long cellsCountCompactedFromMob = 0;
  private volatile long cellsSizeCompactedToMob = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long dataInMemoryWithoutWAL = 0;
  private volatile double percentFileLocal = 0;
  private volatile double percentFileLocalSecondaryRegions = 0;
  private volatile long flushedCellsCount = 0;
  private volatile long compactedCellsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double writeRequestsRatePerSecond = 0;
  private volatile long checkAndMutateChecksFailed = 0;
  private volatile long checkAndMutateChecksPassed = 0;
  private volatile long storefileIndexSize = 0;
  private volatile long totalStaticIndexSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long compactedCellsCount = 0;
  private volatile long majorCompactedCellsCount = 0;
  private volatile long flushedCellsSize = 0;
  private volatile long compactedCellsSize = 0;
  private volatile long majorCompactedCellsSize = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private MobFileCache mobFileCache;
  private CacheStats cacheStats;
  private CacheStats l1Stats = null;
  private CacheStats l2Stats = null;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long checkAndMutateChecksPassed = 0;
  private volatile long storefileIndexSize = 0;
  private volatile long totalStaticIndexSize = 0;
  private volatile long totalStaticBloomSize = 0;
  private volatile long bloomFilterRequestsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long writeRequestsCount = 0;
  private volatile double writeRequestsRatePerSecond = 0;
  private volatile long checkAndMutateChecksFailed = 0;
  private volatile long checkAndMutateChecksPassed = 0;
  private volatile long storefileIndexSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long numMutationsWithoutWAL = 0;
  private volatile long dataInMemoryWithoutWAL = 0;
  private volatile double percentFileLocal = 0;
  private volatile double percentFileLocalSecondaryRegions = 0;
  private volatile long flushedCellsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double mobFileCacheHitRatio = 0;
  private volatile long mobFileCacheEvictedCount = 0;
  private volatile long mobFileCacheCount = 0;
  private volatile long blockedRequestsCount = 0L;
  private volatile long averageRegionSize = 0L;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long flushedCellsCount = 0;
  private volatile long compactedCellsCount = 0;
  private volatile long majorCompactedCellsCount = 0;
  private volatile long flushedCellsSize = 0;
  private volatile long compactedCellsSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long onHeapMemstoreSize = 0;
  private volatile long offHeapMemstoreSize = 0;
  private volatile long storeFileSize = 0;
  private volatile double storeFileSizeGrowthRate = 0;
  private volatile long maxStoreFileCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFileCacheEvictedCount = 0;
  private volatile long mobFileCacheCount = 0;
  private volatile long blockedRequestsCount = 0L;
  private volatile long averageRegionSize = 0L;
  protected final Map<String, ArrayList<Long>> requestsCountCache =
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long storeFileSize = 0;
  private volatile double storeFileSizeGrowthRate = 0;
  private volatile long maxStoreFileCount = 0;
  private volatile long maxStoreFileAge = 0;
  private volatile long minStoreFileAge = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long numStoreFiles = 0;
  private volatile long memstoreSize = 0;
  private volatile long onHeapMemstoreSize = 0;
  private volatile long offHeapMemstoreSize = 0;
  private volatile long storeFileSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long offHeapMemstoreSize = 0;
  private volatile long storeFileSize = 0;
  private volatile double storeFileSizeGrowthRate = 0;
  private volatile long maxStoreFileCount = 0;
  private volatile long maxStoreFileAge = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double readRequestsRatePerSecond = 0;
  private volatile long cpRequestsCount = 0;
  private volatile long filteredReadRequestsCount = 0;
  private volatile long writeRequestsCount = 0;
  private volatile double writeRequestsRatePerSecond = 0;
```

### RedundantFieldInitialization
Field initialization to `0.0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long avgStoreFileAge = 0;
  private volatile long numReferenceFiles = 0;
  private volatile double requestsPerSecond = 0.0;
  private volatile long readRequestsCount = 0;
  private volatile double readRequestsRatePerSecond = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double percentFileLocalSecondaryRegions = 0;
  private volatile long flushedCellsCount = 0;
  private volatile long compactedCellsCount = 0;
  private volatile long majorCompactedCellsCount = 0;
  private volatile long flushedCellsSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long checkAndMutateChecksFailed = 0;
  private volatile long checkAndMutateChecksPassed = 0;
  private volatile long storefileIndexSize = 0;
  private volatile long totalStaticIndexSize = 0;
  private volatile long totalStaticBloomSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long walFileSize = 0;
  private volatile long numStoreFiles = 0;
  private volatile long memstoreSize = 0;
  private volatile long onHeapMemstoreSize = 0;
  private volatile long offHeapMemstoreSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long numWALFiles = 0;
  private volatile long walFileSize = 0;
  private volatile long numStoreFiles = 0;
  private volatile long memstoreSize = 0;
  private volatile long onHeapMemstoreSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long bloomFilterEligibleRequestsCount = 0;
  private volatile long numMutationsWithoutWAL = 0;
  private volatile long dataInMemoryWithoutWAL = 0;
  private volatile double percentFileLocal = 0;
  private volatile double percentFileLocalSecondaryRegions = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long bloomFilterRequestsCount = 0;
  private volatile long bloomFilterNegativeResultsCount = 0;
  private volatile long bloomFilterEligibleRequestsCount = 0;
  private volatile long numMutationsWithoutWAL = 0;
  private volatile long dataInMemoryWithoutWAL = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java

  private BlockCache blockCache;
  private BlockCache l1Cache = null;
  private BlockCache l2Cache = null;
  private MobFileCache mobFileCache;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long minStoreFileAge = 0;
  private volatile long avgStoreFileAge = 0;
  private volatile long numReferenceFiles = 0;
  private volatile double requestsPerSecond = 0.0;
  private volatile long readRequestsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long memstoreSize = 0;
  private volatile long onHeapMemstoreSize = 0;
  private volatile long offHeapMemstoreSize = 0;
  private volatile long storeFileSize = 0;
  private volatile double storeFileSizeGrowthRate = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFlushedCellsSize = 0;
  private volatile long mobScanCellsCount = 0;
  private volatile long mobScanCellsSize = 0;
  private volatile long mobFileCacheAccessCount = 0;
  private volatile long mobFileCacheMissCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFlushCount = 0;
  private volatile long mobFlushedCellsCount = 0;
  private volatile long mobFlushedCellsSize = 0;
  private volatile long mobScanCellsCount = 0;
  private volatile long mobScanCellsSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long numStores = 0;
  private volatile long numWALFiles = 0;
  private volatile long walFileSize = 0;
  private volatile long numStoreFiles = 0;
  private volatile long memstoreSize = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/InternalScan.java`
#### Snippet
```java
public class InternalScan extends Scan {
  private boolean memOnly = false;
  private boolean filesOnly = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/InternalScan.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
public class InternalScan extends Scan {
  private boolean memOnly = false;
  private boolean filesOnly = false;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double storeFileSizeGrowthRate = 0;
  private volatile long maxStoreFileCount = 0;
  private volatile long maxStoreFileAge = 0;
  private volatile long minStoreFileAge = 0;
  private volatile long avgStoreFileAge = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long cellsCountCompactedFromMob = 0;
  private volatile long cellsSizeCompactedToMob = 0;
  private volatile long cellsSizeCompactedFromMob = 0;
  private volatile long mobFlushCount = 0;
  private volatile long mobFlushedCellsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long flushedCellsSize = 0;
  private volatile long compactedCellsSize = 0;
  private volatile long majorCompactedCellsSize = 0;
  private volatile long cellsCountCompactedToMob = 0;
  private volatile long cellsCountCompactedFromMob = 0;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFileCacheCount = 0;
  private volatile long blockedRequestsCount = 0L;
  private volatile long averageRegionSize = 0L;
  protected final Map<String, ArrayList<Long>> requestsCountCache =
    new ConcurrentHashMap<String, ArrayList<Long>>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long cellsSizeCompactedFromMob = 0;
  private volatile long mobFlushCount = 0;
  private volatile long mobFlushedCellsCount = 0;
  private volatile long mobFlushedCellsSize = 0;
  private volatile long mobScanCellsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long maxStoreFileCount = 0;
  private volatile long maxStoreFileAge = 0;
  private volatile long minStoreFileAge = 0;
  private volatile long avgStoreFileAge = 0;
  private volatile long numReferenceFiles = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobScanCellsSize = 0;
  private volatile long mobFileCacheAccessCount = 0;
  private volatile long mobFileCacheMissCount = 0;
  private volatile double mobFileCacheHitRatio = 0;
  private volatile long mobFileCacheEvictedCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double requestsPerSecond = 0.0;
  private volatile long readRequestsCount = 0;
  private volatile double readRequestsRatePerSecond = 0;
  private volatile long cpRequestsCount = 0;
  private volatile long filteredReadRequestsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long cpRequestsCount = 0;
  private volatile long filteredReadRequestsCount = 0;
  private volatile long writeRequestsCount = 0;
  private volatile double writeRequestsRatePerSecond = 0;
  private volatile long checkAndMutateChecksFailed = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFlushedCellsCount = 0;
  private volatile long mobFlushedCellsSize = 0;
  private volatile long mobScanCellsCount = 0;
  private volatile long mobScanCellsSize = 0;
  private volatile long mobFileCacheAccessCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long readRequestsCount = 0;
  private volatile double readRequestsRatePerSecond = 0;
  private volatile long cpRequestsCount = 0;
  private volatile long filteredReadRequestsCount = 0;
  private volatile long writeRequestsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobScanCellsCount = 0;
  private volatile long mobScanCellsSize = 0;
  private volatile long mobFileCacheAccessCount = 0;
  private volatile long mobFileCacheMissCount = 0;
  private volatile double mobFileCacheHitRatio = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long numReferenceFiles = 0;
  private volatile double requestsPerSecond = 0.0;
  private volatile long readRequestsCount = 0;
  private volatile double readRequestsRatePerSecond = 0;
  private volatile long cpRequestsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long maxStoreFileAge = 0;
  private volatile long minStoreFileAge = 0;
  private volatile long avgStoreFileAge = 0;
  private volatile long numReferenceFiles = 0;
  private volatile double requestsPerSecond = 0.0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java

    private long lastRan = 0;
    private long lastStoreFileSize = 0;

    @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long majorCompactedCellsSize = 0;
  private volatile long cellsCountCompactedToMob = 0;
  private volatile long cellsCountCompactedFromMob = 0;
  private volatile long cellsSizeCompactedToMob = 0;
  private volatile long cellsSizeCompactedFromMob = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long majorCompactedCellsCount = 0;
  private volatile long flushedCellsSize = 0;
  private volatile long compactedCellsSize = 0;
  private volatile long majorCompactedCellsSize = 0;
  private volatile long cellsCountCompactedToMob = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  public class RegionServerMetricsWrapperRunnable implements Runnable {

    private long lastRan = 0;
    private long lastStoreFileSize = 0;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long filteredReadRequestsCount = 0;
  private volatile long writeRequestsCount = 0;
  private volatile double writeRequestsRatePerSecond = 0;
  private volatile long checkAndMutateChecksFailed = 0;
  private volatile long checkAndMutateChecksPassed = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long mobFileCacheAccessCount = 0;
  private volatile long mobFileCacheMissCount = 0;
  private volatile double mobFileCacheHitRatio = 0;
  private volatile long mobFileCacheEvictedCount = 0;
  private volatile long mobFileCacheCount = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile long cellsSizeCompactedToMob = 0;
  private volatile long cellsSizeCompactedFromMob = 0;
  private volatile long mobFlushCount = 0;
  private volatile long mobFlushedCellsCount = 0;
  private volatile long mobFlushedCellsSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
  private volatile double percentFileLocal = 0;
  private volatile double percentFileLocalSecondaryRegions = 0;
  private volatile long flushedCellsCount = 0;
  private volatile long compactedCellsCount = 0;
  private volatile long majorCompactedCellsCount = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobStoreScanner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(MobStoreScanner.class);

  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobStoreScanner.java`
#### Snippet
```java
  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
  private final List<MobCell> referencedMobCells;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MobStoreScanner.java`
#### Snippet
```java

  private boolean cacheMobBlocks = false;
  private boolean rawMobScan = false;
  private boolean readEmptyValueOnMobCellMiss = false;
  private final HMobStore mobStore;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionPipeline.java`
#### Snippet
```java
   * </pre>
   */
  private volatile long version = 0;

  public CompactionPipeline(RegionServicesForStores region) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  }

  private State state = null;

  /** Cached file metadata (or overrides as the case may be) */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    private ArrayList<byte[]> stripeEndRows = null;

    private Collection<HStoreFile> compactedFiles = null;
    private Collection<HStoreFile> results = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java

    private Collection<HStoreFile> compactedFiles = null;
    private Collection<HStoreFile> results = null;

    private List<HStoreFile> l0Results = new ArrayList<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  private class CompactionOrFlushMergeCopy {
    private ArrayList<List<HStoreFile>> stripeFiles = null;
    private ArrayList<HStoreFile> level0Files = null;
    private ArrayList<byte[]> stripeEndRows = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    private ArrayList<List<HStoreFile>> stripeFiles = null;
    private ArrayList<HStoreFile> level0Files = null;
    private ArrayList<byte[]> stripeEndRows = null;

    private Collection<HStoreFile> compactedFiles = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
   */
  private class CompactionOrFlushMergeCopy {
    private ArrayList<List<HStoreFile>> stripeFiles = null;
    private ArrayList<HStoreFile> level0Files = null;
    private ArrayList<byte[]> stripeEndRows = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java`
#### Snippet
```java
   */
  private class StripeCompaction extends CompactionContext {
    private StripeCompactionPolicy.StripeCompactionRequest stripeRequest = null;

    @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  private final Comparator<? super Cell> comparator;
  protected int minCellIdx = 0; // the index of the minimal cell (for sub-sets)
  protected int maxCellIdx = 0; // the index of the cell after the maximal cell (for sub-sets)
  private boolean descending = false;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(CellFlatMap.class);
  private final Comparator<? super Cell> comparator;
  protected int minCellIdx = 0; // the index of the minimal cell (for sub-sets)
  protected int maxCellIdx = 0; // the index of the cell after the maximal cell (for sub-sets)
  private boolean descending = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
  protected int minCellIdx = 0; // the index of the minimal cell (for sub-sets)
  protected int maxCellIdx = 0; // the index of the cell after the maximal cell (for sub-sets)
  private boolean descending = false;

  /* C-tor */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureAsyncProtobufLogWriter.java`
#### Snippet
```java
public class SecureAsyncProtobufLogWriter extends AsyncProtobufLogWriter {

  private Encryptor encryptor = null;

  public SecureAsyncProtobufLogWriter(EventLoopGroup eventLoopGroup,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
  private final boolean offheap;
  static ChunkCreator instance;
  static boolean chunkPoolDisabled = false;
  private MemStoreChunkPool dataChunksPool;
  private final int chunkSize;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ReaderBase.java`
#### Snippet
```java
  protected FileSystem fs;
  protected Path path;
  protected long edit = 0;
  protected long fileLength;
  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ReaderBase.java`
#### Snippet
```java
   * Compression context to use reading. Can be null if no compression.
   */
  protected CompressionContext compressionContext = null;
  private boolean emptyCompressionContext = true;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SecureProtobufLogReader.class);

  private Decryptor decryptor = null;
  private static List<String> writerClsNames = new ArrayList<>();
  static {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  // A state before we go into stopped state. At this stage we're closing user
  // space regions.
  private boolean stopping = false;
  private volatile boolean killed = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  private final RegionServerAccounting regionServerAccounting;

  private NamedQueueServiceChore namedQueueServiceChore = null;

  // Block cache
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "MS_SHOULD_BE_FINAL")
  public static boolean TEST_SKIP_REPORTING_TRANSITION = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  // space regions.
  private boolean stopping = false;
  private volatile boolean killed = false;

  private final int threadWakeFrequency;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DualAsyncFSWAL.java`
#### Snippet
```java
  private volatile boolean skipRemoteWAL = false;

  private volatile boolean markerEditOnly = false;

  public DualAsyncFSWAL(FileSystem fs, FileSystem remoteFs, Path rootDir, Path remoteWALDir,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DualAsyncFSWAL.java`
#### Snippet
```java
  private final Path remoteWALDir;

  private volatile boolean skipRemoteWAL = false;

  private volatile boolean markerEditOnly = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/CompressionContext.java`
#### Snippet
```java
    new EnumMap<>(DictionaryIndex.class);
  // Context used for compressing tags
  TagCompressionContext tagCompressionContext = null;
  ValueCompressor valueCompressor = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/CompressionContext.java`
#### Snippet
```java
  // Context used for compressing tags
  TagCompressionContext tagCompressionContext = null;
  ValueCompressor valueCompressor = null;

  public CompressionContext(Class<? extends Dictionary> dictType, boolean recoveredEdits,
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogWriter.java`
#### Snippet
```java
public class SecureProtobufLogWriter extends ProtobufLogWriter {

  private Encryptor encryptor = null;

  @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
  protected WALCellCodec.ByteStringUncompressor byteStringUncompressor;
  protected boolean hasCompression = false;
  protected boolean hasTagCompression = false;
  protected boolean hasValueCompression = false;
  protected Compression.Algorithm valueCompressionType = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
  protected Codec.Decoder cellDecoder;
  protected WALCellCodec.ByteStringUncompressor byteStringUncompressor;
  protected boolean hasCompression = false;
  protected boolean hasTagCompression = false;
  protected boolean hasValueCompression = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
  protected boolean hasTagCompression = false;
  protected boolean hasValueCompression = false;
  protected Compression.Algorithm valueCompressionType = null;
  // walEditsStopOffset is the position of the last byte to read. After reading the last WALEdit
  // entry in the wal, the inputstream's position is equal to walEditsStopOffset.
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java

  // cell codec classname
  private String codecClsName = null;

  // a flag indicate that whether we need to reset compression context when seeking back
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java`
#### Snippet
```java
  protected boolean hasCompression = false;
  protected boolean hasTagCompression = false;
  protected boolean hasValueCompression = false;
  protected Compression.Algorithm valueCompressionType = null;
  // walEditsStopOffset is the position of the last byte to read. After reading the last WALEdit
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java`
#### Snippet
```java
   */
  static class PostOpenDeployTasksThread extends Thread {
    private Throwable exception = null;
    private final Server server;
    private final RegionServerServices services;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/ParallelSeekHandler.java`
#### Snippet
```java
  private long readPoint;
  private CountDownLatch latch;
  private Throwable err = null;

  public ParallelSeekHandler(KeyValueScanner scanner, Cell keyValue, long readPoint,
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
    // Keep around last exception thrown. Clear on successful sync.
    private final BlockingQueue<SyncFuture> syncFutures;
    private volatile SyncFuture takeSyncFuture = null;

    SyncRunner(final String name, final int maxHandlersCount) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
     */
    private final Object safePointWaiter = new Object();
    private volatile boolean shutdown = false;

    /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
     * this WAL fail until WAL is replaced.
     */
    private Exception exception = null;
    /**
     * Object to block on while waiting on safe point.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java`
#### Snippet
```java
  private final SnapshotDescription snapshot;
  private final SnapshotSubprocedurePool taskManager;
  private boolean snapshotSkipFlush = false;

  // the maximum number of attempts we flush
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java`
#### Snippet
```java
  }

  private volatile boolean stopped = false;

  @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
   * seeking to next row/column. TODO: estimate them?
   */
  private long kvsScanned = 0;
  private Cell prevCell = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java

  protected final long readPt;
  private boolean topChanged = false;

  /** An internal constructor. */
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
   * A flag that enables StoreFileScanner parallel-seeking
   */
  private boolean parallelSeekEnabled = false;
  private ExecutorService executor;
  private final Scan scan;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java

  // Used to indicate that the scanner has closed (see HBASE-1107)
  private volatile boolean closing = false;
  private final boolean get;
  private final boolean explicitColumnQuery;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
  private boolean scanUsePread;
  // Indicates whether there was flush during the course of the scan
  private volatile boolean flushed = false;
  // generally we get one file from a flush
  private final List<KeyValueScanner> flushedstoreFileScanners = new ArrayList<>(1);
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
  private long countPerRow = 0;
  private int storeLimit = -1;
  private int storeOffset = 0;

  // Used to indicate that the scanner has closed (see HBASE-1107)
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
  private boolean cacheBlocks;

  private long countPerRow = 0;
  private int storeLimit = -1;
  private int storeOffset = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
   */
  private long kvsScanned = 0;
  private Cell prevCell = null;

  private final long preadMaxBytes;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionProgress.java`
#### Snippet
```java
  public long totalCompactingKVs;
  /** the completed count of key values in currently running compaction */
  public long currentCompactedKVs = 0;
  /** the total size of data processed by the currently running compaction, in bytes */
  public long totalCompactedSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionProgress.java`
#### Snippet
```java
  public long currentCompactedKVs = 0;
  /** the total size of data processed by the currently running compaction, in bytes */
  public long totalCompactedSize = 0;

  /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequestImpl.java`
#### Snippet
```java
  private int priority = NO_PRIORITY;
  private Collection<HStoreFile> filesToCompact;
  private boolean isAfterSplit = false;

  // CompactRequest object creation time.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequestImpl.java`
#### Snippet
```java

  // was this compaction promoted to an off-peak
  private boolean isOffPeak = false;

  private enum DisplayCompactionType {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionContext.java`
#### Snippet
```java
@InterfaceAudience.Private
public abstract class CompactionContext {
  protected CompactionRequestImpl request = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanWildcardColumnTracker.java`
#### Snippet
```java
@InterfaceAudience.Private
public class ScanWildcardColumnTracker implements ColumnTracker {
  private Cell columnCell = null;
  private int currentCount = 0;
  private final int maxVersions;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanWildcardColumnTracker.java`
#### Snippet
```java
public class ScanWildcardColumnTracker implements ColumnTracker {
  private Cell columnCell = null;
  private int currentCount = 0;
  private final int maxVersions;
  private final int minVersions;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
  private int count = 0;

  private Cell curColCell = null;

  private static Cell createStartKey(Scan scan, ScanInfo scanInfo) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.java`
#### Snippet
```java
  private final int versionsAfterFilter;

  private int count = 0;

  private Cell curColCell = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
  private volatile long lastTimeCheckSlowSync = EnvironmentEdgeManager.currentTime();

  protected volatile boolean closed = false;

  protected final AtomicBoolean shutdown = new AtomicBoolean(false);
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
  protected SortedSet<Long> familyVersionStamps = new TreeSet<Long>();
  protected Cell deleteCell = null;
  protected byte[] deleteBuffer = null;
  protected int deleteOffset = 0;
  protected int deleteLength = 0;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java

  protected boolean hasFamilyStamp = false;
  protected long familyStamp = 0L;
  protected SortedSet<Long> familyVersionStamps = new TreeSet<Long>();
  protected Cell deleteCell = null;
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
  protected int deleteLength = 0;
  protected byte deleteType = 0;
  protected long deleteTimestamp = 0L;
  protected final CellComparator comparator;

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
  protected Cell deleteCell = null;
  protected byte[] deleteBuffer = null;
  protected int deleteOffset = 0;
  protected int deleteLength = 0;
  protected byte deleteType = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
  protected byte[] deleteBuffer = null;
  protected int deleteOffset = 0;
  protected int deleteLength = 0;
  protected byte deleteType = 0;
  protected long deleteTimestamp = 0L;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
  protected long familyStamp = 0L;
  protected SortedSet<Long> familyVersionStamps = new TreeSet<Long>();
  protected Cell deleteCell = null;
  protected byte[] deleteBuffer = null;
  protected int deleteOffset = 0;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
public class ScanDeleteTracker implements DeleteTracker {

  protected boolean hasFamilyStamp = false;
  protected long familyStamp = 0L;
  protected SortedSet<Long> familyVersionStamps = new TreeSet<Long>();
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
  protected int deleteOffset = 0;
  protected int deleteLength = 0;
  protected byte deleteType = 0;
  protected long deleteTimestamp = 0L;
  protected final CellComparator comparator;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreRequest.java`
#### Snippet
```java
  private TableName[] fromTables;
  private TableName[] toTables;
  private boolean overwrite = false;

  private RestoreRequest() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreRequest.java`
#### Snippet
```java
  private String restoreRootDir;
  private String backupId;
  private boolean check = false;
  private TableName[] fromTables;
  private TableName[] toTables;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupTableInfo.java`
#### Snippet
```java
   * Snapshot name for offline/online snapshot
   */
  private String snapshotName = null;

  public BackupTableInfo() {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
  protected static class FileDetails {
    /** Maximum key count after compaction (for blooms) */
    public long maxKeyCount = 0;
    /** Earliest put timestamp if major compaction */
    public long earliestPutTs = HConstants.LATEST_TIMESTAMP;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
    public long latestPutTs = HConstants.LATEST_TIMESTAMP;
    /** The last key in the files we're compacting. */
    public long maxSeqId = 0;
    /** Latest memstore read point found in any of the involved files */
    public long maxMVCCReadpoint = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
    public long minSeqIdToKeep = 0;
    /** Total size of the compacted files **/
    private long totalCompactedFilesSize = 0;
  }

```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
    public int maxTagsLength = 0;
    /** Min SeqId to keep during a major compaction **/
    public long minSeqIdToKeep = 0;
    /** Total size of the compacted files **/
    private long totalCompactedFilesSize = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
    public long maxSeqId = 0;
    /** Latest memstore read point found in any of the involved files */
    public long maxMVCCReadpoint = 0;
    /** Max tags length **/
    public int maxTagsLength = 0;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
    public long maxMVCCReadpoint = 0;
    /** Max tags length **/
    public int maxTagsLength = 0;
    /** Min SeqId to keep during a major compaction **/
    public long minSeqIdToKeep = 0;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  public abstract static class StripeCompactionRequest {
    protected CompactionRequestImpl request;
    protected byte[] majorRangeFromRow = null, majorRangeToRow = null;

    public List<Path> execute(StripeCompactor compactor, ThroughputController throughputController)
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  public abstract static class StripeCompactionRequest {
    protected CompactionRequestImpl request;
    protected byte[] majorRangeFromRow = null, majorRangeToRow = null;

    public List<Path> execute(StripeCompactor compactor, ThroughputController throughputController)
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
  private final static Logger LOG = LoggerFactory.getLogger(StripeCompactionPolicy.class);
  // Policy used to compact individual stripes.
  private ExploringCompactionPolicy stripePolicy = null;

  private StripeStoreConfig config;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupHFileCleaner.java`
#### Snippet
```java
  private Configuration conf;
  private Connection connection;
  private long prevReadFromBackupTbl = 0, // timestamp of most recent read from backup:system table
      secondPrevReadFromBackupTbl = 0; // timestamp of 2nd most recent read from backup:system table
  // used by unit test to skip reading backup:system
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupHFileCleaner.java`
#### Snippet
```java
  // used by unit test to skip reading backup:system
  private boolean checkForFullyBackedUpTables = true;
  private List<TableName> fullyBackedUpTables = null;

  private Set<String> getFilenameFromBulkLoad(Map<byte[], List<Path>>[] maps) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupHFileCleaner.java`
#### Snippet
```java
  private Connection connection;
  private long prevReadFromBackupTbl = 0, // timestamp of most recent read from backup:system table
      secondPrevReadFromBackupTbl = 0; // timestamp of 2nd most recent read from backup:system table
  // used by unit test to skip reading backup:system
  private boolean checkForFullyBackedUpTables = true;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupHFileCleaner.java`
#### Snippet
```java
public class BackupHFileCleaner extends BaseHFileCleanerDelegate implements Abortable {
  private static final Logger LOG = LoggerFactory.getLogger(BackupHFileCleaner.class);
  private boolean stopped = false;
  private boolean aborted;
  private Configuration conf;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
  protected String backupId;
  protected List<TableName> tableList;
  protected Map<String, Long> newTimestamps = null;

  protected BackupManager backupManager;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
   * For incremental backup, a location of a backed-up hlogs
   */
  private String hlogTargetDir = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java

  protected Configuration conf = null;
  protected BackupInfo backupInfo = null;
  protected BackupSystemTable systemTable;
  protected final Connection conn;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(BackupManager.class);

  protected Configuration conf = null;
  protected BackupInfo backupInfo = null;
  protected BackupSystemTable systemTable;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(BackupLogCleaner.class);

  private boolean stopped = false;
  private Connection conn;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/regionserver/LogRollRegionServerProcedureManager.java`
#### Snippet
```java
  private ProcedureMemberRpcs memberRpcs;
  private ProcedureMember member;
  private boolean started = false;

  /**
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java
  // Accumulated progress within the whole backup process for the copy operation
  private float progressDone = 0.1f;
  private long bytesCopied = 0;
  private static float INIT_PROGRESS = 0.1f;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHttpServlet.java`
#### Snippet
```java
    final HttpServletRequest request;
    final UserGroupInformation httpUGI;
    String outToken = null;

    HttpKerberosServerAction(HttpServletRequest request, UserGroupInformation httpUGI) {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/HBaseServiceHandler.java`
#### Snippet
```java
  }

  protected ThriftMetrics metrics = null;

  public void initMetrics(ThriftMetrics metrics) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  private int port;
  private boolean isFramed = false;
  private boolean isCompact = false;

  // TODO: We can rip out the ThriftClient piece of it rather than creating a new client every time.
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  private String host;
  private int port;
  private boolean isFramed = false;
  private boolean isCompact = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  private HttpClient httpClient;
  private boolean httpClientCreated = false;
  private boolean isClosed = false;

  private String host;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
  // For HTTP protocol
  private HttpClient httpClient;
  private boolean httpClientCreated = false;
  private boolean isClosed = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftTable.java`
#### Snippet
```java
  private class Scanner implements ResultScanner {
    protected TScan scan;
    protected Result lastResult = null;
    protected final Queue<Result> cache = new ArrayDeque<>();;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseZKTestingUtility.java`
#### Snippet
```java
  public static ZKWatcher getZooKeeperWatcher(HBaseZKTestingUtility testUtil) throws IOException {
    return new ZKWatcher(testUtil.getConfiguration(), "unittest", new Abortable() {
      boolean aborted = false;

      @Override
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  // nextScannerId and scannerMap are used to manage scanner state
  private int nextScannerId = 0;
  private Cache<Integer, ResultScannerWrapper> scannerMap;
  IncrementCoalescer coalescer;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
    private int numRegionServers = 1;
    private List<Integer> rsPorts = null;
    private Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass = null;
    private int numDataNodes = 1;
    private String[] dataNodeHosts = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
    private String[] dataNodeHosts = null;
    private int numZkServers = 1;
    private boolean createRootDir = false;
    private boolean createWALDir = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
    private int numZkServers = 1;
    private boolean createRootDir = false;
    private boolean createWALDir = false;

    private Builder() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
    private Class<? extends HMaster> masterClass = null;
    private int numRegionServers = 1;
    private List<Integer> rsPorts = null;
    private Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass = null;
    private int numDataNodes = 1;
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
  public static final class Builder {
    private int numMasters = 1;
    private int numAlwaysStandByMasters = 0;
    private Class<? extends HMaster> masterClass = null;
    private int numRegionServers = 1;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
    private Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass = null;
    private int numDataNodes = 1;
    private String[] dataNodeHosts = null;
    private int numZkServers = 1;
    private boolean createRootDir = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/StartMiniClusterOption.java`
#### Snippet
```java
    private int numMasters = 1;
    private int numAlwaysStandByMasters = 0;
    private Class<? extends HMaster> masterClass = null;
    private int numRegionServers = 1;
    private List<Integer> rsPorts = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCommonTestingUtility.java`
#### Snippet
```java
   * Directory where we put the data for this instance of HBaseTestingUtility
   */
  private File dataTestDir = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
  public static class MiniHBaseClusterRegionServer extends HRegionServer {
    private Thread shutdownThread = null;
    private User user = null;
    /**
     * List of RegionServers killed so far. ServerName also comprises startCode of a server, so any
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
   */
  public static class MiniHBaseClusterRegionServer extends HRegionServer {
    private Thread shutdownThread = null;
    private User user = null;
    /**
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterImpl.java`
#### Snippet
```java
    .setNameFormat(getClass().getSuperclass() + "-%d").setDaemon(true).build());

  private boolean miniClusterRunning = false;

  private boolean miniHBaseClusterRunning = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterImpl.java`
#### Snippet
```java
  private boolean miniClusterRunning = false;

  private boolean miniHBaseClusterRunning = false;

  TestingHBaseClusterImpl(TestingHBaseClusterOption option) {
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private Configuration conf;
    private int numMasters = 1;
    private int numAlwaysStandByMasters = 0;
    private int numRegionServers = 1;
    private List<Integer> rsPorts = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private boolean createWALDir = false;
    private String externalDfsUri = null;
    private String externalZkConnectString = null;

    private Builder() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private boolean createRootDir = false;
    private boolean createWALDir = false;
    private String externalDfsUri = null;
    private String externalZkConnectString = null;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private String[] dataNodeHosts = null;
    private int numZkServers = 1;
    private boolean createRootDir = false;
    private boolean createWALDir = false;
    private String externalDfsUri = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private List<Integer> rsPorts = null;
    private int numDataNodes = 1;
    private String[] dataNodeHosts = null;
    private int numZkServers = 1;
    private boolean createRootDir = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private int numZkServers = 1;
    private boolean createRootDir = false;
    private boolean createWALDir = false;
    private String externalDfsUri = null;
    private String externalZkConnectString = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterOption.java`
#### Snippet
```java
    private int numAlwaysStandByMasters = 0;
    private int numRegionServers = 1;
    private List<Integer> rsPorts = null;
    private int numDataNodes = 1;
    private String[] dataNodeHosts = null;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  static class ObservedExceptionsInBatch {
    private boolean wrongRegion = false;
    private boolean failedSanityCheck = false;
    private boolean wrongFamily = false;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    volatile boolean flushing = false;
    // Set when a flush has been requested.
    volatile boolean flushRequested = false;
    // Number of compactions running.
    AtomicInteger compacting = new AtomicInteger(0);
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  // set to true if the region is restored from snapshot for reading by ClientSideRegionScanner
  private boolean isRestoredRegion = false;

  public void setRestoredRegion(boolean restoredRegion) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  static class WriteState {
    // Set while a memstore flush is happening.
    volatile boolean flushing = false;
    // Set when a flush has been requested.
    volatile boolean flushRequested = false;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   * region. Requests can override it.
   */
  private boolean isLoadingCfsOnDemandDefault = false;

  private final AtomicInteger majorInProgress = new AtomicInteger(0);
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private volatile RegionCoprocessorHost coprocessorHost;

  private TableDescriptor htableDescriptor = null;
  private RegionSplitPolicy splitPolicy;
  private RegionSplitRestriction splitRestriction;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    private boolean wrongRegion = false;
    private boolean failedSanityCheck = false;
    private boolean wrongFamily = false;

    /** Returns If a {@link WrongRegionException} has been observed. */
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

    protected final HRegion region;
    protected int nextIndexToProcess = 0;
    protected final ObservedExceptionsInBatch observedExceptions;
    // Durability of the batch (highest durability of all operations)
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  /** Saved state from replaying prepare flush cache */
  private PrepareFlushResult prepareFlushResult = null;

  private long lastReplayedSequenceId = HConstants.NO_SEQNUM;
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // Durability of the batch (highest durability of all operations)
    protected Durability durability;
    protected boolean atomic = false;

    public BatchOperation(final HRegion region, T[] operations) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    volatile boolean writesEnabled = true;
    // Set if region is read-only
    volatile boolean readOnly = false;
    // whether the reads are enabled. This is different than readOnly, because readOnly is
    // static in the lifetime of the region, while readsEnabled is dynamic
```

### RedundantFieldInitialization
Field initialization to `0` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private static final class ReplayBatchOperation extends BatchOperation<MutationReplay> {

    private long origLogSeqNum = 0;

    public ReplayBatchOperation(final HRegion region, MutationReplay[] operations,
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   */
  static class ObservedExceptionsInBatch {
    private boolean wrongRegion = false;
    private boolean failedSanityCheck = false;
    private boolean wrongFamily = false;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  // Used for testing.
  private volatile Long timeoutForWriteLock = null;

  private final CellComparator cellComparator;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java
  private static final SequenceFile.CompressionType DEFAULT_TYPE =
    SequenceFile.CompressionType.RECORD;
  private RegionCoprocessorEnvironment env = null;
  private UserProvider userProvider;

```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java
      private S sum;

      long count = 0L;

      @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java
      private S sum = baseSum;

      private R value = null;

      @Override
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java
    private final CompletableFuture<T> future;

    protected boolean finished = false;

    private void completeExceptionally(Throwable error) {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  private static final class FsDatasetAsyncDiskServiceFixer extends Thread {

    private volatile boolean stopped = false;

    private final MiniDFSCluster cluster;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
   * Directory on test filesystem where we put the data for this instance of HBaseTestingUtility
   */
  private Path dataTestDirOnTestFS = null;

  private final AtomicReference<AsyncClusterConnection> asyncConnection = new AtomicReference<>();
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  }

  private Admin hbaseAdmin = null;

  /**
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java

  private MiniDFSCluster dfsCluster = null;
  private FsDatasetAsyncDiskServiceFixer dfsClusterFixer = null;

  private volatile HBaseCluster hbaseCluster = null;
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java

  private volatile HBaseCluster hbaseCluster = null;
  private MiniMRCluster mrCluster = null;

  /** If there is a mini cluster running for this testing utility instance. */
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  private FsDatasetAsyncDiskServiceFixer dfsClusterFixer = null;

  private volatile HBaseCluster hbaseCluster = null;
  private MiniMRCluster mrCluster = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  public static final boolean PRESPLIT_TEST_TABLE = true;

  private MiniDFSCluster dfsCluster = null;
  private FsDatasetAsyncDiskServiceFixer dfsClusterFixer = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final NavigableMap<byte[], List<S>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    class StdCallback implements Batch.Callback<List<S>> {
      S sumVal = null, sumWeights = null;

      public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
    class MaxCallBack implements Batch.Callback<R> {
      R max = null;

      R getMax() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java

    class SumCallBack implements Batch.Callback<S> {
      S sumVal = null;

      public S getSumResult() {
```

### RedundantFieldInitialization
Field initialization to `0L` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
    class StdCallback implements Batch.Callback<Pair<List<S>, Long>> {
      long rowCountVal = 0L;
      S sumVal = null, sumSqVal = null;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    class StdCallback implements Batch.Callback<Pair<List<S>, Long>> {
      long rowCountVal = 0L;
      S sumVal = null, sumSqVal = null;

      public synchronized Pair<List<S>, Long> getStdParams() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    class StdCallback implements Batch.Callback<Pair<List<S>, Long>> {
      long rowCountVal = 0L;
      S sumVal = null, sumSqVal = null;

      public synchronized Pair<List<S>, Long> getStdParams() {
```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
  static class AggregationClientRpcController implements RpcController {
    private String errorText;
    private boolean cancelled = false;
    private boolean failed = false;

```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final NavigableMap<byte[], List<S>> map = new TreeMap<>(Bytes.BYTES_COMPARATOR);
    class StdCallback implements Batch.Callback<List<S>> {
      S sumVal = null, sumWeights = null;

      public synchronized Pair<NavigableMap<byte[], List<S>>, List<S>> getMedianParams() {
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
    class AvgCallBack implements Batch.Callback<Pair<S, Long>> {
      S sum = null;
      Long rowCount = 0L;

```

### RedundantFieldInitialization
Field initialization to `false` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    private String errorText;
    private boolean cancelled = false;
    private boolean failed = false;

    @Override
```

### RedundantFieldInitialization
Field initialization to `null` is redundant
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java`
#### Snippet
```java
    final AggregateRequest requestArg = validateArgAndGetPB(scan, ci, false);
    class MinCallBack implements Batch.Callback<R> {
      private R min = null;

      public R getMinimum() {
```

## RuleId[id=RedundantImplements]
### RedundantImplements
Redundant interface declaration `HeapSize`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCell.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public interface ExtendedCell extends RawCell, HeapSize {
  int CELL_NOT_BASED_ON_CHUNK = -1;

```

### RedundantImplements
Redundant interface declaration `Map`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
@SuppressWarnings({ "unchecked", "rawtypes", "hiding", "TypeParameterShadowing" })
public class CopyOnWriteArrayMap<K, V> extends AbstractMap<K, V>
  implements Map<K, V>, ConcurrentNavigableMap<K, V> {
  private final Comparator<? super K> keyComparator;
  private volatile ArrayHolder<K, V> holder;
```

### RedundantImplements
Redundant interface declaration `MetricHistogram`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/metrics2/lib/MutableRangeHistogram.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public abstract class MutableRangeHistogram extends MutableHistogram implements MetricHistogram {

  public MutableRangeHistogram(MetricsInfo info) {
```

### RedundantImplements
Redundant interface declaration `HeapSize`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Put.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class Put extends Mutation implements HeapSize {
  /**
   * Create a Put operation for the specified row.
```

### RedundantImplements
Redundant interface declaration `Constants`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java

@InterfaceAudience.Private
public class MultiRowResource extends ResourceBase implements Constants {
  private static final Logger LOG = LoggerFactory.getLogger(MultiRowResource.class);

```

### RedundantImplements
Redundant interface declaration `InputFormat`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java`
#### Snippet
```java
@InterfaceAudience.Public
public class MultiTableSnapshotInputFormat extends TableSnapshotInputFormat
  implements InputFormat<ImmutableBytesWritable, Result> {

  private final MultiTableSnapshotInputFormatImpl delegate;
```

### RedundantImplements
Redundant interface declaration `Tool`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class ExportSnapshot extends AbstractHBaseTool implements Tool {
  public static final String NAME = "exportsnapshot";
  /** Configuration prefix for overrides for the source filesystem */
```

### RedundantImplements
Redundant interface declaration `Stoppable`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
@InterfaceStability.Unstable
public class SnapshotManager extends MasterProcedureManager implements Stoppable {
  private static final Logger LOG = LoggerFactory.getLogger(SnapshotManager.class);

```

### RedundantImplements
Redundant interface declaration `RemoteProcedure`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RefreshPeerProcedure.java`
#### Snippet
```java
@InterfaceAudience.Private
public class RefreshPeerProcedure extends ServerRemoteProcedure
  implements PeerProcedureInterface, RemoteProcedure<MasterProcedureEnv, ServerName> {

  private static final Logger LOG = LoggerFactory.getLogger(RefreshPeerProcedure.class);
```

### RedundantImplements
Redundant interface declaration `RemoteProcedure`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ClaimReplicationQueueRemoteProcedure.java`
#### Snippet
```java
@InterfaceAudience.Private
public class ClaimReplicationQueueRemoteProcedure extends ServerRemoteProcedure
  implements ServerProcedureInterface, RemoteProcedure<MasterProcedureEnv, ServerName> {

  private static final Logger LOG =
```

### RedundantImplements
Redundant interface declaration `PeerProcedureInterface`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/AbstractPeerProcedure.java`
#### Snippet
```java
@InterfaceAudience.Private
public abstract class AbstractPeerProcedure<TState> extends AbstractPeerNoLockProcedure<TState>
  implements PeerProcedureInterface {

  private static final Logger LOG = LoggerFactory.getLogger(AbstractPeerProcedure.class);
```

### RedundantImplements
Redundant interface declaration `KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReversedStoreScanner.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ReversedStoreScanner extends StoreScanner implements KeyValueScanner {

  /**
```

### RedundantImplements
Redundant interface declaration `KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java`
#### Snippet
```java
@InterfaceAudience.Private
public class KeyValueHeap extends NonReversedNonLazyKeyValueScanner
  implements KeyValueScanner, InternalScanner {
  private static final Logger LOG = LoggerFactory.getLogger(KeyValueHeap.class);
  protected PriorityQueue<KeyValueScanner> heap = null;
```

### RedundantImplements
Redundant interface declaration `Stoppable`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
public abstract class PressureAwareThroughputController extends Configured
  implements ThroughputController, Stoppable {
  private static final Logger LOG =
    LoggerFactory.getLogger(PressureAwareThroughputController.class);
```

### RedundantImplements
Redundant interface declaration `KeyValueScanner`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java
@InterfaceAudience.Private
public class StoreScanner extends NonReversedNonLazyKeyValueScanner
  implements KeyValueScanner, InternalScanner, ChangedReadersObserver {
  private static final Logger LOG = LoggerFactory.getLogger(StoreScanner.class);
  // In unit tests, the store could be null
```

## RuleId[id=ExceptionNameDoesntEndWithException]
### ExceptionNameDoesntEndWithException
Exception class name `MultiActionResultTooLarge` does not end with 'Exception'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/MultiActionResultTooLarge.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class MultiActionResultTooLarge extends RetryImmediatelyException {
  public MultiActionResultTooLarge(String s) {
    super(s);
```

### ExceptionNameDoesntEndWithException
Exception class name `PluggableRpcQueueNotFound` does not end with 'Exception'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/PluggableRpcQueueNotFound.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public class PluggableRpcQueueNotFound extends RuntimeException {
  public PluggableRpcQueueNotFound(String message) {
    super(message);
```

### ExceptionNameDoesntEndWithException
Exception class name `TIOErrorWithCause` does not end with 'Exception'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  private boolean isReadOnly;

  private static class TIOErrorWithCause extends TIOError {
    private Throwable cause;

```

### ExceptionNameDoesntEndWithException
Exception class name `IOErrorWithCause` does not end with 'Exception'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
  }

  public static class IOErrorWithCause extends IOError {
    private final Throwable cause;

```

## RuleId[id=InstanceofCatchParameter]
### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
      // Hadoop 2.8+, 3.0-a1+ added FileSystem.setStoragePolicy with a default implementation
      // that throws UnsupportedOperationException
      if (e instanceof UnsupportedOperationException) {
        if (LOG.isDebugEnabled()) {
          LOG.debug("The underlying FileSystem implementation doesn't support "
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
      if (e == null) {
        this.rpcClient.failedServers.addToFailedServers(remoteId.getAddress(), t);
        if (t instanceof LinkageError) {
          // probably the hbase hadoop version does not match the running hadoop version
          e = new DoNotRetryIOException(t);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
          // probably the hbase hadoop version does not match the running hadoop version
          e = new DoNotRetryIOException(t);
        } else if (t instanceof IOException) {
          e = (IOException) t;
        } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
        call.setException(e);
      }
      if (e instanceof SocketTimeoutException) {
        // Clean up open calls but don't treat this as a fatal condition,
        // since we expect certain responses to not make it by the specified
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java`
#### Snippet
```java
      LOG.warn(StringUtils.stringifyException(e));
      // Lets get the exception rethrown to get a more meaningful error message than 404
      if (e instanceof AccessDeniedException) {
        throw e;
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
      LOG.error("Exception occurred while processing " + uriInfo.getAbsolutePath() + " : ", e);
      servlet.getMetrics().incrementFailedPutRequests(1);
      if (e instanceof TableNotFoundException) {
        return Response.status(Response.Status.NOT_FOUND).type(MIMETYPE_TEXT)
          .entity("Not found" + CRLF).build();
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
          .entity("Not found" + CRLF).build();
      } else if (
        e instanceof RuntimeException
          || e instanceof JsonMappingException | e instanceof JsonParseException
      ) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
      } else if (
        e instanceof RuntimeException
          || e instanceof JsonMappingException | e instanceof JsonParseException
      ) {
        return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java`
#### Snippet
```java
      } else if (
        e instanceof RuntimeException
          || e instanceof JsonMappingException | e instanceof JsonParseException
      ) {
        return Response.status(Response.Status.BAD_REQUEST).type(MIMETYPE_TEXT)
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java`
#### Snippet
```java
      servlet.getMetrics().incrementFailedPutRequests(1);
      // Avoid re-unwrapping the exception
      if (e instanceof WebApplicationException) {
        throw (WebApplicationException) e;
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/util/RecoverLeaseFSUtils.java`
#### Snippet
```java
        + getLogMessageDetail(nbAttempt, p, startWaiting));
    } catch (IOException e) {
      if (e instanceof LeaseExpiredException && e.getMessage().contains("File does not exist")) {
        // This exception comes out instead of FNFE, fix it
        throw new FileNotFoundException("The given WAL wasn't found at " + p);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/util/RecoverLeaseFSUtils.java`
#### Snippet
```java
        // This exception comes out instead of FNFE, fix it
        throw new FileNotFoundException("The given WAL wasn't found at " + p);
      } else if (e instanceof FileNotFoundException) {
        throw (FileNotFoundException) e;
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java`
#### Snippet
```java
      } catch (IOException e) {
        // do not retry if the exception tells us not to do so
        if (e instanceof DoNotRetryIOException) {
          throw e;
        }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
          CryptoProtocolVersion.supported());
      } catch (Exception e) {
        if (e instanceof RemoteException) {
          throw (RemoteException) e;
        } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java`
#### Snippet
```java
      } catch (IOException e) {
        // do not retry if the exception tells us not to do so
        if (e instanceof DoNotRetryIOException) {
          updateCounters();
          throw e;
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ex`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
          break;
        } catch (IOException ex) {
          if (!(ex instanceof BindException) && !(ex.getCause() instanceof BindException)) {
            throw ex;
          }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `thrown`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java

        final Throwable responseThrowable;
        if (thrown instanceof LinkageError) {
          // probably the hbase hadoop version does not match the running hadoop version
          responseThrowable = new DoNotRetryIOException(thrown);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `thrown`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
          // probably the hbase hadoop version does not match the running hadoop version
          responseThrowable = new DoNotRetryIOException(thrown);
        } else if (thrown instanceof UnsupportedOperationException) {
          // If the method is not present on the server, do not retry.
          responseThrowable = new DoNotRetryIOException(thrown);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcHandler.java`
#### Snippet
```java
      cr.run();
    } catch (Throwable e) {
      if (e instanceof Error) {
        int failedCount = failedHandlerCount.incrementAndGet();
        if (
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallRunner.java`
#### Snippet
```java
      } catch (Throwable e) {
        TraceUtil.setError(ipcServerSpan, e);
        if (e instanceof ServerNotRunningYetException) {
          // If ServerNotRunningYetException, don't spew stack trace.
          if (RpcServer.LOG.isTraceEnabled()) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallRunner.java`
#### Snippet
```java
        errorThrowable = e;
        error = StringUtils.stringifyException(e);
        if (e instanceof Error) {
          throw (Error) e;
        }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ex`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobFileCleanerChore.java`
#### Snippet
```java
                  // When FileBased SFT is active the store dir can contain corrupted or incomplete
                  // files. So read errors are expected. We just skip these files.
                  if (ex instanceof FileNotFoundException) {
                    throw ex;
                  }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
      // putting it on the wire. Its needed to adhere to the pb Service Interface but we don't
      // need to pass it over the wire.
      if (e instanceof ServiceException) {
        if (e.getCause() == null) {
          LOG.debug("Caught a ServiceException with null cause", e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
      metrics.exception(e);

      if (e instanceof LinkageError) throw new DoNotRetryIOException(e);
      if (e instanceof IOException) throw (IOException) e;
      LOG.error("Unexpected throwable object ", e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java

      if (e instanceof LinkageError) throw new DoNotRetryIOException(e);
      if (e instanceof IOException) throw (IOException) e;
      LOG.error("Unexpected throwable object ", e);
      throw new IOException(e.getMessage(), e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/FSHLogProvider.java`
#### Snippet
```java
      return writer;
    } catch (Exception e) {
      if (e instanceof CommonFSUtils.StreamLacksCapabilityException) {
        LOG.error("The RegionServer write ahead log provider for FileSystem implementations "
          + "relies on the ability to call " + e.getMessage() + " for proper operation during "
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
              break;
            } catch (IOException ioe) {
              if (ioe instanceof WALClosedException) {
                LOG.warn("WAL has been closed. Skipping rolling of writer and just remove it", ioe);
                iter.remove();
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ex`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractWALRoller.java`
#### Snippet
```java
        // Abort if we get here. We probably won't recover an IOE. HBASE-1132
        abort("IOE in log roller",
          ex instanceof RemoteException ? ((RemoteException) ex).unwrapRemoteException() : ex);
      } catch (Exception ex) {
        LOG.error("Log rolling failed", ex);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      corrupt = true;
    } catch (IOException e) {
      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
      throw e;
    } finally {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      }
    } catch (IOException e) {
      if (e instanceof FileNotFoundException) {
        // A wal file may not exist anymore. Nothing can be recovered so move on
        LOG.warn("File {} does not exist anymore", path, e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
        return null;
      }
      if (!skipErrors || e instanceof InterruptedIOException) {
        throw e; // Don't mark the file corrupted if interrupted, or not skipErrors
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java`
#### Snippet
```java

          // Only inspect the Exception to consider retry when it's an IOException
          if (e instanceof IOException) {
            String msg = e.getMessage();
            if (
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AsyncFSWALProvider.java`
#### Snippet
```java
      return writer;
    } catch (Exception e) {
      if (e instanceof CommonFSUtils.StreamLacksCapabilityException) {
        LOG.error("The RegionServer async write ahead log provider "
          + "relies on the ability to call " + e.getMessage() + " for proper operation during "
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
      } catch (IOException ioe) {
        if (
          ioe instanceof NotServingRegionException
            || (ioe instanceof RemoteWithExtrasException && ((RemoteWithExtrasException) ioe)
              .unwrapRemoteException() instanceof NotServingRegionException)
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
        if (
          ioe instanceof NotServingRegionException
            || (ioe instanceof RemoteWithExtrasException && ((RemoteWithExtrasException) ioe)
              .unwrapRemoteException() instanceof NotServingRegionException)
        ) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
        });
    } catch (Exception e) {
      if (e instanceof FileNotFoundException) {
        LOG.debug("Dir dose not exist, {}", dir);
      } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java`
#### Snippet
```java
        }
      } catch (IOException e) {
        e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
        LOG.warn("Error while deleting: " + filePath, e);
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      }
    } catch (IOException e) {
      exception = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
    }
    try {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      Throwable error = e;
      if (
        e instanceof InvocationTargetException
          && ((InvocationTargetException) e).getTargetException() != null
      ) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility
      if (
        t instanceof NoClassDefFoundError
          && t.getMessage().contains("org/apache/hadoop/hdfs/protocol/HdfsConstants$SafeModeAction")
      ) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      }
    } catch (IOException ioe) {
      if (ioe instanceof HBaseIOException) {
        throw (HBaseIOException) ioe;
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineTableProcedure.java`
#### Snippet
```java
      }
    } catch (IOException ioe) {
      if (ioe instanceof HBaseIOException) {
        throw (HBaseIOException) ioe;
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
    } catch (Exception e) {
      String msg = null;
      if (e instanceof InterruptedException) {
        msg = "Procedure '" + barrierName + "' aborting due to interrupt!"
          + " Likely due to pool shutdown.";
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
          + " Likely due to pool shutdown.";
        Thread.currentThread().interrupt();
      } else if (e instanceof ForeignException) {
        msg = "Subprocedure '" + barrierName + "' aborting due to a ForeignException!";
      } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java`
#### Snippet
```java
      LOG.info("Procedure '" + procName + "' execution completed");
    } catch (Exception e) {
      if (e instanceof InterruptedException) {
        Thread.currentThread().interrupt();
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
        return true;
      } catch (IOException ioe) {
        if (ioe instanceof RemoteException) {
          if (dropOnDeletedTables && isTableNotFoundException(ioe)) {
            // Only filter the edits to replicate and don't change the entries in replicateContext
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
          }
        } else {
          if (ioe instanceof SocketTimeoutException) {
            // This exception means we waited for more than 60s and nothing
            // happened, the cluster is alive and calling it right away
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
                + "caused by a machine failure or a massive slowdown",
              this.socketTimeoutMultiplier);
          } else if (ioe instanceof ConnectException || ioe instanceof UnknownHostException) {
            LOG.warn("{} Peer is unavailable, rechecking all sinks: ", logPeerId(), ioe);
            chooseSinks();
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
                + "caused by a machine failure or a massive slowdown",
              this.socketTimeoutMultiplier);
          } else if (ioe instanceof ConnectException || ioe instanceof UnknownHostException) {
            LOG.warn("{} Peer is unavailable, rechecking all sinks: ", logPeerId(), ioe);
            chooseSinks();
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
            LOG.warn("{} Peer is unavailable, rechecking all sinks: ", logPeerId(), ioe);
            chooseSinks();
          } else if (ioe instanceof CallTimeoutException) {
            replicateContext
              .setTimeout(ReplicationUtils.getAdaptiveTimeout(initialTimeout, sleepMultiplier));
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        fileStatus = (new WALLink(getConf(), server.getServerName(), wal)).getFileStatus(fs);
      } catch (IOException e) {
        if (e instanceof FileNotFoundException) {
          numWalsNotFound++;
          LOG.warn("WAL " + wal + " couldn't be found, skipping", e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java`
#### Snippet
```java
      return Status.RESIGNED;
    } catch (IOException e) {
      if (e instanceof FileNotFoundException) {
        // A wal file may not exist anymore. Nothing can be recovered so move on
        LOG.warn("Done, WAL {} does not exist anymore", filename, e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java`
#### Snippet
```java
      Throwable cause = e.getCause();
      if (
        e instanceof RetriesExhaustedException && (cause instanceof NotServingRegionException
          || cause instanceof ConnectException || cause instanceof SocketTimeoutException)
      ) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RemoteProcedureResultReporter.java`
#### Snippet
```java
      } catch (IOException e) {
        boolean pause =
          e instanceof ServerNotRunningYetException || e instanceof PleaseHoldException;
        long pauseTime;
        if (pause) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RemoteProcedureResultReporter.java`
#### Snippet
```java
      } catch (IOException e) {
        boolean pause =
          e instanceof ServerNotRunningYetException || e instanceof PleaseHoldException;
        long pauseTime;
        if (pause) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
        ioe = e;
        if (
          (e instanceof FileNotFoundException) || (e.getCause() instanceof FileNotFoundException)
        ) {
          LOG.debug("Fail to read the cell, the mob file " + path + " doesn't exist", e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
        ) {
          LOG.debug("Fail to read the cell, the mob file " + path + " doesn't exist", e);
        } else if (e instanceof CorruptHFileException) {
          LOG.error("The mob file " + path + " is corrupt", e);
          break;
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ex`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
      } catch (IOException ex) {
        IOException remoteEx =
          ex instanceof RemoteException ? ((RemoteException) ex).unwrapRemoteException() : ex;
        LOG.error("Compaction failed " + this, remoteEx);
        if (remoteEx != ex) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    } catch (Throwable t) {
      clearAndClose(memStoreScanners);
      throw t instanceof IOException ? (IOException) t : new IOException(t);
    } finally {
      HStoreFile.decreaseStoreFilesRefeCount(storeFilesToScan);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    } catch (Throwable t) {
      clearAndClose(memStoreScanners);
      throw t instanceof IOException ? (IOException) t : new IOException(t);
    }
  }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
        } catch (Exception e) {
          LOG.warn("Failed validating store file {}, retrying num={}", lastPathName, i, e);
          if (e instanceof IOException) {
            lastException = (IOException) e;
          } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
              compactSplitThread.requestSystemCompaction(region, Thread.currentThread().getName());
            } catch (IOException e) {
              e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
              LOG.error("Cache flush failed for region "
                + Bytes.toStringBinary(region.getRegionInfo().getRegionName()), e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ex`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
      return false;
    } catch (IOException ex) {
      ex = ex instanceof RemoteException ? ((RemoteException) ex).unwrapRemoteException() : ex;
      LOG.error("Cache flush failed" + (region != null
        ? (" for region " + Bytes.toStringBinary(region.getRegionInfo().getRegionName()))
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        // application-level ClientScanner has to continue without bubbling up the exception to
        // the client. See ClientScanner code to see how it deals with these special exceptions.
        if (e instanceof DoNotRetryIOException) {
          throw e;
        }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        // If it is a FileNotFoundException, wrap as a
        // DoNotRetryIOException. This can avoid the retry in ClientScanner.
        if (e instanceof FileNotFoundException) {
          throw new DoNotRetryIOException(e);
        }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BrokenStoreFileCleaner.java`
#### Snippet
```java
      }
    } catch (IOException e) {
      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
      LOG.warn("Error while deleting: " + filePath, e);
    }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        } catch (IOException e) {
          if (EnvironmentEdgeManager.currentTime() > (previousLogTime + 1000)) {
            e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
            if (e instanceof ServerNotRunningYetException) {
              LOG.info("Master isn't available yet, retrying");
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
          if (EnvironmentEdgeManager.currentTime() > (previousLogTime + 1000)) {
            e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
            if (e instanceof ServerNotRunningYetException) {
              LOG.info("Master isn't available yet, retrying");
            } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    } catch (Throwable t) {
      if (!rpcServices.checkOOME(t)) {
        String prefix = t instanceof YouAreDeadException ? "" : "Unhandled: ";
        abort(prefix + t.getMessage(), t);
      }
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        }
      } catch (Throwable e) {
        e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;
        LOG.error("Shutdown / close of WAL failed: " + e);
        LOG.debug("Shutdown / close exception details:", e);
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.java`
#### Snippet
```java
      } catch (IOException e) {
        if (
          e instanceof TableNotFoundException || FutureUtils
            .get(connection.getAdmin().isTableDisabled(region.getRegionInfo().getTable()))
        ) {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java`
#### Snippet
```java
          + this.region.getRegionInfo().getEncodedName();
        this.exception = e;
        if (e instanceof IOException && isRegionStillOpening(region.getRegionInfo(), services)) {
          server.abort(msg, e);
        } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `e`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
        return;
      } catch (IOException e) {
        if (e instanceof ExclusiveOperationException) {
          // sleep, then repeat
          try {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
      }
    } catch (Throwable t) {
      if (t instanceof IOException) {
        throw getTIOError((IOException) t);
      } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
      }
    } catch (Throwable t) {
      if (t instanceof IOException) {
        throw getTIOError((IOException) t);
      } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
      }
    } catch (Throwable t) {
      if (t instanceof IOException) {
        throw getIOError(t);
      } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `t`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHBaseServiceHandler.java`
#### Snippet
```java
      }
    } catch (Throwable t) {
      if (t instanceof IOException) {
        throw getIOError(t);
      } else {
```

### InstanceofCatchParameter
'instanceof' on 'catch' parameter `ioe`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
       * server if it can not finish the sync within 5 minutes.
       */
      if (ioe instanceof WALSyncTimeoutIOException) {
        if (rsServices != null) {
          rsServices.abort("WAL sync timeout,forcing server shutdown", ioe);
```

## RuleId[id=ArrayEquality]
### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
        int length2) {
        // Short circuit equal case
        if (buffer1 == buffer2 && offset1 == offset2 && length1 == length2) {
          return 0;
        }
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    int rightOffset, int rightLen) {
    // short circuit case
    if (left == right && leftOffset == rightOffset && leftLen == rightLen) {
      return true;
    }
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java

        // Short circuit equal case
        if (buffer1 == buffer2 && offset1 == offset2 && length1 == length2) {
          return 0;
        }
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    // Could use Arrays.equals?
    // noinspection SimplifiableConditionalExpression
    if (left == right) return true;
    if (left == null || right == null) return false;
    if (left.length != right.length) return false;
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
    @Override
    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
      if (b1 == b2 && s1 == s2 && l1 == l2) {
        return 0;
      }
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufMagic.java`
#### Snippet
```java
    int offset2, int length2) {
    // Short circuit equal case
    if (buffer1 == buffer2 && offset1 == offset2 && length1 == length2) {
      return 0;
    }
```

### ArrayEquality
Array objects are compared using `!=`, not 'Arrays.equals()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java

    // No need to use Arrays.equals because ENDKEY is null
    if (end != ENDKEY && Bytes.compareTo(start, end) > 0) {
      // don't allow backwards edges
      LOG.debug(
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    // See Javadoc of INVALID_KEY_IN_MAP for more information
    return (result == null) ? sf.getMetadataValue(STRIPE_START_KEY)
      : result == INVALID_KEY_IN_MAP ? INVALID_KEY
      : result;
  }
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    // See Javadoc of INVALID_KEY_IN_MAP for more information
    return (result == null) ? sf.getMetadataValue(STRIPE_END_KEY)
      : result == INVALID_KEY_IN_MAP ? INVALID_KEY
      : result;
  }
```

### ArrayEquality
Array objects are compared using `==`, not 'Arrays.equals()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
  private static final boolean isInvalid(byte[] key) {
    // No need to use Arrays.equals because INVALID_KEY is null
    return key == INVALID_KEY;
  }

```

## RuleId[id=FuseStreamOperations]
### FuseStreamOperations
Stream may be extended replacing 'toArray'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
  private CompletableFuture<Void> handleHDFSAclParallel(List<HDFSAclOperation> operations) {
    List<CompletableFuture<Void>> futures =
      operations.stream().map(this::handleHDFSAcl).collect(Collectors.toList());
    return CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()]));
  }
```

## RuleId[id=NonFinalFieldOfException]
### NonFinalFieldOfException
Non-final field `serverOverloaded` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/HBaseServerException.java`
#### Snippet
```java
@InterfaceAudience.Public
public class HBaseServerException extends HBaseIOException {
  private boolean serverOverloaded;

  public HBaseServerException() {
```

### NonFinalFieldOfException
Non-final field `hostnameAndPort` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
  List<Throwable> exceptions;
  List<Row> actions;
  List<String> hostnameAndPort;

  public RetriesExhaustedWithDetailsException(final String msg) {
```

### NonFinalFieldOfException
Non-final field `exceptions` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
@InterfaceAudience.Public
public class RetriesExhaustedWithDetailsException extends RetriesExhaustedException {
  List<Throwable> exceptions;
  List<Row> actions;
  List<String> hostnameAndPort;
```

### NonFinalFieldOfException
Non-final field `actions` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
public class RetriesExhaustedWithDetailsException extends RetriesExhaustedException {
  List<Throwable> exceptions;
  List<Row> actions;
  List<String> hostnameAndPort;

```

### NonFinalFieldOfException
Non-final field `type` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/RpcThrottlingException.java`
#### Snippet
```java

  private long waitInterval;
  private Type type;

  public RpcThrottlingException(String msg) {
```

### NonFinalFieldOfException
Non-final field `waitInterval` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/RpcThrottlingException.java`
#### Snippet
```java
  private static final String MSG_WAIT = " - wait ";

  private long waitInterval;
  private Type type;

```

### NonFinalFieldOfException
Non-final field `description` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java`
#### Snippet
```java
@InterfaceAudience.Public
public class HBaseSnapshotException extends DoNotRetryIOException {
  private SnapshotDescription description;

  /**
```

### NonFinalFieldOfException
Non-final field `protocol` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/UnknownProtocolException.java`
#### Snippet
```java
@InterfaceAudience.Public
public class UnknownProtocolException extends org.apache.hadoop.hbase.DoNotRetryIOException {
  private Class<?> protocol;

  public UnknownProtocolException(String mesg) {
```

### NonFinalFieldOfException
Non-final field `timeOfFirstFailureMilliSec` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/PreemptiveFastFailException.java`
#### Snippet
```java
public class PreemptiveFastFailException extends ConnectException {
  private static final long serialVersionUID = 7129103682617007177L;
  private long failureCount, timeOfFirstFailureMilliSec, timeOfLatestAttemptMilliSec;

  // If set, we guarantee that no modifications went to server
```

### NonFinalFieldOfException
Non-final field `failureCount` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/PreemptiveFastFailException.java`
#### Snippet
```java
public class PreemptiveFastFailException extends ConnectException {
  private static final long serialVersionUID = 7129103682617007177L;
  private long failureCount, timeOfFirstFailureMilliSec, timeOfLatestAttemptMilliSec;

  // If set, we guarantee that no modifications went to server
```

### NonFinalFieldOfException
Non-final field `timeOfLatestAttemptMilliSec` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/PreemptiveFastFailException.java`
#### Snippet
```java
public class PreemptiveFastFailException extends ConnectException {
  private static final long serialVersionUID = 7129103682617007177L;
  private long failureCount, timeOfFirstFailureMilliSec, timeOfLatestAttemptMilliSec;

  // If set, we guarantee that no modifications went to server
```

### NonFinalFieldOfException
Non-final field `guaranteedClientSideOnly` of exception class
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/PreemptiveFastFailException.java`
#### Snippet
```java

  // If set, we guarantee that no modifications went to server
  private boolean guaranteedClientSideOnly;

  /**
```

### NonFinalFieldOfException
Non-final field `requestedSize` of exception class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/CacheFullException.java`
#### Snippet
```java
public class CacheFullException extends IOException {
  private static final long serialVersionUID = 3265127301824638920L;
  private int requestedSize, bucketIndex;

  CacheFullException(int requestedSize, int bucketIndex) {
```

### NonFinalFieldOfException
Non-final field `bucketIndex` of exception class
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/CacheFullException.java`
#### Snippet
```java
public class CacheFullException extends IOException {
  private static final long serialVersionUID = 3265127301824638920L;
  private int requestedSize, bucketIndex;

  CacheFullException(int requestedSize, int bucketIndex) {
```

### NonFinalFieldOfException
Non-final field `info` of exception class
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupException.java`
#### Snippet
```java
@InterfaceAudience.Private
public class BackupException extends HBaseIOException {
  private BackupInfo info;

  /**
```

### NonFinalFieldOfException
Non-final field `cause` of exception class
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java

  private static class TIOErrorWithCause extends TIOError {
    private Throwable cause;

    public TIOErrorWithCause(Throwable cause) {
```

## RuleId[id=RedundantStringFormatCall]
### RedundantStringFormatCall
Redundant call to `format()`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Random64.java`
#### Snippet
```java
          long cost = EnvironmentEdgeManager.currentTime() - startTime;
          long remainingMs = (long) (1.0 * (totalTestCnt - cnt) * cost / cnt);
          System.out.println(String.format("Progress: %.3f%%, remaining %d minutes",
            100.0 * cnt / totalTestCnt, remainingMs / 60000));
        }
```

### RedundantStringFormatCall
Redundant call to `format()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
          List<Tag> tags = PrivateCellUtil.getTags(cell);
          for (Tag tag : tags) {
            out.print(String.format(" T[%d]: %s", i++, tag.toString()));
          }
        }
```

### RedundantStringFormatCall
Redundant call to `format()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALPrettyPrinter.java`
#### Snippet
```java
          // Pretty output, complete with indentation by atomic action
          if (!outputOnlyRowKey) {
            out.println(String.format(outputTmpl, txn.get("sequence"), txn.get("table"),
              txn.get("region"), new Date(writeTime)));
          }
```

### RedundantStringFormatCall
Redundant call to `format()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        Set<String> tmpTables = new TreeSet<>();
        try {
          LOG.debug(String.format("reading list of tables"));
          tds = this.admin.listTableDescriptors(pattern);
          if (tds == null) {
```

### RedundantStringFormatCall
Redundant call to `format()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/WALProcedurePrettyPrinter.java`
#### Snippet
```java
        long writeTime = key.getWriteTime();
        out.println(
          String.format(KEY_TMPL, sequenceId, FORMATTER.format(Instant.ofEpochMilli(writeTime))));
        for (Cell cell : edit.getCells()) {
          Map<String, Object> op = WALPrettyPrinter.toStringMap(cell);
```

### RedundantStringFormatCall
Redundant call to `format()`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
    // don't even select for compaction if disableCompactions is set to true
    if (!isCompactionsEnabled()) {
      LOG.info(String.format("User has disabled compactions"));
      return Optional.empty();
    }
```

## RuleId[id=ZeroLengthArrayInitialization]
### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/PemReader.java`
#### Snippet
```java
    KeyStore keyStore = KeyStore.getInstance("JKS");
    keyStore.load(null, null);
    keyStore.setKeyEntry("key", key, keyPassword, certificateChain.toArray(new Certificate[0]));
    return keyStore;
  }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
      // skip empty input buffer.
      src.get();
      return new byte[0];
    }
    final int offset = src.getOffset(), start = src.getPosition();
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/nio/ByteBuff.java`
#### Snippet
```java
    return buffers.size() == 1
      ? new SingleByteBuff(refCnt, buffers.get(0))
      : new MultiByteBuff(refCnt, buffers.toArray(new ByteBuffer[0]));
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassLoaderBase.java`
#### Snippet
```java
   */
  public ClassLoaderBase(final ClassLoader parent) {
    super(new URL[] {}, parent);
    Preconditions.checkNotNull(parent, "No parent classloader!");
    this.parent = parent;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/FutureUtils.java`
#### Snippet
```java
   */
  public static <T> CompletableFuture<List<T>> allOf(List<CompletableFuture<T>> futures) {
    return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
      .thenApply(v -> futures.stream().map(f -> f.getNow(null)).collect(toList()));
  }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/trace/TraceUtil.java`
#### Snippet
```java
    try (Scope ignored = span.makeCurrent()) {
      List<CompletableFuture<T>> futures = action.get();
      endSpan(CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])), span);
      return futures;
    }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static boolean isSorted(Collection<byte[]> arrays) {
    if (!CollectionUtils.isEmpty(arrays)) {
      byte[] previous = new byte[0];
      for (byte[] array : arrays) {
        if (Bytes.compareTo(previous, array) > 0) {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  public static byte[] multiple(byte[] srcBytes, int multiNum) {
    if (multiNum <= 0) {
      return new byte[0];
    }
    byte[] result = new byte[srcBytes.length * multiNum];
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
            return result.toArray(new String[result.size()]);
          } catch (Exception e) {
            return new String[0];
          }
        }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
          return this.cache.get(getShortName());
        } catch (ExecutionException e) {
          return new String[0];
        }
      }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/Superusers.java`
#### Snippet
```java
    superUsersBuilder.add(currentUser);

    String[] superUserList = conf.getStrings(SUPERUSER_CONF_KEY, new String[0]);
    for (String name : superUserList) {
      if (AuthUtil.isGroupPrincipal(name)) {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
      this.endIndex = 0;
      this.startIndex = 0;
      this.entries = new COWEntry[] {};
      this.keyComparator = keyComparator;
      this.comparator = comparator;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-protocol-shaded/src/main/java/org/apache/hadoop/hbase/util/ForeignExceptionUtil.java`
#### Snippet
```java
  public static StackTraceElement[] toStackTrace(List<StackTraceElementMessage> traceList) {
    if (traceList == null || traceList.isEmpty()) {
      return new StackTraceElement[0]; // empty array
    }
    StackTraceElement[] trace = new StackTraceElement[traceList.size()];
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/SaslPlainServer.java`
#### Snippet
```java
      return (props == null) || "false".equals(props.get(Sasl.POLICY_NOPLAINTEXT))
        ? new String[] { "PLAIN" }
        : new String[0];
    }
  }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/HttpDoAsClient.java`
#### Snippet
```java
    context.requestInteg(true);

    final byte[] outToken = context.initSecContext(new byte[0], 0, 0);
    StringBuilder outputBuffer = new StringBuilder();
    outputBuffer.append("Negotiate ");
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorOverAsyncBufferedMutator.java`
#### Snippet
```java
    // should get the future array before calling mutator.flush, otherwise we may hit an infinite
    // wait, since someone may add new future to the map after we calling the flush.
    CompletableFuture<?>[] toWait = futures.toArray(new CompletableFuture<?>[0]);
    mutator.flush();
    try {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public TableName[] listTableNames() throws IOException {
    return get(admin.listTableNames()).toArray(new TableName[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public NamespaceDescriptor[] listNamespaceDescriptors() throws IOException {
    return get(admin.listNamespaceDescriptors()).toArray(new NamespaceDescriptor[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public String[] listNamespaces() throws IOException {
    return get(admin.listNamespaces()).toArray(new String[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public TableName[] listTableNamesByNamespace(String name) throws IOException {
    return get(admin.listTableNamesByNamespace(name)).toArray(new TableName[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java`
#### Snippet
```java
  @Override
  public TableName[] listTableNames(Pattern pattern, boolean includeSysTables) throws IOException {
    return get(admin.listTableNames(pattern, includeSysTables)).toArray(new TableName[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ResultScanner.java`
#### Snippet
```java
      }
    }
    return resultSets.toArray(new Result[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  public byte[][] getFamilies() {
    if (hasFamilies()) {
      return this.familyMap.keySet().toArray(new byte[0][0]);
    }
    return null;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchScanResultCache.java`
#### Snippet
```java
      }
    }
    return regroupedResults.toArray(new Result[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientIdGenerator.java`
#### Snippet
```java
      LOG.warn("Failed to get IP address bytes", ex);
    }
    return new byte[0];
  }
}
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterBase.java`
#### Snippet
```java
  @Override
  public byte[] toByteArray() throws IOException {
    return new byte[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java`
#### Snippet
```java

  public NullComparator() {
    super(new byte[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java`
#### Snippet
```java
    final ByteArrayComparable comparator) {
    this.family = Preconditions.checkNotNull(family, "family should not be null.");
    this.qualifier = qualifier == null ? new byte[0] : qualifier;
    this.op = Preconditions.checkNotNull(op, "CompareOperator should not be null");
    this.comparator = Preconditions.checkNotNull(comparator, "Comparator should not be null");
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableOverAsyncTable.java`
#### Snippet
```java
  @Override
  public Result[] get(List<Get> gets) throws IOException {
    return FutureUtils.get(table.getAll(gets)).toArray(new Result[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java`
#### Snippet
```java
    closeZk();
    DoNotRetryIOException error = new DoNotRetryIOException("Client already closed");
    Arrays.stream(tasks.toArray(new Task[0])).forEach(t -> t.closed(error));
    tasks.clear();
  }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static TableName[] getTableNameArray(List<HBaseProtos.TableName> tableNamesList) {
    if (tableNamesList == null) {
      return new TableName[0];
    }
    TableName[] tableNames = new TableName[tableNamesList.size()];
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  public static void createWithParents(ZKWatcher zkw, String znode) throws KeeperException {
    createWithParents(zkw, znode, new byte[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  public static void createAndFailSilent(ZKWatcher zkw, String znode) throws KeeperException {
    createAndFailSilent(zkw, znode, new byte[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/WrapperAsyncFSOutput.java`
#### Snippet
```java
  @Override
  public DatanodeInfo[] getPipeline() {
    return new DatanodeInfo[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
        DataChecksum summer = createChecksum(client);
        locatedBlock = namenode.addBlock(src, client.getClientName(), null,
          toExcludeNodes.toArray(new DatanodeInfo[0]), stat.getFileId(), null, null);
        Map<Channel, DatanodeInfo> datanodes = new IdentityHashMap<>();
        futureList = connectToDataNodes(conf, client, clientName, locatedBlock, 0L, 0L,
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
    public void handlerAdded(ChannelHandlerContext ctx) throws Exception {
      safeWrite(ctx, ctx.alloc().buffer(4).writeInt(SASL_TRANSFER_MAGIC_NUMBER));
      sendSaslMessage(ctx, new byte[0]);
      ctx.flush();
      step++;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
    setupTime(conf, WALInputFormat.END_TIME_KEY);
    String inputDirs = args[0];
    String[] tables = args.length == 1 ? new String[] {} : args[1].split(",");
    String[] tableMap;
    if (args.length > 2) {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java`
#### Snippet
```java
    public String[] getLocations() throws IOException, InterruptedException {
      // TODO: Find the data node with the most blocks for this WAL?
      return new String[] {};
    }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
      @Override
      public String[] getLocations() throws IOException, InterruptedException {
        return new String[] {};
      }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
      this.regionInfo = regionInfo;
      if (locations == null || locations.isEmpty()) {
        this.locations = new String[0];
      } else {
        this.locations = locations.toArray(new String[locations.size()]);
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
    serializations.add(CellSerialization.class.getName());

    conf.setStrings("io.serializations", serializations.toArray(new String[0]));
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java
    try {
      if (zk.exists(path, false) == null) {
        createZNode(path, new byte[0]);
      }
    } catch (KeeperException | InterruptedException e) {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java

    createEphemeralZNode(ChaosConstants.CHAOS_AGENT_REGISTRATION_EPIMERAL_ZNODE
      + ChaosConstants.ZNODE_PATH_SEPARATOR + agentName, new byte[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosAgent.java`
#### Snippet
```java
      createZKConnection(newTaskCreatedWatcher);
      createEphemeralZNode(ChaosConstants.CHAOS_AGENT_REGISTRATION_EPIMERAL_ZNODE
        + ChaosConstants.ZNODE_PATH_SEPARATOR + agentName, new byte[0]);
    } catch (IOException e) {
      LOG.error("Error creating new ZK COnnection for agent: {}", agentName + e);
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
      TableMapReduceUtil.initCredentials(job);
      // Despite the method name this will get delegation token for the filesystem
      TokenCache.obtainTokensForNamenodes(job.getCredentials(), storeDirs.toArray(new Path[0]),
        conf);

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java`
#### Snippet
```java
      FileStatus[] files = CommonFSUtils.listStatus(fs, path);
      if (files == null) {
        return new String[] {};
      }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
      }
    }
    this.locations = paths.toArray(new Path[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/PersistentIOEngine.java`
#### Snippet
```java
    } catch (IOException ioex) {
      LOG.error("Calculating checksum failed, because of ", ioex);
      return new byte[0];
    } catch (NoSuchAlgorithmException e) {
      LOG.error("No such algorithm : " + algorithm + "!");
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/PersistentIOEngine.java`
#### Snippet
```java
    } catch (NoSuchAlgorithmException e) {
      LOG.error("No such algorithm : " + algorithm + "!");
      return new byte[0];
    }
  }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
    convertToFutureOfList(List<CompletableFuture<T>> futures) {
    CompletableFuture<Void> allDoneFuture =
      CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]));
    return allDoneFuture
      .thenApply(v -> futures.stream().map(CompletableFuture::join).collect(Collectors.toList()));
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java

    SimpleServerRpcConnection[] toArray() {
      return connections.toArray(new SimpleServerRpcConnection[0]);
    }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
    try {
      FileStatus[] files = walFS.listStatus(editsDir, WALSplitUtil::isSequenceIdFile);
      return files != null ? files : new FileStatus[0];
    } catch (FileNotFoundException e) {
      return new FileStatus[0];
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
      return files != null ? files : new FileStatus[0];
    } catch (FileNotFoundException e) {
      return new FileStatus[0];
    }
  }
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      LOG.debug("Exception getting table descriptors", e);
    }
    return new TableDescriptor[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/example/HFileArchiveManager.java`
#### Snippet
```java
    String tableNode = this.getTableNode(table);
    LOG.debug("Creating: " + tableNode + ", data: []");
    ZKUtil.createSetData(zooKeeper, tableNode, new byte[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/AsyncRegionReplicationRetryingCaller.java`
#### Snippet
```java
      operationTimeoutNs, rpcTimeoutNs, conn.connConf.getStartLogErrorsCnt());
    this.replica = replica;
    this.entries = entries.toArray(new Entry[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
    }

    return keysArray.toArray(new byte[0][]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
    Path walDirPath = new Path(CommonFSUtils.getWALRootDir(conf), HConstants.HREGION_LOGDIR_NAME);
    FileStatus[] walDirForServerNames = CommonFSUtils.listStatus(fs, walDirPath, filter);
    return walDirForServerNames == null ? new FileStatus[0] : walDirForServerNames;
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/MetaFixer.java`
#### Snippet
```java
    List<Long> pidList = new ArrayList<>();
    for (Set<RegionInfo> regions : calculateMerges(maxMergeCount, report.getOverlaps())) {
      RegionInfo[] regionsArray = regions.toArray(new RegionInfo[] {});
      try {
        pidList.add(this.masterServices.mergeRegions(regionsArray, true, HConstants.NO_NONCE,
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    List<ServerName> serversToExclude) {
    if (hris.isEmpty()) {
      return new TransitRegionStateProcedure[0];
    }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java`
#### Snippet
```java
    @Override
    public byte[] insideBarrier() throws ForeignException {
      return new byte[0];
    }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
    // ProtobufUtil.prependPBMagic does not take care of null
    if (data == null) {
      data = new byte[0];
    }
    try {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.java`
#### Snippet
```java
      }
    }
    byte[] procArgs = family != null ? family.toByteArray() : new byte[0];

    // Kick of the global procedure from the master coordinator to the region servers.
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.java`
#### Snippet
```java
  public byte[] insideBarrier() throws ForeignException {
    // No-Op
    return new byte[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java`
#### Snippet
```java
  private long rpcStartTime;
  private String methodName = "";
  private Object[] params = {};
  private Message packet;
  private boolean snapshot = false;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/BalancerDecisionQueueService.java`
#### Snippet
```java
    }
    List<RecentLogs.BalancerDecision> balancerDecisions =
      Arrays.stream(balancerDecisionQueue.toArray(new RecentLogs.BalancerDecision[0]))
        .collect(Collectors.toList());
    // latest records should be displayed first, hence reverse order sorting
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/BalancerRejectionQueueService.java`
#### Snippet
```java
    }
    List<RecentLogs.BalancerRejection> balancerRejections =
      Arrays.stream(balancerRejectionQueue.toArray(new RecentLogs.BalancerRejection[0]))
        .collect(Collectors.toList());
    // latest records should be displayed first, hence reverse order sorting
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.java`
#### Snippet
```java
    List<
      TooSlowLog.SlowLogPayload> slowLogPayloadList =
        Arrays.stream(slowLogQueue.toArray(new TooSlowLog.SlowLogPayload[0]))
          .filter(e -> e.getType() == TooSlowLog.SlowLogPayload.Type.ALL
            || e.getType() == TooSlowLog.SlowLogPayload.Type.LARGE_LOG)
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.java`
#### Snippet
```java
    getSlowLogPayloads(final AdminProtos.SlowLogResponseRequest request) {
    List<TooSlowLog.SlowLogPayload> slowLogPayloadList =
      Arrays.stream(slowLogQueue.toArray(new TooSlowLog.SlowLogPayload[0]))
        .filter(e -> e.getType() == TooSlowLog.SlowLogPayload.Type.ALL
          || e.getType() == TooSlowLog.SlowLogPayload.Type.SLOW_LOG)
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationBarrierFamilyFormat.java`
#### Snippet
```java
        result = scanner.next();
        if (result == null) {
          return new ReplicationBarrierResult(new long[0], null, Collections.emptyList());
        }
        byte[] regionName = result.getRow();
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
    try {
      StoreEngine<?, ?, ?, ?> se =
        ReflectionUtils.instantiateWithCustomCtor(className, new Class[] {}, new Object[] {});
      se.createComponentsOnce(conf, store, cellComparator);
      return se;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
    try {
      StoreEngine<?, ?, ?, ?> se =
        ReflectionUtils.instantiateWithCustomCtor(className, new Class[] {}, new Object[] {});
      se.createComponentsOnce(conf, store, cellComparator);
      return se;
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultMemStore.java`
#### Snippet
```java
    byte[] fam = Bytes.toBytes("col");
    byte[] qf = Bytes.toBytes("umn");
    byte[] empty = new byte[0];
    MemStoreSizing memStoreSizing = new NonThreadSafeMemStoreSizing();
    for (int i = 0; i < count; i++) {
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
        && "simple".equalsIgnoreCase(conf.get(User.HBASE_SECURITY_CONF_KEY))
    ) {
      return User.createUserForTesting(conf, user.getShortName(), new String[] {});
    }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
     * (and is an open boundary for the first one).
     */
    public byte[][] stripeEndRows = new byte[0][];

    /**
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
    }
    coprocessors.addAll(rsHost.getCoprocessors());
    return coprocessors.toArray(new String[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
  DatanodeInfo[] getPipeline() {
    AsyncFSOutput output = this.fsOut;
    return output != null ? output.getPipeline() : new DatanodeInfo[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
      }
    }
    return new DatanodeInfo[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java`
#### Snippet
```java
  public byte[] insideBarrier() throws ForeignException {
    flushSnapshot();
    return new byte[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java`
#### Snippet
```java
  private static StackTraceElement[] toStackTrace(List<StackTraceElementMessage> traceList) {
    if (traceList == null || traceList.isEmpty()) {
      return new StackTraceElement[0]; // empty array
    }
    StackTraceElement[] trace = new StackTraceElement[traceList.size()];
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/FullTableBackupClient.java`
#### Snippet
```java
      }

      String[] args = argsList.toArray(new String[0]);

      String jobname = "Full-Backup_" + backupInfo.getBackupId() + "_" + table.getNameAsString();
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/LogRollMasterProcedureManager.java`
#### Snippet
```java

    List<NameStringPair> conf = desc.getConfigurationList();
    byte[] data = new byte[0];
    if (conf.size() > 0) {
      // Get backup root path
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    Set<String> tables = new HashSet<>(Arrays.asList(existingTables));
    Arrays.asList(toRemove).forEach(table -> tables.remove(table));
    return tables.toArray(new String[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      return Bytes.toString(data).split(",");
    }
    return new String[0];
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    Set<String> tables = new HashSet<>(Arrays.asList(existingTables));
    tables.addAll(Arrays.asList(newTables));
    return tables.toArray(new String[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java`
#### Snippet
```java
      }
    }
    return actions.toArray(new Permission.Action[0]);
  }
}
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
      }
    }
    return actions.toArray(new Permission.Action[0]);
  }

```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  public Result mutateRow(RowMutations rm, long nonceGroup, long nonce) throws IOException {
    final List<Mutation> m = rm.getMutations();
    OperationStatus[] statuses = batchMutate(m.toArray(new Mutation[0]), true, nonceGroup, nonce);

    List<Result> results = new ArrayList<>();
```

### ZeroLengthArrayInitialization
Allocation of zero length array
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
      fams.add(Bytes.toBytes(family));
    }
    return createTable(tableName, fams.toArray(new byte[0][]));
  }

```

## RuleId[id=NonExceptionNameEndsWithException]
### NonExceptionNameEndsWithException
Non-exception class name `ConsumerWithException` ends with 'Exception'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java

  @FunctionalInterface
  private interface ConsumerWithException<T, U> {
    void accept(T t, U u) throws IOException;
  }
```

## RuleId[id=OptionalGetWithoutIsPresent]
### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshot.java`
#### Snippet
```java
      builder.setInViolation(status.inViolation);
      if (status.isInViolation()) {
        builder.setViolationPolicy(ProtobufUtil.toProtoViolationPolicy(status.getPolicy().get()));
      }
      return builder.build();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/SaslClientAuthenticationProviders.java`
#### Snippet
```java
    Optional<SaslClientAuthenticationProvider> optional = providers.stream()
      .filter((p) -> p instanceof SimpleSaslClientAuthenticationProvider).findFirst();
    return new Pair<>(optional.get(), null);
  }

```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
      .initialCapacity((int) Math.ceil((1.2 * maximumSizeInBytes) / avgBlockSize)).build();
    this.maxBlockSize = maxBlockSize;
    this.policy = cache.policy().eviction().get();
    this.stats = new CacheStats(getClass().getSimpleName());

```

### OptionalGetWithoutIsPresent
`OptionalLong.getAsLong()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
  @Override
  public long getCurrentSize() {
    return policy.weightedSize().getAsLong();
  }

```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hf = HFile.createReader(fs, hfile.getPath(), CacheConfig.DISABLED, true, getConf());
          Optional<Cell> startKv = hf.getFirstKey();
          start = CellUtil.cloneRow(startKv.get());
          Optional<Cell> endKv = hf.getLastKey();
          end = CellUtil.cloneRow(endKv.get());
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          start = CellUtil.cloneRow(startKv.get());
          Optional<Cell> endKv = hf.getLastKey();
          end = CellUtil.cloneRow(endKv.get());
        } catch (Exception ioe) {
          LOG.warn("Problem reading orphan file " + hfile + ", skipping");
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
                (reader.getFirstKey() != null)
                  && ((storeFirstKey == null) || (comparator.compare(storeFirstKey,
                    ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey().get()).getKey()) > 0))
              ) {
                storeFirstKey = ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey().get()).getKey();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
                    ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey().get()).getKey()) > 0))
              ) {
                storeFirstKey = ((KeyValue.KeyOnlyKeyValue) reader.getFirstKey().get()).getKey();
              }
              if (
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
                (reader.getLastKey() != null)
                  && ((storeLastKey == null) || (comparator.compare(storeLastKey,
                    ((KeyValue.KeyOnlyKeyValue) reader.getLastKey().get()).getKey())) < 0)
              ) {
                storeLastKey = ((KeyValue.KeyOnlyKeyValue) reader.getLastKey().get()).getKey();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
                    ((KeyValue.KeyOnlyKeyValue) reader.getLastKey().get()).getKey())) < 0)
              ) {
                storeLastKey = ((KeyValue.KeyOnlyKeyValue) reader.getLastKey().get()).getKey();
              }
              reader.close();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
              + " for family " + builder.getNameAsString());
          }
          byte[] first = reader.getFirstRowKey().get();
          byte[] last = reader.getLastRowKey().get();

```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
          }
          byte[] first = reader.getFirstRowKey().get();
          byte[] last = reader.getLastRowKey().get();

          LOG.info("Trying to figure out region boundaries hfile=" + hfile + " first="
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java`
#### Snippet
```java
      case SYNC_REPLICATION_UPDATE_LAST_PUSHED_SEQ_ID_FOR_SERIAL_PEER:
        try {
          setLastPushedSequenceId(env, env.getReplicationPeerManager().getPeerConfig(peerId).get());
        } catch (Exception e) {
          throw suspend(env.getMasterConfiguration(),
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java`
#### Snippet
```java
        return Flow.HAS_MORE_STATE;
      case REPLAY_REMOTE_WAL_IN_PEER:
        replayRemoteWAL(env.getReplicationPeerManager().getPeerConfig(peerId).get().isSerial());
        setNextState(
          PeerSyncReplicationStateTransitionState.TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE);
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java`
#### Snippet
```java
  protected void reopenRegions(MasterProcedureEnv env) {
    addChildProcedure(
      env.getReplicationPeerManager().getPeerConfig(peerId).get().getTableCFsMap().keySet().stream()
        .map(ReopenTableRegionsProcedure::new).toArray(ReopenTableRegionsProcedure[]::new));
  }
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SpaceViolationPolicyEnforcementFactory.java`
#### Snippet
```java
      throw new IllegalArgumentException(tableName + " is not in violation. Snapshot=" + snapshot);
    }
    switch (status.getPolicy().get()) {
      case DISABLE:
        enforcement = new DisableTableViolationPolicyEnforcement();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServer.java`
#### Snippet
```java
    // Create hbase-metrics module based metrics. The registry should already be registered by the
    // MetricsRegionServerSource
    metricRegistry = MetricRegistries.global().get(serverSource.getMetricRegistryInfo()).get();

    // create and use metrics from the new hbase-metrics based registry.
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
    }
    Cell midKey = optionalMidKey.get();
    Cell firstKey = reader.getFirstKey().get();
    Cell lastKey = reader.getLastKey().get();
    // if the midkey is the same as the first or last keys, we cannot (ever) split this region.
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
    Cell midKey = optionalMidKey.get();
    Cell firstKey = reader.getFirstKey().get();
    Cell lastKey = reader.getLastKey().get();
    // if the midkey is the same as the first or last keys, we cannot (ever) split this region.
    if (
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
          InetSocketAddress isa = rpcServices.getSocketAddress();
          // here getActiveMaster() is definitely not null.
          String isaHostName = InetAddresses.isInetAddress(getActiveMaster().get().getHostname())
            ? isa.getAddress().getHostAddress()
            : isa.getHostName();
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
          final byte[] first, last;
          try {
            first = reader.getFirstRowKey().get();
            last = reader.getLastRowKey().get();
            LOG.debug("Trying to figure out region boundaries hfile=" + hfile + " first="
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
          try {
            first = reader.getFirstRowKey().get();
            last = reader.getLastRowKey().get();
            LOG.debug("Trying to figure out region boundaries hfile=" + hfile + " first="
              + Bytes.toStringBinary(first) + " last=" + Bytes.toStringBinary(last));
```

### OptionalGetWithoutIsPresent
`Optional.get()` without 'isPresent()' check
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/client/coprocessor/AsyncAggregationClient.java`
#### Snippet
```java
    CompletableFuture<R> future, AsyncTable<AdvancedScanResultConsumer> table,
    ColumnInterpreter<R, S, P, Q, T> ci, Scan scan, NavigableMap<byte[], S> sumByRegion) {
    double halfSum = ci.divideForAvg(sumByRegion.values().stream().reduce(ci::add).get(), 2L);
    S movingSum = null;
    byte[] startRow = null;
```

## RuleId[id=ConstantValue]
### ConstantValue
Condition `Class.forName("org.apache.hadoop.conf.ConfServlet") != null` is always `true`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java`
#### Snippet
```java
    boolean isShowConf = false;
    try {
      if (Class.forName("org.apache.hadoop.conf.ConfServlet") != null) {
        isShowConf = true;
      }
```

### ConstantValue
Condition `nextCapacity > MAX_ARRAY_SIZE` is always `false`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteArrayOutputStream.java`
#### Snippet
```java
      // but make sure there is enough if twice the existing capacity is still too small
      nextCapacity = Math.max(nextCapacity, capacityNeeded);
      if (nextCapacity > MAX_ARRAY_SIZE) {
        throw new BufferOverflowException();
      }
```

### ConstantValue
Value `withTags` is always 'false'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    }
    if (cell instanceof ExtendedCell) {
      return ((ExtendedCell) cell).getSerializedSize(withTags);
    }
    return length(cell.getRowLength(), cell.getFamilyLength(), cell.getQualifierLength(),
```

### ConstantValue
Value `withTags` is always 'false'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
    }
    return length(cell.getRowLength(), cell.getFamilyLength(), cell.getQualifierLength(),
      cell.getValueLength(), cell.getTagsLength(), withTags);
  }

```

### ConstantValue
Condition `provider != null` is always `true`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Encryption.java`
#### Snippet
```java
  public static Key getSecretKeyForSubject(String subject, Configuration conf) throws IOException {
    KeyProvider provider = getKeyProvider(conf);
    if (provider != null) {
      try {
        Key[] keys = provider.getKeys(new String[] { subject });
```

### ConstantValue
Condition `subjectAlts != null` is always `true`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/tls/HBaseHostnameVerifier.java`
#### Snippet
```java
  void verify(final String host, final X509Certificate cert) throws SSLException {
    final List<SubjectName> subjectAlts = getSubjectAltNames(cert);
    if (subjectAlts != null && !subjectAlts.isEmpty()) {
      Optional<InetAddress> inetAddress = parseIpAddress(host);
      if (inetAddress.isPresent()) {
```

### ConstantValue
Result of `getRowLength()` is always '0'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
      return ClassSize.align(FIXED_OVERHEAD)
        // array overhead
        + (getRowLength() == 0 ? ClassSize.sizeOfByteArray(getRowLength()) : getRowLength())
        + (getFamilyLength() == 0
          ? ClassSize.sizeOfByteArray(getFamilyLength())
```

### ConstantValue
Result of `getFamilyLength()` is always '0'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
        + (getRowLength() == 0 ? ClassSize.sizeOfByteArray(getRowLength()) : getRowLength())
        + (getFamilyLength() == 0
          ? ClassSize.sizeOfByteArray(getFamilyLength())
          : getFamilyLength());
    }
```

### ConstantValue
Value `res` is always 'false'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    // row
    boolean res = CellUtil.matchingRows(a, b);
    if (!res) return res;

    // family
```

### ConstantValue
Value `res` is always 'false'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/PrivateCellUtil.java`
#### Snippet
```java
    // family
    res = CellUtil.matchingColumn(a, b);
    if (!res) return res;

    // timestamp: later sorts first
```

### ConstantValue
Condition `a0 == 255` is always `true`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    if (a0 == 253) return 7;
    if (a0 == 254) return 8;
    if (a0 == 255) return 9;
    throw unexpectedHeader(src.peek());
  }
```

### ConstantValue
Condition `null != field` is always `true`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
    for (; null != cl; cl = cl.getSuperclass()) {
      Field[] field = cl.getDeclaredFields();
      if (null != field) {
        for (Field aField : field) {
          if (Modifier.isStatic(aField.getModifiers())) continue;
```

### ConstantValue
Condition `memStoreDeltaSize < 0` is always `true`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsHeapMemoryManagerSourceImpl.java`
#### Snippet
```java
    if (memStoreDeltaSize >= 0) {
      incMemStoreSizeHistogram.add(memStoreDeltaSize);
    } else if (memStoreDeltaSize < 0) {
      decMemStoreSizeHistogram.add(-memStoreDeltaSize);
    }
```

### ConstantValue
Condition `blockCacheDeltaSize < 0` is always `true`
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsHeapMemoryManagerSourceImpl.java`
#### Snippet
```java
    if (blockCacheDeltaSize >= 0) {
      incBlockCacheSizeHistogram.add(blockCacheDeltaSize);
    } else if (blockCacheDeltaSize < 0) {
      decBlockCacheSizeHistogram.add(-blockCacheDeltaSize);
    }
```

### ConstantValue
Condition `family != null` is always `true`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    String timestampStr = humanReadableTimestamp(timestamp);
    byte type = b[o + l - 1];
    return row + "/" + family + (family != null && family.length() > 0 ? ":" : "") + qualifier + "/"
      + timestampStr + "/" + Type.codeToType(type);
  }
```

### ConstantValue
Condition `rightDelimiter < 0` is always `true` when reached
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
        } else if (rightDelimiter < 0 && leftDelimiter >= 0) {
          return 1;
        } else if (leftDelimiter < 0 && rightDelimiter < 0) {
          return 0;
        }
```

### ConstantValue
Condition `rightDelimiter < 0` is always `true` when reached
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
        } else if (rightDelimiter < 0 && leftDelimiter >= 0) {
          return 1;
        } else if (leftDelimiter < 0 && rightDelimiter < 0) {
          return 0;
        }
```

### ConstantValue
Condition `framed` is always `false`
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java

    TTransport transport = new TSocket(new TConfiguration(), host, port, timeout);
    if (framed) {
      transport = new TFramedTransport(transport);
    } else if (secure) {
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")), ByteBuffer.wrap(invalid),
      writeToWal));
    client.mutateRow(demoTable, ByteBuffer.wrap(bytes("foo")), mutations, dummyAttributes);

    // this row name is valid utf8
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    mutations.add(
      new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")), ByteBuffer.wrap(valid), writeToWal));
    client.mutateRow(demoTable, ByteBuffer.wrap(valid), mutations, dummyAttributes);

    // non-utf8 is now allowed in row names because HBase stores values as binary
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")), ByteBuffer.wrap(invalid),
      writeToWal));
    client.mutateRow(demoTable, ByteBuffer.wrap(invalid), mutations, dummyAttributes);

    // Run a scanner on the rows we just created
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    System.out.println("Starting scanner...");
    int scanner =
      client.scannerOpen(demoTable, ByteBuffer.wrap(bytes("")), columnNames, dummyAttributes);

    while (true) {
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("unused:")),
        ByteBuffer.wrap(bytes("DELETE_ME")), writeToWal));
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));
      client.deleteAllRow(demoTable, ByteBuffer.wrap(row), dummyAttributes);
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
        ByteBuffer.wrap(bytes("DELETE_ME")), writeToWal));
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));
      client.deleteAllRow(demoTable, ByteBuffer.wrap(row), dummyAttributes);

```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));
      client.deleteAllRow(demoTable, ByteBuffer.wrap(row), dummyAttributes);

      // sleep to force later timestamp
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:foo")),
        ByteBuffer.wrap(bytes("FOO")), writeToWal));
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
        ByteBuffer.wrap(bytes("FOO")), writeToWal));
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

      Mutation m;
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      m.value = ByteBuffer.wrap(bytes("-1"));
      mutations.add(m);
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      mutations.add(m);
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

      mutations = new ArrayList<>();
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      mutations.add(new Mutation(false, ByteBuffer.wrap(bytes("entry:sqr")),
        ByteBuffer.wrap(bytes(Integer.toString(i * i))), writeToWal));
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
        ByteBuffer.wrap(bytes(Integer.toString(i * i))), writeToWal));
      client.mutateRow(demoTable, ByteBuffer.wrap(row), mutations, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

      // sleep to force later timestamp
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      m.isDelete = true;
      // shouldn't override latest
      client.mutateRowTs(demoTable, ByteBuffer.wrap(row), mutations, 1, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
      // shouldn't override latest
      client.mutateRowTs(demoTable, ByteBuffer.wrap(row), mutations, 1, dummyAttributes);
      printRow(client.getRow(demoTable, ByteBuffer.wrap(row), dummyAttributes));

      List<TCell> versions = client.getVer(demoTable, ByteBuffer.wrap(row),
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

      List<TCell> versions = client.getVer(demoTable, ByteBuffer.wrap(row),
        ByteBuffer.wrap(bytes("entry:num")), 10, dummyAttributes);
      printVersions(ByteBuffer.wrap(row), versions);

```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java

      List<TCell> result = client.get(demoTable, ByteBuffer.wrap(row),
        ByteBuffer.wrap(bytes("entry:foo")), dummyAttributes);

      if (!result.isEmpty()) {
```

### ConstantValue
Value `dummyAttributes` is always 'null'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    System.out.println("Starting scanner...");
    scanner = client.scannerOpenWithStop(demoTable, ByteBuffer.wrap(bytes("00020")),
      ByteBuffer.wrap(bytes("00040")), columnNames, dummyAttributes);

    while (true) {
```

### ConstantValue
Value `param` is always 'null'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java

      final AtomicInteger counter = concurrentCounterCache.getUnchecked(addr);
      Call call = new Call(nextCallId(), md, param, hrc.cellScanner(), returnType,
        hrc.getCallTimeout(), hrc.getPriority(), new RpcCallback<Call>() {
          @Override
```

### ConstantValue
Condition `hasAttributes` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
      s.append(", {");
      // step 2: printing attributes
      if (hasAttributes) {
        s.append("TABLE_ATTRIBUTES => {");

```

### ConstantValue
Condition `ignoreTS` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
        }
      }
    } else if (!ignoreValue && ignoreTS) {
      for (Cell cell : list) {
        if (
```

### ConstantValue
Condition `tags != null` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
    stringMap.put("vlen", c.getValueLength());
    List<Tag> tags = PrivateCellUtil.getTags(c);
    if (tags != null) {
      List<String> tagsString = new ArrayList<>(tags.size());
      for (Tag t : tags) {
```

### ConstantValue
Condition `CellUtil.cloneValue(kv) == null` is always `false` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java`
#### Snippet
```java
  @Override
  public BigDecimal getValue(byte[] colFamily, byte[] colQualifier, Cell kv) throws IOException {
    if (kv == null || CellUtil.cloneValue(kv) == null) {
      return null;
    }
```

### ConstantValue
Condition `other != null` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java`
#### Snippet
```java
    }
    DependentColumnFilter other = (DependentColumnFilter) o;
    return other != null && super.areSerializedFieldsEqual(other)
      && Bytes.equals(this.getFamily(), other.getFamily())
      && Bytes.equals(this.getQualifier(), other.getQualifier())
```

### ConstantValue
Condition `removeCfs != null` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs != null && !removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `!removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Result of `removeCfs.isEmpty()` is always 'false'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `cfs != null` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `cfs != null && (removeCfs == null || removeCfs.isEmpty())` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs == null || removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs != null` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newExcludeTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs != null && !removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newExcludeTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `!removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newExcludeTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Result of `removeCfs.isEmpty()` is always 'false'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
            newExcludeTableCfs.put(table, Lists.newArrayList(cfSet));
          }
        } else if (cfs == null && (removeCfs != null && !removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `cfs != null` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `cfs != null && (removeCfs == null || removeCfs.isEmpty())` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs == null || removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `removeCfs.isEmpty()` is always `true` when reached
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
          throw new ReplicationException("Cannot remove cf of table: " + table
            + " which doesn't specify cfs from exclude-table-cfs config in peer: " + id);
        } else if (cfs != null && (removeCfs == null || removeCfs.isEmpty())) {
          throw new ReplicationException("Cannot remove table: " + table
            + " which has specified cfs from exclude-table-cfs config in peer: " + id);
```

### ConstantValue
Condition `dbs != null` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
      String tName = tabName;
      List<String> dbs = Splitter.on('.').splitToList(tabName);
      if (dbs != null && dbs.size() == 2) {
        Iterator<String> ii = dbs.iterator();
        ns = ii.next();
```

### ConstantValue
Condition `i < fuzzyKeyMeta.length` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
          toInc = i;
        }
      } else if (i < fuzzyKeyMeta.length && fuzzyKeyMeta[i] == -1 /* fixed */) {
        if (order.lt((row[i + offset] & 0xFF), (fuzzyKeyBytes[i] & 0xFF))) {
          // if setting value for any fixed position increased the original array,
```

### ConstantValue
Condition `hasSettings` at the left side of assignment expression is always `false`. Can be simplified
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
  public static boolean isEmptyQuota(final Quotas quotas) {
    boolean hasSettings = false;
    hasSettings |= quotas.hasThrottle();
    hasSettings |= quotas.hasBypassGlobals();
    // Only when there is a space quota, make sure there's actually both fields provided
```

### ConstantValue
Value `includeCompactionState` is always 'true'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
    builder.setRegion(region);
    if (includeCompactionState) {
      builder.setCompactionState(includeCompactionState);
    }
    if (includeBestSplitRow) {
```

### ConstantValue
Value `includeBestSplitRow` is always 'true'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
    }
    if (includeBestSplitRow) {
      builder.setBestSplitRow(includeBestSplitRow);
    }
    return builder.build();
```

### ConstantValue
Condition `ctor != null` is always `true`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureUtil.java`
#### Snippet
```java

      Constructor<?> ctor = clazz.getConstructor();
      assert ctor != null;
      if (!Modifier.isPublic(ctor.getModifiers())) {
        throw new Exception("the " + clazz + " constructor is not public");
```

### ConstantValue
Condition `ctor != null` is always `true`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureUtil.java`
#### Snippet
```java
      @SuppressWarnings("rawtypes")
      Constructor<? extends Procedure> ctor = clazz.asSubclass(Procedure.class).getConstructor();
      assert ctor != null : "no constructor found";
      if (!Modifier.isPublic(ctor.getModifiers())) {
        throw new Exception("the " + clazz + " constructor is not public");
```

### ConstantValue
Value `override` is always 'false'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    if (lockEntry == null && !override) {
      LOG.debug("Waited {} ms, but {} is still running, skipping bypass with force={}", lockWait,
        procedure, override);
      return false;
    } else if (lockEntry == null) {
```

### ConstantValue
Value `override` is always 'true'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    } else if (lockEntry == null) {
      LOG.debug("Waited {} ms, but {} is still running, begin bypass with force={}", lockWait,
        procedure, override);
    }
    try {
```

### ConstantValue
Value `recursive` is always 'true'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
            v -> {
              try {
                bypassProcedure(v.getProcId(), lockWait, override, recursive);
              } catch (IOException e) {
                LOG.warn("Recursive bypass of pid={}", v.getProcId(), e);
```

### ConstantValue
Condition `(reExecute && subprocs == null) || !reExecute` is always `true`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      }

      assert (reExecute && subprocs == null) || !reExecute;
    } while (reExecute);

```

### ConstantValue
Condition `subprocs == null` is always `true` when reached
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      }

      assert (reExecute && subprocs == null) || !reExecute;
    } while (reExecute);

```

### ConstantValue
Condition `!reExecute` is always `true` when reached
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      }

      assert (reExecute && subprocs == null) || !reExecute;
    } while (reExecute);

```

### ConstantValue
Value `reExecute` is always 'false'
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
      }

      assert (reExecute && subprocs == null) || !reExecute;
    } while (reExecute);

```

### ConstantValue
Condition `message instanceof ScanRequest` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      return null;
    }
    if (message instanceof ScanRequest) {
      ScanRequest scanRequest = (ScanRequest) message;
      String regionName = getStringForByteString(scanRequest.getRegion().getValue());
```

### ConstantValue
Condition `components != null` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
    builder.setVersion(version);
    String[] components = version.split("\\.");
    if (components != null && components.length > 2) {
      builder.setVersionMajor(Integer.parseInt(components[0]));
      builder.setVersionMinor(Integer.parseInt(components[1]));
```

### ConstantValue
Condition `parseFrom == null` is always `false`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      Class<?> c = Class.forName(type, true, ClassLoaderHolder.CLASS_LOADER);
      Method parseFrom = c.getMethod(funcName, byte[].class);
      if (parseFrom == null) {
        throw new IOException("Unable to locate function: " + funcName + " in type: " + type);
      }
```

### ConstantValue
Condition `parseFrom == null` is always `false`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      Class<?> c = Class.forName(type, true, ClassLoaderHolder.CLASS_LOADER);
      Method parseFrom = c.getMethod(funcName, byte[].class);
      if (parseFrom == null) {
        throw new IOException("Unable to locate function: " + funcName + " in type: " + type);
      }
```

### ConstantValue
Condition `m instanceof ScanRequest` is always `true`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static String getShortTextFormat(Message m) {
    if (m == null) return "null";
    if (m instanceof ScanRequest) {
      // This should be small and safe to output. No data.
      return TextFormat.shortDebugString(m);
```

### ConstantValue
Value `isRetry` is always 'false'
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java
                return;
              }
              LOG.debug("Node {} already deleted, retry={}", path, isRetry);
              TraceUtil.setError(span, e);
              throw e;
```

### ConstantValue
Condition `node == null` is always `false`
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
    assert node != null : "expected node to update procId=" + procId;
    assert node.contains(procId) : "expected procId=" + procId + " in the node";
    if (node == null) {
      throw new NullPointerException("pid=" + procId);
    }
```

### ConstantValue
Condition `deleteAllAtEOJ` is always `true`
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
        getAndPrintRowContents(table);

        if (deleteAllAtEOJ) {
          deleteRow(table);
        }
```

### ConstantValue
Condition `deleteAllAtEOJ` is always `true`
in `hbase-archetypes/hbase-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/client/HelloHBase.java`
#### Snippet
```java
      }

      if (deleteAllAtEOJ) {
        deleteNamespaceAndTable(admin);
      }
```

### ConstantValue
Value `hasParser` is always 'true'
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java
    }

    HAS_PARSER = hasParser;
  }
}
```

### ConstantValue
Condition `tableNames == null` is always `false`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java`
#### Snippet
```java
    // expecting exactly one path
    Path[] tableNames = FileInputFormat.getInputPaths(job);
    if (tableNames == null || tableNames.length > 1) {
      throw new IOException("expecting one table name");
    }
```

### ConstantValue
Value `raw` is always 'true'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java
    boolean raw = Boolean.parseBoolean(conf.get(RAW_SCAN));
    if (raw) {
      s.setRaw(raw);
    }
    for (String columnFamily : conf.getTrimmedStrings(TableInputFormat.SCAN_COLUMN_FAMILY)) {
```

### ConstantValue
Condition `endRow.length != 0` is always `true` when reached
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
      endRow[0] = -1;
    }
    if (startRow.length == 0 && endRow.length != 0) {
      startRow = new byte[1];
      startRow[0] = 0;
```

### ConstantValue
Condition `startRow.length != 0` is always `true`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
      startRow[0] = 0;
    }
    if (startRow.length != 0 && endRow.length == 0) {
      endRow = new byte[startRow.length];
      for (int k = 0; k < startRow.length; k++) {
```

### ConstantValue
Condition `readingSnapshot && dstTableName == null` is always `false`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      }

      if (readingSnapshot && dstTableName == null) {
        printUsage("The --new.name=<table> for destination table should be "
          + "provided when copying data from snapshot .");
```

### ConstantValue
Condition `dstTableName == null` is always `false` when reached
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java`
#### Snippet
```java
      }

      if (readingSnapshot && dstTableName == null) {
        printUsage("The --new.name=<table> for destination table should be "
          + "provided when copying data from snapshot .");
```

### ConstantValue
Condition `deleteAllAtEOJ` is always `true`
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
        getAndPrintRowContents(table);

        if (deleteAllAtEOJ) {
          deleteRow(table);
        }
```

### ConstantValue
Condition `deleteAllAtEOJ` is always `true`
in `hbase-archetypes/hbase-shaded-client-project/src/main/java/org/apache/hbase/archetypes/exemplars/shaded_client/HelloHBase.java`
#### Snippet
```java
      }

      if (deleteAllAtEOJ) {
        deleteNamespaceAndTable(admin);
      }
```

### ConstantValue
Value `put` is always 'null'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
          PrivateCellUtil.createFirstOnRow(key.get(), key.getOffset(), (short) key.getLength()))
      ) {
        processKV(key, result, context, put, delete);
      }
    }
```

### ConstantValue
Value `delete` is always 'null'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
          PrivateCellUtil.createFirstOnRow(key.get(), key.getOffset(), (short) key.getLength()))
      ) {
        processKV(key, result, context, put, delete);
      }
    }
```

### ConstantValue
Condition `values.length == 0` is always `false` when reached
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
      return null;
    }
    if (keys.length == 0 || values.length == 0) {
      LOG.error("keys and values arrays can not be empty;");
      return null;
```

### ConstantValue
Condition `max <= min` is always `false`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/CostFunction.java`
#### Snippet
```java
      return 0;
    }
    if (max <= min || Math.abs(max - min) <= COST_EPSILON) {
      return 0;
    }
```

### ConstantValue
Condition `requestLog != null` is always `true`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
    RequestLog requestLog = HttpRequestLog.getRequestLog(name);

    if (requestLog != null) {
      RequestLogHandler requestLogHandler = new RequestLogHandler();
      requestLogHandler.setRequestLog(requestLog);
```

### ConstantValue
Condition `fetchFromTail` is always `false`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      for (int i = 0; i <= numToOffload;) {
        RegionInfo hri = regions.get(i); // fetch from head
        if (fetchFromTail) {
          hri = regions.get(regions.size() - 1 - i);
        }
```

### ConstantValue
Value `fetchFromTail` is always 'false'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
        }

        addRegionPlan(regionsToMove, fetchFromTail, si, regionsToReturn);

        underloadedServers.put(si, numToTake - 1);
```

### ConstantValue
Value `fetchFromTail` is always 'false'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
      int numTaken = 0;
      while (numTaken < numToTake && 0 < regionsToMove.size()) {
        addRegionPlan(regionsToMove, fetchFromTail, server.getKey().getServerName(),
          regionsToReturn);
        numTaken++;
```

### ConstantValue
Value `fetchFromTail` is always 'false'
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java

    if (min != max) {
      balanceOverall(regionsToReturn, serverBalanceInfo, fetchFromTail, regionsToMove, max, min);
    }

```

### ConstantValue
Condition `this.regionThreads != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java`
#### Snippet
```java
   */
  public void join() {
    if (this.regionThreads != null) {
      for (Thread t : this.regionThreads) {
        if (t.isAlive()) {
```

### ConstantValue
Condition `this.masterThreads != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java`
#### Snippet
```java
      }
    }
    if (this.masterThreads != null) {
      for (Thread t : this.masterThreads) {
        if (t.isAlive()) {
```

### ConstantValue
Value `b` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
        boolean b = delegate.next();
        if (!b) {
          return b;
        }
        // constrain the bottom.
```

### ConstantValue
Value `b` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java
        boolean b = delegate.seekTo();
        if (!b) {
          return b;
        }
        // Check key.
```

### ConstantValue
Condition `args[0] != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
                res != null && args != null && args.length == 3
                  && "getBlockLocations".equals(method.getName()) && res instanceof LocatedBlocks
                  && args[0] instanceof String && args[0] != null
              ) {
                lrb.reorderBlocks(conf, (LocatedBlocks) res, (String) args[0]);
```

### ConstantValue
Value `useHBaseChecksum` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.java`
#### Snippet
```java
      this.streamNoFsChecksum = (link != null) ? link.open(fsNc) : fsNc.open(path);
      setStreamOptions(streamNoFsChecksum);
      this.useHBaseChecksumConfigured = this.useHBaseChecksum = useHBaseChecksum;
      // Close the checksum stream; we will reopen it if we get an HBase checksum failure.
      this.stream.close();
```

### ConstantValue
Value `isHBaseChecksum` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java`
#### Snippet
```java
      boolean isHBaseChecksum = fsdis.shouldUseHBaseChecksum();
      assert !isHBaseChecksum; // Initially we must read with FS checksum.
      FixedFileTrailer.readFromStream(fsdis.getStream(isHBaseChecksum), size);
      return true;
    } catch (IllegalArgumentException e) {
```

### ConstantValue
Condition `block != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
        // in the retrieved data block. Hence returned the block so that
        // the ref count can be decremented
        if (block != null) {
          block.release();
        }
```

### ConstantValue
Condition `0 <= i` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java

      int i = -pos - 1;
      assert 0 <= i && i <= blockKeys.length;
      return i - 1;
    }
```

### ConstantValue
Condition `0 <= i` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java

      int i = -pos - 1;
      assert 0 <= i && i <= blockKeys.length;
      return i - 1;
    }
```

### ConstantValue
Value `cacheOnly` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
          cacheConf.getBlockCache().ifPresent(cache -> {
            if (cacheable && cacheConf.shouldCacheBlockOnRead(category)) {
              cache.cacheBlock(cacheKey, hfileBlock, cacheConf.isInMemory(), cacheOnly);
            }
          });
```

### ConstantValue
Condition `this.encoding != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java`
#### Snippet
```java
  public void startBlockEncoding(HFileBlockEncodingContext encodingCtx, DataOutputStream out)
    throws IOException {
    if (this.encoding != null && this.encoding != DataBlockEncoding.NONE) {
      this.encoding.getEncoder().startBlockEncoding(encodingCtx, out);
    }
```

### ConstantValue
Condition `0 <= i` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java

      int i = -pos - 1;
      assert 0 <= i && i <= blockKeys.length;
      return i - 1;
    }
```

### ConstantValue
Condition `block != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
        // in the retrieved data block. Hence returned the block so that
        // the ref count can be decremented
        if (block != null) {
          block.release();
        }
```

### ConstantValue
Value `repeat` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.java`
#### Snippet
```java
      }
      if (victimCache != null) {
        value = victimCache.getBlock(cacheKey, caching, repeat, updateCacheMetrics);
        if ((value != null) && caching) {
          cacheBlock(cacheKey, value);
```

### ConstantValue
Value `repeat` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
      if (victimHandler != null && !repeat) {
        // The handler will increase result's refCnt for RPC, so need no extra retain.
        Cacheable result = victimHandler.getBlock(cacheKey, caching, repeat, updateCacheMetrics);
        // Promote this to L1.
        if (result != null) {
```

### ConstantValue
Value `repeat` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
      if (victimHandler != null && !repeat) {
        // The handler will increase result's refCnt for RPC, so need no extra retain.
        Cacheable result = victimHandler.getBlock(cacheKey, caching, repeat, updateCacheMetrics);
        // Promote this to L1.
        if (result != null) {
```

### ConstantValue
Value `doVerificationThruHBaseChecksum` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
            String msg = "HBase checksum verification failed for file " + pathName + " at offset "
              + offset + " filesize " + fileSize + " but this cannot happen because doVerify is "
              + doVerificationThruHBaseChecksum;
            HFile.LOG.warn(msg);
            throw new IOException(msg); // cannot happen case here
```

### ConstantValue
Value `doVerificationThruHBaseChecksum` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
          doVerificationThruHBaseChecksum = false;
          blk = readBlockDataInternal(is, offset, onDiskSizeWithHeaderL, pread,
            doVerificationThruHBaseChecksum, updateMetrics, intoHeap);
          if (blk != null) {
            HFile.LOG.warn(
```

### ConstantValue
Condition `!doVerificationThruHBaseChecksum` is always `true` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
          }
        }
        if (blk == null && !doVerificationThruHBaseChecksum) {
          String msg =
            "readBlockData failed, possibly due to " + "checksum verification failed for file "
```

### ConstantValue
Value `doVerificationThruHBaseChecksum` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
          }
        }
        if (blk == null && !doVerificationThruHBaseChecksum) {
          String msg =
            "readBlockData failed, possibly due to " + "checksum verification failed for file "
```

### ConstantValue
Value `usesHBaseChecksum` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
      fileContextBuilder.withBytesPerCheckSum(0);
      // Need to fix onDiskDataSizeWithHeader; there are not checksums after-block-data
      onDiskDataSizeWithHeader = onDiskSizeWithoutHeader + headerSize(usesHBaseChecksum);
    }
    fileContext = fileContextBuilder.build();
```

### ConstantValue
Condition `param instanceof RegionServerStatusProtos.ReportRegionStateTransitionRequest` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
    // However changing that would require a pretty drastic change and should be done for
    // the next major release and not as a fix for HBASE-14239
    if (param instanceof RegionServerStatusProtos.ReportRegionStateTransitionRequest) {
      return true;
    }
```

### ConstantValue
Value `param` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
    // However changing that would require a pretty drastic change and should be done for
    // the next major release and not as a fix for HBASE-14239
    if (param instanceof RegionServerStatusProtos.ReportRegionStateTransitionRequest) {
      return true;
    }
```

### ConstantValue
Condition `param instanceof RegionServerStatusProtos.RegionServerStartupRequest` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
      return true;
    }
    if (param instanceof RegionServerStatusProtos.RegionServerStartupRequest) {
      return true;
    }
```

### ConstantValue
Value `param` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
      return true;
    }
    if (param instanceof RegionServerStatusProtos.RegionServerStartupRequest) {
      return true;
    }
```

### ConstantValue
Condition `param instanceof RegionServerStatusProtos.RegionServerReportRequest` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
      return true;
    }
    if (param instanceof RegionServerStatusProtos.RegionServerReportRequest) {
      return true;
    }
```

### ConstantValue
Value `param` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
      return true;
    }
    if (param instanceof RegionServerStatusProtos.RegionServerReportRequest) {
      return true;
    }
```

### ConstantValue
Value `cr` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/AdaptiveLifoCoDelCallQueue.java`
#### Snippet
```java
      }
      if (cr == null) {
        return cr;
      }
      if (needToDrop(cr)) {
```

### ConstantValue
Condition `!HConstants.HREGION_LOGDIR_NAME.endsWith("/")` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    }
    startPathSB.append(HConstants.HREGION_LOGDIR_NAME);
    if (!HConstants.HREGION_LOGDIR_NAME.endsWith("/")) {
      startPathSB.append('/');
    }
```

### ConstantValue
Result of `HConstants.HREGION_LOGDIR_NAME.endsWith("/")` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    }
    startPathSB.append(HConstants.HREGION_LOGDIR_NAME);
    if (!HConstants.HREGION_LOGDIR_NAME.endsWith("/")) {
      startPathSB.append('/');
    }
```

### ConstantValue
Condition `reader == null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    Exception ee = null;
    org.apache.hadoop.hbase.wal.WAL.Reader reader = null;
    while (reader == null && attempt++ < maxAttempts) {
      try {
        // Detect if this is a new file, if so get a new reader else
```

### ConstantValue
Condition `reader == null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
        ee = npe;
      }
      if (reader == null) {
        // sleep before next attempt
        try {
```

### ConstantValue
Value `corrupt` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      if (cancel != null && !cancel.progress()) {
        cancelled = true;
        return new SplitWALResult(false, corrupt);
      }
      walReader = getReader(walStatus, this.skipErrors, cancel);
```

### ConstantValue
Value `corrupt` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      if (walReader == null) {
        LOG.warn("Nothing in {}; empty?", wal);
        return new SplitWALResult(true, corrupt);
      }
      LOG.info("Open {} took {}ms", wal, EnvironmentEdgeManager.currentTime() - startTS);
```

### ConstantValue
Value `corrupt` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
          if (cancel != null && !cancel.progress()) {
            cancelled = true;
            return new SplitWALResult(false, corrupt);
          }
        }
```

### ConstantValue
Condition `buffer != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/OutputSink.java`
#### Snippet
```java
        }

        assert buffer != null;
        try {
          writeBuffer(buffer);
```

### ConstantValue
Value `pattern` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        try {
          LOG.debug(String.format("reading list of tables"));
          tds = this.admin.listTableDescriptors(pattern);
          if (tds == null) {
            tds = Collections.emptyList();
```

### ConstantValue
Value `stop` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/OOMEChecker.java`
#### Snippet
```java
      }
    }
    return stop;
  }
}
```

### ConstantValue
Condition `splits == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
      // remove endpoints, which are included in the splits list

      return splits == null ? null : Arrays.copyOfRange(splits, 1, splits.length - 1);
    }

```

### ConstantValue
Value `superEq` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
      boolean superEq = super.equals(o);
      if (!superEq) {
        return superEq;
      }

```

### ConstantValue
Condition `movedRegions != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
        return false;
      } finally {
        if (movedRegions != null) {
          writeFile(filename, movedRegions);
        }
```

### ConstantValue
Condition `conf == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java`
#### Snippet
```java
    if (!isGeneralBloomEnabled(conf)) {
      LOG.trace("Bloom filters are disabled by configuration for " + writer.getPath()
        + (conf == null ? " (configuration is null)" : ""));
      return null;
    } else if (bloomType == BloomType.NONE) {
```

### ConstantValue
Condition `conf == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java`
#### Snippet
```java
    if (!isDeleteFamilyBloomEnabled(conf)) {
      LOG.info("Delete Bloom filters are disabled by configuration for " + writer.getPath()
        + (conf == null ? " (configuration is null)" : ""));
      return null;
    }
```

### ConstantValue
Condition `ranges.isEmpty()` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
        }

      } else if (ranges.isEmpty()) {
        if (problemKey != null) {
          LOG.warn("reached end of problem group: " + Bytes.toStringBinary(key));
```

### ConstantValue
Condition `isDeployed` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      // We shouldn't have record of this region at all then!
      assert false : "Entry for region with no data";
    } else if (!inMeta && !inHdfs && isDeployed) {
      errors.reportError(ERROR_CODE.NOT_IN_META_HDFS,
        "Region " + descriptiveName + ", key=" + key + ", not on HDFS or in hbase:meta but "
```

### ConstantValue
Condition `inHdfs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      }

    } else if (!inMeta && inHdfs && !isDeployed) {
      if (hbi.isMerged()) {
        // This region has already been merged, the remaining hdfs file will be
```

### ConstantValue
Condition `inHdfs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      }

    } else if (!inMeta && inHdfs && isDeployed) {
      errors.reportError(ERROR_CODE.NOT_IN_META, "Region " + descriptiveName
        + " not in META, but deployed on " + Joiner.on(", ").join(hbi.getDeployedOn()));
```

### ConstantValue
Condition `isDeployed` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      }

    } else if (!inMeta && inHdfs && isDeployed) {
      errors.reportError(ERROR_CODE.NOT_IN_META, "Region " + descriptiveName
        + " not in META, but deployed on " + Joiner.on(", ").join(hbi.getDeployedOn()));
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

      // ========== Cases where the region is in hbase:meta =============
    } else if (inMeta && inHdfs && !isDeployed && splitParent) {
      // check whether this is an actual error, or just transient state where parent
      // is not cleaned
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        resetSplitParent(hbi);
      }
    } else if (inMeta && !inHdfs && !isDeployed) {
      errors.reportError(ERROR_CODE.NOT_IN_HDFS_OR_DEPLOYED, "Region " + descriptiveName
        + " found in META, but not in HDFS " + "or deployed on any region server.");
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        deleteMetaRegion(hbi);
      }
    } else if (inMeta && !inHdfs && isDeployed) {
      errors.reportError(ERROR_CODE.NOT_IN_HDFS,
        "Region " + descriptiveName + " found in META, but not in HDFS, " + "and deployed on "
```

### ConstantValue
Condition `isDeployed` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        deleteMetaRegion(hbi);
      }
    } else if (inMeta && !inHdfs && isDeployed) {
      errors.reportError(ERROR_CODE.NOT_IN_HDFS,
        "Region " + descriptiveName + " found in META, but not in HDFS, " + "and deployed on "
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        deleteMetaRegion(hbi);
      }
    } else if (inMeta && inHdfs && !isDeployed && shouldBeDeployed) {
      errors.reportError(ERROR_CODE.NOT_DEPLOYED,
        "Region " + descriptiveName + " not deployed on any region server.");
```

### ConstantValue
Condition `inHdfs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        deleteMetaRegion(hbi);
      }
    } else if (inMeta && inHdfs && !isDeployed && shouldBeDeployed) {
      errors.reportError(ERROR_CODE.NOT_DEPLOYED,
        "Region " + descriptiveName + " not deployed on any region server.");
```

### ConstantValue
Condition `shouldBeDeployed` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        deleteMetaRegion(hbi);
      }
    } else if (inMeta && inHdfs && !isDeployed && shouldBeDeployed) {
      errors.reportError(ERROR_CODE.NOT_DEPLOYED,
        "Region " + descriptiveName + " not deployed on any region server.");
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        "Region " + descriptiveName + " not deployed on any region server.");
      tryAssignmentRepair(hbi, "Trying to fix unassigned region...");
    } else if (inMeta && inHdfs && isDeployed && !shouldBeDeployed) {
      errors.reportError(ERROR_CODE.SHOULD_NOT_BE_DEPLOYED,
        "Region " + descriptiveName + " should not be deployed according "
```

### ConstantValue
Condition `inHdfs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        "Region " + descriptiveName + " not deployed on any region server.");
      tryAssignmentRepair(hbi, "Trying to fix unassigned region...");
    } else if (inMeta && inHdfs && isDeployed && !shouldBeDeployed) {
      errors.reportError(ERROR_CODE.SHOULD_NOT_BE_DEPLOYED,
        "Region " + descriptiveName + " should not be deployed according "
```

### ConstantValue
Condition `isDeployed` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        "Region " + descriptiveName + " not deployed on any region server.");
      tryAssignmentRepair(hbi, "Trying to fix unassigned region...");
    } else if (inMeta && inHdfs && isDeployed && !shouldBeDeployed) {
      errors.reportError(ERROR_CODE.SHOULD_NOT_BE_DEPLOYED,
        "Region " + descriptiveName + " should not be deployed according "
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isMultiplyDeployed) {
      errors.reportError(ERROR_CODE.MULTI_DEPLOYED,
        "Region " + descriptiveName + " is listed in hbase:meta on region server "
```

### ConstantValue
Condition `inHdfs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isMultiplyDeployed) {
      errors.reportError(ERROR_CODE.MULTI_DEPLOYED,
        "Region " + descriptiveName + " is listed in hbase:meta on region server "
```

### ConstantValue
Condition `inMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
      errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META,
        "Region " + descriptiveName + " listed in hbase:meta on region server "
```

### ConstantValue
Condition `inMeta && inHdfs && isDeployed && !deploymentMatchesMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
      errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META,
        "Region " + descriptiveName + " listed in hbase:meta on region server "
```

### ConstantValue
Condition `inHdfs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
      errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META,
        "Region " + descriptiveName + " listed in hbase:meta on region server "
```

### ConstantValue
Condition `isDeployed` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
      errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META,
        "Region " + descriptiveName + " listed in hbase:meta on region server "
```

### ConstantValue
Condition `!deploymentMatchesMeta` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
      errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META,
        "Region " + descriptiveName + " listed in hbase:meta on region server "
```

### ConstantValue
Value `deploymentMatchesMeta` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          hbi.getDeployedOn());
      }
    } else if (inMeta && inHdfs && isDeployed && !deploymentMatchesMeta) {
      errors.reportError(ERROR_CODE.SERVER_DOES_NOT_MATCH_META,
        "Region " + descriptiveName + " listed in hbase:meta on region server "
```

### ConstantValue
Condition `valid` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
            && (currentRegionBoundariesInformation.metaFirstKey != null)
        ) {
          valid = valid && comparator.compare(currentRegionBoundariesInformation.storesFirstKey,
            currentRegionBoundariesInformation.metaFirstKey) >= 0;
        }
```

### ConstantValue
Condition `servers.size() > 1` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        if (servers.isEmpty()) {
          assignMetaReplica(i);
        } else if (servers.size() > 1) {
          errors.reportError(ERROR_CODE.MULTI_META_REGION, "hbase:meta, replicaId "
            + metaHbckRegionInfo.getReplicaId() + " is found on more than one region.");
```

### ConstantValue
Value `b` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

  public static byte[] keyOnly(byte[] b) {
    if (b == null) return b;
    int rowlength = Bytes.toShort(b, 0);
    byte[] result = new byte[rowlength];
```

### ConstantValue
Condition `servers != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
            ServerName server = servers.get(p.ordinal());
            Float currentLocality = 0f;
            if (servers != null) {
              currentLocality = regionLocality.get(server.getHostname());
              if (currentLocality == null) {
```

### ConstantValue
Condition `i == 2` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        } else if (i == 1) {
          copy = "secondary";
        } else if (i == 2) {
          copy = "tertiary";
        }
```

### ConstantValue
Condition `oldServers != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
            ServerName oldServer = oldServers.get(p.ordinal());
            Float oldLocality = 0f;
            if (oldServers != null) {
              oldLocality = regionLocality.get(oldServer.getHostname());
              if (oldLocality == null) {
```

### ConstantValue
Condition `i == 2` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        } else if (i == 1) {
          System.out.print("secondary ");
        } else if (i == 2) {
          System.out.print("tertiary ");
        }
```

### ConstantValue
Condition `param instanceof RegionServerStatusProtos.RegionServerReportRequest` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterAnnotationReadingPriorityFunction.java`
#### Snippet
```java
    }
    // also use HIGH_QOS for region server report
    if (param instanceof RegionServerStatusProtos.RegionServerReportRequest) {
      return HConstants.HIGH_QOS;
    }
```

### ConstantValue
Value `param` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterAnnotationReadingPriorityFunction.java`
#### Snippet
```java
    }
    // also use HIGH_QOS for region server report
    if (param instanceof RegionServerStatusProtos.RegionServerReportRequest) {
      return HConstants.HIGH_QOS;
    }
```

### ConstantValue
Condition `this.metricsMasterFilesystem != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
    splitTime = EnvironmentEdgeManager.currentTime() - splitTime;

    if (this.metricsMasterFilesystem != null) {
      if (filter == META_FILTER) {
        this.metricsMasterFilesystem.addMetaWALSplit(splitTime, splitLogSize);
```

### ConstantValue
Condition `sl != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
      server.getAssignmentManager().reportOnlineRegions(serverName,
        newLoad.getRegionMetrics().keySet());
      if (sl != null && server.metricsMaster != null) {
        // Up our metrics.
        server.metricsMaster.incrementRequests(
```

### ConstantValue
Value `ri` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/janitor/ReportMakingVisitor.java`
#### Snippet
```java
    if (ri == null) {
      this.report.emptyRegionInfo.add(metaTableRow.getRow());
      return ri;
    }

```

### ConstantValue
Value `clusterId` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      LOG.warn("Cluster ID file does not exist at {}", idPath);
    }
    return clusterId;
  }

```

### ConstantValue
Condition `!shortCircuitSkipChecksum` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
            + "it, see https://issues.apache.org/jira/browse/HBASE-6868."
          : ""));
      assert !shortCircuitSkipChecksum; // this will fail if assertions are on
    }
    checkShortCircuitReadBufferSize(conf);
```

### ConstantValue
Value `shortCircuitSkipChecksum` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
            + "it, see https://issues.apache.org/jira/browse/HBASE-6868."
          : ""));
      assert !shortCircuitSkipChecksum; // this will fail if assertions are on
    }
    checkShortCircuitReadBufferSize(conf);
```

### ConstantValue
Condition `ss != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java`
#### Snippet
```java
    FileSystem fs = mfs.getFileSystem();
    List<SnapshotDescription> ss = getCompletedSnapshots(new Path(rootDir, oldSnapshotDir), false);
    if (ss != null && !ss.isEmpty()) {
      LOG.error("Snapshots from an earlier release were found under: " + oldSnapshotDir);
      LOG.error("Please rename the directory as " + HConstants.SNAPSHOT_DIR_NAME);
```

### ConstantValue
Condition `task != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
          break;
        }
        if (task != null) {
          LOG.trace("Removing {}", task.filePath);
          boolean succeed;
```

### ConstantValue
Condition `mobTableDir != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java`
#### Snippet
```java

      // Delete the table directory where the mob files are saved
      if (mobTableDir != null && fs.exists(mobTableDir)) {
        if (!fs.delete(mobTableDir, true)) {
          throw new IOException("Couldn't delete mob dir " + mobTableDir);
```

### ConstantValue
Condition `mergeRegionFs != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java`
#### Snippet
```java
      mergedFiles.addAll(mergeStoreFiles(env, regionFs, mergeRegionFs, mergedRegion));
    }
    assert mergeRegionFs != null;
    mergeRegionFs.commitMergedRegion(mergedFiles, env);

```

### ConstantValue
Condition `queue == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
      final TableQueue queue = getTableQueue(table);
      final LockAndQueue tableLock = locking.getTableLock(table);
      if (queue == null) return true;

      if (queue.isEmpty() && tableLock.tryExclusiveLock(procedure)) {
```

### ConstantValue
Condition `!delegate.isEmpty()` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java`
#### Snippet
```java
    try {
      delegate.add(e);
      if (!delegate.isEmpty()) {
        notEmpty.signal();
      }
```

### ConstantValue
Result of `delegate.isEmpty()` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java`
#### Snippet
```java
    try {
      delegate.add(e);
      if (!delegate.isEmpty()) {
        notEmpty.signal();
      }
```

### ConstantValue
Condition `newInViolation` is always `true` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SpaceQuotaRefresherChore.java`
#### Snippet
```java
            }
            getManager().disableViolationPolicyEnforcement(tableName);
          } else if (currInViolation && newInViolation) {
            if (LOG.isTraceEnabled()) {
              LOG.trace("Switching quota violation policy on " + tableName + " from "
```

### ConstantValue
Condition `userPerms != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
        // Are there permissions for this user?
        List<Permission> userPerms = kvPerms.get(user.getShortName());
        if (userPerms != null) {
          results.addAll(userPerms);
        }
```

### ConstantValue
Condition `results != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
          for (String group : groupNames) {
            List<Permission> groupPerms = kvPerms.get(AuthUtil.toGroupEntry(group));
            if (results != null) {
              results.addAll(groupPerms);
            }
```

### ConstantValue
Value `meta` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    if (!meta && !metaLoaded) {
      throw new PleaseHoldException(
        "Master not fully online; hbase:meta=" + meta + ", metaLoaded=" + metaLoaded);
    }
  }
```

### ConstantValue
Value `metaLoaded` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    if (!meta && !metaLoaded) {
      throw new PleaseHoldException(
        "Master not fully online; hbase:meta=" + meta + ", metaLoaded=" + metaLoaded);
    }
  }
```

### ConstantValue
Condition `delInfo.tags.isEmpty()` is always `true` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityNewVersionBehaivorTracker.java`
#### Snippet
```java
    Byte putCellVisTagsFormat = VisibilityUtils.extractVisibilityTags(put, putVisTags);
    return putVisTags.isEmpty() == delInfo.tags.isEmpty()
      && ((putVisTags.isEmpty() && delInfo.tags.isEmpty())
        || VisibilityLabelServiceManager.getInstance().getVisibilityLabelService()
          .matchVisibility(putVisTags, putCellVisTagsFormat, delInfo.tags, delInfo.format));
```

### ConstantValue
Condition `labelsRegion == null` is always `false` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
  public List<String> getUserAuths(byte[] user, boolean systemCall) throws IOException {
    assert (labelsRegion != null || systemCall);
    if (systemCall || labelsRegion == null) {
      return this.labelsCache.getUserAuths(Bytes.toString(user));
    }
```

### ConstantValue
Condition `labelsRegion == null` is always `false` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
  public List<String> getGroupAuths(String[] groups, boolean systemCall) throws IOException {
    assert (labelsRegion != null || systemCall);
    if (systemCall || labelsRegion == null) {
      return this.labelsCache.getGroupAuths(groups);
    }
```

### ConstantValue
Condition `visibilityLabelFilter != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    Filter visibilityLabelFilter =
      VisibilityUtils.createVisibilityLabelFilter(e.getEnvironment().getRegion(), authorizations);
    if (visibilityLabelFilter != null) {
      Filter filter = get.getFilter();
      if (filter != null) {
```

### ConstantValue
Condition `visibilityLabelFilter != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java`
#### Snippet
```java
    Filter visibilityLabelFilter =
      VisibilityUtils.createVisibilityLabelFilter(region, authorizations);
    if (visibilityLabelFilter != null) {
      Filter filter = scan.getFilter();
      if (filter != null) {
```

### ConstantValue
Condition `opMeterName == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java`
#### Snippet
```java
      // Mark access type ["get", "put", "delete"] metric
      String opMeterName = opMeterName(op);
      if (opMeterName == null || opMeterName.isEmpty()) {
        return;
      }
```

### ConstantValue
Value `sinkPeer` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
      sinkPeer = getReplicationSink();
    } catch (IOException e) {
      this.onReplicateWALEntryException(entriesHashCode, e, sinkPeer);
      resultCompletableFuture.completeExceptionally(e);
      return resultCompletableFuture;
```

### ConstantValue
Value `bypass` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
    boolean bypass = false;
    if (observerOperation == null) {
      return bypass;
    }
    List<E> envs = coprocEnvironments.get();
```

### ConstantValue
Condition `bypass` at the left side of assignment expression is always `false`. Can be simplified
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
      }
      // Internal to shouldBypass, it checks if obeserverOperation#isBypassable().
      bypass |= observerOperation.shouldBypass();
      observerOperation.postEnvCall();
      if (bypass) {
```

### ConstantValue
Value `childrenPaths` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
      Thread.sleep(sleepTime);
    }
    return childrenPaths;
  }

```

### ConstantValue
Condition `m == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
  public static Pair<String, String> getReferredToRegionAndFile(final String referenceFile) {
    Matcher m = REF_NAME_PATTERN.matcher(referenceFile);
    if (m == null || !m.matches()) {
      LOG.warn("Failed match of store file name {}", referenceFile);
      throw new IllegalArgumentException("Failed match of store file name " + referenceFile);
```

### ConstantValue
Condition `m == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
  public static Path getReferredToFile(final Path p) {
    Matcher m = REF_NAME_PATTERN.matcher(p.getName());
    if (m == null || !m.matches()) {
      LOG.warn("Failed match of store file name {}", p.toString());
      throw new IllegalArgumentException("Failed match of store file name " + p.toString());
```

### ConstantValue
Condition `initialPath != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
      return false;
    }
    if (initialPath != o.initialPath && initialPath != null && !initialPath.equals(o.initialPath)) {
      return false;
    }
```

### ConstantValue
Condition `reference != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
      return false;
    }
    if (reference != o.reference && reference != null && !reference.equals(o.reference)) {
      return false;
    }
```

### ConstantValue
Condition `link != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
      return false;
    }
    if (link != o.link && link != null && !link.equals(o.link)) {
      return false;
    }
```

### ConstantValue
Condition `set == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
      if (entry.getValue() instanceof Set) {
        Set<byte[]> set = (Set<byte[]>) entry.getValue();
        if (set == null || set.isEmpty()) {
          get.addFamily(col);
        } else {
```

### ConstantValue
Condition `list == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
      } else if (entry.getValue() instanceof List) {
        List<Cell> list = (List<Cell>) entry.getValue();
        if (list == null || list.isEmpty()) {
          get.addFamily(col);
        } else {
```

### ConstantValue
Condition `considerCellTs` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
              ) {
                byte type = col.getTypeByte();
                if (considerCellTs) {
                  curColCheckTs = col.getTimestamp();
                }
```

### ConstantValue
Condition `acls != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
        List<UserPermission> acls =
          PermissionStorage.getUserTablePermissions(conf, tableName, null, null, null, false);
        if (acls != null) {
          tableAcls.put(tableName, acls);
        }
```

### ConstantValue
Condition `storeFileTracker != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
    this.storeFileTracker = createStoreFileTracker(conf, store);
    assert compactor != null && compactionPolicy != null && storeFileManager != null
      && storeFlusher != null && storeFileTracker != null;
  }

```

### ConstantValue
Condition `region.stores != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionWrapperImpl.java`
#### Snippet
```java
      long avgAgeNumerator = 0;
      long numHFiles = 0;
      if (region.stores != null) {
        for (HStore store : region.stores.values()) {
          tempNumStoreFiles += store.getStorefilesCount();
```

### ConstantValue
Condition `stores == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionWrapperImpl.java`
#### Snippet
```java
  public long getNumStores() {
    Map<byte[], HStore> stores = this.region.stores;
    if (stores == null) {
      return 0;
    }
```

### ConstantValue
Condition `c != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
      }
      // Finally we can compact something.
      assert c != null;

      tracker.beforeExecution(store);
```

### ConstantValue
Condition `largeThreads > 0` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java

    // if we have throttle threads, make sure the user also specified size
    Preconditions.checkArgument(largeThreads > 0 && smallThreads > 0);

    final String n = Thread.currentThread().getName();
```

### ConstantValue
Value `onOrOff` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
        reInitializeCompactionsExecutors();
      }
      setCompactionsEnabled(onOrOff);
      return;
    }
```

### ConstantValue
Value `onOrOff` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
    }

    setCompactionsEnabled(onOrOff);
    LOG.info("Interrupting running compactions because user switched off compactions");
    interrupt();
```

### ConstantValue
Condition `lastException != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
        lastException = e;
      }
      if (lastException != null && i < (flushRetriesNumber - 1)) {
        try {
          Thread.sleep(pauseTime);
```

### ConstantValue
Value `state` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  NextState setScannerState(NextState state) {
    if (!NextState.isValidState(state)) {
      throw new IllegalArgumentException("Cannot set to invalid state: " + state);
    }

```

### ConstantValue
Condition `regionToFlush == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
      }

      if ((regionToFlush == null || regionToFlushSize == 0) && bestRegionReplicaSize == 0) {
        // A concurrency issue (such as splitting region) may happen such that the online region
        // seen by getCopyOfOnlineRegionsSortedByXX() method is no longer eligible to
```

### ConstantValue
Condition `regionToFlush == null` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java

      if (
        regionToFlush == null || (bestRegionReplica != null
          && ServerRegionReplicaUtil.isRegionReplicaStoreFileRefreshEnabled(conf)
          && (bestRegionReplicaSize > secondaryMultiplier * regionToFlushSize))
```

### ConstantValue
Condition `region != null` is always `true` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
    } catch (IOException ex) {
      ex = ex instanceof RemoteException ? ((RemoteException) ex).unwrapRemoteException() : ex;
      LOG.error("Cache flush failed" + (region != null
        ? (" for region " + Bytes.toStringBinary(region.getRegionInfo().getRegionName()))
        : ""), ex);
```

### ConstantValue
Value `processed` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        }
        if (processed != null) {
          builder.setProcessed(processed);
        }
        boolean clientCellBlockSupported = isClientCellBlockSupport(context);
```

### ConstantValue
Condition `edits != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
          walEntries.add(walEntry);
        }
        if (edits != null && !edits.isEmpty()) {
          // HBASE-17924
          // sort to improve lock efficiency
```

### ConstantValue
Condition `Double.isNaN(tempBlockedRequestsCount)` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java`
#### Snippet
```java
        float localityIndex =
          hdfsBlocksDistribution.getBlockLocalityIndex(regionServer.getServerName().getHostname());
        tempPercentFileLocal = Double.isNaN(tempBlockedRequestsCount) ? 0 : (localityIndex * 100);

        float localityIndexSecondaryRegions = hdfsBlocksDistributionSecondaryRegions
```

### ConstantValue
Condition `filesForL0 != null` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
            filesForL0 = conflictingFiles;
          }
          if (filesForL0 != null) {
            for (HStoreFile sf : filesForL0) {
              insertFileIntoStripe(getLevel0Copy(), sf);
```

### ConstantValue
Value `descending` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
    final int greaterCellIndex = getValidIndex(toKey, toInclusive, false);
    if (descending) {
      return createSubCellFlatMap(greaterCellIndex, lessCellIndex, descending);
    } else {
      return createSubCellFlatMap(lessCellIndex, greaterCellIndex, descending);
```

### ConstantValue
Value `descending` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CellFlatMap.java`
#### Snippet
```java
      return createSubCellFlatMap(greaterCellIndex, lessCellIndex, descending);
    } else {
      return createSubCellFlatMap(lessCellIndex, greaterCellIndex, descending);
    }
  }
```

### ConstantValue
Value `pool` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
    // do not create offheap chunk on demand
    if (pool && this.offheap) {
      chunk = new OffheapChunk(size, id, chunkType, pool);
    } else {
      chunk = new OnheapChunk(size, id, chunkType, pool);
```

### ConstantValue
Condition `memStoreChunkPool != null` is always `true` when reached
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java`
#### Snippet
```java
    MemStoreChunkPool memStoreChunkPool = new MemStoreChunkPool(label, chunkSize, chunkType,
      maxCount, initialCount, poolSizePercentage);
    if (heapMemoryManager != null && memStoreChunkPool != null) {
      // Register with Heap Memory manager
      heapMemoryManager.registerTuneObserver(memStoreChunkPool);
```

### ConstantValue
Condition `tries > 0` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        // Log if we had to retry else don't log unless TRACE. We want to
        // know if were successful after an attempt showed in logs as failed.
        if (tries > 0 || LOG.isTraceEnabled()) {
          LOG.info("TRANSITION REPORTED " + request);
        }
```

### ConstantValue
Value `previous` is always 'true'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
      LOG.info("Received CLOSE for the region:" + encodedName + " , which we are already "
        + "trying to OPEN. Cancelling OPENING.");
      if (!regionsInTransitionInRS.replace(Bytes.toBytes(encodedName), previous, Boolean.FALSE)) {
        // The replace failed. That should be an exceptional case, but theoretically it can happen.
        // We're going to try to do a standard close then.
```

### ConstantValue
Value `logRollNeeded` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
          // is lower than or equals 1, we consider it as a new writer.
          if (this.numEntries.get() <= 1) {
            return logRollNeeded;
          }
          // Once the live datanode number and the replicas return to normal,
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    Result defaultResult = null;
    if (this.coprocEnvironments.isEmpty()) {
      return defaultResult;
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    Result defaultResult = null;
    if (coprocEnvironments.isEmpty()) {
      return defaultResult;
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
      regionObserverGetter, defaultResult, bypassable) {
      @Override
      public Result call(RegionObserver observer) throws IOException {
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
      regionObserverGetter, defaultResult, bypassable) {
      @Override
      public Result call(RegionObserver observer) throws IOException {
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    Result defaultResult = null;
    if (this.coprocEnvironments.isEmpty()) {
      return defaultResult;
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    Result defaultResult = null;
    if (coprocEnvironments.isEmpty()) {
      return defaultResult;
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
      regionObserverGetter, defaultResult, bypassable) {
      @Override
      public Result call(RegionObserver observer) throws IOException {
```

### ConstantValue
Value `defaultResult` is always 'null'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    }
    return execOperationWithResult(new ObserverOperationWithResult<RegionObserver, Result>(
      regionObserverGetter, defaultResult, bypassable) {
      @Override
      public Result call(RegionObserver observer) throws IOException {
```

### ConstantValue
Condition `coprocEnvironments.isEmpty()` is always `false`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
      return;
    }
    execOperation(coprocEnvironments.isEmpty() ? null : new RegionObserverOperationWithoutResult() {
      @Override
      public void call(RegionObserver observer) throws IOException {
```

### ConstantValue
Value `result` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java`
#### Snippet
```java
    long mcTime = getNextMajorCompactTime(filesToCompact);
    if (filesToCompact == null || filesToCompact.isEmpty() || mcTime == 0) {
      return result;
    }
    // TODO: Use better method for determining stamp of last major (HBASE-2990)
```

### ConstantValue
Condition `ret > 0` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ExplicitColumnTracker.java`
#### Snippet
```java
      // of interest. Advance the ExplicitColumnTracker state to next
      // column of interest, and check again.
      if (ret > 0) {
        ++this.index;
        if (done()) {
```

### ConstantValue
Condition `finished` is always `true`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java`
#### Snippet
```java
      progressSet.remove(progress);
    }
    assert finished : "We should have exited the method on all error paths";
    assert writer != null : "Writer should be non-null if no error";
    return commitWriter(writer, fd, request);
```

### ConstantValue
Value `canDropDeletesNoL0` is always 'false'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java`
#### Snippet
```java
        // If we need to compact L0, see if we can add something to it, and drop deletes.
        StripeCompactionRequest result =
          selectSingleStripeCompaction(si, !shouldSelectL0Files(si), canDropDeletesNoL0, isOffpeak);
        if (result != null) {
          return result;
```

### ConstantValue
Condition `restoreImageSet != null` is always `true`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
      restoreImages(arr, table, tTableArray[i], isOverwrite);
      restoreImageSet.addAll(list);
      if (restoreImageSet != null && !restoreImageSet.isEmpty()) {
        LOG.info("Restore includes the following image(s):");
        for (BackupImage image : restoreImageSet) {
```

### ConstantValue
Condition `restoreImageSet != null && !restoreImageSet.isEmpty()` is always `true`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
      restoreImages(arr, table, tTableArray[i], isOverwrite);
      restoreImageSet.addAll(list);
      if (restoreImageSet != null && !restoreImageSet.isEmpty()) {
        LOG.info("Restore includes the following image(s):");
        for (BackupImage image : restoreImageSet) {
```

### ConstantValue
Condition `!restoreImageSet.isEmpty()` is always `true` when reached
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
      restoreImages(arr, table, tTableArray[i], isOverwrite);
      restoreImageSet.addAll(list);
      if (restoreImageSet != null && !restoreImageSet.isEmpty()) {
        LOG.info("Restore includes the following image(s):");
        for (BackupImage image : restoreImageSet) {
```

### ConstantValue
Result of `restoreImageSet.isEmpty()` is always 'false'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
      restoreImages(arr, table, tTableArray[i], isOverwrite);
      restoreImageSet.addAll(list);
      if (restoreImageSet != null && !restoreImageSet.isEmpty()) {
        LOG.info("Restore includes the following image(s):");
        for (BackupImage image : restoreImageSet) {
```

### ConstantValue
Condition `33 * type.hashCode() + backupId != null` is always `true`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
  @Override
  public int hashCode() {
    int hash = 33 * type.hashCode() + backupId != null ? backupId.hashCode() : 0;
    if (backupRootDir != null) {
      hash = 33 * hash + backupRootDir.hashCode();
```

### ConstantValue
Value `res` is always 'true'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
        }
        System.out
          .println("MERGE repair: renamed from " + tmpPath + " to " + destPath + " res=" + res);
      } else {
        checkRemoveBackupImages(fs, backupRoot, backupIds);
```

### ConstantValue
Condition `snapshotDone` is always `true`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
        // Fail delete operation
        // Step 1
        if (snapshotDone) {
          if (BackupSystemTable.snapshotExists(conn)) {
            BackupSystemTable.restoreFromSnapshot(conn);
```

### ConstantValue
Condition `deleteSessionStarted` is always `true`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java

      } finally {
        if (deleteSessionStarted) {
          sysTable.finishBackupExclusiveOperation();
        }
```

### ConstantValue
Condition `data == null` is always `false`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
        TableName tn = TableName.valueOf(tabName);
        byte[] data = CellUtil.cloneValue(cell);
        if (data == null) {
          throw new IOException("Data of last backup data from backup system table "
            + "is empty. Create a backup first.");
```

### ConstantValue
Condition `data != null` is always `true`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
            + "is empty. Create a backup first.");
        }
        if (data != null && data.length > 0) {
          HashMap<String, Long> lastBackup =
            fromTableServerTimestampProto(BackupProtos.TableServerTimestamp.parseFrom(data));
```

### ConstantValue
Value `t` is always 'null'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftMetrics.java`
#### Snippet
```java
  protected static Throwable unwrap(Throwable t) {
    if (t == null) {
      return t;
    }
    if (t instanceof TIOError || t instanceof IOError) {
```

### ConstantValue
Condition `qualifier != null` is always `true`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java
        long timestamp = cell.getTimestamp();
        column.setFamily(family);
        if (qualifier != null) {
          column.setQualifier(qualifier);
        }
```

## RuleId[id=MethodOverridesStaticMethod]
### MethodOverridesStaticMethod
Method `main()` tries to override a static method of a superclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java`
#### Snippet
```java
   * can also be dumped using http services. e.g. "curl http://master:16010/dump"
   */
  public static void main(String[] args) throws Exception {
    HBaseConfiguration.create().writeXml(System.out);
  }
```

### MethodOverridesStaticMethod
Method `login()` tries to override a static method of a superclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
     * @throws IOException exception from UserGroupInformation.loginUserFromKeytab
     */
    public static void login(String keytabLocation, String principalName) throws IOException {
      if (isSecurityEnabled()) {
        UserGroupInformation.loginUserFromKeytab(principalName, keytabLocation);
```

### MethodOverridesStaticMethod
Method `createUserForTesting()` tries to override a static method of a superclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
     * @see User#createUserForTesting(org.apache.hadoop.conf.Configuration, String, String[])
     */
    public static User createUserForTesting(Configuration conf, String name, String[] groups) {
      synchronized (UserProvider.class) {
        if (!(UserProvider.groups instanceof TestingGroups)) {
```

### MethodOverridesStaticMethod
Method `login()` tries to override a static method of a superclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
     * @param localhost        the local hostname
     */
    public static void login(Configuration conf, String fileConfKey, String principalConfKey,
      String localhost) throws IOException {
      if (isSecurityEnabled()) {
```

### MethodOverridesStaticMethod
Method `isSecurityEnabled()` tries to override a static method of a superclass
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java

    /** Returns the result of {@code UserGroupInformation.isSecurityEnabled()}. */
    public static boolean isSecurityEnabled() {
      return UserGroupInformation.isSecurityEnabled();
    }
```

### MethodOverridesStaticMethod
Method `getInstance()` tries to override a static method of a superclass
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java`
#### Snippet
```java
   */
  @SuppressWarnings("unchecked")
  public static <T> T getInstance(Class<T> klass) {
    synchronized (SingletonStorage.INSTANCE.lock) {
      T instance = (T) SingletonStorage.INSTANCE.instances.get(klass);
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BigDecimalComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static BigDecimalComparator parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    ComparatorProtos.BigDecimalComparator proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryComponentComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static BinaryComponentComparator parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    ComparatorProtos.BinaryComponentComparator proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SubstringComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static SubstringComparator parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    ComparatorProtos.SubstringComparator proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static ColumnPrefixFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.ColumnPrefixFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 1, "Expected 1 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 4, "Expected 4 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static ColumnRangeFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.ColumnRangeFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static SkipFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.SkipFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/InclusiveStopFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static InclusiveStopFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.InclusiveStopFilter proto;
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/InclusiveStopFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 1, "Expected 1 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 1, "Expected 1 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static ColumnCountGetFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.ColumnCountGetFilter proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RandomRowFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static RandomRowFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.RandomRowFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static SingleColumnValueFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.SingleColumnValueFilter proto;
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 4 || filterArguments.size() == 6,
      "Expected 4 or 6 but got: %s", filterArguments.size());
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    @SuppressWarnings("rawtypes") // for arguments
    ArrayList arguments = CompareFilter.extractArguments(filterArguments);
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static ValueFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.ValueFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static FirstKeyOnlyFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    // There is nothing to deserialize. Why do this at all?
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.isEmpty(), "Expected 0 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(
      filterArguments.size() == 2 || filterArguments.size() == 3 || filterArguments.size() == 5,
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static DependentColumnFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.DependentColumnFilter proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static KeyOnlyFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.KeyOnlyFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument((filterArguments.isEmpty() || filterArguments.size() == 1),
      "Expected: 0 or 1 but got: %s", filterArguments.size());
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static NullComparator parseFrom(final byte[] pbBytes) throws DeserializationException {
    try {
      // Just parse. Don't use what we parse since on end we are returning new NullComparator.
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static BinaryPrefixComparator parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    ComparatorProtos.BinaryPrefixComparator proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterList.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static FilterList parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.FilterList proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 2, "Expected 2 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static ColumnPaginationFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.ColumnPaginationFilter proto;
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java`
#### Snippet
```java
   * @return a ColumnValueFilter
   */
  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 4, "Expect 4 arguments: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static ColumnValueFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.ColumnValueFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static QualifierFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.QualifierFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    ArrayList<?> arguments = CompareFilter.extractArguments(filterArguments);
    CompareOperator compareOp = (CompareOperator) arguments.get(0);
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static PrefixFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.PrefixFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 1, "Expected 1 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    byte[][] prefixes = new byte[filterArguments.size()][];
    for (int i = 0; i < filterArguments.size(); i++) {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static MultipleColumnPrefixFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.MultipleColumnPrefixFilter proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static MultiRowRangeFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.MultiRowRangeFilter proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static PageFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.PageFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    Preconditions.checkArgument(filterArguments.size() == 1, "Expected 1 but got: %s",
      filterArguments.size());
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static RegexStringComparator parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    ComparatorProtos.RegexStringComparator proto;
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyValueMatchingQualifiersFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static FirstKeyValueMatchingQualifiersFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    throw new DeserializationException(
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static WhileMatchFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.WhileMatchFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static RowFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.RowFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    @SuppressWarnings("rawtypes") // for arguments
    ArrayList arguments = CompareFilter.extractArguments(filterArguments);
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BitComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static BitComparator parseFrom(final byte[] pbBytes) throws DeserializationException {
    ComparatorProtos.BitComparator proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static FamilyFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.FamilyFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    ArrayList<?> arguments = CompareFilter.extractArguments(filterArguments);
    CompareOperator compareOp = (CompareOperator) arguments.get(0);
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/LongComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static LongComparator parseFrom(final byte[] pbBytes) throws DeserializationException {
    ComparatorProtos.LongComparator proto;
    try {
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    ArrayList<Long> timestamps = new ArrayList<>(filterArguments.size());
    for (int i = 0; i < filterArguments.size(); i++) {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static TimestampsFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.TimestampsFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryComparator.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static BinaryComparator parseFrom(final byte[] pbBytes) throws DeserializationException {
    ComparatorProtos.BinaryComparator proto;
    try {
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static SingleColumnValueExcludeFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    FilterProtos.SingleColumnValueExcludeFilter proto;
```

### MethodOverridesStaticMethod
Method `createFilterFromArguments()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java`
#### Snippet
```java
  }

  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
    SingleColumnValueFilter tempFilter =
      (SingleColumnValueFilter) SingleColumnValueFilter.createFilterFromArguments(filterArguments);
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static FuzzyRowFilter parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.FuzzyRowFilter proto;
    try {
```

### MethodOverridesStaticMethod
Method `valueOf()` tries to override a static method of a superclass
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/StartcodeAgnosticServerName.java`
#### Snippet
```java
  }

  public static StartcodeAgnosticServerName valueOf(final String hostnameAndPort, long startcode) {
    return new StartcodeAgnosticServerName(Addressing.parseHostname(hostnameAndPort),
      Addressing.parsePort(hostnameAndPort), startcode);
```

### MethodOverridesStaticMethod
Method `get()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java`
#### Snippet
```java
   * checksum verification is done by HBase.
   */
  static public FileSystem get(Configuration conf) throws IOException {
    return new HFileSystem(conf, true);
  }
```

### MethodOverridesStaticMethod
Method `create()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/CachedMobFile.java`
#### Snippet
```java
  }

  public static CachedMobFile create(FileSystem fs, Path path, Configuration conf,
    CacheConfig cacheConf) throws IOException {
    // XXX: primaryReplica is only used for constructing the key of block cache so it is not a
```

### MethodOverridesStaticMethod
Method `main()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/MajorCompactorTTL.java`
#### Snippet
```java
  }

  public static void main(String[] args) throws Exception {
    ToolRunner.run(HBaseConfiguration.create(), new MajorCompactorTTL(), args);
  }
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java`
#### Snippet
```java
   * @see #toByteArray
   */
  public static FilterWrapper parseFrom(final byte[] pbBytes) throws DeserializationException {
    FilterProtos.FilterWrapper proto;
    try {
```

### MethodOverridesStaticMethod
Method `main()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupMajorCompactionTTL.java`
#### Snippet
```java
  }

  public static void main(String[] args) throws Exception {
    ToolRunner.run(HBaseConfiguration.create(), new RSGroupMajorCompactionTTL(), args);
  }
```

### MethodOverridesStaticMethod
Method `createSecretKey()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java`
#### Snippet
```java
  }

  public static SecretKey createSecretKey(byte[] raw) {
    return SecretManager.createSecretKey(raw);
  }
```

### MethodOverridesStaticMethod
Method `parseFrom()` tries to override a static method of a superclass
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlFilter.java`
#### Snippet
```java
   * @see #toByteArray()
   */
  public static AccessControlFilter parseFrom(final byte[] pbBytes)
    throws DeserializationException {
    // no implementation, server-side use only
```

### MethodOverridesStaticMethod
Method `main()` tries to override a static method of a superclass
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java`
#### Snippet
```java
   * Start up the Thrift2 server.
   */
  public static void main(String[] args) throws Exception {
    final Configuration conf = HBaseConfiguration.create();
    // for now, only time we return is on an argument error.
```

### MethodOverridesStaticMethod
Method `randomFreePort()` tries to override a static method of a superclass
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  }

  public static int randomFreePort() {
    return HBaseCommonTestingUtility.randomFreePort();
  }
```

## RuleId[id=StringConcatenationInsideStringBufferAppend]
### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CompoundConfiguration.java`
#### Snippet
```java
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("CompoundConfiguration: " + this.configs.size() + " configs");
    for (ImmutableConfigMap m : this.configs) {
      sb.append(m);
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `PrintWriter.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
      pw.append("exception");
      if (this.exceptions.size() > 1) {
        pw.append(" #" + i);
      }
      pw.append(" from " + server + " for "
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `PrintWriter.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java
        pw.append(" #" + i);
      }
      pw.append(" from " + server + " for "
        + ((action == null) ? "unknown key" : Bytes.toStringBinary(action.getRow())));
      if (t != null) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
    public String toString() {
      StringBuilder sb = new StringBuilder(1024);
      sb.append("Master: " + getMasterName());

      int backupMastersSize = getBackupMasterNames().size();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java

      int backupMastersSize = getBackupMasterNames().size();
      sb.append("\nNumber of backup masters: " + backupMastersSize);
      if (backupMastersSize > 0) {
        for (ServerName serverName : getBackupMasterNames()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      if (backupMastersSize > 0) {
        for (ServerName serverName : getBackupMasterNames()) {
          sb.append("\n  " + serverName);
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      int serversSize = getLiveServerMetrics().size();
      int serversNameSize = getServersName().size();
      sb.append(
        "\nNumber of live region servers: " + (serversSize > 0 ? serversSize : serversNameSize));
      if (serversSize > 0) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      if (serversSize > 0) {
        for (ServerName serverName : getLiveServerMetrics().keySet()) {
          sb.append("\n  " + serverName.getServerName());
        }
      } else if (serversNameSize > 0) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      } else if (serversNameSize > 0) {
        for (ServerName serverName : getServersName()) {
          sb.append("\n  " + serverName.getServerName());
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java

      int deadServerSize = getDeadServerNames().size();
      sb.append("\nNumber of dead region servers: " + deadServerSize);
      if (deadServerSize > 0) {
        for (ServerName serverName : getDeadServerNames()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      if (deadServerSize > 0) {
        for (ServerName serverName : getDeadServerNames()) {
          sb.append("\n  " + serverName);
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java

      int unknownServerSize = getUnknownServerNames().size();
      sb.append("\nNumber of unknown region servers: " + unknownServerSize);
      if (unknownServerSize > 0) {
        for (ServerName serverName : getUnknownServerNames()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      if (unknownServerSize > 0) {
        for (ServerName serverName : getUnknownServerNames()) {
          sb.append("\n  " + serverName);
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      }

      sb.append("\nAverage load: " + getAverageLoad());
      sb.append("\nNumber of requests: " + getRequestCount());
      sb.append("\nNumber of regions: " + getRegionCount());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java

      sb.append("\nAverage load: " + getAverageLoad());
      sb.append("\nNumber of requests: " + getRequestCount());
      sb.append("\nNumber of regions: " + getRegionCount());

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      sb.append("\nAverage load: " + getAverageLoad());
      sb.append("\nNumber of requests: " + getRequestCount());
      sb.append("\nNumber of regions: " + getRegionCount());

      int ritSize = getRegionStatesInTransition().size();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java

      int ritSize = getRegionStatesInTransition().size();
      sb.append("\nNumber of regions in transition: " + ritSize);
      if (ritSize > 0) {
        for (RegionState state : getRegionStatesInTransition()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterMetricsBuilder.java`
#### Snippet
```java
      if (ritSize > 0) {
        for (RegionState state : getRegionStatesInTransition()) {
          sb.append("\n  " + state.toDescriptiveString());
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java`
#### Snippet
```java
            moreThanOneB = true;
          }
          sb.append(CellUtil.getCellKeyAsString(cell) + "+="
            + Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
        }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultiRowRangeFilter.java`
#### Snippet
```java
    if (details) {
      for (RowRange range : invalidRanges) {
        sb.append("Invalid range: start row => " + Bytes.toString(range.startRow) + ", stop row => "
          + Bytes.toString(range.stopRow)).append('\n');
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerDescription.java`
#### Snippet
```java
  public String toString() {
    StringBuilder builder = new StringBuilder("id : ").append(id);
    builder.append(", enabled : " + enabled);
    builder.append(", config : " + config);
    builder.append(", syncReplicationState : " + syncReplicationState);
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerDescription.java`
#### Snippet
```java
    StringBuilder builder = new StringBuilder("id : ").append(id);
    builder.append(", enabled : " + enabled);
    builder.append(", config : " + config);
    builder.append(", syncReplicationState : " + syncReplicationState);
    return builder.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerDescription.java`
#### Snippet
```java
    builder.append(", enabled : " + enabled);
    builder.append(", config : " + config);
    builder.append(", syncReplicationState : " + syncReplicationState);
    return builder.toString();
  }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java

    if (hasException()) {
      sb.append(", exception=" + getException());
    }

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
        sb.append(",");
      }
      sb.append(sourceCf + ":" + destCf);
    }
    conf.set(CF_RENAME_PROP, sb.toString());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
      builder.append(" : (");
      if (c.isNeeded()) {
        builder.append("multiplier=" + c.getMultiplier());
        builder.append(", ");
        double cost = c.cost();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
        builder.append(", ");
        double cost = c.cost();
        builder.append("imbalance=" + cost);
        if (cost >= minCostNeedBalance) {
          builder.append(", need balance");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilter.java`
#### Snippet
```java
    StringBuilder sb = new StringBuilder();
    sb.append(BloomFilterUtil.formatStats(this));
    sb.append(BloomFilterUtil.STATS_RECORD_SEP + "Number of chunks: " + numChunks);
    sb.append(BloomFilterUtil.STATS_RECORD_SEP + ((comparator != null)
      ? "Comparator: " + comparator.getClass().getSimpleName()
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilter.java`
#### Snippet
```java
    sb.append(BloomFilterUtil.formatStats(this));
    sb.append(BloomFilterUtil.STATS_RECORD_SEP + "Number of chunks: " + numChunks);
    sb.append(BloomFilterUtil.STATS_RECORD_SEP + ((comparator != null)
      ? "Comparator: " + comparator.getClass().getSimpleName()
      : "Comparator: " + Bytes.BYTES_RAWCOMPARATOR.getClass().getSimpleName()));
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
    public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append("size=" + rootCount).append("\n");
      for (int i = 0; i < rootCount; i++) {
        sb.append("key=").append((blockKeys[i])).append("\n  offset=").append(blockOffsets[i])
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      for (int i = 0; i < rootCount; i++) {
        sb.append("key=").append((blockKeys[i])).append("\n  offset=").append(blockOffsets[i])
          .append(", dataSize=" + blockDataSizes[i]).append("\n");
      }
      return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
    public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append("size=" + rootCount).append("\n");
      for (int i = 0; i < rootCount; i++) {
        sb.append("key=").append(KeyValue.keyToString(blockKeys[i])).append("\n  offset=")
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
      for (int i = 0; i < rootCount; i++) {
        sb.append("key=").append(KeyValue.keyToString(blockKeys[i])).append("\n  offset=")
          .append(blockOffsets[i]).append(", dataSize=" + blockDataSizes[i]).append("\n");
      }
      return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
    public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append("size=" + rootCount).append("\n");
      for (int i = 0; i < rootCount; i++) {
        sb.append("key=").append((blockKeys[i])).append("\n  offset=").append(blockOffsets[i])
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpIndexBlockEncoder.java`
#### Snippet
```java
      for (int i = 0; i < rootCount; i++) {
        sb.append("key=").append((blockKeys[i])).append("\n  offset=").append(blockOffsets[i])
          .append(", dataSize=" + blockDataSizes[i]).append("\n");
      }
      return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
          bytesToFreeWithoutExtra += bytesToFreeForBucket[i];
          if (msgBuffer != null) {
            msgBuffer.append("Free for bucketSize(" + stats[i].itemSize() + ")="
              + StringUtils.byteDesc(bytesToFreeForBucket[i]) + ", ");
          }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      }
      if (msgBuffer != null) {
        msgBuffer.append("Free for total=" + StringUtils.byteDesc(bytesToFreeWithoutExtra) + ", ");
      }

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    sb.append("{ meta => ");
    sb.append((metaEntry != null) ? metaEntry.hri.getRegionNameAsString() : "null");
    sb.append(", hdfs => " + getHdfsRegionDir());
    sb.append(", deployed => " + Joiner.on(", ").join(deployedEntries));
    sb.append(", replicaId => " + getReplicaId());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    sb.append((metaEntry != null) ? metaEntry.hri.getRegionNameAsString() : "null");
    sb.append(", hdfs => " + getHdfsRegionDir());
    sb.append(", deployed => " + Joiner.on(", ").join(deployedEntries));
    sb.append(", replicaId => " + getReplicaId());
    sb.append(" }");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
    sb.append(", hdfs => " + getHdfsRegionDir());
    sb.append(", deployed => " + Joiner.on(", ").join(deployedEntries));
    sb.append(", replicaId => " + getReplicaId());
    sb.append(" }");
    return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterUtil.java`
#### Snippet
```java
    long m = bloomFilter.getMaxKeys();

    sb.append("BloomSize: " + bloomFilter.getByteSize() + STATS_RECORD_SEP);
    sb.append("No of Keys in bloom: " + k + STATS_RECORD_SEP);
    sb.append("Max Keys for bloom: " + m);
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterUtil.java`
#### Snippet
```java

    sb.append("BloomSize: " + bloomFilter.getByteSize() + STATS_RECORD_SEP);
    sb.append("No of Keys in bloom: " + k + STATS_RECORD_SEP);
    sb.append("Max Keys for bloom: " + m);
    if (m > 0) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterUtil.java`
#### Snippet
```java
    sb.append("BloomSize: " + bloomFilter.getByteSize() + STATS_RECORD_SEP);
    sb.append("No of Keys in bloom: " + k + STATS_RECORD_SEP);
    sb.append("Max Keys for bloom: " + m);
    if (m > 0) {
      sb.append(STATS_RECORD_SEP + "Percentage filled: "
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterUtil.java`
#### Snippet
```java
    sb.append("Max Keys for bloom: " + m);
    if (m > 0) {
      sb.append(STATS_RECORD_SEP + "Percentage filled: "
        + NumberFormat.getPercentInstance().format(k * 1.0 / m));
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
    for (byte[] k : splits) {
      sb.setLength(0); // clear out existing buffer, if any.
      sb.append(Bytes.toStringBinary(k) + ":\t");
      for (HbckRegionInfo r : regions.get(k)) {
        sb.append("[ " + r.toString() + ", " + Bytes.toStringBinary(r.getEndKey()) + "]\t");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      sb.append(Bytes.toStringBinary(k) + ":\t");
      for (HbckRegionInfo r : regions.get(k)) {
        sb.append("[ " + r.toString() + ", " + Bytes.toStringBinary(r.getEndKey()) + "]\t");
      }
      hbck.getErrors().print(sb.toString());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      sb.append("    Deployed on: ");
      for (ServerName server : tInfo.deployedOn) {
        sb.append(" " + server.toString());
      }
      errors.print(sb.toString());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
  public synchronized String toString() {
    StringBuilder builder = new StringBuilder();
    builder.append("QuotaState(ts=" + getLastUpdate());
    if (isBypass()) {
      builder.append(" bypass");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      if (globalLimiter != NoopQuotaLimiter.get()) {
        // builder.append(" global-limiter");
        builder.append(" " + globalLimiter);
      }
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
  public synchronized String toString() {
    StringBuilder builder = new StringBuilder();
    builder.append("UserQuotaState(ts=" + getLastUpdate());
    if (bypassGlobals) builder.append(" bypass-globals");

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
        builder.append(" [");
        for (TableName table : tableLimiters.keySet()) {
          builder.append(" " + table);
        }
        builder.append(" ]");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
        builder.append(" [");
        for (String ns : namespaceLimiters.keySet()) {
          builder.append(" " + ns);
        }
        builder.append(" ]");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/GlobalQuotaSettingsImpl.java`
#### Snippet
```java
    }
    if (bypassGlobals != null) {
      builder.append(" { GLOBAL_BYPASS => " + bypassGlobals + " } ");
    }
    if (spaceProto != null) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    builder.append("TimeBasedLimiter(");
    if (!reqsLimiter.isBypass()) {
      builder.append("reqs=" + reqsLimiter);
    }
    if (!reqSizeLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!reqSizeLimiter.isBypass()) {
      builder.append(" resSize=" + reqSizeLimiter);
    }
    if (!writeReqsLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!writeReqsLimiter.isBypass()) {
      builder.append(" writeReqs=" + writeReqsLimiter);
    }
    if (!writeSizeLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!writeSizeLimiter.isBypass()) {
      builder.append(" writeSize=" + writeSizeLimiter);
    }
    if (!readReqsLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!readReqsLimiter.isBypass()) {
      builder.append(" readReqs=" + readReqsLimiter);
    }
    if (!readSizeLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!readSizeLimiter.isBypass()) {
      builder.append(" readSize=" + readSizeLimiter);
    }
    if (!reqCapacityUnitLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!reqCapacityUnitLimiter.isBypass()) {
      builder.append(" reqCapacityUnit=" + reqCapacityUnitLimiter);
    }
    if (!writeCapacityUnitLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!writeCapacityUnitLimiter.isBypass()) {
      builder.append(" writeCapacityUnit=" + writeCapacityUnitLimiter);
    }
    if (!readCapacityUnitLimiter.isBypass()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/TimeBasedLimiter.java`
#### Snippet
```java
    }
    if (!readCapacityUnitLimiter.isBypass()) {
      builder.append(" readCapacityUnit=" + readCapacityUnitLimiter);
    }
    builder.append(')');
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java`
#### Snippet
```java
          }
          first = false;
          sb.append(entry.getKey() + '=');
          sb.append(entry.getValue());
        }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.java`
#### Snippet
```java
      sb.append(childExps.get(i));
      if (i < this.childExps.size() - 1) {
        sb.append(" " + this.op + " ");
      }
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java`
#### Snippet
```java
        long delta = entry.getTimeStamp() - previousEntry.getTimeStamp();
        if (delta != 0) {
          sb.append(" (+" + delta + " ms)");
        }
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/ThreadMonitoring.java`
#### Snippet
```java
    sb.append(indent).append("  Waited count: ").append(info.getWaitedCount()).append("\n");
    if (contention) {
      sb.append(indent).append("  Blocked time: " + info.getBlockedTime()).append("\n");
      sb.append(indent).append("  Waited time: " + info.getWaitedTime()).append("\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/ThreadMonitoring.java`
#### Snippet
```java
    if (contention) {
      sb.append(indent).append("  Blocked time: " + info.getBlockedTime()).append("\n");
      sb.append(indent).append("  Waited time: " + info.getWaitedTime()).append("\n");
    }
    if (state == Thread.State.WAITING) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    for (ReplicationPeerDescription peer : peers) {
      ReplicationPeerConfig peerConfig = peer.getPeerConfig();
      sb.append("Peer: " + peer.getPeerId() + "\n");
      sb.append("    " + "State: " + (peer.isEnabled() ? "ENABLED" : "DISABLED") + "\n");
      sb.append("    " + "Cluster Name: " + peerConfig.getClusterKey() + "\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      ReplicationPeerConfig peerConfig = peer.getPeerConfig();
      sb.append("Peer: " + peer.getPeerId() + "\n");
      sb.append("    " + "State: " + (peer.isEnabled() ? "ENABLED" : "DISABLED") + "\n");
      sb.append("    " + "Cluster Name: " + peerConfig.getClusterKey() + "\n");
      sb.append("    " + "Replication Endpoint: " + peerConfig.getReplicationEndpointImpl() + "\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      sb.append("Peer: " + peer.getPeerId() + "\n");
      sb.append("    " + "State: " + (peer.isEnabled() ? "ENABLED" : "DISABLED") + "\n");
      sb.append("    " + "Cluster Name: " + peerConfig.getClusterKey() + "\n");
      sb.append("    " + "Replication Endpoint: " + peerConfig.getReplicationEndpointImpl() + "\n");
      currentConf = peerConfig.getConfiguration();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      sb.append("    " + "State: " + (peer.isEnabled() ? "ENABLED" : "DISABLED") + "\n");
      sb.append("    " + "Cluster Name: " + peerConfig.getClusterKey() + "\n");
      sb.append("    " + "Replication Endpoint: " + peerConfig.getReplicationEndpointImpl() + "\n");
      currentConf = peerConfig.getConfiguration();
      // Only show when we have a custom configuration for the peer
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      // Only show when we have a custom configuration for the peer
      if (currentConf.size() > 1) {
        sb.append("    " + "Peer Configuration: " + currentConf + "\n");
      }
      sb.append("    " + "Peer Table CFs: " + peerConfig.getTableCFsMap() + "\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        sb.append("    " + "Peer Configuration: " + currentConf + "\n");
      }
      sb.append("    " + "Peer Table CFs: " + peerConfig.getTableCFsMap() + "\n");
      sb.append("    " + "Peer Namespaces: " + peerConfig.getNamespaces() + "\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      }
      sb.append("    " + "Peer Table CFs: " + peerConfig.getTableCFsMap() + "\n");
      sb.append("    " + "Peer Namespaces: " + peerConfig.getNamespaces() + "\n");
    }
    return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    List<ServerName> deadServers;

    sb.append("Dumping replication queue info for RegionServer: [" + regionserver + "]" + "\n");
    sb.append("    Queue znode: " + queueId + "\n");
    sb.append("    PeerID: " + queueInfo.getPeerId() + "\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java

    sb.append("Dumping replication queue info for RegionServer: [" + regionserver + "]" + "\n");
    sb.append("    Queue znode: " + queueId + "\n");
    sb.append("    PeerID: " + queueInfo.getPeerId() + "\n");
    sb.append("    Recovered: " + queueInfo.isQueueRecovered() + "\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    sb.append("Dumping replication queue info for RegionServer: [" + regionserver + "]" + "\n");
    sb.append("    Queue znode: " + queueId + "\n");
    sb.append("    PeerID: " + queueInfo.getPeerId() + "\n");
    sb.append("    Recovered: " + queueInfo.isQueueRecovered() + "\n");
    deadServers = queueInfo.getDeadRegionServers();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    sb.append("    Queue znode: " + queueId + "\n");
    sb.append("    PeerID: " + queueInfo.getPeerId() + "\n");
    sb.append("    Recovered: " + queueInfo.isQueueRecovered() + "\n");
    deadServers = queueInfo.getDeadRegionServers();
    if (deadServers.isEmpty()) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      sb.append("    No dead RegionServers found in this queue." + "\n");
    } else {
      sb.append("    Dead RegionServers: " + deadServers + "\n");
    }
    sb.append("    Was deleted: " + isDeleted + "\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      sb.append("    Dead RegionServers: " + deadServers + "\n");
    }
    sb.append("    Was deleted: " + isDeleted + "\n");
    sb.append("    Number of WALs in replication queue: " + wals.size() + "\n");
    peersQueueSize.addAndGet(queueInfo.getPeerId(), wals.size());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    }
    sb.append("    Was deleted: " + isDeleted + "\n");
    sb.append("    Number of WALs in replication queue: " + wals.size() + "\n");
    peersQueueSize.addAndGet(queueInfo.getPeerId(), wals.size());

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    for (String wal : wals) {
      long position = queueStorage.getWALPosition(regionserver, queueInfo.getPeerId(), wal);
      sb.append("    Replication position for " + wal + ": "
        + (position > 0 ? position : "0" + " (not started or nothing to replicate)") + "\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    if (hdfs) {
      FileSystem fs = FileSystem.get(getConf());
      sb.append("    Total size of WALs on HDFS for this queue: "
        + StringUtils.humanSize(getTotalWALSize(fs, wals, regionserver)) + "\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    StringBuilder sb = new StringBuilder();
    if (!deletedQueues.isEmpty()) {
      sb.append("Found " + deletedQueues.size() + " deleted queues"
        + ", run hbck -fixReplication in order to remove the deleted replication queues\n");
      for (String deletedQueue : deletedQueues) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        + ", run hbck -fixReplication in order to remove the deleted replication queues\n");
      for (String deletedQueue : deletedQueues) {
        sb.append("    " + deletedQueue + "\n");
      }
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    }
    if (!deadRegionServers.isEmpty()) {
      sb.append("Found " + deadRegionServers.size() + " dead regionservers"
        + ", restart one regionserver to transfer the queues of dead regionservers\n");
      for (String deadRs : deadRegionServers) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        + ", restart one regionserver to transfer the queues of dead regionservers\n");
      for (String deadRs : deadRegionServers) {
        sb.append("    " + deadRs + "\n");
      }
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      sb.append("Dumping all peers's number of WALs in replication queue\n");
      for (Map.Entry<String, Long> entry : peersQueueSize.asMap().entrySet()) {
        sb.append(
          "    PeerId: " + entry.getKey() + " , sizeOfLogQueue: " + entry.getValue() + "\n");
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
      }
    }
    sb.append("    Total size of WALs on HDFS: " + StringUtils.humanSize(totalSizeOfWALs) + "\n");
    if (numWalsNotFound > 0) {
      sb.append("    ERROR: There are " + numWalsNotFound + " WALs not found!!!\n");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    sb.append("    Total size of WALs on HDFS: " + StringUtils.humanSize(totalSizeOfWALs) + "\n");
    if (numWalsNotFound > 0) {
      sb.append("    ERROR: There are " + numWalsNotFound + " WALs not found!!!\n");
    }
    return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java
      .append(getTotalBufferLimit()).append("B\n");
    for (ReplicationSourceInterface source : this.sources.values()) {
      stats.append("Normal source for cluster " + source.getPeerId() + ": ");
      stats.append(source.getStats() + "\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java
    for (ReplicationSourceInterface source : this.sources.values()) {
      stats.append("Normal source for cluster " + source.getPeerId() + ": ");
      stats.append(source.getStats() + "\n");
    }
    for (ReplicationSourceInterface oldSource : oldsources) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java
    }
    for (ReplicationSourceInterface oldSource : oldsources) {
      stats.append("Recovered source for cluster/machine(s) " + oldSource.getPeerId() + ": ");
      stats.append(oldSource.getStats() + "\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java
    for (ReplicationSourceInterface oldSource : oldsources) {
      stats.append("Recovered source for cluster/machine(s) " + oldSource.getPeerId() + ": ");
      stats.append(oldSource.getStats() + "\n");
    }
    return stats.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
          // we do not want blocked flushes
          newTuneDirection = StepDirection.INCREASE_MEMSTORE_SIZE;
          tunerLog.append(
            "Going to increase memstore size due to" + blockedFlushCount + " blocked flushes.");
        } else {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
    Iterator<Runnable> it = lq.iterator();
    while (it.hasNext()) {
      queueLists.append("    " + it.next().toString());
      queueLists.append("\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
      it = lq.iterator();
      while (it.hasNext()) {
        queueLists.append("    " + it.next().toString());
        queueLists.append("\n");
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplit.java`
#### Snippet
```java
    it = lq.iterator();
    while (it.hasNext()) {
      queueLists.append("    " + it.next().toString());
      queueLists.append("\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java

    while (it.hasNext()) {
      queueList.append("    " + it.next().toString());
      queueList.append("\n");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    if (!LOG.isDebugEnabled()) return;
    StringBuilder sb = new StringBuilder();
    sb.append("\n" + string + "; current stripe state is as such:");
    sb.append("\n level 0 with ").append(state.level0Files.size())
      .append(" files: " + TraditionalBinaryPrefix
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
    sb.append("\n" + string + "; current stripe state is as such:");
    sb.append("\n level 0 with ").append(state.level0Files.size())
      .append(" files: " + TraditionalBinaryPrefix
        .long2String(StripeCompactionPolicy.getTotalFileSize(state.level0Files), "", 1) + ";");
    for (int i = 0; i < state.stripeFiles.size(); ++i) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
      sb.append("\n stripe ending in ").append(endRow).append(" with ")
        .append(state.stripeFiles.get(i).size())
        .append(" files: " + TraditionalBinaryPrefix.long2String(
          StripeCompactionPolicy.getTotalFileSize(state.stripeFiles.get(i)), "", 1) + ";");
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.java`
#### Snippet
```java
    if (LOG.isDebugEnabled()) {
      StringBuilder sb = new StringBuilder();
      sb.append("Executing compaction with " + targetBoundaries.size() + " boundaries:");
      for (byte[] tb : targetBoundaries) {
        sb.append(" [").append(Bytes.toString(tb)).append("]");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
  protected String obtainBackupMetaDataStr(BackupInfo backupInfo) {
    StringBuilder sb = new StringBuilder();
    sb.append("type=" + backupInfo.getType() + ",tablelist=");
    for (TableName table : backupInfo.getTables()) {
      sb.append(table + ";");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
    sb.append("type=" + backupInfo.getType() + ",tablelist=");
    for (TableName table : backupInfo.getTables()) {
      sb.append(table + ";");
    }
    if (sb.lastIndexOf(";") > 0) {
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
      sb.delete(sb.lastIndexOf(";"), sb.lastIndexOf(";") + 1);
    }
    sb.append(",targetRootDir=" + backupInfo.getBackupRootDir());

    return sb.toString();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
    StringBuilder sb = new StringBuilder();
    sb.append("{");
    sb.append("ID=" + backupId).append(",");
    sb.append("Type=" + getType()).append(",");
    sb.append("Tables=" + getTableListAsString()).append(",");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
    sb.append("{");
    sb.append("ID=" + backupId).append(",");
    sb.append("Type=" + getType()).append(",");
    sb.append("Tables=" + getTableListAsString()).append(",");
    sb.append("State=" + getState()).append(",");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
    sb.append("ID=" + backupId).append(",");
    sb.append("Type=" + getType()).append(",");
    sb.append("Tables=" + getTableListAsString()).append(",");
    sb.append("State=" + getState()).append(",");
    Calendar cal = Calendar.getInstance();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
    sb.append("Type=" + getType()).append(",");
    sb.append("Tables=" + getTableListAsString()).append(",");
    sb.append("State=" + getState()).append(",");
    Calendar cal = Calendar.getInstance();
    cal.setTimeInMillis(getStartTs());
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
    cal.setTimeInMillis(getStartTs());
    Date date = cal.getTime();
    sb.append("Start time=" + date).append(",");
    if (state == BackupState.FAILED) {
      sb.append("Failed message=" + getFailedMsg()).append(",");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
    sb.append("Start time=" + date).append(",");
    if (state == BackupState.FAILED) {
      sb.append("Failed message=" + getFailedMsg()).append(",");
    } else if (state == BackupState.RUNNING) {
      sb.append("Phase=" + getPhase()).append(",");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
      sb.append("Failed message=" + getFailedMsg()).append(",");
    } else if (state == BackupState.RUNNING) {
      sb.append("Phase=" + getPhase()).append(",");
    } else if (state == BackupState.COMPLETE) {
      cal = Calendar.getInstance();
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
      cal.setTimeInMillis(getCompleteTs());
      date = cal.getTime();
      sb.append("End time=" + date).append(",");
    }
    sb.append("Progress=" + getProgress() + "%");
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java`
#### Snippet
```java
      sb.append("End time=" + date).append(",");
    }
    sb.append("Progress=" + getProgress() + "%");
    sb.append("}");

```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        && (this.maxFlushedSeqId + this.flushPerChanges < this.mvcc.getReadPoint())
    ) {
      whyFlush.append("more than max edits, " + this.flushPerChanges + ", since last flush");
      return true;
    }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      if (s.timeOfOldestEdit() < now - modifiedFlushCheckInterval) {
        // we have an old enough edit in the memstore, flush
        whyFlush.append(s.toString() + " has an old edit so flush to free WALs");
        return true;
      }
```

### StringConcatenationInsideStringBufferAppend
String concatenation as argument to `StringBuilder.append()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  void throwException(String title, String regionName) {
    StringBuilder buf = new StringBuilder();
    buf.append(title + ", ");
    buf.append(getRegionInfo().toString());
    buf.append(getRegionInfo().isMetaRegion() ? " meta region " : " ");
```

## RuleId[id=IOResource]
### IOResource
'DataInputStream' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java`
#### Snippet
```java
    ByteArrayInputStream bais = new ByteArrayInputStream(encodedDataWithHeader, bytesToSkip,
      encodedDataWithHeader.length - bytesToSkip);
    final DataInputStream dis = new DataInputStream(bais);

    return new Iterator<Cell>() {
```

### IOResource
'DataOutputStream' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java`
#### Snippet
```java
      // wrap with Crypto AES
      byte[] wrapped = cryptoAES.wrap(buf, off, len);
      DataOutputStream dob = new DataOutputStream(out);
      dob.writeInt(wrapped.length);
      dob.write(wrapped, 0, wrapped.length);
```

### IOResource
'DataInputStream' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java`
#### Snippet
```java
    private void readNextRpcPacket() throws IOException {
      LOG.debug("reading next wrapped RPC packet");
      DataInputStream dis = new DataInputStream(in);
      int rpcLen = dis.readInt();
      byte[] rpcBuf = new byte[rpcLen];
```

### IOResource
'TableRecordReader' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java`
#### Snippet
```java
    // if no table record reader was provided use default
    final TableRecordReader trr =
      this.tableRecordReader == null ? new TableRecordReader() : this.tableRecordReader;
    trr.setStartRow(tSplit.getStartRow());
    trr.setEndRow(tSplit.getEndRow());
```

### IOResource
'TableRecordReader' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
    LOG.info("Input split length: " + StringUtils.humanReadableInt(tSplit.getLength()) + " bytes.");
    final TableRecordReader trr =
      this.tableRecordReader != null ? this.tableRecordReader : new TableRecordReader();
    Scan sc = new Scan(this.scan);
    sc.withStartRow(tSplit.getStartRow());
```

### IOResource
'FileInputStream' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java`
#### Snippet
```java

  private static FileInputStream deleteFileOnClose(final File file) throws IOException {
    return new FileInputStream(file) {
      private File myFile;

```

### IOResource
'RandomAccessFile' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java`
#### Snippet
```java
      String filePath = filePaths[i];
      try {
        rafs[i] = new RandomAccessFile(filePath, "rw");
        long totalSpace = new File(filePath).getTotalSpace();
        if (totalSpace < sizePerFile) {
```

### IOResource
'RandomAccessFile' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java`
#### Snippet
```java
      LOG.warn("Caught ClosedChannelException accessing BucketCache, reopening file: "
        + filePaths[accessFileNum], ioe);
      rafs[accessFileNum] = new RandomAccessFile(filePaths[accessFileNum], "rw");
      fileChannels[accessFileNum] = rafs[accessFileNum].getChannel();
    } finally {
```

### IOResource
'FSDataInputStreamWrapper' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
       * TestLazyDataBlockDecompression. Refactor?
       */
      FSDataInputStreamWrapper fsdis = new FSDataInputStreamWrapper(fs, file);
      long fileSize = fs.getFileStatus(file).getLen();
      FixedFileTrailer trailer = FixedFileTrailer.readFromStream(fsdis.getStream(false), fileSize);
```

### IOResource
'FileInputStream' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
   */
  private FileInputStream deleteFileOnClose(final File file) throws IOException {
    return new FileInputStream(file) {
      private File myFile;

```

### IOResource
'FSTableDescriptors' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
   */
  private void loadTableInfosForTablesWithNoRegion() throws IOException {
    Map<String, TableDescriptor> allTables = new FSTableDescriptors(getConf()).getAll();
    for (TableDescriptor htd : allTables.values()) {
      if (checkMetaOnly && !htd.isMetaTable()) {
```

### IOResource
'PrintWriter' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
  protected HBaseFsck printUsageAndExit() {
    StringWriter sw = new StringWriter(2048);
    PrintWriter out = new PrintWriter(sw);
    out.println("");
    out.println("-----------------------------------------------------------------------");
```

### IOResource
'TSocket' should be opened in front of a 'try' block and closed in the corresponding 'finally' block
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftConnection.java`
#### Snippet
```java
      TTransport tTransport = null;
      try {
        TSocket sock = new TSocket(connection.getHost(), connection.getPort());
        sock.setSocketTimeout(connection.getOperationTimeout());
        sock.setConnectTimeout(connection.getConnectTimeout());
```

## RuleId[id=RedundantOperationOnEmptyContainer]
### RedundantOperationOnEmptyContainer
Collection `skipServerSet` is always empty
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
        if (rackSkipSet.size() == getTotalNumberOfRacks()) {
          // remove the last two added and break
          skipServerSet.remove(secondaryAndTertiary[0]);
          skipServerSet.remove(secondaryAndTertiary[1]);
          break;
```

### RedundantOperationOnEmptyContainer
Collection `skipServerSet` is always empty
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
          // remove the last two added and break
          skipServerSet.remove(secondaryAndTertiary[0]);
          skipServerSet.remove(secondaryAndTertiary[1]);
          break;
        }
```

## RuleId[id=OptionalIsPresent]
### OptionalIsPresent
Can be replaced with single expression in functional style
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeProviderSelector.java`
#### Snippet
```java
    Optional<Token<?>> optional = user.getTokens().stream()
      .filter((t) -> SHADE_TOKEN_KIND_TEXT.equals(t.getKind())).findFirst();
    if (optional.isPresent()) {
      return new Pair<>(shade, optional.get());
    }
```

### OptionalIsPresent
Can be replaced with single expression in functional style
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
    addListener(ClientMetaTableAccessor.getTableState(metaTable, tableName),
      (tableState, error) -> {
        completeCheckTableState(future, tableState.isPresent() ? tableState.get() : null, error,
          TableState.State.ENABLED, tableName);
      });
```

### OptionalIsPresent
Can be replaced with single expression in functional style
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
    addListener(ClientMetaTableAccessor.getTableState(metaTable, tableName),
      (tableState, error) -> {
        completeCheckTableState(future, tableState.isPresent() ? tableState.get() : null, error,
          TableState.State.DISABLED, tableName);
      });
```

### OptionalIsPresent
Can be replaced with single expression in functional style
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java

        heap.recordBlockSize(blockSize -> {
          if (rpcCall.isPresent()) {
            rpcCall.get().incrementResponseBlockSize(blockSize);
          }
```

## RuleId[id=Java8MapForEach]
### Java8MapForEach
Can be replaced with 'Map.forEach()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
      ProcedureDescription.newBuilder().setSignature(signature).setInstance(instance);
    if (props != null && !props.isEmpty()) {
      props.entrySet().forEach(entry -> builder.addConfiguration(
        NameStringPair.newBuilder().setName(entry.getKey()).setValue(entry.getValue()).build()));
    }
```

### Java8MapForEach
Can be replaced with 'Map.forEach()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
      UpdateRSGroupConfigRequest.newBuilder().setGroupName(groupName);
    if (configuration != null) {
      configuration.entrySet().forEach(e -> request.addConfiguration(
        NameStringPair.newBuilder().setName(e.getKey()).setValue(e.getValue()).build()));
    }
```

### Java8MapForEach
Can be replaced with 'Map.forEach()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckChore.java`
#### Snippet
```java
      StringBuffer sb = new StringBuffer();
      sb.append("Regions by state: ");
      stateCountMap.entrySet().forEach(e -> {
        sb.append(e.getKey());
        sb.append('=');
```

### Java8MapForEach
Can be replaced with 'Map.forEach()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    if (serverManager != null) {
      final Map<ServerName, ServerMetrics> map = new HashMap<>();
      serverManager.getOnlineServers().entrySet().forEach(e -> map.put(e.getKey(), e.getValue()));
      return map;
    }
```

### Java8MapForEach
Can be replaced with 'Map.forEach()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    Map<ServerName, Set<byte[]>> rsReportsSnapshot = new HashMap<>();
    synchronized (rsReports) {
      rsReports.entrySet().forEach(e -> rsReportsSnapshot.put(e.getKey(), e.getValue()));
    }
    return rsReportsSnapshot;
```

## RuleId[id=FieldMayBeStatic]
### FieldMayBeStatic
Field `STALE_CACHE_CLEAN_CHORE_INTERVAL_IN_MILLISECONDS` may be 'static'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java`
#### Snippet
```java
    LoggerFactory.getLogger(CatalogReplicaLoadBalanceSimpleSelector.class);
  private final long STALE_CACHE_TIMEOUT_IN_MILLISECONDS = 3000; // 3 seconds
  private final int STALE_CACHE_CLEAN_CHORE_INTERVAL_IN_MILLISECONDS = 1500; // 1.5 seconds
  private final int REFRESH_REPLICA_COUNT_CHORE_INTERVAL_IN_MILLISECONDS = 60000; // 1 minute

```

### FieldMayBeStatic
Field `STALE_CACHE_TIMEOUT_IN_MILLISECONDS` may be 'static'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java`
#### Snippet
```java
  private static final Logger LOG =
    LoggerFactory.getLogger(CatalogReplicaLoadBalanceSimpleSelector.class);
  private final long STALE_CACHE_TIMEOUT_IN_MILLISECONDS = 3000; // 3 seconds
  private final int STALE_CACHE_CLEAN_CHORE_INTERVAL_IN_MILLISECONDS = 1500; // 1.5 seconds
  private final int REFRESH_REPLICA_COUNT_CHORE_INTERVAL_IN_MILLISECONDS = 60000; // 1 minute
```

### FieldMayBeStatic
Field `REFRESH_REPLICA_COUNT_CHORE_INTERVAL_IN_MILLISECONDS` may be 'static'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CatalogReplicaLoadBalanceSimpleSelector.java`
#### Snippet
```java
  private final long STALE_CACHE_TIMEOUT_IN_MILLISECONDS = 3000; // 3 seconds
  private final int STALE_CACHE_CLEAN_CHORE_INTERVAL_IN_MILLISECONDS = 1500; // 1.5 seconds
  private final int REFRESH_REPLICA_COUNT_CHORE_INTERVAL_IN_MILLISECONDS = 60000; // 1 minute

  /**
```

### FieldMayBeStatic
Field `DEFAULT_TIMEOUT` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java`
#### Snippet
```java
  // Maximum number of retries before taking bold actions
  protected final int maxRetriesMultiplier;
  private final int DEFAULT_TIMEOUT = 20000;
  private final int getEntriesTimeout;
  private final int shipEditsTimeout;
```

### FieldMayBeStatic
Field `columnFamilyOption` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java

  private final String fileOption = "f";
  private final String columnFamilyOption = "cf";
  private final String regionOption = "r";
  private final String tableNameOption = "t";
```

### FieldMayBeStatic
Field `tableNameOption` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
  private final String columnFamilyOption = "cf";
  private final String regionOption = "r";
  private final String tableNameOption = "t";

  private final String cmdString = "sft";
```

### FieldMayBeStatic
Field `cmdString` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
  private final String tableNameOption = "t";

  private final String cmdString = "sft";

  private String namespace;
```

### FieldMayBeStatic
Field `regionOption` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
  private final String fileOption = "f";
  private final String columnFamilyOption = "cf";
  private final String regionOption = "r";
  private final String tableNameOption = "t";

```

### FieldMayBeStatic
Field `fileOption` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFilePrettyPrinter.java`
#### Snippet
```java
  private Options options = new Options();

  private final String fileOption = "f";
  private final String columnFamilyOption = "cf";
  private final String regionOption = "r";
```

## RuleId[id=RedundantLengthCheck]
### RedundantLengthCheck
Redundant array length check
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java`
#### Snippet
```java
      }
    }
    if (statuses == null || statuses.length == 0) {
      return; // no remote files at all
    }
```

### RedundantLengthCheck
Redundant array length check
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
    // Calculate length of tags area
    int tagsLength = 0;
    if (tags != null && tags.length > 0) {
      for (Tag t : tags) {
        tagsLength += t.getValueLength() + Tag.INFRASTRUCTURE_SIZE;
```

### RedundantLengthCheck
Redundant array length check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        // the dst does not exist, and the src a dir, src becomes dst. (foo/b)
        FileStatus[] hfiles = fs.listStatus(src);
        if (hfiles != null && hfiles.length > 0) {
          for (FileStatus hfile : hfiles) {
            success = fs.rename(hfile.getPath(), dst);
```

### RedundantLengthCheck
Redundant array length check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
    }
    Scan s = new Scan();
    if (groups != null && groups.length > 0) {
      for (String group : groups) {
        s.addColumn(LABELS_TABLE_FAMILY, Bytes.toBytes(AuthUtil.toGroupEntry(group)));
```

## RuleId[id=UtilityClassWithoutPrivateConstructor]
### UtilityClassWithoutPrivateConstructor
Class `TagBuilderFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TagBuilderFactory.java`
#### Snippet
```java
 */
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
public final class TagBuilderFactory {

  public static TagBuilder create() {
```

### UtilityClassWithoutPrivateConstructor
Class `TagType` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TagType.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public final class TagType {
  // Please declare new Tag Types here to avoid step on pre-existing tag types.
  public static final byte ACL_TAG_TYPE = (byte) 1;
```

### UtilityClassWithoutPrivateConstructor
Class `KeyValueTestUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java`
#### Snippet
```java

@InterfaceAudience.Private
public class KeyValueTestUtil {

  public static KeyValue create(String row, String family, String qualifier, long timestamp,
```

### UtilityClassWithoutPrivateConstructor
Class `KeyValueUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class KeyValueUtil {

  private static final Logger LOG = LoggerFactory.getLogger(KeyValueUtil.class);
```

### UtilityClassWithoutPrivateConstructor
Class `StreamUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/util/StreamUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class StreamUtils {

  public static void writeRawVInt32(OutputStream output, int value) throws IOException {
```

### UtilityClassWithoutPrivateConstructor
Class `ByteRangeUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class ByteRangeUtils {
  public static int numEqualPrefixBytes(ByteRange left, ByteRange right, int rightInnerOffset) {
    int maxCompares = Math.min(left.getLength(), right.getLength() - rightInnerOffset);
```

### UtilityClassWithoutPrivateConstructor
Class `PrettyPrinter` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PrettyPrinter.java`
#### Snippet
```java

@InterfaceAudience.Private
public final class PrettyPrinter {

  private static final Logger LOG = LoggerFactory.getLogger(PrettyPrinter.class);
```

### UtilityClassWithoutPrivateConstructor
Class `ConcurrentMapUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcurrentMapUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ConcurrentMapUtils {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `OrderedBytes` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class OrderedBytes {

  /*
```

### UtilityClassWithoutPrivateConstructor
Class `AvlTree` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static class AvlTree {
    /**
     * Return the node that matches the specified key or null in case of node not found.
```

### UtilityClassWithoutPrivateConstructor
Class `AvlIterableList` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/AvlUtil.java`
#### Snippet
```java
   */
  @InterfaceAudience.Private
  public static class AvlIterableList {
    /**
     * Return the successor of the current node
```

### UtilityClassWithoutPrivateConstructor
Class `MD5Hash` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/MD5Hash.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class MD5Hash {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `ClassSize` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ClassSize {
  private static final Logger LOG = LoggerFactory.getLogger(ClassSize.class);

```

### UtilityClassWithoutPrivateConstructor
Class `Addressing` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Addressing.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class Addressing {
  public static final String VALID_PORT_REGEX = "[\\d]+";
  public static final String HOSTNAME_PORT_SEPARATOR = ":";
```

### UtilityClassWithoutPrivateConstructor
Class `ReflectionUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ReflectionUtils.java`
#### Snippet
```java

@InterfaceAudience.Private
public class ReflectionUtils {
  @SuppressWarnings({ "unchecked", "TypeParameterUnusedInFormals" })
  public static <T> T instantiateWithCustomCtor(String className, Class<?>[] ctorArgTypes,
```

### UtilityClassWithoutPrivateConstructor
Class `VersionInfo` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class VersionInfo {
  private static final Logger LOG = LoggerFactory.getLogger(VersionInfo.class.getName());

```

### UtilityClassWithoutPrivateConstructor
Class `Classes` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Classes.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class Classes {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `Threads` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Threads.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class Threads {
  private static final Logger LOG = LoggerFactory.getLogger(Threads.class);

```

### UtilityClassWithoutPrivateConstructor
Class `ComparerHolder` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  }

  static class ComparerHolder {
    static final String UNSAFE_COMPARER_NAME = ComparerHolder.class.getName() + "$UnsafeComparer";

```

### UtilityClassWithoutPrivateConstructor
Class `ConverterHolder` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java
  }

  static class ConverterHolder {
    static final String UNSAFE_CONVERTER_NAME =
      ConverterHolder.class.getName() + "$UnsafeConverter";
```

### UtilityClassWithoutPrivateConstructor
Class `ConverterHolder` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
  }

  static class ConverterHolder {
    static final String UNSAFE_CONVERTER_NAME =
      ConverterHolder.class.getName() + "$UnsafeConverter";
```

### UtilityClassWithoutPrivateConstructor
Class `LexicographicalComparerHolder` has only 'static' members, and lacks a 'private' constructor
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java
   * available.
   */
  static class LexicographicalComparerHolder {
    static final String UNSAFE_COMPARER_NAME =
      LexicographicalComparerHolder.class.getName() + "$UnsafeComparer";
```

### UtilityClassWithoutPrivateConstructor
Class `CatalogFamilyFormat` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/CatalogFamilyFormat.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class CatalogFamilyFormat {

  private static final Logger LOG = LoggerFactory.getLogger(CatalogFamilyFormat.class);
```

### UtilityClassWithoutPrivateConstructor
Class `Writables` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/util/Writables.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class Writables {
  /**
   * Get the Writable's contents as a byte array
```

### UtilityClassWithoutPrivateConstructor
Class `IPCUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/IPCUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
class IPCUtil {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `RegionInfoDisplay` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfoDisplay.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class RegionInfoDisplay {
  public final static String DISPLAY_KEYS_KEY = "hbase.display.keys";
  public final static byte[] HIDDEN_END_KEY = Bytes.toBytes("hidden-end-key");
```

### UtilityClassWithoutPrivateConstructor
Class `PackagePrivateFieldAccessor` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/PackagePrivateFieldAccessor.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class PackagePrivateFieldAccessor {

  public static void setMvccReadPoint(Scan scan, long mvccReadPoint) {
```

### UtilityClassWithoutPrivateConstructor
Class `ConnectionFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionFactory.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class ConnectionFactory {

  public static final String HBASE_CLIENT_ASYNC_CONNECTION_IMPL =
```

### UtilityClassWithoutPrivateConstructor
Class `ClientUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientUtil.java`
#### Snippet
```java

@InterfaceAudience.Private
public class ClientUtil {

  public static boolean areScanStartRowAndStopRowEqual(byte[] startRow, byte[] stopRow) {
```

### UtilityClassWithoutPrivateConstructor
Class `RequestControllerFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestControllerFactory.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public final class RequestControllerFactory {
  public static final String REQUEST_CONTROLLER_IMPL_CONF_KEY =
    "hbase.client.request.controller.impl";
```

### UtilityClassWithoutPrivateConstructor
Class `ParseConstants` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseConstants.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public final class ParseConstants {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `QuotaSettingsFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettingsFactory.java`
#### Snippet
```java

@InterfaceAudience.Public
public class QuotaSettingsFactory {
  static class QuotaGlobalsSettingsBypass extends QuotaSettings {
    private final boolean bypassGlobals;
```

### UtilityClassWithoutPrivateConstructor
Class `SlowLogTableAccessor` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/slowlog/SlowLogTableAccessor.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class SlowLogTableAccessor {

  private static final Logger LOG = LoggerFactory.getLogger(SlowLogTableAccessor.class);
```

### UtilityClassWithoutPrivateConstructor
Class `SaslUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/SaslUtil.java`
#### Snippet
```java

@InterfaceAudience.Private
public class SaslUtil {
  private static final Logger LOG = LoggerFactory.getLogger(SaslUtil.class);
  public static final String SASL_DEFAULT_REALM = "default";
```

### UtilityClassWithoutPrivateConstructor
Class `AccessControlClient` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlClient.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class AccessControlClient {
  public static final TableName ACL_TABLE_NAME =
    TableName.valueOf(NamespaceDescriptor.SYSTEM_NAMESPACE_NAME_STR, "acl");
```

### UtilityClassWithoutPrivateConstructor
Class `ShadedAccessControlUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/ShadedAccessControlUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ShadedAccessControlUtil {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `VisibilityConstants` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java`
#### Snippet
```java

@InterfaceAudience.Private
public final class VisibilityConstants {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `VisibilityLabelsValidator` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsValidator.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class VisibilityLabelsValidator {
  private static final boolean[] validAuthChars = new boolean[256];

```

### UtilityClassWithoutPrivateConstructor
Class `VisibilityClient` has only 'static' members, and lacks a 'private' constructor
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityClient.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class VisibilityClient {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `Driver` has only 'static' members, and lacks a 'private' constructor
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/Driver.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.TOOLS)
@InterfaceStability.Stable
public class Driver {

  private static ProgramDriver pgd = new ProgramDriver();
```

### UtilityClassWithoutPrivateConstructor
Class `TableMapReduceUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java`
#### Snippet
```java
@InterfaceAudience.Public
@SuppressWarnings({ "rawtypes", "unchecked" })
public class TableMapReduceUtil {
  private static final Logger LOG = LoggerFactory.getLogger(TableMapReduceUtil.class);

```

### UtilityClassWithoutPrivateConstructor
Class `Options` has only 'static' members, and lacks a 'private' constructor
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java

  // Command line options and defaults.
  static final class Options {
    static final Option SNAPSHOT = new Option(null, "snapshot", true, "Snapshot to restore.");
    static final Option TARGET_NAME =
```

### UtilityClassWithoutPrivateConstructor
Class `ChaosService` has only 'static' members, and lacks a 'private' constructor
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosService.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ChaosService {

  private static final Logger LOG = LoggerFactory.getLogger(ChaosService.class.getName());
```

### UtilityClassWithoutPrivateConstructor
Class `TableSnapshotInputFormatImpl` has only 'static' members, and lacks a 'private' constructor
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSnapshotInputFormatImpl.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class TableSnapshotInputFormatImpl {
  // TODO: Snapshots files are owned in fs by the hbase user. There is no
  // easy way to delegate access.
```

### UtilityClassWithoutPrivateConstructor
Class `ChaosConstants` has only 'static' members, and lacks a 'private' constructor
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosConstants.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public final class ChaosConstants {

  /* Base ZNode for whole Chaos Testing */
```

### UtilityClassWithoutPrivateConstructor
Class `ChaosUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ChaosUtils {

  public static String getHostName() throws UnknownHostException {
```

### UtilityClassWithoutPrivateConstructor
Class `TableMapReduceUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
@SuppressWarnings({ "rawtypes", "unchecked" })
@InterfaceAudience.Public
public class TableMapReduceUtil {
  private static final Logger LOG = LoggerFactory.getLogger(TableMapReduceUtil.class);
  public static final String TABLE_INPUT_CLASS_KEY = "hbase.table.input.class";
```

### UtilityClassWithoutPrivateConstructor
Class `MemorySizeUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/util/MemorySizeUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class MemorySizeUtil {

  public static final String MEMSTORE_SIZE_KEY = "hbase.regionserver.global.memstore.size";
```

### UtilityClassWithoutPrivateConstructor
Class `CacheableDeserializerIdManager` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheableDeserializerIdManager.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class CacheableDeserializerIdManager {
  private static final Map<Integer, CacheableDeserializer<Cacheable>> registeredDeserializers =
    new ConcurrentHashMap<>();
```

### UtilityClassWithoutPrivateConstructor
Class `HFileBlockIndex` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class HFileBlockIndex {

  private static final Logger LOG = LoggerFactory.getLogger(HFileBlockIndex.class);
```

### UtilityClassWithoutPrivateConstructor
Class `BlockCacheUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class BlockCacheUtil {

  private static final Logger LOG = LoggerFactory.getLogger(BlockCacheUtil.class);
```

### UtilityClassWithoutPrivateConstructor
Class `ChecksumUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ChecksumUtil {
  public static final Logger LOG = LoggerFactory.getLogger(ChecksumUtil.class);

```

### UtilityClassWithoutPrivateConstructor
Class `Header` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java

  // TODO: encapsulate Header related logic in this inner class.
  static class Header {
    // Format of header is:
    // 8 bytes - block magic
```

### UtilityClassWithoutPrivateConstructor
Class `JVMClusterUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class JVMClusterUtil {
  private static final Logger LOG = LoggerFactory.getLogger(JVMClusterUtil.class);

```

### UtilityClassWithoutPrivateConstructor
Class `HBaseConfTool` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseConfTool.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class HBaseConfTool {
  public static void main(String args[]) {
    if (args.length < 1) {
```

### UtilityClassWithoutPrivateConstructor
Class `RegionSplitter` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class RegionSplitter {
  private static final Logger LOG = LoggerFactory.getLogger(RegionSplitter.class);

```

### UtilityClassWithoutPrivateConstructor
Class `DirectMemoryUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/DirectMemoryUtils.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public class DirectMemoryUtils {
  private static final Logger LOG = LoggerFactory.getLogger(DirectMemoryUtils.class);
  private static final String MEMORY_USED = "MemoryUsed";
```

### UtilityClassWithoutPrivateConstructor
Class `HBaseFsckRepair` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class HBaseFsckRepair {
  private static final Logger LOG = LoggerFactory.getLogger(HBaseFsckRepair.class);

```

### UtilityClassWithoutPrivateConstructor
Class `CompressionTest` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.TOOLS)
@InterfaceStability.Evolving
public class CompressionTest {
  private static final Logger LOG = LoggerFactory.getLogger(CompressionTest.class);

```

### UtilityClassWithoutPrivateConstructor
Class `MasterRegionFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/MasterRegionFactory.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public final class MasterRegionFactory {

  // Use the character $ to let the log cleaner know that this is not the normal wal file.
```

### UtilityClassWithoutPrivateConstructor
Class `QuotaLimiterFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaLimiterFactory.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public class QuotaLimiterFactory {
  public static QuotaLimiter fromThrottle(final Throttle throttle) {
    return TimeBasedLimiter.fromThrottle(throttle);
```

### UtilityClassWithoutPrivateConstructor
Class `ReplicationProtobufUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.java`
#### Snippet
```java

@InterfaceAudience.Private
public class ReplicationProtobufUtil {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `SecurityUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/SecurityUtil.java`
#### Snippet
```java
@InterfaceAudience.Private
@InterfaceStability.Evolving
public class SecurityUtil {

  /**
```

### UtilityClassWithoutPrivateConstructor
Class `TokenUtil` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class TokenUtil {
  // This class is referenced indirectly by User out in common; instances are created by reflection
  private static final Logger LOG = LoggerFactory.getLogger(TokenUtil.class);
```

### UtilityClassWithoutPrivateConstructor
Class `SnapshotScannerHDFSAclStorage` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.java`
#### Snippet
```java
  }

  static final class SnapshotScannerHDFSAclStorage {
    /**
     * Add a new CF in HBase acl table to record if the HBase read permission is synchronized to
```

### UtilityClassWithoutPrivateConstructor
Class `VisibilityUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class VisibilityUtils {

  private static final Logger LOG = LoggerFactory.getLogger(VisibilityUtils.class);
```

### UtilityClassWithoutPrivateConstructor
Class `Options` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java`
#### Snippet
```java
  private static final Logger LOG = LoggerFactory.getLogger(SnapshotInfo.class);

  static final class Options {
    static final Option SNAPSHOT =
      new Option(null, "snapshot", true, "The name of the snapshot to be detailed.");
```

### UtilityClassWithoutPrivateConstructor
Class `LogHandlerUtils` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/LogHandlerUtils.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class LogHandlerUtils {

  private static int getTotalFiltersCount(AdminProtos.SlowLogResponseRequest request) {
```

### UtilityClassWithoutPrivateConstructor
Class `MetricsCoprocessor` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetricsCoprocessor.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class MetricsCoprocessor {

  // Master coprocessor metrics
```

### UtilityClassWithoutPrivateConstructor
Class `ShutdownHook` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ShutdownHook {
  private static final Logger LOG = LoggerFactory.getLogger(ShutdownHook.class);
  private static final String CLIENT_FINALIZER_DATA_METHOD = "clientFinalizer";
```

### UtilityClassWithoutPrivateConstructor
Class `FlushPolicyFactory` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/FlushPolicyFactory.java`
#### Snippet
```java
 */
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
public class FlushPolicyFactory {

  private static final Logger LOG = LoggerFactory.getLogger(FlushPolicyFactory.class);
```

### UtilityClassWithoutPrivateConstructor
Class `StoreFileComparators` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileComparators.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
final class StoreFileComparators {
  /**
   * Comparator that compares based on the Sequence Ids of the the store files. Bulk loads that did
```

### UtilityClassWithoutPrivateConstructor
Class `Compressor` has only 'static' members, and lacks a 'private' constructor
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class Compressor {
  /**
   * Command line tool to compress and uncompress WALs.
```

## RuleId[id=DataFlowIssue]
### DataFlowIssue
Variable is already assigned to this value
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
        } else {
          tagsArray = Bytes.copy(tagsBuffer, 0, this.tagsLength);
          tOffset = 0;
        }
      }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java`
#### Snippet
```java
        } else {
          tagsBuf = ByteBuffer.wrap(Bytes.copy(tagsBuffer, 0, this.tagsLength));
          tOffset = 0;
        }
      }
```

### DataFlowIssue
Method invocation `longValue` may produce `NullPointerException`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
      return Long.valueOf(0);
    }
    return decodeNumericValue(src).longValue();
  }

```

### DataFlowIssue
Method invocation `doubleValue` may produce `NullPointerException`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
      return Double.POSITIVE_INFINITY;
    } else {
      return decodeNumericValue(src).doubleValue();
    }
  }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Addressing.java`
#### Snippet
```java
        local = NetworkInterface.getByInetAddress(addr) != null;
      } catch (SocketException e) {
        local = false;
      }
    }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java`
#### Snippet
```java
      try {
        initTempDir(conf);
        dynamicJarsEnabled = true;
      } catch (Exception e) {
        LOG.error("Disabling the DynamicClassLoader as it failed to initialize its temp directory."
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/HBTop.java`
#### Snippet
```java

          case "r":
            initialMode = Mode.REGION;
            break;

```

### DataFlowIssue
Method invocation `plus` may produce `NullPointerException`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/Record.java`
#### Snippet
```java
        return entry(k, values.get(k));
      }
      return entry(k, values.get(k).plus(o.values.get(k)));
    }));
  }
```

### DataFlowIssue
Argument `o.values.get(k)` might be null
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/Record.java`
#### Snippet
```java
        return entry(k, values.get(k));
      }
      return entry(k, values.get(k).plus(o.values.get(k)));
    }));
  }
```

### DataFlowIssue
Unboxing of `keyPress.getCharacter()` may produce `NullPointerException`
in `hbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/terminal/impl/KeyPressGenerator.java`
#### Snippet
```java
    if (
      keyPress.isCtrl() && keyPress.getType() == KeyPress.Type.Character
        && keyPress.getCharacter() == 'c'
    ) {
      System.exit(0);
```

### DataFlowIssue
Method invocation `stream` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
        .map(ProtobufUtil::toReplicationLoadSource).collect(Collectors.toList()))
      .addAllTasks(
        metrics.getTasks().stream().map(ProtobufUtil::toServerTask).collect(Collectors.toList()))
      .setReportStartTime(metrics.getLastReportTimestamp())
      .setReportEndTime(metrics.getReportTimestamp());
```

### DataFlowIssue
Condition `param instanceof ClientProtos.MultiRequest` is redundant and can be replaced with a null check
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
      cs.setStartTime(EnvironmentEdgeManager.currentTime());

      if (param instanceof ClientProtos.MultiRequest) {
        ClientProtos.MultiRequest req = (ClientProtos.MultiRequest) param;
        int numActions = 0;
```

### DataFlowIssue
Method invocation `putHeaders` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/IPCUtil.java`
#### Snippet
```java
    RPCTInfo.Builder traceBuilder = RPCTInfo.newBuilder();
    GlobalOpenTelemetry.getPropagators().getTextMapPropagator().inject(Context.current(),
      traceBuilder, (carrier, key, value) -> carrier.putHeaders(key, value));
    builder.setTraceInfo(traceBuilder.build());
    builder.setMethodName(call.md.getName());
```

### DataFlowIssue
Dereference of `rawResults` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncScanSingleRegionRpcRetryingCaller.java`
#### Snippet
```java
      if (resp.hasCursor()) {
        cursor = Optional.of(ProtobufUtil.toCursor(resp.getCursor()));
      } else if (scan.isNeedCursorResult() && rawResults.length > 0) {
        // It is size limit exceed and we need to return the last Result's row.
        // When user setBatch and the scanner is reopened, the server may return Results that
```

### DataFlowIssue
Method invocation `getRegionLocation` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncMetaRegionLocator.java`
#### Snippet
```java
        }
      }
      HRegionLocation oldLoc = oldLocs.getRegionLocation(replicaId);
      if (
        oldLoc != null && (oldLoc.getSeqNum() > loc.getSeqNum()
```

### DataFlowIssue
Method invocation `isStale` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchScanResultCache.java`
#### Snippet
```java
    for (;;) {
      Result r = partialResults.pollFirst();
      stale = stale || r.isStale();
      int newCellCount = cellCount + r.size();
      if (newCellCount > batch) {
```

### DataFlowIssue
Method invocation `size` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BatchScanResultCache.java`
#### Snippet
```java
      Result r = partialResults.pollFirst();
      stale = stale || r.isStale();
      int newCellCount = cellCount + r.size();
      if (newCellCount > batch) {
        // We have more cells than expected, so split the current result
```

### DataFlowIssue
Method invocation `getSecond` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
        while (!nextRows.isEmpty() && !lessThan(currentCell, nextRows.peek().getFirst())) {
          Pair<byte[], Pair<byte[], byte[]>> head = nextRows.poll();
          Pair<byte[], byte[]> fuzzyData = head.getSecond();
          updateWith(currentCell, fuzzyData);
        }
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/EncryptionUtil.java`
#### Snippet
```java
    throws IOException {
    return wrapKey(conf,
      conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName()),
      new SecretKeySpec(key, algorithm));
  }
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/EncryptionUtil.java`
#### Snippet
```java
    Key key;
    String masterKeyName =
      conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName());
    try {
      // First try the master key
```

### DataFlowIssue
Method invocation `getCause` may produce `NullPointerException`
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java`
#### Snippet
```java
    }
    // TableNotFound may also be buried one level deep
    if (exp instanceof TableNotFoundException || exp.getCause() instanceof TableNotFoundException) {
      throw new WebApplicationException(
        Response.status(Response.Status.NOT_FOUND).type(MIMETYPE_TEXT)
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
              col = rowspec.getColumns()[i++];
            } catch (ArrayIndexOutOfBoundsException e) {
              col = null;
            }
          }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
            col = rowspec.getColumns()[i++];
          } catch (ArrayIndexOutOfBoundsException e) {
            col = null;
          }
        }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
            col = rowspec.getColumns()[i++];
          } catch (ArrayIndexOutOfBoundsException e) {
            col = null;
          }
        }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java
  public static MutationProto toMutation(final MutationType type, final Mutation mutation,
    MutationProto.Builder builder, long nonce) throws IOException {
    builder = getMutationBuilderAndSetCommonFields(type, mutation, builder);
    if (nonce != HConstants.NO_NONCE) {
      builder.setNonce(nonce);
```

### DataFlowIssue
Dereference of `ZKUtil.listChildrenNoWatch(zkw, znodeToProcess)` may produce `NullPointerException`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
      zkw.getConfiguration().get("zookeeper.znode.replication.peers.state", "peer-state");
    int pblen = ProtobufUtil.lengthOfPBMagic();
    for (String child : ZKUtil.listChildrenNoWatch(zkw, znodeToProcess)) {
      if (!child.equals(peerState)) {
        continue;
```

### DataFlowIssue
Dereference of `peerStateData` may produce `NullPointerException`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
        ReplicationProtos.ReplicationState.Builder builder =
          ReplicationProtos.ReplicationState.newBuilder();
        ProtobufUtil.mergeFrom(builder, peerStateData, pblen, peerStateData.length - pblen);
        sb.append(builder.getState().name());
      } catch (IOException ipbe) {
```

### DataFlowIssue
Dereference of `ZKUtil.listChildrenNoWatch(zkw, peersZnode)` may produce `NullPointerException`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
    int pblen = ProtobufUtil.lengthOfPBMagic();
    sb.append("\n").append(peersZnode).append(": ");
    for (String peerIdZnode : ZKUtil.listChildrenNoWatch(zkw, peersZnode)) {
      String znodeToProcess = ZNodePaths.joinZNode(peersZnode, peerIdZnode);
      byte[] data;
```

### DataFlowIssue
Dereference of `data` may produce `NullPointerException`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
        ReplicationProtos.ReplicationPeer.Builder builder =
          ReplicationProtos.ReplicationPeer.newBuilder();
        ProtobufUtil.mergeFrom(builder, data, pblen, data.length - pblen);
        String clusterKey = builder.getClusterkey();
        sb.append("\n").append(znodeToProcess).append(": ").append(clusterKey);
```

### DataFlowIssue
Dereference of `ZKUtil.listChildrenNoWatch(zkw, znodeToProcess)` may produce `NullPointerException`
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKDump.java`
#### Snippet
```java
        }
      }
      for (String zNodeChild : ZKUtil.listChildrenNoWatch(zkw, znodeToProcess)) {
        stack.add(ZNodePaths.joinZNode(znodeToProcess, zNodeChild));
      }
```

### DataFlowIssue
Method invocation `close` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java`
#### Snippet
```java
      } finally {
        if (this.m_mutator == null) {
          conn.close();
          conn = null;
        }
```

### DataFlowIssue
Dereference of `ZKUtil.listChildrenNoWatch(zookeeper, sb.toString())` may produce `NullPointerException`
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java`
#### Snippet
```java
      for (String levelOne : levelOneDirs) {
        sb.append(ZNodePaths.ZNODE_PATH_SEPARATOR).append(levelOne);
        for (String levelTwo : ZKUtil.listChildrenNoWatch(zookeeper, sb.toString())) {
          sb.append(ZNodePaths.ZNODE_PATH_SEPARATOR).append(levelTwo);
          for (String znode : ZKUtil.listChildrenNoWatch(zookeeper, sb.toString())) {
```

### DataFlowIssue
Dereference of `ZKUtil.listChildrenNoWatch(zookeeper, sb.toString())` may produce `NullPointerException`
in `hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java`
#### Snippet
```java
        for (String levelTwo : ZKUtil.listChildrenNoWatch(zookeeper, sb.toString())) {
          sb.append(ZNodePaths.ZNODE_PATH_SEPARATOR).append(levelTwo);
          for (String znode : ZKUtil.listChildrenNoWatch(zookeeper, sb.toString())) {
            if (znode.endsWith(suffix)) {
              sb.append(ZNodePaths.ZNODE_PATH_SEPARATOR).append(znode);
```

### DataFlowIssue
Method invocation `getFirst` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java`
#### Snippet
```java
  public static Job createSubmittableJob(Configuration conf, String[] args) throws IOException {
    Triple<TableName, Scan, Path> arguments = ExportUtils.getArgumentsFromCommandLine(conf, args);
    String tableName = arguments.getFirst().getNameAsString();
    Path outputDir = arguments.getThird();
    Job job = Job.getInstance(conf, conf.get(JOB_NAME_CONF_KEY, NAME + "_" + tableName));
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
      startRow = new byte[1];
      endRow = new byte[1];
      startRow[0] = 0;
      endRow[0] = -1;
    }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
    if (startRow.length == 0 && endRow.length != 0) {
      startRow = new byte[1];
      startRow[0] = 0;
    }
    if (startRow.length != 0 && endRow.length == 0) {
```

### DataFlowIssue
Dereference of `splitKeys` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
    // Split Region into n chunks evenly
    byte[][] splitKeys = Bytes.split(startRow, endRow, true, n - 1);
    for (int i = 0; i < splitKeys.length - 1; i++) {
      // In the table input format for single table we do not need to
      // store the scan object in table split because it can be memory intensive and redundant
```

### DataFlowIssue
Method invocation `add` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
              }
              if (CellUtil.isDelete(cell)) {
                del.add(cell);
              } else {
                put.add(cell);
```

### DataFlowIssue
Method invocation `add` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
                del.add(cell);
              } else {
                put.add(cell);
              }
              context.getCounter(Counter.CELLS_WRITTEN).increment(1);
```

### DataFlowIssue
Argument `path` might be null
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
          throw new IOException("Invalid File Type: " + inputInfo.getType().toString());
      }
      return new Path(outputArchive, path);
    }

```

### DataFlowIssue
Argument `targetCell` might be null
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
            }
            // add a tombstone to exactly match the target cell that is missing on the source
            delete.addColumn(CellUtil.cloneFamily(targetCell), CellUtil.cloneQualifier(targetCell),
              targetCell.getTimestamp());
          }
```

### DataFlowIssue
Method invocation `getValueArray` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
              LOG.debug("Different values: ");
              LOG.debug("  source cell: " + sourceCell + " value: "
                + Bytes.toString(sourceCell.getValueArray(), sourceCell.getValueOffset(),
                  sourceCell.getValueLength()));
              LOG.debug("  target cell: " + targetCell + " value: "
```

### DataFlowIssue
Method invocation `getValueArray` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SyncTable.java`
#### Snippet
```java
                  sourceCell.getValueLength()));
              LOG.debug("  target cell: " + targetCell + " value: "
                + Bytes.toString(targetCell.getValueArray(), targetCell.getValueOffset(),
                  targetCell.getValueLength()));
            }
```

### DataFlowIssue
Method invocation `shutdown` may produce `NullPointerException`
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosService.java`
#### Snippet
```java
      execute(args, conf);
    } finally {
      if (authChore != null) choreChaosService.shutdown();
    }
  }
```

### DataFlowIssue
Method invocation `get` may produce `NullPointerException`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java`
#### Snippet
```java
        byte[] family = CellUtil.cloneFamily(kv);
        if (writeMultipleTables) {
          tableNameBytes = MultiTableHFileOutputFormat.getTableName(row.get());
          tableNameBytes = TableName.valueOf(tableNameBytes).getNameWithNamespaceInclAsString()
            .getBytes(Charset.defaultCharset());
```

### DataFlowIssue
Method invocation `write` may produce `NullPointerException`
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java`
#### Snippet
```java
        }
        if (jsonpcb != null) {
          writer.write(");");
        }
        if (writer != null) {
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
    // Walk down least loaded, filling each to the min
    int neededRegions = 0; // number of regions needed to bring all up to min
    fetchFromTail = false;

    Map<ServerName, Integer> underloadedServers = new HashMap<>();
```

### DataFlowIssue
Method invocation `setNumRegionsAdded` may produce `NullPointerException`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
        }
        regionsToMove.add(new RegionPlan(region, server.getKey().getServerName(), null));
        balanceInfo.setNumRegionsAdded(balanceInfo.getNumRegionsAdded() - 1);
        balanceInfo.setNextRegionForUnload(balanceInfo.getNextRegionForUnload() + 1);
        totalNumMoved++;
```

### DataFlowIssue
Method invocation `setNumRegionsAdded` may produce `NullPointerException`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
          regionsToReturn);
        numTaken++;
        balanceInfo.setNumRegionsAdded(balanceInfo.getNumRegionsAdded() + 1);
      }
    }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthCheckChore.java`
#### Snippet
```java
          stop = true;
        } else {
          stop = false;
        }
      } else {
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthCheckChore.java`
#### Snippet
```java
        numTimesUnhealthy = 1;
        startWindow = EnvironmentEdgeManager.currentTime();
        stop = false;
      }
    }
```

### DataFlowIssue
Dereference of `exception` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
      }
    }
    throw exception;
  }

```

### DataFlowIssue
Dereference of `exception` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/FileLink.java`
#### Snippet
```java
        }
      }
      throw exception;
    }

```

### DataFlowIssue
Dereference of `readyChunk` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java`
#### Snippet
```java
    ReadyChunk readyChunk = readyChunks.peek();

    BloomFilterChunk readyChunkBloom = readyChunk.chunk;
    readyChunkBloom.writeBloom(out);
  }
```

### DataFlowIssue
Method invocation `getBlockType` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
          block.release();
        }
      } while (!block.getBlockType().isData());
      return block;
    }
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.java`
#### Snippet
```java
      trailer.setEncryptionKey(EncryptionUtil.wrapKey(
        cryptoContext.getConf(), cryptoContext.getConf()
          .get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName()),
        cryptoContext.getKey()));
    }
```

### DataFlowIssue
Argument `head` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruCachedBlockQueue.java`
#### Snippet
```java
    } else {
      LruCachedBlock head = queue.peek();
      if (cb.compareTo(head) > 0) {
        heapSize += cb.heapSize();
        heapSize -= head.heapSize();
```

### DataFlowIssue
Method invocation `getValue` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/CachedEntryQueue.java`
#### Snippet
```java
      cacheSize += entry.getValue().getLength();
    } else {
      BucketEntry head = queue.peek().getValue();
      if (BucketEntry.COMPARATOR.compare(entry.getValue(), head) > 0) {
        cacheSize += entry.getValue().getLength();
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
   */
  public ByteBuffer getMetaData(ByteBuffer bb) {
    bb = addMetaData(bb, true);
    bb.flip();
    return bb;
```

### DataFlowIssue
Method invocation `getBlockType` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
        public HFileBlock nextBlockWithBlockType(BlockType blockType) throws IOException {
          HFileBlock blk = nextBlock();
          if (blk.getBlockType() != blockType) {
            throw new IOException(
              "Expected block of type " + blockType + " but found " + blk.getBlockType());
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
  public void serialize(ByteBuffer destination, boolean includeNextBlockMetadata) {
    this.buf.get(destination, 0, getSerializedLength() - BLOCK_METADATA_SPACE);
    destination = addMetaData(destination, includeNextBlockMetadata);

    // Make it ready for reading. flip sets position to zero and limit to current position which
```

### DataFlowIssue
Array access `secondaryAndTertiary[0]` may produce `NullPointerException`
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
        if (rackSkipSet.size() == getTotalNumberOfRacks()) {
          // remove the last two added and break
          skipServerSet.remove(secondaryAndTertiary[0]);
          skipServerSet.remove(secondaryAndTertiary[1]);
          break;
```

### DataFlowIssue
Method invocation `finish` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java

    if (printStats) {
      fileStats.finish(printStatRanges);
      out.println("Stats:\n" + fileStats);
    }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
            try {
              // Blocks
              entries = getRAMQueueEntries(inputQueue, entries);
            } catch (InterruptedException ie) {
              if (!cacheEnabled || !writerEnabled) {
```

### DataFlowIssue
Condition `param instanceof ScanRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java

  private boolean isScanRequest(final RequestHeader header, final Message param) {
    return param instanceof ScanRequest;
  }

```

### DataFlowIssue
Condition `param instanceof MultiRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
  protected boolean isWriteRequest(final RequestHeader header, final Message param) {
    // TODO: Is there a better way to do this?
    if (param instanceof MultiRequest) {
      MultiRequest multi = (MultiRequest) param;
      for (RegionAction regionAction : multi.getRegionActionList()) {
```

### DataFlowIssue
Method invocation `stream` may produce `NullPointerException`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          }
          CompletableFuture<?>[] compactFutures =
            locations.stream().filter(l -> l.getRegion() != null)
              .filter(l -> !l.getRegion().isOffline()).filter(l -> l.getServerName() != null)
              .map(l -> compact(l.getServerName(), l.getRegion(), major, columnFamily))
```

### DataFlowIssue
Condition `param instanceof MutateRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.java`
#### Snippet
```java
      }
    }
    if (param instanceof MutateRequest) {
      return true;
    }
```

### DataFlowIssue
Condition `param instanceof ClientProtos.ScanRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
    }
    responseInfo.put("param", stringifiedParam);
    if (param instanceof ClientProtos.ScanRequest && rsRpcServices != null) {
      ClientProtos.ScanRequest request = ((ClientProtos.ScanRequest) param);
      String scanDetails;
```

### DataFlowIssue
Condition `param instanceof ClientProtos.MultiRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
      }
    }
    if (param instanceof ClientProtos.MultiRequest) {
      int numGets = 0;
      int numMutations = 0;
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
        LOG.warn("Try to recover the WAL lease " + path, lnre);
        recoverLease(conf, path);
        reader = null;
        ee = lnre;
      } catch (NullPointerException npe) {
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
        // Just sleep and retry. Will require re-reading compressed WALs for compressionContext.
        LOG.warn("Got NPE opening reader, will retry.");
        reader = null;
        ee = npe;
      }
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
      serverName = ServerName.parseServerName(logDirName);
    } catch (IllegalArgumentException | IllegalStateException ex) {
      serverName = null;
      LOG.warn("Cannot parse a server name from path=" + logFile + "; " + ex.getMessage());
    }
```

### DataFlowIssue
Argument `files` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
    });
    if (ArrayUtils.isNotEmpty(files)) {
      Arrays.asList(files).forEach(status -> filesSorted.add(status.getPath()));
    }
    return filesSorted;
```

### DataFlowIssue
Casting `m` to `Delete` may produce `ClassCastException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
      }
      if (CellUtil.isDelete(cell)) {
        ((Delete) m).add(cell);
      } else {
        ((Put) m).add(cell);
```

### DataFlowIssue
Casting `m` to `Put` may produce `ClassCastException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitUtil.java`
#### Snippet
```java
        ((Delete) m).add(cell);
      } else {
        ((Put) m).add(cell);
      }
      m.setDurability(durability);
```

### DataFlowIssue
The call to 'warnOrThrowExceptionForFailure' always fails, according to its method contracts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/TableDescriptorChecker.java`
#### Snippet
```java
    // Meta table shouldn't be set as read only, otherwise it will impact region assignments
    if (td.isReadOnly() && TableName.isMetaTableName(td.getTableName())) {
      warnOrThrowExceptionForFailure(false, "Meta table can't be set as read only.", null);
    }

```

### DataFlowIssue
The call to 'warnOrThrowExceptionForFailure' always fails, according to its method contracts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/TableDescriptorChecker.java`
#### Snippet
```java
      }
    } catch (IOException e) {
      warnOrThrowExceptionForFailure(false, e.getMessage(), e);
    }
  }
```

### DataFlowIssue
Dereference of `status` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
    throws IOException {
    FileStatus[] status = CommonFSUtils.listStatus(fs, dir, TABLEINFO_PATHFILTER);
    for (FileStatus file : status) {
      Path path = file.getPath();
      int sequenceId = getTableInfoSequenceIdAndFileLength(path).sequenceId;
```

### DataFlowIssue
Dereference of `findActiveMaster(masters)` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
    });

    return findActiveMaster(masters).master.getServerName().toString();
  }

```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ZKDataMigrator.java`
#### Snippet
```java
        switch (state) {
          case ENABLED:
            newState = TableState.State.ENABLED;
            break;
          case DISABLED:
```

### DataFlowIssue
Method invocation `getHostname` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
    // normally, see HBASE-27304 for details.
    ServerName master = admin.getClusterMetrics(EnumSet.of(Option.MASTER)).getMasterName();
    if (InetAddresses.isInetAddress(master.getHostname())) {
      if (!InetAddresses.isInetAddress(this.hostname)) {
        this.hostname = InetAddress.getByName(this.hostname).getHostAddress();
```

### DataFlowIssue
Method invocation `getHostname` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionMover.java`
#### Snippet
```java
  private void stripMaster(List<ServerName> regionServers) throws IOException {
    ServerName master = admin.getClusterMetrics(EnumSet.of(Option.MASTER)).getMasterName();
    stripServer(regionServers, master.getHostname(), master.getPort());
  }

```

### DataFlowIssue
Method invocation `getFirst` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      }

      LOG.info("This group range is [" + Bytes.toStringBinary(range.getFirst()) + ", "
        + Bytes.toStringBinary(range.getSecond()) + "]");

```

### DataFlowIssue
Method invocation `getHdfsHRI` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      for (HbckRegionInfo hi : daughters) {
        if (Bytes.compareTo(hi.getHdfsHRI().getStartKey(), range.getFirst()) == 0) {
          if (parent.getHdfsHRI().getRegionId() < hi.getHdfsHRI().getRegionId()) {
            daughterA = hi;
          }
```

### DataFlowIssue
Method invocation `getHdfsHRI` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
        }
        if (Bytes.compareTo(hi.getHdfsHRI().getEndKey(), range.getSecond()) == 0) {
          if (parent.getHdfsHRI().getRegionId() < hi.getHdfsHRI().getRegionId()) {
            daughterB = hi;
          }
```

### DataFlowIssue
Method invocation `getHdfsHRI` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      // daughters must share the same regionID and we should have a parent too
      if (
        daughterA.getHdfsHRI().getRegionId() != daughterB.getHdfsHRI().getRegionId()
          || parent == null
      ) {
```

### DataFlowIssue
Method invocation `getHdfsHRI` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      // daughters must share the same regionID and we should have a parent too
      if (
        daughterA.getHdfsHRI().getRegionId() != daughterB.getHdfsHRI().getRegionId()
          || parent == null
      ) {
```

### DataFlowIssue
Method invocation `getFirst` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
      // from start key to end Key
      RegionInfo newRegion = RegionInfoBuilder.newBuilder(htd.getTableName())
        .setStartKey(range.getFirst()).setEndKey(range.getSecond()).build();
      HRegion region = HBaseFsckRepair.createHDFSRegionDir(conf, newRegion, htd);
      LOG.info("[" + thread + "] Created new empty container region: " + newRegion
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
      } catch (IOException e) {
        LOG.warn("Failed to archive " + currentFile + " on try #" + i, e);
        success = false;
      }
    }
```

### DataFlowIssue
Method invocation `getFilePath` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
    int count = 0;

    fsDelegationToken.acquireDelegationToken(queue.peek().getFilePath().getFileSystem(getConf()));
    bulkToken = FutureUtils.get(conn.prepareBulkLoad(tableName));
    Pair<Multimap<ByteBuffer, LoadQueueItem>, Set<String>> pair = null;
```

### DataFlowIssue
Condition `param instanceof RegionServerStatusProtos.ReportRegionStateTransitionRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterAnnotationReadingPriorityFunction.java`
#### Snippet
```java
    // running. Hence all reports of meta transition should always be in a different thread.
    // This keeps from deadlocking the cluster.
    if (param instanceof RegionServerStatusProtos.ReportRegionStateTransitionRequest) {
      // Regions are moving. Lets see which ones.
      RegionServerStatusProtos.ReportRegionStateTransitionRequest tRequest =
```

### DataFlowIssue
Argument `servers` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/DrainingServerTracker.java`
#### Snippet
```java
    List<String> servers =
      ZKUtil.listChildrenAndWatchThem(watcher, watcher.getZNodePaths().drainingZNode);
    add(servers);
  }

```

### DataFlowIssue
Argument `newNodes` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/DrainingServerTracker.java`
#### Snippet
```java
        final List<String> newNodes =
          ZKUtil.listChildrenAndWatchThem(watcher, watcher.getZNodePaths().drainingZNode);
        add(newNodes);
      } catch (KeeperException e) {
        abortable.abort("Unexpected zk exception getting RS nodes", e);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
    RegionServerStartupResponse.Builder resp =
      addConfig(RegionServerStartupResponse.newBuilder(), HConstants.HBASE_DIR);
    resp = addConfig(resp, "fs.defaultFS");
    return addConfig(resp, "hbase.master.info.port");
  }
```

### DataFlowIssue
Argument `switchType` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
          server.cpHost.preSetSplitOrMergeEnabled(newValue, switchType);
        }
        server.getSplitOrMergeStateStore().setSplitOrMergeEnabled(newValue, switchType);
        if (server.cpHost != null) {
          server.cpHost.postSetSplitOrMergeEnabled(newValue, switchType);
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
        User caller = RpcServer.getRequestUser().orElse(null);
        String userName =
          request.hasUserName() ? request.getUserName().toStringUtf8() : caller.getShortName();
        List<Permission> permissions = new ArrayList<>();
        for (int i = 0; i < request.getPermissionCount(); i++) {
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
        }
        server.getMasterCoprocessorHost().preHasUserPermissions(userName, permissions);
        if (!caller.getShortName().equals(userName)) {
          List<String> groups = AccessChecker.getUserGroups(userName);
          caller = new InputUser(userName, groups.toArray(new String[groups.size()]));
```

### DataFlowIssue
Dereference of `snapshotDirs` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java`
#### Snippet
```java
    // iterate over all the cached snapshots and see if we need to update some, it is not an
    // expensive operation if we do not reload the manifest of snapshots.
    for (FileStatus snapshotDir : snapshotDirs) {
      String name = snapshotDir.getPath().getName();
      SnapshotDirectoryInfo files = snapshots.get(name);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      cpHost.preGetTableNames(htds, regex);
    }
    htds = getTableDescriptors(htds, namespace, regex, null, includeSysTables);
    if (cpHost != null) {
      cpHost.postGetTableNames(htds, regex);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
      cpHost.preGetTableDescriptors(tableNameList, htds, regex);
    }
    htds = getTableDescriptors(htds, namespace, regex, tableNameList, includeSysTables);
    if (cpHost != null) {
      cpHost.postGetTableDescriptors(tableNameList, htds, regex);
```

### DataFlowIssue
Method invocation `size` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java`
#### Snippet
```java
        SnapshotManifest.open(env.getMasterConfiguration(), mfs.getFileSystem(),
          SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, mfs.getRootDir()), snapshot);
      int snapshotRegionCount = manifest.getRegionManifestsMap().size();
      int tableRegionCount =
        ProcedureSyncWait.getMasterQuotaManager(env).getRegionCountOfTable(tableName);
```

### DataFlowIssue
Method invocation `size` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java`
#### Snippet
```java

      ProcedureSyncWait.getMasterQuotaManager(env).checkNamespaceTableAndRegionQuota(getTableName(),
        manifest.getRegionManifestsMap().size());
    }

```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java`
#### Snippet
```java
    // Put for parent
    Put putOfMerged = MetaTableAccessor.makePutFromRegionInfo(child, time);
    putOfMerged = addMergeRegions(putOfMerged, Arrays.asList(parents));
    // Set initial state to CLOSED.
    // NOTE: If initial state is not set to CLOSED then merged region gets added with the
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/QuotaState.java`
#### Snippet
```java
      globalLimiter = NoopQuotaLimiter.get();
    } else {
      globalLimiter = QuotaLimiterFactory.update(globalLimiter, other.globalLimiter);
    }
    lastUpdate = other.lastUpdate;
```

### DataFlowIssue
Method invocation `getUGI` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.java`
#### Snippet
```java
      ugi = user.get().getUGI();
    } else {
      ugi = User.getCurrent().getUGI();
    }
    TableName table = region.getTableDescriptor().getTableName();
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/UserQuotaState.java`
#### Snippet
```java
          limiter = entry.getValue();
        } else {
          limiter = QuotaLimiterFactory.update(limiter, entry.getValue());
        }
        map.put(entry.getKey(), limiter);
```

### DataFlowIssue
Method invocation `getParams` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
      }
    }
    result.getParams().addExtraParam("filterUser", filterUser);
    logResult(result);
    if (!result.isAllowed()) {
```

### DataFlowIssue
Method invocation `isAllowed` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
    }
    logResult(result);
    if (!result.isAllowed()) {
      throw new AccessDeniedException("Insufficient permissions " + result.toContextString());
    }
```

### DataFlowIssue
Method invocation `getParams` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
      }
    }
    result.getParams().addExtraParam("filterUser", filterUser);
    logResult(result);
    if (!result.isAllowed()) {
```

### DataFlowIssue
Method invocation `isAllowed` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
    }
    logResult(result);
    if (!result.isAllowed()) {
      throw new AccessDeniedException("Insufficient permissions " + result.toContextString());
    }
```

### DataFlowIssue
Method invocation `isAllowed` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
    }
    logResult(result);
    if (!result.isAllowed()) {
      throw new AccessDeniedException("Insufficient permissions " + result.toContextString());
    }
```

### DataFlowIssue
Argument `nodeList` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
          final List<ZKUtil.NodeAndData> nodeList =
            ZKUtil.getChildDataAndWatchForNewChildren(watcher, aclZNode, false);
          refreshNodes(nodeList);
        } catch (KeeperException ke) {
          String msg = "ZooKeeper error while reading node children data for path " + path;
```

### DataFlowIssue
Method invocation `getDefaultName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslServerAuthenticationProvider.java`
#### Snippet
```java
      if (pc != null) {
        TokenIdentifier tokenIdentifier =
          HBaseSaslRpcServer.getIdentifier(nc.getDefaultName(), secretManager);
        attemptingUser.set(tokenIdentifier.getUser());
        char[] password = getPassword(tokenIdentifier);
```

### DataFlowIssue
Casting `top` to `NonLeafExpressionNode` may produce `ClassCastException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/ExpressionParser.java`
#### Snippet
```java
        throw new ParseException("Error parsing expression " + expS + " at column : " + index);
      }
      if (!top.isSingleNode() && ((NonLeafExpressionNode) top).getChildExps().size() != 1) {
        throw new ParseException("Error parsing expression " + expS + " at column : " + index);
      }
```

### DataFlowIssue
Dereference of `visibilityLabels` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
      labels.clear();
      ordinalVsLabels.clear();
      for (VisibilityLabel visLabel : visibilityLabels) {
        String label = Bytes.toString(visLabel.getLabel().toByteArray());
        labels.put(label, visLabel.getOrdinal());
```

### DataFlowIssue
Method invocation `getUserAuthsList` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
      this.userAuths.clear();
      this.groupAuths.clear();
      for (UserAuthorizations userAuths : multiUserAuths.getUserAuthsList()) {
        String user = Bytes.toString(userAuths.getUser().toByteArray());
        if (AuthUtil.isGroupPrincipal(user)) {
```

### DataFlowIssue
Method invocation `getName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
      // Ensure all servers are of same rsgroup.
      for (Address server : servers) {
        String tmpGroup = getRSGroupOfServer(server).getName();
        if (!tmpGroup.equals(srcGrp.getName())) {
          throw new ConstraintException("Move server request should only come from one source "
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java`
#### Snippet
```java
    }
    if (LOG.isTraceEnabled()) {
      LOG.trace("Current active user name is " + user.getShortName());
    }
    return user;
```

### DataFlowIssue
Argument `parent` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java`
#### Snippet
```java
        String parent = ZKUtil.getParent(path);
        // if its the end barrier, the procedure can be completed
        if (isReachedNode(parent)) {
          receivedReachedGlobalBarrier(path);
          return;
```

### DataFlowIssue
Argument `ZKUtil.getParent(path)` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java
          if (isAcquiredPathNode(path)) {
            // node wasn't present when we created the watch so zk event triggers acquire
            coordinator.memberAcquiredBarrier(ZKUtil.getNodeName(ZKUtil.getParent(path)),
              ZKUtil.getNodeName(path));
          } else if (isReachedPathNode(path)) {
```

### DataFlowIssue
Argument `ZKUtil.getParent(path)` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.java`
#### Snippet
```java

            // TODO Nothing enforces that acquire and reached znodes from showing up in wrong order.
            String procName = ZKUtil.getNodeName(ZKUtil.getParent(path));
            String member = ZKUtil.getNodeName(path);
            // get the data from the procedure member
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
      ) {
        // remove namespace directories default HDFS acls for skip tables
        op = HDFSAclOperation.OperationType.REMOVE;
        aclType = HDFSAclOperation.AclType.DEFAULT;
      }
```

### DataFlowIssue
Condition `param instanceof ClientProtos.MultiRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.java`
#### Snippet
```java
    int numMutations = 0;
    int numServiceCalls = 0;
    if (param instanceof ClientProtos.MultiRequest) {
      ClientProtos.MultiRequest multi = (ClientProtos.MultiRequest) param;
      for (ClientProtos.RegionAction regionAction : multi.getRegionActionList()) {
```

### DataFlowIssue
Method invocation `getValue` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java`
#### Snippet
```java
        }
      } else {
        boolean valueIsNull = comparator.getValue() == null || comparator.getValue().length == 0;
        if (result.isEmpty() && valueIsNull) {
          matches = true;
```

### DataFlowIssue
Argument `op` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java`
#### Snippet
```java
          Cell kv = result.get(0);
          int compareResult = PrivateCellUtil.compareValue(kv, comparator);
          matches = matches(op, compareResult);
        }
      }
```

### DataFlowIssue
Method invocation `getStoresList` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/BulkLoadCellFilter.java`
#### Snippet
```java
      return cell;
    }
    List<StoreDescriptor> storesList = bld.getStoresList();
    // Copy the StoreDescriptor list and update it as storesList is a unmodifiableList
    List<StoreDescriptor> copiedStoresList = new ArrayList<>(storesList);
```

### DataFlowIssue
Method invocation `getStoresList` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java`
#### Snippet
```java
      if (CellUtil.matchingQualifier(cell, WALEdit.BULK_LOAD)) {
        BulkLoadDescriptor bld = WALEdit.getBulkLoadDescriptor(cell);
        List<StoreDescriptor> stores = bld.getStoresList();
        int totalStores = stores.size();
        for (int j = 0; j < totalStores; j++) {
```

### DataFlowIssue
Method invocation `equals` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/PeerProcedureHandlerImpl.java`
#### Snippet
```java
        // Reset peer config if refresh source failed
        peer.setPeerConfig(oldConfig);
        peer.setPeerState(oldState.equals(PeerState.ENABLED));
      }
      peerLock.unlock();
```

### DataFlowIssue
Method invocation `equals` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/PeerProcedureHandlerImpl.java`
#### Snippet
```java
      if (!success && peer != null) {
        // Reset peer state if refresh source failed
        peer.setPeerState(oldState.equals(PeerState.ENABLED));
      }
      peerLock.unlock();
```

### DataFlowIssue
Method invocation `getStoresList` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
        try {
          BulkLoadDescriptor bld = WALEdit.getBulkLoadDescriptor(cells.get(i));
          List<StoreDescriptor> stores = bld.getStoresList();
          int totalStores = stores.size();
          for (int j = 0; j < totalStores; j++) {
```

### DataFlowIssue
Argument `path` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
        if (!fs.exists(path)) {
          // There is a chance that wal has moved to oldWALs directory, so look there also.
          path = AbstractFSWALProvider.findArchivedLog(path, conf);
          // path can be null if unable to locate in archiveDir.
        }
```

### DataFlowIssue
Method invocation `getStoresList` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.java`
#### Snippet
```java
        try {
          BulkLoadDescriptor bld = WALEdit.getBulkLoadDescriptor(cells.get(i));
          List<StoreDescriptor> stores = bld.getStoresList();
          int totalStores = stores.size();
          for (int j = 0; j < totalStores; j++) {
```

### DataFlowIssue
Method invocation `getName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSourceShipper.java`
#### Snippet
```java
    try {
      startPosition = this.replicationQueues.getWALPosition(source.getServer().getServerName(),
        peerClusterZNode, this.logQueue.getQueue(walGroupId).peek().getName());
      LOG.trace("Recovered queue started with log {} at position {}",
        this.logQueue.getQueue(walGroupId).peek(), startPosition);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java`
#### Snippet
```java
              LOG.warn("Exception checking for local table " + tableName, iox);
              // we can't drop edits without full assurance, so we assume table exists.
              exist = true;
            }
          }
```

### DataFlowIssue
Method invocation `getReplicate` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
          if (CellUtil.matchingQualifier(cell, WALEdit.BULK_LOAD)) {
            BulkLoadDescriptor bld = WALEdit.getBulkLoadDescriptor(cell);
            if (bld.getReplicate()) {
              if (bulkLoadsPerClusters == null) {
                bulkLoadsPerClusters = new HashMap<>();
```

### DataFlowIssue
Casting `mutation` to `Delete` may produce `ClassCastException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
            }
            if (CellUtil.isDelete(cell)) {
              ((Delete) mutation).add(cell);
            } else {
              ((Put) mutation).add(cell);
```

### DataFlowIssue
Casting `mutation` to `Put` may produce `ClassCastException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
              ((Delete) mutation).add(cell);
            } else {
              ((Put) mutation).add(cell);
            }
            previousCell = cell;
```

### DataFlowIssue
Method invocation `stream` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
    queueStorage = ReplicationStorageFactory.getReplicationQueueStorage(zkw, getConf());
    Set<ServerName> liveRegionServers = ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().rsZNode)
      .stream().map(ServerName::parseServerName).collect(Collectors.toSet());

    // Loops each peer on each RS and dumps the queues
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
      length = status.getLen() - status.getLen() / 2;
    } else {
      start = 0;
      length = status.getLen() / 2;
    }
```

### DataFlowIssue
Dereference of `exToThrow` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
          }
        }
        throw exToThrow;
      } else {
        // HFile Reference
```

### DataFlowIssue
Dereference of `exToThrow` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
          }
        }
        throw exToThrow;
      } else {
        status = fs.getFileStatus(initialPath);
```

### DataFlowIssue
Dereference of `exToThrow` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
        }
      }
      throw exToThrow;
    } else {
      return computeHDFSBlocksDistributionInternal(fs);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
    if (earlyMemstoreSufficientCheck && earlyBlockCacheSufficientCheck) {
      // Both memstore and block cache memory seems to be sufficient. No operation required.
      newTuneDirection = StepDirection.NEUTRAL;
      tunerLog.append("Going to do nothing because no changes are needed.");
    } else if (earlyMemstoreSufficientCheck) {
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
        ) {
          // Everything is fine no tuning required
          newTuneDirection = StepDirection.NEUTRAL;
        } else if (
          (double) cacheMissCount
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.java`
#### Snippet
```java
          tunerLog.append(
            "Going to do nothing because we " + "could not determine best tuning direction");
          newTuneDirection = StepDirection.NEUTRAL;
        }
      }
```

### DataFlowIssue
Condition `service instanceof AccessControlService` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    // Don't intercept calls to our own AccessControlService, we check for
    // appropriate permissions in the service handlers
    if (shouldCheckExecPermission && !(service instanceof AccessControlService)) {
      requirePermission(ctx,
        "invoke(" + service.getDescriptorForType().getName() + "." + methodName + ")",
```

### DataFlowIssue
Method invocation `getDescriptorForType` will produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    if (shouldCheckExecPermission && !(service instanceof AccessControlService)) {
      requirePermission(ctx,
        "invoke(" + service.getDescriptorForType().getName() + "." + methodName + ")",
        getTableName(ctx.getEnvironment()), null, null, Action.EXEC);
    }
```

### DataFlowIssue
Method invocation `getName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
        User caller = RpcServer.getRequestUser().orElse(null);
        if (LOG.isDebugEnabled()) {
          LOG.debug("Received request from {} to grant access permission {}", caller.getName(),
            perm.toString());
        }
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
        if (LOG.isDebugEnabled()) {
          LOG.debug("Received request from {} to revoke access permission {}",
            caller.getShortName(), perm.toString());
        }
        preGrantOrRevoke(caller, "revoke", perm);
```

### DataFlowIssue
Argument `table` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
          // Only wrap the filter if we are enforcing authorizations
          if (authorizationEnabled) {
            Filter ourFilter = new AccessControlFilter(getAuthManager(), user, table,
              AccessControlFilter.Strategy.CHECK_TABLE_AND_CF_ONLY, cfVsMaxVersions);
            // wrap any existing filter
```

### DataFlowIssue
Argument `table` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
        // Only wrap the filter if we are enforcing authorizations
        if (authorizationEnabled) {
          Filter ourFilter = new AccessControlFilter(getAuthManager(), user, table,
            AccessControlFilter.Strategy.CHECK_CELL_DEFAULT, cfVsMaxVersions);
          // wrap any existing filter
```

### DataFlowIssue
Condition `param instanceof BulkLoadHFileRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSAnnotationReadingPriorityFunction.java`
#### Snippet
```java
      return header.getPriority();
    }
    if (param instanceof BulkLoadHFileRequest) {
      return HConstants.BULKLOAD_QOS;
    }
```

### DataFlowIssue
Method invocation `getClass` will produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSAnnotationReadingPriorityFunction.java`
#### Snippet
```java
    }

    String cls = param.getClass().getName();
    Class<? extends Message> rpcArgClass = argumentToClassMap.get(cls);
    RegionSpecifier regionSpecifier = null;
```

### DataFlowIssue
Condition `param instanceof ScanRequest` is redundant and can be replaced with a null check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSAnnotationReadingPriorityFunction.java`
#### Snippet
```java
  @Override
  public long getDeadline(RequestHeader header, Message param) {
    if (param instanceof ScanRequest) {
      ScanRequest request = (ScanRequest) param;
      if (!request.hasScannerId()) {
```

### DataFlowIssue
Method invocation `getCause` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java`
#### Snippet
```java
      return null;
    } else if (
      (ioe instanceof FileNotFoundException) || (ioe.getCause() instanceof FileNotFoundException)
    ) {
      // The region is re-opened when FileNotFoundException is thrown.
```

### DataFlowIssue
The call to 'checkArgument' always fails, according to its method contracts
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    // Check that we do not try to compact the same StoreFile twice.
    if (!Collections.disjoint(filesCompacting, filesToAdd)) {
      Preconditions.checkArgument(false, "%s overlaps with %s", filesToAdd, filesCompacting);
    }
    filesCompacting.addAll(filesToAdd);
```

### DataFlowIssue
Dereference of `lastException` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
      }
    }
    throw lastException;
  }

```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
    for (HRegion region : regions) {
      try {
        stats = stats.append(this.server.clearRegionBlockCache(region));
      } catch (Exception e) {
        stats.addException(region.getRegionInfo().getRegionName(), e);
```

### DataFlowIssue
Method invocation `incrementResponseExceptionSize` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        NameBytesPair pair = ResponseConverter.buildException(ie);
        resultOrExceptionBuilder.setException(pair);
        context.incrementResponseExceptionSize(pair.getSerializedSize());
      }
      if (hasResultOrException) {
```

### DataFlowIssue
Method invocation `getClientVersionInfo` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
        // retrying with the same scannerId only to get USE on the next RPC, we directly throw
        // a special exception to save an RPC.
        if (VersionInfoUtil.hasMinimumVersion(rpcCall.getClientVersionInfo(), 1, 4)) {
          // 1.4.0+ clients know how to handle
          throw new ScannerResetException("Scanner is closed on the server-side", e);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
        // The new boundaries should match old stripe boundaries, so we should get exact matches.
        if (isOpen(firstStartRow)) {
          removeFrom = 0;
        } else {
          removeFrom = findStripeIndexByEndRow(firstStartRow);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java`
#### Snippet
```java
            LOG.debug("Unable to unwrap key with WAL key '" + walKeyName + "'");
          }
          key = null;
        }
      }
```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.java`
#### Snippet
```java
      if (key == null) {
        String masterKeyName =
          conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName());
        try {
          // Then, try the cluster master key
```

### DataFlowIssue
Method invocation `getTxid` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.java`
#### Snippet
```java
  // confirm non-empty before calling
  private static long getLastTxid(Deque<FSWALEntry> queue) {
    return queue.peekLast().getTxid();
  }

```

### DataFlowIssue
Method invocation `getShortName` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractProtobufLogWriter.java`
#### Snippet
```java
      builder.setEncryptionKey(UnsafeByteOperations.unsafeWrap(EncryptionUtil.wrapKey(conf,
        conf.get(HConstants.CRYPTO_WAL_KEY_NAME_CONF_KEY,
          conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName())),
        key)));

```

### DataFlowIssue
Method invocation `contains` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java`
#### Snippet
```java
    Collection<HStoreFile> expiredStores = new ArrayList<>();
    for (HStoreFile sf : files) {
      if (isEmptyStoreFile(sf) && !filesCompacting.contains(sf)) {
        expiredStores.add(sf);
        continue;
```

### DataFlowIssue
Method invocation `update` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/FileBasedStoreFileTracker.java`
#### Snippet
```java
        builder.addStoreFile(toStoreFileEntry(info));
      }
      backedFile.update(builder);
    }
  }
```

### DataFlowIssue
Method invocation `update` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/FileBasedStoreFileTracker.java`
#### Snippet
```java
        builder.addStoreFile(toStoreFileEntry(info));
      }
      backedFile.update(builder);
      for (String name : compactedFileNames) {
        storefiles.remove(name);
```

### DataFlowIssue
Method invocation `load` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/FileBasedStoreFileTracker.java`
#### Snippet
```java
  @Override
  protected List<StoreFileInfo> doLoadStoreFiles(boolean readOnly) throws IOException {
    StoreFileList list = backedFile.load(readOnly);
    if (list == null) {
      return Collections.emptyList();
```

### DataFlowIssue
Method invocation `update` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/FileBasedStoreFileTracker.java`
#### Snippet
```java
        builder.addStoreFile(toStoreFileEntry(info));
      }
      backedFile.update(builder);
      for (StoreFileInfo info : newFiles) {
        storefiles.put(info.getPath().getName(), info);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java`
#### Snippet
```java
      type = BackupCommand.CREATE;
    } else if (BackupCommand.HELP.name().equalsIgnoreCase(cmd)) {
      type = BackupCommand.HELP;
    } else if (BackupCommand.DELETE.name().equalsIgnoreCase(cmd)) {
      type = BackupCommand.DELETE;
```

### DataFlowIssue
Method invocation `getBackupRootDir` may produce `NullPointerException`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupCommands.java`
#### Snippet
```java
      // Check if backup .tmp exists
      BackupInfo bInfo = sysTable.readBackupInfo(backupIds[0]);
      String backupRoot = bInfo.getBackupRootDir();
      FileSystem fs = FileSystem.get(new Path(backupRoot).toUri(), new Configuration());
      String backupId = BackupUtils.findMostRecentBackupId(backupIds);
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
        if (type == BackupType.INCREMENTAL) {
          // Update incremental backup set
          tableList = excludeNonExistingTables(tableList, nonExistingTableList);
        } else {
          // Throw exception only in full mode - we try to backup non-existing table
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    };
    FileSystem walFs = CommonFSUtils.getWALFileSystem(c);
    logFiles = BackupUtils.getFiles(walFs, logDir, logFiles, filter);
    logFiles = BackupUtils.getFiles(walFs, oldLogDir, logFiles, filter);
    return logFiles;
```

### DataFlowIssue
Variable is already assigned to this value
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
    FileSystem walFs = CommonFSUtils.getWALFileSystem(c);
    logFiles = BackupUtils.getFiles(walFs, logDir, logFiles, filter);
    logFiles = BackupUtils.getFiles(walFs, oldLogDir, logFiles, filter);
    return logFiles;
  }
```

### DataFlowIssue
Method invocation `getTableNames` may produce `NullPointerException`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java
        BackupInfo bInfo = table.readBackupInfo(backupId);

        allSet.addAll(bInfo.getTableNames());
      }
    }
```

### DataFlowIssue
Method invocation `getBackupRootDir` may produce `NullPointerException`
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupMergeJob.java`
#### Snippet
```java

      BackupInfo bInfo = table.readBackupInfo(backupIds[0]);
      String backupRoot = bInfo.getBackupRootDir();

      for (int i = 0; i < tableNames.length; i++) {
```

### DataFlowIssue
Argument `path` might be null
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
          files = mapForSrc[srcIdx].get(fam);
        }
        files.add(new Path(path));
        if (LOG.isDebugEnabled()) {
          LOG.debug("found bulk loaded file : " + tbl + " " + Bytes.toString(fam) + " " + path);
```

### DataFlowIssue
Method invocation `close` may produce `NullPointerException`
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java`
#### Snippet
```java
    scannerMap = CacheBuilder.newBuilder().expireAfterAccess(cacheTimeout, TimeUnit.MILLISECONDS)
      .removalListener((RemovalListener<Integer,
        ResultScanner>) removalNotification -> removalNotification.getValue().close())
      .build();
  }
```

### DataFlowIssue
Argument `files` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      }
      deleteRecoveredEdits(fs,
        Stream.of(files).map(FileStatus::getPath).collect(Collectors.toList()));
    }

```

### DataFlowIssue
Method invocation `getValue` may produce `NullPointerException`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          } else {
            boolean valueIsNull =
              comparator.getValue() == null || comparator.getValue().length == 0;
            if (result.isEmpty() && valueIsNull) {
              matches = op != CompareOperator.NOT_EQUAL;
```

### DataFlowIssue
Argument `op` might be null
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
              cellTs = kv.getTimestamp();
              int compareResult = PrivateCellUtil.compareValue(kv, comparator);
              matches = matches(op, compareResult);
            }
          }
```

### DataFlowIssue
Method invocation `getFirst` may produce `NullPointerException`
in `hbase-endpoint/src/main/java/org/apache/hadoop/hbase/coprocessor/Export.java`
#### Snippet
```java
    Triple<TableName, Scan, Path> arguments =
      ExportUtils.getArgumentsFromCommandLine(conf, otherArgs);
    return run(conf, arguments.getFirst(), arguments.getSecond(), arguments.getThird());
  }

```

### DataFlowIssue
Method invocation `getName` may produce `NullPointerException`
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    // Else distributed filesystem. Make a new instance per daemon. Below
    // code is taken from the AppendTestUtil over in hdfs.
    String username = User.getCurrent().getName() + differentiatingSuffix;
    User user = User.createUserForTesting(c, username, new String[] { "supergroup" });
    return user;
```

## RuleId[id=DeprecatedIsStillUsed]
### DeprecatedIsStillUsed
Deprecated member 'DEPRECATED_MAX_BUFFER_COUNT_KEY' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
   */
  @Deprecated
  static final String DEPRECATED_MAX_BUFFER_COUNT_KEY = "hbase.ipc.server.reservoir.initial.max";

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'DEPRECATED_ALLOCATOR_POOL_ENABLED_KEY' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
   */
  @Deprecated
  public static final String DEPRECATED_ALLOCATOR_POOL_ENABLED_KEY =
    "hbase.ipc.server.reservoir.enabled";

```

### DeprecatedIsStillUsed
Deprecated member 'DEPRECATED_BUFFER_SIZE_KEY' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBuffAllocator.java`
#### Snippet
```java
   */
  @Deprecated
  static final String DEPRECATED_BUFFER_SIZE_KEY = "hbase.ipc.server.reservoir.initial.buffer.size";

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'RS_HOSTNAME_KEY' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/DNS.java`
#### Snippet
```java
  @Deprecated
  @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
  public static final String RS_HOSTNAME_KEY = "hbase.regionserver.hostname";

  static {
```

### DeprecatedIsStillUsed
Deprecated member 'getAuthChore' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/AuthUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static ScheduledChore getAuthChore(Configuration conf) throws IOException {
    if (!isAuthRenewalChoreEnabled(conf)) {
      return null;
```

### DeprecatedIsStillUsed
Deprecated member 'oswrite' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java`
#### Snippet
```java
   */
  @Deprecated
  public static long oswrite(final KeyValue kv, final OutputStream out, final boolean withTags)
    throws IOException {
    ByteBufferUtils.putInt(out, kv.getSerializedSize(withTags));
```

### DeprecatedIsStillUsed
Deprecated member 'NAMESPACE_TABLE_NAME' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/TableName.java`
#### Snippet
```java
   */
  @Deprecated
  public static final TableName NAMESPACE_TABLE_NAME =
    valueOf(NamespaceDescriptor.SYSTEM_NAMESPACE_NAME_STR, "namespace");

```

### DeprecatedIsStillUsed
Deprecated member 'DEFAULT_HBASE_SPLIT_COORDINATED_BY_ZK' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   */
  @Deprecated
  public static final boolean DEFAULT_HBASE_SPLIT_COORDINATED_BY_ZK = false;

  public static final String HBASE_SPLIT_WAL_MAX_SPLITTER = "hbase.regionserver.wal.max.splitters";
```

### DeprecatedIsStillUsed
Deprecated member 'REPLAY_QOS' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   */
  @Deprecated
  public static final int REPLAY_QOS = 6;
  public static final int REGION_REPLICATION_QOS = REPLAY_QOS;
  public static final int QOS_THRESHOLD = 10;
```

### DeprecatedIsStillUsed
Deprecated member 'DEFAULT_META_REPLICA_NUM' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   */
  @Deprecated
  public static final int DEFAULT_META_REPLICA_NUM = 1;

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'SLOW_LOG_SYS_TABLE_CHORE_DURATION_KEY' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
  // since <need to know the version number> and will be removed in <version number>
  // Instead use hbase.regionserver.named.queue.chore.duration config property
  public static final String SLOW_LOG_SYS_TABLE_CHORE_DURATION_KEY =
    "hbase.slowlog.systable.chore.duration";
  // Default 10 mins.
```

### DeprecatedIsStillUsed
Deprecated member 'META_REPLICAS_NUM' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   */
  @Deprecated
  public static final String META_REPLICAS_NUM = "hbase.meta.replica.count";
  /**
   * @deprecated Since 2.4.0, will be removed in 4.0.0. Please change the meta replicas number by
```

### DeprecatedIsStillUsed
Deprecated member 'HBASE_CLIENT_PAUSE_FOR_CQTBE' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java
   */
  @Deprecated
  public static final String HBASE_CLIENT_PAUSE_FOR_CQTBE = "hbase.client.pause.cqtbe";

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'containsTable' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java`
#### Snippet
```java
   */
  @Deprecated
  public boolean containsTable(TableName table) {
    return tables.contains(table);
  }
```

### DeprecatedIsStillUsed
Deprecated member 'addTable' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java`
#### Snippet
```java
   */
  @Deprecated
  public void addTable(TableName table) {
    tables.add(table);
  }
```

### DeprecatedIsStillUsed
Deprecated member 'tables' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java`
#### Snippet
```java
   */
  @Deprecated
  private final SortedSet<TableName> tables;

  private final Map<String, String> configuration;
```

### DeprecatedIsStillUsed
Deprecated member 'addAllTables' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java`
#### Snippet
```java
   */
  @Deprecated
  public void addAllTables(Collection<TableName> arg) {
    tables.addAll(arg);
  }
```

### DeprecatedIsStillUsed
Deprecated member 'getWrongWALRegionDir' is still used
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
   */
  @Deprecated
  public static Path getWrongWALRegionDir(final Configuration conf, final TableName tableName,
    final String encodedRegionName) throws IOException {
    Path wrongTableDir = new Path(new Path(getWALRootDir(conf), tableName.getNamespaceAsString()),
```

### DeprecatedIsStillUsed
Deprecated member 'LogQueryFilter' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/LogQueryFilter.java`
#### Snippet
```java
@InterfaceStability.Evolving
@Deprecated
public class LogQueryFilter {

  private String regionName;
```

### DeprecatedIsStillUsed
Deprecated member 'getWriteBufferPeriodicFlushTimerTickMs' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
   */
  @Deprecated
  public long getWriteBufferPeriodicFlushTimerTickMs() {
    return writeBufferPeriodicFlushTimerTickMs;
  }
```

### DeprecatedIsStillUsed
Deprecated member 'getImplementationClassName' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
   */
  @Deprecated
  public String getImplementationClassName() {
    return this.implementationClassName;
  }
```

### DeprecatedIsStillUsed
Deprecated member 'setWriteBufferPeriodicFlushTimerTickMs' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
   */
  @Deprecated
  public BufferedMutatorParams setWriteBufferPeriodicFlushTimerTickMs(long timerTickMs) {
    this.writeBufferPeriodicFlushTimerTickMs = timerTickMs;
    return this;
```

### DeprecatedIsStillUsed
Deprecated member 'implementationClassName' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
   */
  @Deprecated
  public BufferedMutatorParams implementationClassName(String implementationClassName) {
    this.implementationClassName = implementationClassName;
    return this;
```

### DeprecatedIsStillUsed
Deprecated member 'getPool' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutatorParams.java`
#### Snippet
```java
   */
  @Deprecated
  public ExecutorService getPool() {
    return pool;
  }
```

### DeprecatedIsStillUsed
Deprecated member 'batchCoprocessorService' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   */
  @Deprecated
  default <R extends Message> void batchCoprocessorService(
    Descriptors.MethodDescriptor methodDescriptor, Message request, byte[] startKey, byte[] endKey,
    R responsePrototype, Batch.Callback<R> callback) throws ServiceException, Throwable {
```

### DeprecatedIsStillUsed
Deprecated member 'batchCoprocessorService' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   */
  @Deprecated
  default <R extends Message> Map<byte[], R> batchCoprocessorService(
    Descriptors.MethodDescriptor methodDescriptor, Message request, byte[] startKey, byte[] endKey,
    R responsePrototype) throws ServiceException, Throwable {
```

### DeprecatedIsStillUsed
Deprecated member 'CoprocessorBlockingRpcCallback' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/CoprocessorBlockingRpcCallback.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
class CoprocessorBlockingRpcCallback<R> implements RpcCallback<R> {
  private R result;
  private boolean resultSet = false;
```

### DeprecatedIsStillUsed
Deprecated member 'setAsyncPrefetch' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
   */
  @Deprecated
  public Scan setAsyncPrefetch(boolean asyncPrefetch) {
    this.asyncPrefetch = asyncPrefetch;
    return this;
```

### DeprecatedIsStillUsed
Deprecated member 'NAMESPACE_TABLEDESC' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   */
  @Deprecated
  public static final TableDescriptor NAMESPACE_TABLEDESC =
    TableDescriptorBuilder.newBuilder(TableName.NAMESPACE_TABLE_NAME)
      .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(NAMESPACE_FAMILY_INFO_BYTES)
```

### DeprecatedIsStillUsed
Deprecated member 'NAMESPACE_FAMILY_INFO_BYTES' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @Deprecated
  public final static byte[] NAMESPACE_FAMILY_INFO_BYTES = Bytes.toBytes(NAMESPACE_FAMILY_INFO);

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'NORMALIZER_TARGET_REGION_SIZE_KEY' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  public static final String NORMALIZER_TARGET_REGION_SIZE = "NORMALIZER_TARGET_REGION_SIZE";
  @Deprecated
  private static final Bytes NORMALIZER_TARGET_REGION_SIZE_KEY =
    new Bytes(Bytes.toBytes(NORMALIZER_TARGET_REGION_SIZE));

```

### DeprecatedIsStillUsed
Deprecated member 'NAMESPACE_COL_DESC_BYTES' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
  @InterfaceAudience.Private
  @Deprecated
  public final static byte[] NAMESPACE_COL_DESC_BYTES = Bytes.toBytes("d");

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'FirstKeyValueMatchingQualifiersFilter' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FirstKeyValueMatchingQualifiersFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
@Deprecated
public class FirstKeyValueMatchingQualifiersFilter extends FirstKeyOnlyFilter {

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'snapshotCleanupZNode' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZNodePaths.java`
#### Snippet
```java
   */
  @Deprecated
  public final String snapshotCleanupZNode;

  public ZNodePaths(Configuration conf) {
```

### DeprecatedIsStillUsed
Deprecated member 'switchZNode' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZNodePaths.java`
#### Snippet
```java
   */
  @Deprecated
  public final String switchZNode;
  // znode of indicating master maintenance mode
  public final String masterMaintZNode;
```

### DeprecatedIsStillUsed
Deprecated member 'regionNormalizerZNode' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZNodePaths.java`
#### Snippet
```java
   */
  @Deprecated
  public final String regionNormalizerZNode;
  // znode containing the state of all switches, currently there are split and merge child node.
  /**
```

### DeprecatedIsStillUsed
Deprecated member 'balancerZNode' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZNodePaths.java`
#### Snippet
```java
   */
  @Deprecated
  public final String balancerZNode;
  // znode containing the state of region normalizer
  /**
```

### DeprecatedIsStillUsed
Deprecated member 'tableZNode' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZNodePaths.java`
#### Snippet
```java
  // MirroringTableStateManager. To be removed in hbase3.
  @Deprecated
  public final String tableZNode;
  // znode containing the unique cluster ID
  public final String clusterIdZNode;
```

### DeprecatedIsStillUsed
Deprecated member 'BitSetNode' is still used
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/BitSetNode.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
class BitSetNode {
  private static final long WORD_MASK = 0xffffffffffffffffL;
  private static final int ADDRESS_BITS_PER_WORD = 6;
```

### DeprecatedIsStillUsed
Deprecated member 'ProcedureWALFile' is still used
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureWALFile.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
class ProcedureWALFile implements Comparable<ProcedureWALFile> {
  private static final Logger LOG = LoggerFactory.getLogger(ProcedureWALFile.class);

```

### DeprecatedIsStillUsed
Deprecated member 'abortProcedureAsync' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   */
  @Deprecated
  Future<Boolean> abortProcedureAsync(long procId, boolean mayInterruptIfRunning)
    throws IOException;

```

### DeprecatedIsStillUsed
Deprecated member 'ZKLeaderManager' is still used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class ZKLeaderManager extends ZKListener {
  private static final Logger LOG = LoggerFactory.getLogger(ZKLeaderManager.class);

```

### DeprecatedIsStillUsed
Deprecated member 'ProcedureStoreTracker' is still used
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/ProcedureStoreTracker.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
class ProcedureStoreTracker {
  private static final Logger LOG = LoggerFactory.getLogger(ProcedureStoreTracker.class);

```

### DeprecatedIsStillUsed
Deprecated member 'ZKSplitLog' is still used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public final class ZKSplitLog {
  private static final Logger LOG = LoggerFactory.getLogger(ZKSplitLog.class);

```

### DeprecatedIsStillUsed
Deprecated member 'hasPermission' is still used
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static boolean hasPermission(RpcController controller,
    AccessControlService.BlockingInterface protocol, TableName tableName, byte[] columnFamily,
    byte[] columnQualifier, String userName, Permission.Action[] actions) throws ServiceException {
```

### DeprecatedIsStillUsed
Deprecated member 'getChildDataAndWatchForNewChildren' is still used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static List<NodeAndData> getChildDataAndWatchForNewChildren(ZKWatcher zkw, String baseNode)
    throws KeeperException {
    return getChildDataAndWatchForNewChildren(zkw, baseNode, true);
```

### DeprecatedIsStillUsed
Deprecated member 'updateExistingNodeData' is still used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static void updateExistingNodeData(ZKWatcher zkw, String znode, byte[] data,
    int expectedVersion) throws KeeperException {
    try {
```

### DeprecatedIsStillUsed
Deprecated member 'getChildDataAndWatchForNewChildren' is still used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static List<NodeAndData> getChildDataAndWatchForNewChildren(ZKWatcher zkw, String baseNode,
    boolean throwOnInterrupt) throws KeeperException {
    List<String> nodes = ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);
```

### DeprecatedIsStillUsed
Deprecated member 'NodeAndData' is still used
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static class NodeAndData {
    private String node;
    private byte[] data;
```

### DeprecatedIsStillUsed
Deprecated member 'START' is still used
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java
   */
  @Deprecated
  public static final String START = "hbase.simpletotalorder.start";

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'END' is still used
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java
   */
  @Deprecated
  public static final String END = "hbase.simpletotalorder.end";

  static final String START_BASE64 = "hbase.simpletotalorder.start.base64";
```

### DeprecatedIsStillUsed
Deprecated member 'HBASE_RSGROUP_LOADBALANCER_CLASS' is still used
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java`
#### Snippet
```java
   */
  @Deprecated
  String HBASE_RSGROUP_LOADBALANCER_CLASS = "hbase.rsgroup.grouploadbalancer.class";

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'name' is still used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
     */
    @Deprecated
    private String name;
    /**
     * @see #addEndpoint(URI)
```

### DeprecatedIsStillUsed
Deprecated member 'bindAddress' is still used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
     */
    @Deprecated
    private String bindAddress;
    /**
     * @see #addEndpoint(URI)
```

### DeprecatedIsStillUsed
Deprecated member 'port' is still used
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
     */
    @Deprecated
    private int port = -1;

    /**
```

### DeprecatedIsStillUsed
Deprecated member 'SplitLogCounters' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogCounters.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class SplitLogCounters {
  private SplitLogCounters() {
  }
```

### DeprecatedIsStillUsed
Deprecated member 'SplitLogTask' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogTask.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class SplitLogTask {
  private final ServerName originServer;
  private final ZooKeeperProtos.SplitLogTask.State state;
```

### DeprecatedIsStillUsed
Deprecated member 'getHBase1CompatibleName' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java`
#### Snippet
```java
   */
  @Deprecated
  private String getHBase1CompatibleName(final String comparator) {
    if (comparator.equals(CellComparatorImpl.class.getName())) {
      return KeyValue.COMPARATOR.getClass().getName();
```

### DeprecatedIsStillUsed
Deprecated member 'getValueString' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileScanner.java`
#### Snippet
```java
   */
  @Deprecated
  String getValueString();

  /** Returns Reader that underlies this Scanner instance. */
```

### DeprecatedIsStillUsed
Deprecated member 'getKeyString' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileScanner.java`
#### Snippet
```java
   */
  @Deprecated
  String getKeyString();

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'SimpleServerRpcConnection' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
class SimpleServerRpcConnection extends ServerRpcConnection {

  final SocketChannel channel;
```

### DeprecatedIsStillUsed
Deprecated member 'COMPACTION' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALEdit.java`
#### Snippet
```java
  @Deprecated
  @InterfaceAudience.Private
  public static final byte[] COMPACTION = Bytes.toBytes("HBASE::COMPACTION");

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'REGION_REPLICA_REPLICATION_PEER' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ServerRegionReplicaUtil.java`
#### Snippet
```java
   */
  @Deprecated
  public static final String REGION_REPLICA_REPLICATION_PEER = "region_replica_replication";

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'HbckErrorReporter' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckErrorReporter.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public interface HbckErrorReporter {
  enum ERROR_CODE {
    UNKNOWN,
```

### DeprecatedIsStillUsed
Deprecated member 'splitLogManager' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
   */
  @Deprecated
  private final SplitLogManager splitLogManager;

  // Is the fileystem ok?
```

### DeprecatedIsStillUsed
Deprecated member 'PATTERN' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.java`
#### Snippet
```java
   */
  @Deprecated
  private static final Pattern PATTERN = Pattern.compile(".*pv2-\\d{20}.log");

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'MetaQueue' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MetaQueue.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
class MetaQueue extends Queue<TableName> {

  protected MetaQueue(LockStatus lockStatus) {
```

### DeprecatedIsStillUsed
Deprecated member 'MetaProcedureInterface' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MetaProcedureInterface.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public interface MetaProcedureInterface {

  enum MetaOperationType {
```

### DeprecatedIsStillUsed
Deprecated member 'RecoverMetaProcedure' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class RecoverMetaProcedure
  extends StateMachineProcedure<MasterProcedureEnv, MasterProcedureProtos.RecoverMetaState>
  implements MetaProcedureInterface {
```

### DeprecatedIsStillUsed
Deprecated member 'getMetaLock' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
   */
  @Deprecated
  LockAndQueue getMetaLock() {
    return metaLock;
  }
```

### DeprecatedIsStillUsed
Deprecated member 'MoveRegionProcedure' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MoveRegionProcedure.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class MoveRegionProcedure extends AbstractStateMachineRegionProcedure<MoveRegionState> {
  private RegionPlan plan;

```

### DeprecatedIsStillUsed
Deprecated member 'UnassignProcedure' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/UnassignProcedure.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class UnassignProcedure extends RegionTransitionProcedure {

  protected volatile ServerName hostingServer;
```

### DeprecatedIsStillUsed
Deprecated member 'AssignProcedure' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignProcedure.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class AssignProcedure extends RegionTransitionProcedure {

  private boolean forceNewPlan = false;
```

### DeprecatedIsStillUsed
Deprecated member 'MIN_REGION_COUNT_KEY' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java`
#### Snippet
```java
   */
  @Deprecated
  static final String MIN_REGION_COUNT_KEY = "hbase.normalizer.min.region.count";
  static final String MERGE_MIN_REGION_COUNT_KEY = "hbase.normalizer.merge.min.region.count";
  static final int DEFAULT_MERGE_MIN_REGION_COUNT = 3;
```

### DeprecatedIsStillUsed
Deprecated member 'RSGroupAdminClient' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class RSGroupAdminClient {
  private RSGroupAdminService.BlockingInterface stub;
  private Admin admin;
```

### DeprecatedIsStillUsed
Deprecated member 'RSGroupAdminEndpoint' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminEndpoint.java`
#### Snippet
```java
@CoreCoprocessor
@InterfaceAudience.Private
public class RSGroupAdminEndpoint implements MasterCoprocessor {
  // Only instance of RSGroupInfoManager. RSGroup aware load balancers ask for this instance on
  // their setup.
```

### DeprecatedIsStillUsed
Deprecated member 'postReplicateLogEntries' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default void postReplicateLogEntries(
    final ObserverContext<RegionServerCoprocessorEnvironment> ctx) throws IOException {
  }
```

### DeprecatedIsStillUsed
Deprecated member 'preReplicateLogEntries' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default void preReplicateLogEntries(final ObserverContext<RegionServerCoprocessorEnvironment> ctx)
    throws IOException {
  }
```

### DeprecatedIsStillUsed
Deprecated member 'HasMasterServices' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/HasMasterServices.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public interface HasMasterServices {
  /** Returns An instance of RegionServerServices, an object NOT for Coprocessor consumption. */
  MasterServices getMasterServices();
```

### DeprecatedIsStillUsed
Deprecated member 'createAndPrepare' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContextImpl.java`
#### Snippet
```java
  @Deprecated
  // TODO: Remove this method, ObserverContext should not depend on RpcServer
  public static <E extends CoprocessorEnvironment> ObserverContext<E> createAndPrepare(E env) {
    ObserverContextImpl<E> ctx = new ObserverContextImpl<>(RpcServer.getRequestUser().orElse(null));
    ctx.prepare(env);
```

### DeprecatedIsStillUsed
Deprecated member 'postWALWrite' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/WALObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default void postWALWrite(ObserverContext<? extends WALCoprocessorEnvironment> ctx,
    RegionInfo info, WALKey logKey, WALEdit logEdit) throws IOException {
  }
```

### DeprecatedIsStillUsed
Deprecated member 'preWALWrite' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/WALObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default void preWALWrite(ObserverContext<? extends WALCoprocessorEnvironment> ctx,
    RegionInfo info, WALKey logKey, WALEdit logEdit) throws IOException {
  }
```

### DeprecatedIsStillUsed
Deprecated member 'SplitLogWorkerCoordination' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/SplitLogWorkerCoordination.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public interface SplitLogWorkerCoordination {

  /**
```

### DeprecatedIsStillUsed
Deprecated member 'SplitLogManagerCoordination' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination.java`
#### Snippet
```java
@InterfaceAudience.Private
@Deprecated
public interface SplitLogManagerCoordination {
  /**
   * Detail class that shares data between coordination and split log manager
```

### DeprecatedIsStillUsed
Deprecated member 'ZkCoordinatedStateManager' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkCoordinatedStateManager.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
public class ZkCoordinatedStateManager implements CoordinatedStateManager {
  protected ZKWatcher watcher;
  protected SplitLogWorkerCoordination splitLogWorkerCoordination;
```

### DeprecatedIsStillUsed
Deprecated member 'KeyPrefixRegionSplitPolicy' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class KeyPrefixRegionSplitPolicy extends IncreasingToUpperBoundRegionSplitPolicy {
  private static final Logger LOG = LoggerFactory.getLogger(KeyPrefixRegionSplitPolicy.class);
  @Deprecated
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndPut' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndPut(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    byte[] family, byte[] qualifier, CompareOperator op, ByteArrayComparable comparator, Put put,
    boolean result) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'postMutationBeforeWAL' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default Cell postMutationBeforeWAL(ObserverContext<RegionCoprocessorEnvironment> ctx,
    MutationType opType, Mutation mutation, Cell oldCell, Cell newCell) throws IOException {
    return newCell;
```

### DeprecatedIsStillUsed
Deprecated member 'postCheckAndPut' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean postCheckAndPut(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    Filter filter, Put put, boolean result) throws IOException {
    return result;
```

### DeprecatedIsStillUsed
Deprecated member 'postCheckAndDelete' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean postCheckAndDelete(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    Filter filter, Delete delete, boolean result) throws IOException {
    return result;
```

### DeprecatedIsStillUsed
Deprecated member 'preStoreFileReaderOpen' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  // Passing InterfaceAudience.Private args FSDataInputStreamWrapper, CacheConfig and Reference.
  // This is fine as the hook is deprecated any way.
  default StoreFileReader preStoreFileReaderOpen(ObserverContext<RegionCoprocessorEnvironment> ctx,
    FileSystem fs, Path p, FSDataInputStreamWrapper in, long size, CacheConfig cacheConf,
    Reference r, StoreFileReader reader) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndPutAfterRowLock' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndPutAfterRowLock(ObserverContext<RegionCoprocessorEnvironment> c,
    byte[] row, Filter filter, Put put, boolean result) throws IOException {
    return result;
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndDelete' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndDelete(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    Filter filter, Delete delete, boolean result) throws IOException {
    return result;
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndPut' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndPut(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    Filter filter, Put put, boolean result) throws IOException {
    return result;
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndPutAfterRowLock' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndPutAfterRowLock(ObserverContext<RegionCoprocessorEnvironment> c,
    byte[] row, byte[] family, byte[] qualifier, CompareOperator op, ByteArrayComparable comparator,
    Put put, boolean result) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndDeleteAfterRowLock' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndDeleteAfterRowLock(ObserverContext<RegionCoprocessorEnvironment> c,
    byte[] row, byte[] family, byte[] qualifier, CompareOperator op, ByteArrayComparable comparator,
    Delete delete, boolean result) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndDeleteAfterRowLock' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndDeleteAfterRowLock(ObserverContext<RegionCoprocessorEnvironment> c,
    byte[] row, Filter filter, Delete delete, boolean result) throws IOException {
    return result;
```

### DeprecatedIsStillUsed
Deprecated member 'postCheckAndPut' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean postCheckAndPut(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    byte[] family, byte[] qualifier, CompareOperator op, ByteArrayComparable comparator, Put put,
    boolean result) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'postCheckAndDelete' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean postCheckAndDelete(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    byte[] family, byte[] qualifier, CompareOperator op, ByteArrayComparable comparator,
    Delete delete, boolean result) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'preCheckAndDelete' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   */
  @Deprecated
  default boolean preCheckAndDelete(ObserverContext<RegionCoprocessorEnvironment> c, byte[] row,
    byte[] family, byte[] qualifier, CompareOperator op, ByteArrayComparable comparator,
    Delete delete, boolean result) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'postStoreFileReaderOpen' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  // Passing InterfaceAudience.Private args FSDataInputStreamWrapper, CacheConfig and Reference.
  // This is fine as the hook is deprecated any way.
  default StoreFileReader postStoreFileReaderOpen(ObserverContext<RegionCoprocessorEnvironment> ctx,
    FileSystem fs, Path p, FSDataInputStreamWrapper in, long size, CacheConfig cacheConf,
    Reference r, StoreFileReader reader) throws IOException {
```

### DeprecatedIsStillUsed
Deprecated member 'DelimitedKeyPrefixRegionSplitPolicy' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitPolicy.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class DelimitedKeyPrefixRegionSplitPolicy extends IncreasingToUpperBoundRegionSplitPolicy {

  private static final Logger LOG =
```

### DeprecatedIsStillUsed
Deprecated member 'getFlushPressure' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java`
#### Snippet
```java
   */
  @Deprecated
  double getFlushPressure();

  /** Returns the metrics tracker for the region server */
```

### DeprecatedIsStillUsed
Deprecated member 'RowTooBigException' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RowTooBigException.java`
#### Snippet
```java
@edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "NM_SAME_SIMPLE_NAME_AS_SUPERCLASS",
    justification = "Temporary glue. To be removed")
public class RowTooBigException extends org.apache.hadoop.hbase.client.RowTooBigException {
  public RowTooBigException(String message) {
    super(message);
```

### DeprecatedIsStillUsed
Deprecated member 'SCANNER_ALREADY_CLOSED' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
  // which has already been exhausted.
  @Deprecated
  private static final IOException SCANNER_ALREADY_CLOSED = new IOException() {

    private static final long serialVersionUID = -4305297078988180130L;
```

### DeprecatedIsStillUsed
Deprecated member 'RS_HOSTNAME_DISABLE_MASTER_REVERSEDNS_KEY' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  @Deprecated
  @InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.CONFIG)
  final static String RS_HOSTNAME_DISABLE_MASTER_REVERSEDNS_KEY =
    "hbase.regionserver.hostname.disable.master.reversedns";

```

### DeprecatedIsStillUsed
Deprecated member 'readCompressed' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
   */
  @Deprecated
  static byte[] readCompressed(DataInput in, Dictionary dict) throws IOException {
    byte status = in.readByte();

```

### DeprecatedIsStillUsed
Deprecated member 'writeCompressed' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java`
#### Snippet
```java
   */
  @Deprecated
  static void writeCompressed(byte[] data, int offset, int length, DataOutput out, Dictionary dict)
    throws IOException {
    short dictIdx = Dictionary.NOT_IN_DICTIONARY;
```

### DeprecatedIsStillUsed
Deprecated member 'WALSplitterHandler' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/WALSplitterHandler.java`
#### Snippet
```java
@Deprecated
@InterfaceAudience.Private
public class WALSplitterHandler extends EventHandler {
  private static final Logger LOG = LoggerFactory.getLogger(WALSplitterHandler.class);
  private final ServerName serverName;
```

### DeprecatedIsStillUsed
Deprecated member 'postInstantiateDeleteTracker' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
   */
  @Deprecated
  public DeleteTracker postInstantiateDeleteTracker(DeleteTracker result) throws IOException {
    if (this.coprocEnvironments.isEmpty()) {
      return result;
```

### DeprecatedIsStillUsed
Deprecated member 'replayWALFlushStartMarker' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
   */
  @Deprecated
  PrepareFlushResult replayWALFlushStartMarker(FlushDescriptor flush) throws IOException {
    long flushSeqId = flush.getFlushSequenceNumber();

```

### DeprecatedIsStillUsed
Deprecated member 'replayWALFlushCommitMarker' is still used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "NN_NAKED_NOTIFY",
      justification = "Intentional; post memstore flush")
  void replayWALFlushCommitMarker(FlushDescriptor flush) throws IOException {
    MonitoredTask status = TaskMonitor.get().createStatus("Committing flush " + this);

```

### DeprecatedIsStillUsed
Deprecated member 'HBaseTestingUtility' is still used
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
@InterfaceAudience.Public
@Deprecated
public class HBaseTestingUtility extends HBaseZKTestingUtility {

  /**
```

## RuleId[id=Convert2MethodRef]
### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/metrics/impl/GlobalMetricRegistriesAdapter.java`
#### Snippet
```java
    this.registeredSources = new HashMap<>();
    this.helper = new DefaultMetricsSystemHelper();
    executor.getExecutor().scheduleAtFixedRate(() -> this.doRun(), 10, 10, TimeUnit.SECONDS);
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java
      Scan scan = new Scan();
      Map<byte[], Export.Response> result = Export.run(conf, tableName, scan, output);
      final long totalOutputRows = result.values().stream().mapToLong(v -> v.getRowCount()).sum();
      final long totalOutputCells = result.values().stream().mapToLong(v -> v.getCellCount()).sum();
      System.out.println("table:" + tableName);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java
      Map<byte[], Export.Response> result = Export.run(conf, tableName, scan, output);
      final long totalOutputRows = result.values().stream().mapToLong(v -> v.getRowCount()).sum();
      final long totalOutputCells = result.values().stream().mapToLong(v -> v.getCellCount()).sum();
      System.out.println("table:" + tableName);
      System.out.println("output:" + output);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/UserMetricsBuilder.java`
#### Snippet
```java
    @Override
    public long getWriteRequestCount() {
      return clientMetricsMap.values().stream().map(c -> c.getWriteRequestsCount()).reduce(0L,
        Long::sum);
    }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/UserMetricsBuilder.java`
#### Snippet
```java
    @Override
    public long getFilteredReadRequests() {
      return clientMetricsMap.values().stream().map(c -> c.getFilteredReadRequestsCount())
        .reduce(0L, Long::sum);
    }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/UserMetricsBuilder.java`
#### Snippet
```java
    @Override
    public long getReadRequestCount() {
      return clientMetricsMap.values().stream().map(c -> c.getReadRequestsCount()).reduce(0L,
        Long::sum);
    }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java`
#### Snippet
```java
      ? null
      : clone.getAttributesMap().entrySet().stream()
        .collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue(), (k, v) -> {
          throw new RuntimeException("collisions!!!");
        }, () -> new TreeMap<>()));
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java`
#### Snippet
```java
      ? null
      : clone.getAttributesMap().entrySet().stream()
        .collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue(), (k, v) -> {
          throw new RuntimeException("collisions!!!");
        }, () -> new TreeMap<>()));
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java`
#### Snippet
```java
        .collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue(), (k, v) -> {
          throw new RuntimeException("collisions!!!");
        }, () -> new TreeMap<>()));
    this.priority = clone.getPriority();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java`
#### Snippet
```java
              .setOverride(override).setRecursive(recursive).build());
        } catch (Throwable t) {
          LOG.error(pids.stream().map(i -> i.toString()).collect(Collectors.joining(", ")), t);
          throw t;
        }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
        // quota to send a candidate request.
        toSend = tableCache.getCandidate();
        toSend.ifPresent(r -> tableCache.send(r));
      }
      futureResultList.forEach(RegionLocationsFutureResult::complete);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java`
#### Snippet
```java
        // quota to send a candidate request.
        toSend = tableCache.getCandidate();
        toSend.ifPresent(r -> tableCache.send(r));
      }
      futureResultList.forEach(RegionLocationsFutureResult::complete);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
    this.ts = clone.getTimestamp();
    this.familyMap = clone.getFamilyCellMap().entrySet().stream()
      .collect(Collectors.toMap(e -> e.getKey(), e -> new ArrayList<>(e.getValue()), (k, v) -> {
        throw new RuntimeException("collisions!!!");
      }, () -> new TreeMap<>(Bytes.BYTES_COMPARATOR)));
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ZKConnectionRegistry.java`
#### Snippet
```java
      addListener(
        zk.list(znodePaths.baseZNode).thenApply(children -> children.stream()
          .filter(c -> this.znodePaths.isMetaZNodePrefix(c)).collect(Collectors.toList())),
        (metaReplicaZNodes, error) -> {
          if (error != null) {
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTable.java`
#### Snippet
```java
   */
  default CompletableFuture<Boolean> exists(Get get) {
    return get(toCheckExistenceOnly(get)).thenApply(r -> r.getExists());
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTable.java`
#### Snippet
```java
  default List<CompletableFuture<Boolean>> exists(List<Get> gets) {
    return get(toCheckExistenceOnly(gets)).stream()
      .<CompletableFuture<Boolean>> map(f -> f.thenApply(r -> r.getExists())).collect(toList());
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
      if (operations != null) {
        remoteDispatch(getKey(), operations);
        operations.stream().filter(operation -> operation.storeInDispatchedQueue())
          .forEach(operation -> dispatchedOperations.add(operation));
        this.operations = null;
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
        remoteDispatch(getKey(), operations);
        operations.stream().filter(operation -> operation.storeInDispatchedQueue())
          .forEach(operation -> dispatchedOperations.add(operation));
        this.operations = null;
      }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
  public void addNode(final TRemote key) {
    assert key != null : "Tried to add a node with a null key";
    nodeMap.computeIfAbsent(key, k -> new BufferNode(k));
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-metrics/src/main/java/org/apache/hadoop/hbase/metrics/impl/FastLongHistogram.java`
#### Snippet
```java

    long getNumAtOrBelow(long val) {
      return Arrays.stream(counts).mapToLong(c -> c.sum()).limit(getIndex(val) + 1).sum();
    }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
              "multi exception: {}; running operations sequentially "
                + "(runSequentialOnMultiFailure=true); {}",
              ke.toString(), ops.stream().map(o -> o.toString()).collect(Collectors.joining(",")));
            processSequentially(zkw, ops);
            break;
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java`
#### Snippet
```java
    if (isByTable) {
      loadOfAllTable.forEach((tableName, loadOfOneTable) -> {
        updateBalancerTableLoadInfo(tableName, loadOfOneTable);
      });
    } else {
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodesPlan.java`
#### Snippet
```java
    // Make a deep copy so changes don't harm our copy of favoredNodesMap.
    return this.favoredNodesMap.entrySet().stream()
      .collect(Collectors.toMap(k -> k.getKey(), v -> new ArrayList<ServerName>(v.getValue())));
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaRegionLocationCache.java`
#### Snippet
```java
    // Explicitly iterate instead of new ArrayList<>(snapshot.values()) because the underlying
    // ArrayValueCollection does not implement toArray().
    snapshot.values().forEach(location -> result.add(location));
    return result;
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyServerRpcConnection.java`
#### Snippet
```java
      return;
    }
    this.callCleanup = () -> buf.release();
    ByteBuff byteBuff = new SingleByteBuff(buf.nioBuffer());
    try {
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          ListTableDescriptorsByStateRequest.newBuilder().setIsEnabled(isEnabled).build(),
          (s, c, req, done) -> s.listTableDescriptorsByState(c, req, done),
          (resp) -> ProtobufUtil.toTableDescriptorList(resp)))
      .call();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          ListTableDescriptorsByNamespaceRequest.newBuilder().setNamespaceName(name).build(),
          (s, c, req, done) -> s.listTableDescriptorsByNamespace(c, req, done),
          (resp) -> ProtobufUtil.toTableDescriptorList(resp)))
      .call();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        List<TableDescriptor>> call(controller, stub, request,
          (s, c, req, done) -> s.getTableDescriptors(c, req, done),
          (resp) -> ProtobufUtil.toTableDescriptorList(resp)))
      .call();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          RequestConverter.buildGetOnlineRegionRequest(),
          (s, c, req, done) -> s.getOnlineRegion(c, req, done),
          resp -> ProtobufUtil.getRegionInfos(resp)))
      .serverName(serverName).call();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          ListNamespaceDescriptorsRequest.newBuilder().build(),
          (s, c, req, done) -> s.listNamespaceDescriptors(c, req, done),
          (resp) -> ProtobufUtil.toNamespaceDescriptorList(resp)))
      .call();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        BalanceResponse> call(controller, stub, ProtobufUtil.toBalanceRequest(request),
          (s, c, req, done) -> s.balance(c, req, done),
          (resp) -> ProtobufUtil.toBalanceResponse(resp)))
      .call();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
        locations.stream().filter(l -> l.getRegion() != null)
          .filter(l -> !l.getRegion().isOffline()).filter(l -> l.getServerName() != null)
          .collect(Collectors.groupingBy(l -> l.getServerName(),
            Collectors.mapping(l -> l.getRegion(), Collectors.toList())));
      List<CompletableFuture<CacheEvictionStats>> futures = new ArrayList<>();
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java`
#### Snippet
```java
          .filter(l -> !l.getRegion().isOffline()).filter(l -> l.getServerName() != null)
          .collect(Collectors.groupingBy(l -> l.getServerName(),
            Collectors.mapping(l -> l.getRegion(), Collectors.toList())));
      List<CompletableFuture<CacheEvictionStats>> futures = new ArrayList<>();
      CacheEvictionStatsAggregator aggregator = new CacheEvictionStatsAggregator();
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALProvider.java`
#### Snippet
```java
  default WALFileLengthProvider getWALFileLengthProvider() {
    return path -> getWALs().stream().map(w -> w.getLogFileSizeIfBeingWritten(path))
      .filter(o -> o.isPresent()).findAny().orElse(OptionalLong.empty());
  }
}
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
              // collecting files, MOB included currently being written
              regionMobs.addAll(store.getStoreFilesBeingWritten().stream()
                .map(path -> path.getName()).collect(Collectors.toList()));

              referencedMOBs
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
        if (LOG.isDebugEnabled()) {
          LOG.debug("Found: {} active mob refs for table={}",
            referencedMOBs.values().stream().map(inner -> inner.values())
              .flatMap(lists -> lists.stream()).mapToInt(lists -> lists.size()).sum(),
            htd.getTableName().getNameAsString());
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
          LOG.debug("Found: {} active mob refs for table={}",
            referencedMOBs.values().stream().map(inner -> inner.values())
              .flatMap(lists -> lists.stream()).mapToInt(lists -> lists.size()).sum(),
            htd.getTableName().getNameAsString());
        }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
          LOG.debug("Found: {} active mob refs for table={}",
            referencedMOBs.values().stream().map(inner -> inner.values())
              .flatMap(lists -> lists.stream()).mapToInt(lists -> lists.size()).sum(),
            htd.getTableName().getNameAsString());
        }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java`
#### Snippet
```java
        try (Stream<Path> stream = Files.list(jarPath)) {
          List<Path> files =
            stream.filter((path) -> Files.isRegularFile(path)).collect(Collectors.toList());

          for (Path file : files) {
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        .filter(
          peerConfig -> peerConfig.getPeerConfig().needToReplicate(cleanReplicationBarrierTable))
        .map(peerConfig -> peerConfig.getPeerId()).collect(Collectors.toList());
      try {
        List<String> batch = new ArrayList<>();
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServicesVersionWrapper.java`
#### Snippet
```java
  public MasterRpcServicesVersionWrapper(MasterRpcServices masterRpcServices) {
    this.masterRpcServices = masterRpcServices;
    this.startupCall = (c, req) -> masterRpcServices.regionServerStartup(c, req);
    this.reportCall = (c, req) -> masterRpcServices.regionServerReport(c, req);
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServicesVersionWrapper.java`
#### Snippet
```java
    this.masterRpcServices = masterRpcServices;
    this.startupCall = (c, req) -> masterRpcServices.regionServerStartup(c, req);
    this.reportCall = (c, req) -> masterRpcServices.regionServerReport(c, req);
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitWALManager.java`
#### Snippet
```java
      FileStatus[] files = CommonFSUtils.listStatus(fs, splitDir);
      LOG.warn("PathIsNotEmptyDirectoryException {}",
        Arrays.stream(files).map(f -> f.getPath()).collect(Collectors.toList()));
      throw e;
    }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
        // but these deleted regions are not added back to RegionStates,
        // so we can safely remove deleted regions.
        removeDeletedRegionFromLoadedFlushedSequenceIds();
      }, "RemoveDeletedRegionSyncThread").start();
      int flushPeriod =
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
      }
      return serverName;
    }).filter(s -> s != null).collect(Collectors.toSet());
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java`
#### Snippet
```java
    List<Thread> oldWALsCleaner = new ArrayList<>(size);
    for (int i = 0; i < size; i++) {
      Thread cleaner = new Thread(() -> deleteFile());
      cleaner.setName("OldWALsCleaner-" + i);
      cleaner.setDaemon(true);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/migrate/RollingUpgradeChore.java`
#### Snippet
```java
        TableDescriptor td = entry.getValue();
        return StringUtils.isEmpty(td.getValue(StoreFileTrackerFactory.TRACKER_IMPL));
      }).limit(concurrentCount).collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue()));
    } catch (IOException e) {
      LOG.warn("Failed to migrate StoreFileTracker", e);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/migrate/RollingUpgradeChore.java`
#### Snippet
```java
        TableDescriptor td = entry.getValue();
        return StringUtils.isEmpty(td.getValue(StoreFileTrackerFactory.TRACKER_IMPL));
      }).limit(concurrentCount).collect(Collectors.toMap(e -> e.getKey(), e -> e.getValue()));
    } catch (IOException e) {
      LOG.warn("Failed to migrate StoreFileTracker", e);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    createProcedureExecutor();
    Map<Class<?>, List<Procedure<MasterProcedureEnv>>> procsByType = procedureExecutor
      .getActiveProceduresNoCopy().stream().collect(Collectors.groupingBy(p -> p.getClass()));

    // Create Assignment Manager
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    this.regionServerTracker.upgrade(
      procsByType.getOrDefault(ServerCrashProcedure.class, Collections.emptyList()).stream()
        .map(p -> (ServerCrashProcedure) p).map(p -> p.getServerName()).collect(Collectors.toSet()),
      Sets.union(rsListStorage.getAll(), walManager.getLiveServersFromWALDir()),
      walManager.getSplittingServersFromWALDir());
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java`
#### Snippet
```java
      case REOPEN_TABLE_REGIONS_CONFIRM_REOPENED:
        regions = regions.stream().map(env.getAssignmentManager().getRegionStates()::checkReopened)
          .filter(l -> l != null).collect(Collectors.toList());
        if (regions.isEmpty()) {
          return Flow.NO_MORE_STATE;
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
  List<LockedResource> getLocks() {
    List<LockedResource> lockedResources = new ArrayList<>();
    addToLockedResources(lockedResources, serverLocks, sn -> sn.getServerName(),
      LockedResourceType.SERVER);
    addToLockedResources(lockedResources, namespaceLocks, Function.identity(),
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
    addToLockedResources(lockedResources, namespaceLocks, Function.identity(),
      LockedResourceType.NAMESPACE);
    addToLockedResources(lockedResources, tableLocks, tn -> tn.getNameAsString(),
      LockedResourceType.TABLE);
    addToLockedResources(lockedResources, regionLocks, Function.identity(),
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SchemaLocking.java`
#### Snippet
```java
    addToLockedResources(lockedResources, peerLocks, Function.identity(), LockedResourceType.PEER);
    addToLockedResources(lockedResources, ImmutableMap.of(TableName.META_TABLE_NAME, metaLock),
      tn -> tn.getNameAsString(), LockedResourceType.META);
    return lockedResources;
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.java`
#### Snippet
```java
  private Procedure<MasterProcedureEnv>[] createRemoteSnapshotProcedures(MasterProcedureEnv env) {
    return env.getAssignmentManager().getTableRegions(snapshotTable, true).stream()
      .filter(r -> RegionReplicaUtil.isDefaultReplica(r))
      .map(r -> new SnapshotRegionProcedure(snapshot, r)).toArray(SnapshotRegionProcedure[]::new);
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.java`
#### Snippet
```java
  private Stream<RegionInfo> getDefaultRegionReplica(MasterProcedureEnv env) {
    return env.getAssignmentManager().getTableRegions(snapshotTable, false).stream()
      .filter(r -> RegionReplicaUtil.isDefaultReplica(r));
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.java`
#### Snippet
```java
      env.getMasterConfiguration().getInt("hbase.snapshot.remote.verify.threshold", 10000);
    List<RegionInfo> regions = env.getAssignmentManager().getTableRegions(snapshotTable, false)
      .stream().filter(r -> RegionReplicaUtil.isDefaultReplica(r)).collect(Collectors.toList());
    int numRegions = regions.size();

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.java`
#### Snippet
```java
          }
          return proc;
        }).filter(p -> p != null).toArray(TransitRegionStateProcedure[]::new);
    if (regionReplication == DEFAULT_REGION_REPLICA) {
      // this is the default case
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  /** Returns Return the regions of the table and filter them. */
  private List<RegionInfo> getRegionsOfTable(TableName table, Predicate<RegionStateNode> filter) {
    return getTableRegionStateNodes(table).stream().filter(filter).map(n -> n.getRegionInfo())
      .collect(Collectors.toList());
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
  public List<HRegionLocation> getRegionsOfTableForReopen(TableName tableName) {
    return getTableRegionStateNodes(tableName).stream().map(this::createRegionForReopen)
      .filter(r -> r != null).collect(Collectors.toList());
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
   */
  public ServerStateNode getOrCreateServer(final ServerName serverName) {
    return serverMap.computeIfAbsent(serverName, key -> new ServerStateNode(key));
  }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java`
#### Snippet
```java
  public List<String> getSerialPeerIdsBelongsTo(TableName tableName) {
    return peers.values().stream().filter(p -> p.getPeerConfig().isSerial())
      .filter(p -> p.getPeerConfig().needToReplicate(tableName)).map(p -> p.getPeerId())
      .collect(Collectors.toList());
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java`
#### Snippet
```java

    // Compute the total size of all snapshots against our table
    final long totalSnapshotSize = snapshotSizes.stream().mapToLong((sws) -> sws.getSize()).sum();

    writeLock.lock();
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
   */
  public TransitRegionStateProcedure[] createAssignProcedures(List<RegionInfo> hris) {
    return hris.stream().map(hri -> regionStates.getOrCreateRegionStateNode(hri))
      .map(regionNode -> createAssignProcedure(regionNode, null)).sorted(AssignmentManager::compare)
      .toArray(TransitRegionStateProcedure[]::new);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    return regionStates.getTableRegionStateNodes(tableName).stream()
      .filter(regionNode -> regionNode.getRegionInfo().getReplicaId() >= newReplicaCount)
      .map(this::forceCreateUnssignProcedure).filter(p -> p != null)
      .toArray(TransitRegionStateProcedure[]::new);
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
            rsn.unlock();
          }
        }).filter(p -> p != null).toArray(TransitRegionStateProcedure[]::new);
    if (procs.length > 0) {
      master.getMasterProcedureExecutor().submitProcedures(procs);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
  public TransitRegionStateProcedure[] createUnassignProceduresForDisabling(TableName tableName) {
    return regionStates.getTableRegionStateNodes(tableName).stream()
      .map(this::forceCreateUnssignProcedure).filter(p -> p != null)
      .toArray(TransitRegionStateProcedure[]::new);
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
    createAssignProcedures(Map<ServerName, List<RegionInfo>> assignments) {
    return assignments.entrySet().stream()
      .flatMap(e -> e.getValue().stream().map(hri -> regionStates.getOrCreateRegionStateNode(hri))
        .map(regionNode -> createAssignProcedure(regionNode, e.getKey())))
      .sorted(AssignmentManager::compare).toArray(TransitRegionStateProcedure[]::new);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    String targetGroupName, String sourceGroupName) throws IOException {
    moveRegionsBetweenGroups(movedServers, srcGrpServers, targetGroupName, sourceGroupName,
      rs -> getRegions(rs), info -> {
        try {
          String groupName = RSGroupUtil.getRSGroupInfo(masterServices, this, info.getTable())
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java`
#### Snippet
```java
    RSGroupInfo rsGroupInfo = getRSGroupInfo(groupName);
    rsGroupInfo.getConfiguration().forEach((k, v) -> rsGroupInfo.removeConfiguration(k));
    configuration.forEach((k, v) -> rsGroupInfo.setConfiguration(k, v));
    flushConfig();
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.java`
#### Snippet
```java
      }
    }
    LongAccumulator totalToDecrement = new LongAccumulator((a, b) -> a + b, 0);
    entryReader.entryBatchQueue.forEach(w -> {
      entryReader.entryBatchQueue.remove(w);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALActionListener.java`
#### Snippet
```java
    // For replay, or if all the cells are markers, do not need to store replication scope.
    if (
      logEdit.isReplay() || logEdit.getCells().stream().allMatch(c -> WALEdit.isMetaEditFamily(c))
    ) {
      ((WALKeyImpl) logKey).clearReplicationScope();
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java`
#### Snippet
```java
        LOG.info("Found [--distributed], will poll each RegionServer.");
        Set<String> peerIds =
          peers.stream().map((peer) -> peer.getPeerId()).collect(Collectors.toSet());
        System.out.println(dumpQueues(zkw, peerIds, opts.isHdfs()));
        System.out.println(dumpReplicationSummary());
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
    this.storeFileWriterCreationTrackerFactory = storeEngine.requireWritingToTmpDirFirst()
      ? () -> null
      : () -> new StoreFileWriterCreationTracker();
    refreshStoreSizeAndTotalBytes();

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.java`
#### Snippet
```java

    this.refCnt = RefCnt.create(() -> {
      recycleChunks();
    });

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      // order is preserved as its expected from the client
      if (!atomic) {
        Arrays.sort(mArray, (v1, v2) -> Row.COMPARATOR.compare(v1, v2));
      }

```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java`
#### Snippet
```java
    private HFileContext fileContext;
    private boolean shouldDropCacheBehind;
    private Supplier<Collection<HStoreFile>> compactedFilesSupplier = () -> Collections.emptySet();
    private String fileStoragePolicy;
    // this is used to track the creation of the StoreFileWriter, mainly used for the SFT
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ImmutableMemStoreLAB.java`
#### Snippet
```java
    this.mslabs = mslabs;
    this.refCnt = RefCnt.create(() -> {
      closeMSLABs();
    });
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        } else {
          LOG.debug("Waiting on {}", this.regionsInTransitionInRS.keySet().stream()
            .map(e -> Bytes.toString(e)).collect(Collectors.joining(", ")));
        }
        if (sleepInterrupted(200)) {
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
    TableDescriptor htd) {
    return htd.getCoprocessorDescriptors().stream().map(cp -> {
      Path path = cp.getJarPath().map(p -> new Path(p)).orElse(null);
      Configuration ourConf;
      if (!cp.getProperties().isEmpty()) {
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java`
#### Snippet
```java
        ourConf = new Configuration(false);
        HBaseConfiguration.merge(ourConf, conf);
        cp.getProperties().forEach((k, v) -> ourConf.set(k, v));
      } else {
        ourConf = conf;
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFile.java`
#### Snippet
```java
    NavigableMap<Long, List<Path>> toDelete =
      loadedSeqId >= 0 ? seqId2TrackFiles.tailMap(loadedSeqId, false) : seqId2TrackFiles;
    toDelete.values().stream().flatMap(l -> l.stream()).forEach(file -> {
      ForkJoinPool.commonPool().execute(() -> {
        LOG.info("Deleting track file {}", file);
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
  private String[] disjoin(String[] existingTables, String[] toRemove) {
    Set<String> tables = new HashSet<>(Arrays.asList(existingTables));
    Arrays.asList(toRemove).forEach(table -> tables.remove(table));
    return tables.toArray(new String[0]);
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      res.advance();
      String[] tables = cellValueToBackupSet(res.current());
      return Arrays.asList(tables).stream().map(item -> TableName.valueOf(item))
        .collect(Collectors.toList());
    }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java

  public HRegionServer getRegionServer(ServerName serverName) {
    return hbaseCluster.getRegionServers().stream().map(t -> t.getRegionServer())
      .filter(r -> r.getServerName().equals(serverName)).findFirst().orElse(null);
  }
```

### Convert2MethodRef
Lambda can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private HStore getStore(Cell cell) {
    return stores.entrySet().stream().filter(e -> CellUtil.matchingFamily(cell, e.getKey()))
      .map(e -> e.getValue()).findFirst().orElse(null);
  }

```

## RuleId[id=ComparatorCombinators]
### ComparatorCombinators
Can be replaced with 'Comparator.comparingInt'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBatchRpcRetryingCaller.java`
#### Snippet
```java
      RequestConverter.buildNoDataRegionActions(entry.getKey(),
        entry.getValue().actions.stream()
          .sorted((a1, a2) -> Integer.compare(a1.getOriginalIndex(), a2.getOriginalIndex()))
          .collect(Collectors.toList()),
        cells, multiRequestBuilder, regionActionBuilder, actionBuilder, mutationBuilder, nonceGroup,
```

### ComparatorCombinators
Can be replaced with 'Comparator' chain
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
  transient final IdReadWriteLock<Long> offsetLock;

  private final NavigableSet<BlockCacheKey> blocksByHFile = new ConcurrentSkipListSet<>((a, b) -> {
    int nameComparison = a.getHfileName().compareTo(b.getHfileName());
    if (nameComparison != 0) {
      return nameComparison;
    }
    return Long.compare(a.getOffset(), b.getOffset());
  });

  /** Statistics thread schedule pool (for heavy debugging, could remove) */
```

### ComparatorCombinators
Can be replaced with 'Comparator.comparingInt'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/compaction/ClusterCompactionQueues.java`
#### Snippet
```java
      return compactionQueues.entrySet().stream()
        .filter(entry -> !compactingServers.contains(entry.getKey()))
        .max(Map.Entry.comparingByValue((o1, o2) -> Integer.compare(o1.size(), o2.size())))
        .map(Map.Entry::getKey);
    } finally {
```

### ComparatorCombinators
Can be replaced with 'Comparator.comparing'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/DeadServer.java`
#### Snippet
```java
    }

    Collections.sort(res, (o1, o2) -> o1.getSecond().compareTo(o2.getSecond()));
    return res;
  }
```

### ComparatorCombinators
Can be replaced with 'Comparator.comparingLong'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java`
#### Snippet
```java
  static Optional<HStoreFile> getLargestFile(Collection<HStoreFile> candidates) {
    return candidates.stream().filter(f -> f.getReader() != null)
      .max((f1, f2) -> Long.compare(f1.getReader().length(), f2.getReader().length()));
  }

```

### ComparatorCombinators
Can be replaced with 'Comparator.reverseOrder'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFile.java`
#### Snippet
```java
      return Collections.emptyNavigableMap();
    }
    TreeMap<Long, List<Path>> map = new TreeMap<>((l1, l2) -> l2.compareTo(l1));
    for (FileStatus status : statuses) {
      Path file = status.getPath();
```

## RuleId[id=RedundantCollectionOperation]
### RedundantCollectionOperation
Unnecessary 'containsKey()' check
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java
    // After master restarts, merged regions' RIT state may not be cleaned,
    // making sure they are cleaned here
    if (regionInTransition.containsKey(regionInfo)) {
      regionInTransition.remove(regionInfo);
    }
```

## RuleId[id=AbstractMethodCallInConstructor]
### AbstractMethodCallInConstructor
Call to 'abstract' method `getBootstrapNodes()` during object construction
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AbstractRpcBasedConnectionRegistry.java`
#### Snippet
```java
    rpcClient = RpcClientFactory.createClient(conf, null);
    rpcControllerFactory = RpcControllerFactory.instantiate(conf);
    populateStubs(getBootstrapNodes(conf));
    // could return null here is refresh interval is less than zero
    registryEndpointRefresher =
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `createNamedQueueRecord()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
      this.msgInterval = conf.getInt("hbase.regionserver.msginterval", 3 * 1000);
      this.sleeper = new Sleeper(this.msgInterval, this);
      this.namedQueueRecorder = createNamedQueueRecord();
      this.rpcServices = createRpcServices();
      useThisHostnameInstead = getUseThisHostnameInstead(conf);
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `createRpcServices()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
      this.sleeper = new Sleeper(this.msgInterval, this);
      this.namedQueueRecorder = createNamedQueueRecord();
      this.rpcServices = createRpcServices();
      useThisHostnameInstead = getUseThisHostnameInstead(conf);
      InetSocketAddress addr = rpcServices.getSocketAddress();
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getUseThisHostnameInstead()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
      this.namedQueueRecorder = createNamedQueueRecord();
      this.rpcServices = createRpcServices();
      useThisHostnameInstead = getUseThisHostnameInstead(conf);
      InetSocketAddress addr = rpcServices.getSocketAddress();

```

### AbstractMethodCallInConstructor
Call to 'abstract' method `login()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
        HConstants.ZK_CLIENT_KERBEROS_PRINCIPAL, hostName);
      // login the server principal (if using secure Hadoop)
      login(userProvider, hostName);
      // init superusers and add the server principal (if using security)
      // or process owner as default super user.
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getProcessName()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
      Superusers.initialize(conf);
      zooKeeper =
        new ZKWatcher(conf, getProcessName() + ":" + addr.getPort(), this, canCreateBaseZNode());

      this.configurationManager = new ConfigurationManager();
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `canCreateBaseZNode()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
      Superusers.initialize(conf);
      zooKeeper =
        new ZKWatcher(conf, getProcessName() + ":" + addr.getPort(), this, canCreateBaseZNode());

      this.configurationManager = new ConfigurationManager();
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `clusterMode()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseServerBase.java`
#### Snippet
```java
      this.metaRegionLocationCache = new MetaRegionLocationCache(zooKeeper);

      if (clusterMode()) {
        if (
          conf.getBoolean(HBASE_SPLIT_WAL_COORDINATED_BY_ZK, DEFAULT_HBASE_SPLIT_COORDINATED_BY_ZK)
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getRpcSchedulerFactoryClass()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
    final RpcSchedulerFactory rpcSchedulerFactory;
    try {
      rpcSchedulerFactory = getRpcSchedulerFactoryClass(conf).asSubclass(RpcSchedulerFactory.class)
        .getDeclaredConstructor().newInstance();
    } catch (NoSuchMethodException | InvocationTargetException | InstantiationException
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getDNSServerType()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
      throw new IllegalArgumentException(e);
    }
    String hostname = DNS.getHostname(conf, getDNSServerType());
    int port = conf.getInt(getPortConfigName(), getDefaultPort());
    // Creation of a HSA will force a resolve.
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getPortConfigName()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
    }
    String hostname = DNS.getHostname(conf, getDNSServerType());
    int port = conf.getInt(getPortConfigName(), getDefaultPort());
    // Creation of a HSA will force a resolve.
    final InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getDefaultPort()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
    }
    String hostname = DNS.getHostname(conf, getDNSServerType());
    int port = conf.getInt(getPortConfigName(), getDefaultPort());
    // Creation of a HSA will force a resolve.
    final InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getHostname()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
    // Creation of a HSA will force a resolve.
    final InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
    final InetSocketAddress bindAddress = new InetSocketAddress(getHostname(conf, hostname), port);
    if (initialIsa.getAddress() == null) {
      throw new IllegalArgumentException("Failed resolve of " + initialIsa);
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `createPriority()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
      throw new IllegalArgumentException("Failed resolve of " + initialIsa);
    }
    priority = createPriority();
    // Using Address means we don't get the IP too. Shorten it more even to just the host name
    // w/o the domain.
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `defaultReservoirEnabled()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
    ConnectionUtils.setServerSideHConnectionRetriesConfig(conf, name, LOG);
    boolean reservoirEnabled =
      conf.getBoolean(ByteBuffAllocator.ALLOCATOR_POOL_ENABLED_KEY, defaultReservoirEnabled());
    try {
      // use final bindAddress for this server.
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getServices()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
    try {
      // use final bindAddress for this server.
      rpcServer = RpcServerFactory.createRpcServer(server, name, getServices(), bindAddress, conf,
        rpcSchedulerFactory.create(conf, this, server), reservoirEnabled);
    } catch (BindException be) {
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `getPortConfigName()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HBaseRpcServicesBase.java`
#### Snippet
```java
        rpcSchedulerFactory.create(conf, this, server), reservoirEnabled);
    } catch (BindException be) {
      throw new IOException(be.getMessage() + ". To switch ports use the '" + getPortConfigName()
        + "' configuration property.", be.getCause() != null ? be.getCause() : be);
    }
```

### AbstractMethodCallInConstructor
Call to 'abstract' method `parseFrom()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/BooleanStateStore.java`
#### Snippet
```java
    super(masterRegion, stateName, watcher, zkPath);
    byte[] state = getState();
    this.on = state == null || parseFrom(state);
  }

```

## RuleId[id=NonSerializableFieldInSerializableClass]
### NonSerializableFieldInSerializableClass
Non-serializable field 'conf' in a Serializable class
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
    };

    private final Configuration conf;
    private final String compressName;
    private final String confKey;
```

### NonSerializableFieldInSerializableClass
Non-serializable field 'lock' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/CompatibilitySingletonFactory.java`
#### Snippet
```java
    INSTANCE;

    private final Object lock = new Object();
    private final Map<Class, Object> instances = new HashMap<>();
  }
```

### NonSerializableFieldInSerializableClass
Non-serializable field 'thriftOne' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/thrift/MetricsThriftServerSourceFactoryImpl.java`
#### Snippet
```java
    INSTANCE;

    private MetricsThriftServerSourceImpl thriftOne;
    private MetricsThriftServerSourceImpl thriftTwo;
  }
```

### NonSerializableFieldInSerializableClass
Non-serializable field 'thriftTwo' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/thrift/MetricsThriftServerSourceFactoryImpl.java`
#### Snippet
```java

    private MetricsThriftServerSourceImpl thriftOne;
    private MetricsThriftServerSourceImpl thriftTwo;
  }

```

### NonSerializableFieldInSerializableClass
Non-serializable field 'source' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceFactoryImpl.java`
#### Snippet
```java
    INSTANCE;

    private final MetricsReplicationSourceImpl source = new MetricsReplicationSourceImpl();
  }

```

### NonSerializableFieldInSerializableClass
Non-serializable field 'tblAggImpl' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java
    private MetricsRegionAggregateSourceImpl regionAggImpl;
    private MetricsUserAggregateSourceImpl userAggImpl;
    private MetricsTableAggregateSourceImpl tblAggImpl;
    private MetricsHeapMemoryManagerSourceImpl heapMemMngImpl;
  }
```

### NonSerializableFieldInSerializableClass
Non-serializable field 'aggLock' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java
    INSTANCE;

    private final Object aggLock = new Object();
    private MetricsRegionAggregateSourceImpl regionAggImpl;
    private MetricsUserAggregateSourceImpl userAggImpl;
```

### NonSerializableFieldInSerializableClass
Non-serializable field 'userAggImpl' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java
    private final Object aggLock = new Object();
    private MetricsRegionAggregateSourceImpl regionAggImpl;
    private MetricsUserAggregateSourceImpl userAggImpl;
    private MetricsTableAggregateSourceImpl tblAggImpl;
    private MetricsHeapMemoryManagerSourceImpl heapMemMngImpl;
```

### NonSerializableFieldInSerializableClass
Non-serializable field 'heapMemMngImpl' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java
    private MetricsUserAggregateSourceImpl userAggImpl;
    private MetricsTableAggregateSourceImpl tblAggImpl;
    private MetricsHeapMemoryManagerSourceImpl heapMemMngImpl;
  }

```

### NonSerializableFieldInSerializableClass
Non-serializable field 'regionAggImpl' in a Serializable class
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java

    private final Object aggLock = new Object();
    private MetricsRegionAggregateSourceImpl regionAggImpl;
    private MetricsUserAggregateSourceImpl userAggImpl;
    private MetricsTableAggregateSourceImpl tblAggImpl;
```

## RuleId[id=CatchMayIgnoreException]
### CatchMayIgnoreException
Empty `catch` block
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java`
#### Snippet
```java
    try {
      accessDeniedClazz = Class.forName("org.apache.hadoop.hbase.security.AccessDeniedException");
    } catch (ClassNotFoundException e) {
    }
  }
```

### CatchMayIgnoreException
Empty `catch` block
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
    try {
      zkw.getRecoverableZooKeeper().delete(delete.getPath(), delete.getVersion());
    } catch (KeeperException.NoNodeException nne) {
    } catch (InterruptedException ie) {
      zkw.interruptedException(ie);
```

### CatchMayIgnoreException
Empty `catch` block
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputHelper.java`
#### Snippet
```java
    try {
      Thread.sleep(ConnectionUtils.getPauseTime(100, retry));
    } catch (InterruptedException e) {
    }
  }
```

### CatchMayIgnoreException
'catch' parameter named `ignored` is used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.java`
#### Snippet
```java
    try {
      socket.shutdownOutput();
    } catch (Exception ignored) {
      if (SimpleRpcServer.LOG.isTraceEnabled()) {
        SimpleRpcServer.LOG.trace("Ignored exception", ignored);
```

### CatchMayIgnoreException
'catch' parameter named `ignored` is used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.java`
#### Snippet
```java
    try {
      socket.close();
    } catch (Exception ignored) {
      if (SimpleRpcServer.LOG.isTraceEnabled()) {
        SimpleRpcServer.LOG.trace("Ignored exception", ignored);
```

### CatchMayIgnoreException
'catch' parameter named `ignored` is used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
                if (key.isAcceptable()) doAccept(key);
              }
            } catch (IOException ignored) {
              if (LOG.isTraceEnabled()) LOG.trace("ignored", ignored);
            }
```

### CatchMayIgnoreException
'catch' parameter named `ignored` is used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
          acceptChannel.close();
          selector.close();
        } catch (IOException ignored) {
          if (LOG.isTraceEnabled()) LOG.trace("ignored", ignored);
        }
```

### CatchMayIgnoreException
Empty `catch` block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
        try {
          Thread.sleep(retryInterval);
        } catch (InterruptedException e) {
        }
      }
```

### CatchMayIgnoreException
'catch' parameter named `ignored` is used
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java`
#### Snippet
```java
      testCompression(a);
      return true;
    } catch (IOException ignored) {
      LOG.warn("Can't instantiate codec: " + codec, ignored);
      return false;
```

### CatchMayIgnoreException
Empty `catch` block
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.java`
#### Snippet
```java
        isFile =
          isFile && !(Boolean) Methods.call(FileStatus.class, status, "isSymlink", null, null);
      } catch (Exception e) {
      }
      return isFile;
```

## RuleId[id=MismatchedJavadocCode]
### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Hbck.java`
#### Snippet
```java
   *                  parent procedure (Dangerous but useful in case where child procedure has been
   *                  'lost'). Does not always work. Experimental.
   * @return true if procedure is marked for bypass successfully, false otherwise
   */
  List<Boolean> bypassProcedure(List<Long> pids, long waitTime, boolean override, boolean recursive)
```

### MismatchedJavadocCode
Method is specified to return 'false' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * This is a server-side call so it prevents any data from being transferred to the client.
   * @param gets the Gets
   * @return Array of boolean. True if the specified Get matches one or more keys, false if not.
   * @throws IOException e
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTable.java`
#### Snippet
```java
     * Specify a RowMutations to commit if the check succeeds.
     * @param mutation mutations to perform if check succeeds
     * @return true if the new mutation was executed, false otherwise. The return value will be
     *         wrapped by a {@link CompletableFuture}.
     */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTable.java`
#### Snippet
```java
   * <p>
   * This is a server-side call so it prevents any data from being transfered to the client.
   * @return true if the specified Get matches one or more keys, false if not. The return value will
   *         be wrapped by a {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTable.java`
#### Snippet
```java
     * Specify a RowMutations to commit if the check succeeds.
     * @param mutation mutations to perform if check succeeds
     * @return true if the new mutation was executed, false otherwise. The return value will be
     *         wrapped by a {@link CompletableFuture}.
     */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query the current state of the auto snapshot cleanup based on TTL.
   * @return true if the auto snapshot cleanup is enabled, false otherwise. The return value will be
   *         wrapped by a {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * Check if a table is disabled.
   * @param tableName name of table to check
   * @return true if table is off-line. The return value will be wrapped by a
   *         {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Check whether master is in maintenance mode
   * @return true if master is in maintenance mode, false otherwise. The return value will be
   *         wrapped by a {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * Check if a replication peer is enabled.
   * @param peerId id of replication peer to check
   * @return true if replication peer is enabled. The return value will be wrapped by a
   *         {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * Invoke region normalizer. Can NOT run for various reasons. Check logs.
   * @param ntfp limit to tables matching the specified filter.
   * @return true if region normalizer ran, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Invoke region normalizer. Can NOT run for various reasons. Check logs.
   * @return true if region normalizer ran, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * Check if a table is available.
   * @param tableName name of table to check
   * @return true if all regions of the table are available. The return value will be wrapped by a
   *         {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query the current state of the Merge switch.
   * @return true if the switch is on, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query the current state of the Split switch.
   * @return true if the switch is on, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * @param instance  The instance name of the procedure
   * @param props     Property/Value pairs of properties passing to the procedure
   * @return true if the specified procedure is finished successfully, false if it is still running.
   *         The value is wrapped by {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query the current state of the cleaner chore.
   * @return true if cleaner chore is on, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * Check if a table is enabled.
   * @param tableName name of table to check
   * @return true if table is on-line. The return value will be wrapped by a
   *         {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * @param procId                ID of the procedure to abort
   * @param mayInterruptIfRunning if the proc completed at least one step, should it be aborted?
   * @return true if aborted, false if procedure already completed or does not exist. the value is
   *         wrapped by {@link CompletableFuture}
   * @deprecated since 2.1.1 and will be removed in 4.0.0.
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query the current state of the region normalizer
   * @return true if region normalizer is on, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query the current state of the balancer.
   * @return true if the balance switch is on, false otherwise. The return value will be wrapped by
   *         a {@link CompletableFuture}.
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Query on the catalog janitor state.
   * @return true if the catalog janitor is on, false otherwise. The return value will be wrapped by
   *         a {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
  /**
   * Ask for cleaner chore to run.
   * @return true if cleaner chore ran, false otherwise. The return value will be wrapped by a
   *         {@link CompletableFuture}
   */
```

### MismatchedJavadocCode
Method is specified to return set but the return type is array
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java`
#### Snippet
```java
   * times in case of machine failure in the middle of the execution.
   * @param env the environment passed to the ProcedureExecutor
   * @return a set of sub-procedures to run or ourselves if there is more work to do or null if the
   *         procedure is done.
   * @throws ProcedureYieldException     the procedure will be added back to the queue and retried
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
   *                  condition, the procedure can be successfully bypassed.
   * @param recursive We will do an expensive search for children of each pid. EXPENSIVE!
   * @return true if bypass success
   * @throws IOException IOException
   */
```

### MismatchedJavadocCode
Method is specified to return list but the return type is array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
  /**
   * List available namespaces
   * @return List of namespace names
   * @throws IOException if a remote or network exception occurs
   */
```

### MismatchedJavadocCode
Method is specified to return array but the return type is list
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
  /**
   * Helper that delegates to getClusterMetrics().getMasterCoprocessorNames().
   * @return an array of master coprocessors
   * @see org.apache.hadoop.hbase.ClusterMetrics#getMasterCoprocessorNames()
   */
```

### MismatchedJavadocCode
Method is specified to return list but the return type is array
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
  /**
   * List available namespace descriptors
   * @return List of descriptors
   * @throws IOException if a remote or network exception occurs
   */
```

### MismatchedJavadocCode
Method is specified to return list but the return type is map
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java`
#### Snippet
```java
  /**
   * Assign regions to the previously hosting region server
   * @return List of plans
   */
  @NonNull
```

### MismatchedJavadocCode
Method is specified to return number but the return type is boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/HFileContentValidator.java`
#### Snippet
```java
   * Check HFile contents are readable by HBase 2.
   * @param conf used configuration
   * @return number of HFiles corrupted HBase
   * @throws IOException if a remote or network exception occurs
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
  /**
   * Checks if the specified snapshot is done.
   * @return true if the snapshot is in file system ready to use, false if the snapshot is in the
   *         process of completing
   * @throws ServiceException wrapping UnknownSnapshotException if invalid snapshot, or a wrapped
```

### MismatchedJavadocCode
Method is specified to return 'false' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
  /**
   * Checks if the specified snapshot is done.
   * @return true if the snapshot is in file system ready to use, false if the snapshot is in the
   *         process of completing
   * @throws ServiceException wrapping UnknownSnapshotException if invalid snapshot, or a wrapped
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
  /**
   * Checks if the specified procedure is done.
   * @return true if the procedure is done, false if the procedure is in the process of completing
   * @throws ServiceException if invalid procedure or failed procedure with progress failure reason.
   */
```

### MismatchedJavadocCode
Method is specified to return 'false' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java`
#### Snippet
```java
  /**
   * Checks if the specified procedure is done.
   * @return true if the procedure is done, false if the procedure is in the process of completing
   * @throws ServiceException if invalid procedure or failed procedure with progress failure reason.
   */
```

### MismatchedJavadocCode
Method is specified to return list but the return type is set
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
  /**
   * Returns the list of ordinals of labels associated with the groups
   * @return the list of ordinals
   */
  public Set<Integer> getGroupAuthsAsOrdinals(String[] groups) {
```

### MismatchedJavadocCode
Method is specified to return list but the return type is set
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java`
#### Snippet
```java
   * Returns the list of ordinals of labels associated with the user
   * @param user Not null value.
   * @return the list of ordinals
   */
  public Set<Integer> getUserAuthsAsOrdinals(String user) {
```

### MismatchedJavadocCode
Method is specified to return set but the return type is list
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
   * the valid store files.
   * @param familyName Column Family Name
   * @return a set of {@link StoreFileInfo} for the specified family.
   */
  public List<StoreFileInfo> getStoreFiles(final String familyName, final boolean validate)
```

### MismatchedJavadocCode
Method is specified to return set but the return type is list
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
   * the valid store files.
   * @param familyName Column Family Name
   * @return a set of {@link StoreFileInfo} for the specified family.
   */
  public List<StoreFileInfo> getStoreFiles(final String familyName) throws IOException {
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
  /**
   * Atomically bulk load several HFiles into an open region
   * @return true if successful, false is failed but recoverably (no action)
   * @throws ServiceException if failed unrecoverably
   */
```

### MismatchedJavadocCode
Method is specified to return 'true' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java`
#### Snippet
```java
   * to do many mutations on a single row. Use checkAndMutate to do one checkAndMutate at a time.
   * @param checkAndMutate the CheckAndMutate object
   * @return true if mutations were applied, false otherwise
   * @throws IOException if an error occurred in this method
   */
```

### MismatchedJavadocCode
Method is specified to return 'false' but its return type is not boolean
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Region.java`
#### Snippet
```java
   * to do many mutations on a single row. Use checkAndMutate to do one checkAndMutate at a time.
   * @param checkAndMutate the CheckAndMutate object
   * @return true if mutations were applied, false otherwise
   * @throws IOException if an error occurred in this method
   */
```

### MismatchedJavadocCode
Method is specified to return set but the return type is array
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
   * Calculate region boundaries and add all the column families to the table descriptor
   * @param regionDirList region dir list
   * @return a set of keys to store the boundaries
   */
  byte[][] generateBoundaryKeys(ArrayList<Path> regionDirList) throws IOException {
```

## RuleId[id=AssignmentToForLoopParameter]
### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
      if (s == 7) {
        ret.put((byte) (t | (ord.apply(a[offset + i]) & 0x7f)));
        i++;
        // explicitly reset t -- clean up overflow buffer after decoding
        // a full cycle and retain assertion condition below. This happens
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java`
#### Snippet
```java

        b[size++] = d;
        i += 3; // skip 3
      } else {
        b[size++] = (byte) ch;
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
          && filterStringAsByteArray[i + 1] == ParseConstants.SINGLE_QUOTE
      ) {
        i++;
        continue;
      }
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      ) {
        unescapedArgument[count++] = filterStringAsByteArray[i + 1];
        i++;
      } else {
        unescapedArgument[count++] = filterStringAsByteArray[i];
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      } else if (checkForOr(filterStringAsByteArray, i)) {
        // OR found
        i += ParseConstants.OR_ARRAY.length - 1;
        reduce(operatorStack, filterStack, ParseConstants.OR_BUFFER);
        operatorStack.push(ParseConstants.OR_BUFFER);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      } else if (checkForAnd(filterStringAsByteArray, i)) {
        // AND found
        i += ParseConstants.AND_ARRAY.length - 1;
        reduce(operatorStack, filterStack, ParseConstants.AND_BUFFER);
        operatorStack.push(ParseConstants.AND_BUFFER);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      } else if (checkForSkip(filterStringAsByteArray, i)) {
        // SKIP found
        i += ParseConstants.SKIP_ARRAY.length - 1;
        reduce(operatorStack, filterStack, ParseConstants.SKIP_BUFFER);
        operatorStack.push(ParseConstants.SKIP_BUFFER);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      } else if (checkForWhile(filterStringAsByteArray, i)) {
        // WHILE found
        i += ParseConstants.WHILE_ARRAY.length - 1;
        reduce(operatorStack, filterStack, ParseConstants.WHILE_BUFFER);
        operatorStack.push(ParseConstants.WHILE_BUFFER);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
        // SimpleFilterExpression found
        byte[] filterSimpleExpression = extractFilterSimpleExpression(filterStringAsByteArray, i);
        i += (filterSimpleExpression.length - 1);
        filter = parseSimpleFilterExpression(filterSimpleExpression);
        filterStack.push(filter);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
            if (isQuoteUnescaped(filterStringAsByteArray, j)) {
              argumentEndIndex = j;
              i = j + 1;
              byte[] filterArgument = createUnescapdArgument(filterStringAsByteArray,
                argumentStartIndex, argumentEndIndex);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `j`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
            } else {
              // To jump over the second escaped quote
              j++;
            }
          } else if (j == filterStringAsByteArray.length - 1) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
          ) {
            argumentEndIndex = j - 1;
            i = j;
            byte[] filterArgument = new byte[argumentEndIndex - argumentStartIndex + 1];
            Bytes.putBytes(filterArgument, 0, filterStringAsByteArray, argumentStartIndex,
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
        } else {
          // To skip the next quote that has been escaped
          i++;
        }
      }
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
          }
        }
        i = j - 1;
        // In the table input format for single table we do not need to
        // store the scan object in table split because it can be memory intensive and redundant
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `len`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java`
#### Snippet
```java
          // Copy 256 bytes from ByteBuff to update the checksum each time, if the remaining
          // bytes is less than 256, then just update the remaining bytes.
          len = Math.min(CHECKSUM_BUF_SIZE, remain);
          data.get(buf, 0, len);
          dataChecksum.update(buf, 0, len);
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
        } else if (cmd.equals("-interval")) {
          // user has specified an interval for canary breaths (-interval N)
          i++;

          if (i == args.length) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          conf.setBoolean(HBASE_CANARY_USE_REGEX, true);
        } else if (cmd.equals("-t")) {
          i++;

          if (i == args.length) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          conf.setLong(HBASE_CANARY_TIMEOUT, timeout);
        } else if (cmd.equals("-writeTableTimeout")) {
          i++;

          if (i == args.length) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          conf.setLong(HBASE_CANARY_REGION_WRITE_TABLE_TIMEOUT, configuredWriteTableTimeout);
        } else if (cmd.equals("-writeTable")) {
          i++;

          if (i == args.length) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          conf.set(HBASE_CANARY_REGION_WRITE_TABLE_NAME, args[i]);
        } else if (cmd.equals("-f")) {
          i++;
          if (i == args.length) {
            System.err.println("-f needs a boolean value argument (true|false).");
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          conf.setBoolean(HBASE_CANARY_FAIL_ON_ERROR, Boolean.parseBoolean(args[i]));
        } else if (cmd.equals("-readTableTimeouts")) {
          i++;
          if (i == args.length) {
            System.err.println("-readTableTimeouts needs a comma-separated list of read "
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
          conf.set(HBASE_CANARY_REGION_READ_TABLE_TIMEOUT, readTableTimeoutsStr);
        } else if (cmd.equals("-permittedZookeeperFailures")) {
          i++;

          if (i == args.length) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        }
        try {
          long timelag = Long.parseLong(args[++i]);
          setTimeLag(timelag);
        } catch (NumberFormatException e) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        }
        try {
          sleepBeforeRerun = Long.parseLong(args[++i]);
        } catch (NumberFormatException e) {
          errors.reportError(ERROR_CODE.WRONG_USAGE, "-sleepBeforeRerun needs a numeric value.");
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          return printUsageAndExit();
        }
        setSidelineDir(args[++i]);
      } else if (cmd.equals("-fix")) {
        errors.reportError(ERROR_CODE.WRONG_USAGE,
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        }
        try {
          int maxOverlapsToSideline = Integer.parseInt(args[++i]);
          setMaxOverlapsToSideline(maxOverlapsToSideline);
        } catch (NumberFormatException e) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
        }
        try {
          int maxMerge = Integer.parseInt(args[++i]);
          setMaxMerge(maxMerge);
        } catch (NumberFormatException e) {
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
      } else if (cmd.equals("-cleanReplicationBarrier")) {
        setCleanReplicationBarrier(true);
        if (args[++i].startsWith("-")) {
          printUsageAndExit();
        }
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java`
#### Snippet
```java
          waitProcedure(regionLocks[i], procedure);
          hasLock = false;
          while (i-- > 0) {
            regionLocks[i].releaseExclusiveLock(procedure);
          }
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationBarrierFamilyFormat.java`
#### Snippet
```java
    for (int i = 0; i < bytes.length; i++) {
      if (bytes[i] == ESCAPE_BYTE) {
        i++;
        if (bytes[i] == SEPARATED_BYTE) {
          parents.add(bos.toByteArray());
```

### AssignmentToForLoopParameter
Assignment to for-loop parameter `i`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
          insertFileIntoStripe(level0Files, badSf);
          ensureLevel0Metadata(badSf);
          --i;
        }
      }
```

## RuleId[id=UnnecessaryToStringCall]
### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/IndexBlockEncoding.java`
#### Snippet
```java
        throw new RuntimeException(
          String.format("Two data block encoder algorithms '%s' and '%s' have " + "the same id %d",
            idArray[algo.id].toString(), algo.toString(), (int) algo.id));
      }
      idArray[algo.id] = algo;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java`
#### Snippet
```java
        throw new RuntimeException(
          String.format("Two data block encoder algorithms '%s' and '%s' have " + "the same id %d",
            idArray[algo.id].toString(), algo.toString(), (int) algo.id));
      }
      idArray[algo.id] = algo;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java`
#### Snippet
```java
    } catch (IOException e) {
      throw new RuntimeException(String.format("Bug in encoding part of algorithm %s. "
        + "Probably it requested more bytes than are available.", toString()), e);
    }
    return baosBytes;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java`
#### Snippet
```java
      if (validFileEncountered == false) {
        // all items returned by globStatus() are directories
        throw new FileNotFoundException("No file found matching " + pathPattern1.toString());
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/SaslPlainServer.java`
#### Snippet
```java
      }
    } catch (Exception e) {
      throw new SaslException("PLAIN auth failed: " + e.toString(), e);
    } finally {
      completed = true;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
        throw new IllegalStateException("Illegal WAL directory specified. "
          + "WAL directories are not permitted to be under root directory: rootDir="
          + rootDir.toString() + ", qualifiedWALDir=" + qualifiedWalDir);
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift/DemoClient.java`
#### Snippet
```java
    for (ColumnDescriptor col2 : client.getColumnDescriptors(demoTable).values()) {
      System.out.println("column with name: " + ClientUtils.utf8(col2.name));
      System.out.println(col2.toString());

      columnNames.add(col2.name);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcConnection.java`
#### Snippet
```java
        if (LOG.isTraceEnabled()) {
          LOG.trace("Found no valid authentication method from providers={} with tokens={}",
            providers.toString(), ticket.getTokens());
        }
        throw new RuntimeException("Found no valid authentication method from options");
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
            && remoteId.getAddress().getHostName().equals(sn.getHostname())
        ) {
          LOG.info("The server on " + sn.toString() + " is dead - stopping the connection "
            + connection.remoteId);
          connections.remove(remoteId, connection);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionOverAsyncConnection.java`
#### Snippet
```java
    ThreadPoolExecutor tpe = new ThreadPoolExecutor(threads, threads, keepAliveTime,
      TimeUnit.SECONDS, workQueue,
      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(toString() + "-shared-%d").build());
    tpe.allowCoreThreadTimeOut(true);
    return tpe;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java

      String value = cp.getJarPath().orElse("") + "|" + cp.getClassName() + "|"
        + Integer.toString(cp.getPriority()) + "|" + kvString.toString();
      return setCoprocessorToMap(value);
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java

    return String.format("%s (%d/%d): [%s]", this.getClass().getSimpleName(), count,
      this.sortedPrefixes.size(), prefixes.toString());
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithAND.java`
#### Snippet
```java
  protected String formatLogFilters(List<Filter> logFilters) {
    return String.format("FilterList AND (%d/%d): %s", logFilters.size(), this.size(),
      logFilters.toString());
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java`
#### Snippet
```java

    return String.format("%s (%d/%d): [%s] canHint: [%b]", this.getClass().getSimpleName(), count,
      this.timestamps.size(), tsList.toString(), canHint);
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
  protected String formatLogFilters(List<Filter> logFilters) {
    return String.format("FilterList OR (%d/%d): %s", logFilters.size(), this.size(),
      logFilters.toString());
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaSettings.java`
#### Snippet
```java
    if (tableName != null) {
      builder.append("TABLE => '");
      builder.append(tableName.toString());
      builder.append("', ");
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/NamespacePermission.java`
#### Snippet
```java
  protected String rawExpression() {
    StringBuilder raw = new StringBuilder("namespace=").append(namespace).append(", ");
    return raw.toString() + super.rawExpression();
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaTableUtil.java`
#### Snippet
```java
    if (c == null) {
      throw new IllegalArgumentException("Result did not contain the expected column "
        + QUOTA_POLICY_COLUMN + ", " + result.toString());
    }
    ByteString buffer =
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java`
#### Snippet
```java
        .append(qualifier == null ? null : Bytes.toString(qualifier)).append(", ");
    }
    return raw.toString() + super.rawExpression();
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    if (replicateAllUserTables) {
      if (excludeNamespaces != null) {
        builder.append("excludeNamespaces=").append(excludeNamespaces.toString()).append(",");
      }
      if (excludeTableCFsMap != null) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
      }
      if (excludeTableCFsMap != null) {
        builder.append("excludeTableCFsMap=").append(excludeTableCFsMap.toString()).append(",");
      }
    } else {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
    } else {
      if (namespaces != null) {
        builder.append("namespaces=").append(namespaces.toString()).append(",");
      }
      if (tableCFsMap != null) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeerConfig.java`
#### Snippet
```java
      }
      if (tableCFsMap != null) {
        builder.append("tableCFs=").append(tableCFsMap.toString()).append(",");
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
      // If null here, it means node has been removed because it crashed. This happens when server
      // is expired in ServerManager. ServerCrashProcedure may or may not have run.
      throw new NoServerDispatchException(key.toString() + "; " + rp.toString());
    }
    node.add(rp);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/RemoteProcedureDispatcher.java`
#### Snippet
```java
    // Check our node still in the map; could have been removed by #removeNode.
    if (!nodeMap.containsValue(node)) {
      throw new NoNodeDispatchException(key.toString() + "; " + rp.toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureEvent.java`
#### Snippet
```java
    ready = false;
    if (LOG.isTraceEnabled()) {
      LOG.trace("Suspend " + toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureEvent.java`
#### Snippet
```java
    ready = true;
    if (LOG.isTraceEnabled()) {
      LOG.trace("Unsuspend " + toString());
    }
    // wakeProcedure adds to the front of queue, so we start from last in the
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java`
#### Snippet
```java
    Procedure<TEnvironment> procedure) {
    Preconditions.checkArgument(procedure.getState() == ProcedureState.RUNNABLE,
      "NOT RUNNABLE! " + procedure.toString());

    // Procedures can suspend themselves. They skip out by throwing a ProcedureSuspendedException.
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
      table.put(put);
      if (LOG.isTraceEnabled()) {
        LOG.trace("PUT " + put.toString());
      }
      servlet.getMetrics().incrementSucessfulPutRequests(1);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java

      if (LOG.isTraceEnabled()) {
        LOG.trace("CHECK-AND-PUT " + put.toString() + ", returns " + retValue);
      }
      if (!retValue) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
        puts.add(put);
        if (LOG.isTraceEnabled()) {
          LOG.trace("PUT " + put.toString());
        }
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java

      if (LOG.isDebugEnabled()) {
        LOG.debug("APPEND " + append.toString());
      }
      Result result = table.append(append);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
      servlet.getMetrics().incrementSucessfulDeleteRequests(1);
      if (LOG.isTraceEnabled()) {
        LOG.trace("DELETE " + delete.toString());
      }
    } catch (Exception e) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java

      if (LOG.isTraceEnabled()) {
        LOG.trace("CHECK-AND-DELETE " + delete.toString() + ", returns " + retValue);
      }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java

      if (LOG.isDebugEnabled()) {
        LOG.debug("INCREMENT " + increment.toString());
      }
      Result result = table.increment(increment);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java`
#### Snippet
```java
      return client.delete(cacheKey.toString()).get();
    } catch (InterruptedException e) {
      LOG.warn("Error deleting " + cacheKey.toString(), e);
      Thread.currentThread().interrupt();
    } catch (ExecutionException e) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-external-blockcache/src/main/java/org/apache/hadoop/hbase/io/hfile/MemcachedBlockCache.java`
#### Snippet
```java
    } catch (ExecutionException e) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("Error deleting " + cacheKey.toString(), e);
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKWatcher.java`
#### Snippet
```java
   */
  public String prefix(final String str) {
    return this.toString() + " " + str;
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java`
#### Snippet
```java
              "multi exception: {}; running operations sequentially "
                + "(runSequentialOnMultiFailure=true); {}",
              ke.toString(), ops.stream().map(o -> o.toString()).collect(Collectors.joining(",")));
            processSequentially(zkw, ops);
            break;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
      admin.deleteTable(tableName);
    } catch (IOException e) {
      LOG.error(format("***Dry run: Failed to delete table '%s'.***%n%s", tableName, e.toString()));
      return;
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
          }
          if (mapperClass.equals(TsvImporterTextMapper.class)) {
            usage(TsvImporterTextMapper.class.toString()
              + " should not be used for non bulkloading case. use "
              + TsvImporterMapper.class.toString() + " or custom mapper whose value type is Put.");
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
            usage(TsvImporterTextMapper.class.toString()
              + " should not be used for non bulkloading case. use "
              + TsvImporterMapper.class.toString() + " or custom mapper whose value type is Put.");
            System.exit(-1);
          }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java`
#### Snippet
```java
      if (pos == 0) {
        // falls before the beginning of the file.
        throw new RuntimeException("Key outside start/stop range: " + key.toString());
      }
      pos--;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableSnapshotInputFormatImpl.java`
#### Snippet
```java
    for (String snapshotName : snapshots) {
      Path restoreSnapshotDir =
        new Path(baseRestoreDir, snapshotName + "__" + UUID.randomUUID().toString());
      rtn.put(snapshotName, restoreSnapshotDir);
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-it/src/main/java/org/apache/hadoop/hbase/chaos/ChaosService.java`
#### Snippet
```java
        break;
      default:
        LOG.error("Service Name not known : " + serviceName.toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/InfoServer.java`
#### Snippet
```java
    builder.setName(name)
      .addEndpoint(URI
        .create(httpConfig.getSchemePrefix() + HostAndPort.fromParts(bindAddress, port).toString()))
      .setAppDir(HBASE_APP_DIR).setFindPort(findPort).setConf(c);
    String logDir = System.getProperty("hbase.log.dir");
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        float avgLocality = 100 * (favoredNodesLocalitySummary[p.ordinal()] / (float) totalRegions);
        System.out.println("\t\tThe expected avg locality if all regions" + " on the "
          + p.toString() + " region servers: " + df.format(avgLocality) + " %");
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
        sb.append(e.getValue().size());
      }
      LOG.warn("Input " + sb.toString());
    }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java
          + "skipping; unassigned regions?");
        if (LOG.isTraceEnabled()) {
          LOG.trace("EMPTY SERVERNAME " + clusterState.toString());
        }
        continue;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
        // of the root dir and see if that fully-qualified URI matches.
        FileSystem rootFS = rootPath.getFileSystem(getConf());
        String qualifiedFile = rootFS.getUri().toString() + file.toString();
        if (!qualifiedFile.startsWith(rootString)) {
          err.println(
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
      simpleReporter.report();

      return metricsOutput.toString() + "Key of biggest row: " + Bytes.toStringBinary(biggestRow);
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java`
#### Snippet
```java
        out.println(Bytes.toBoolean(e.getValue()));
      } else if (Bytes.equals(e.getKey(), HFileInfo.LASTKEY)) {
        out.println(new KeyValue.KeyOnlyKeyValue(e.getValue()).toString());
      } else {
        out.println(Bytes.toStringBinary(e.getValue()));
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
        persistToFile();
      } catch (IOException ex) {
        LOG.error("Unable to persist data on exit: " + ex.toString(), ex);
      } catch (InterruptedException e) {
        LOG.warn("Failed to persist data on exit", e);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java`
#### Snippet
```java
      long totalSize = bucketAllocator.getTotalSize();
      if (LOG.isDebugEnabled() && msgBuffer != null) {
        LOG.debug("Free started because \"" + why + "\"; " + msgBuffer.toString()
          + " of current used=" + StringUtils.byteDesc(currentSize) + ", actual cacheSize="
          + StringUtils.byteDesc(realCacheSize.sum()) + ", total="
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
          "Expected HEADER=" + Bytes.toStringBinary(RPC_HEADER) + " but received HEADER="
            + Bytes.toStringBinary(preambleBuffer.array(), 0, RPC_HEADER.length) + " from "
            + toString());
        return false;
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/ServerRpcConnection.java`
#### Snippet
```java
      // The provider may be null if we failed to parse the header of the request
      ", authName=" + (provider == null ? "unknown" : provider.getSaslAuthMethod().getName())
      + " from " + toString();
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
              }
            } else {
              String errMsg = String.format("Corrupted MOB reference: %s", c.toString());
              throw new IOException(errMsg);
            }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java`
#### Snippet
```java
      if (foundTableNames.size() > 0) {
        System.err.println("Cannot pass a tablename when using the -regionserver "
          + "option, tablenames:" + foundTableNames.toString());
        this.errorCode = USAGE_EXIT_CODE;
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
    if (this.fs.exists(tabledir)) {
      if (!this.fs.delete(tabledir, true)) {
        throw new IOException("Failed delete of " + tabledir.toString());
      }
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
    } catch (InvocationTargetException ite) {
      Throwable target = ite.getTargetException();
      throw new RuntimeException("Failed construction of Master: " + hmc.toString()
        + ((target.getCause() != null) ? target.getCause().getMessage() : ""), target);
    } catch (Exception e) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java`
#### Snippet
```java
    } catch (InvocationTargetException ite) {
      Throwable target = ite.getTargetException();
      throw new RuntimeException("Failed construction of RegionServer: " + hrsc.toString()
        + ((target.getCause() != null) ? target.getCause().getMessage() : ""), target);
    } catch (Exception e) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomFilterUtil.java`
#### Snippet
```java
      } catch (NumberFormatException nfe) {
        message = "Number format exception when parsing " + PREFIX_LENGTH_KEY + " for BloomType "
          + bloomFilterType.toString() + ":" + prefixLengthString;
        throw new IllegalArgumentException(message, nfe);
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmPauseMonitor.java`
#### Snippet
```java
            GcTimes diff = gcTimesAfterSleep.get(name).subtract(gcTimesBeforeSleep.get(name));
            if (diff.gcCount != 0) {
              gcDiffs.add("GC pool '" + name + "' had collection(s): " + diff.toString());
            }
          }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
            && Bytes.compareTo(hi.getHdfsHRI().getEndKey(), range.getSecond()) == 0
        ) {
          LOG.info("This is a parent for this group: " + hi.toString());
          parent = hi;
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
        }

        LOG.info("Before sideline big overlapped region: " + regionToSideline.toString());
        Path sidelineRegionDir = hbck.sidelineRegionDir(fs, TO_BE_LOADED, regionToSideline);
        if (sidelineRegionDir != null) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
          sidelinedRegions.put(sidelineRegionDir, regionToSideline);
          LOG.info("After sidelined big overlapped region: "
            + regionToSideline.getRegionNameAsString() + " to " + sidelineRegionDir.toString());
          hbck.fixes++;
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java
      LOG.info("Acquired " + lockRequestStr);
    } else {
      LOG.info(String.format("Failed acquire in %s %s of %s", timeout, timeUnit.toString(),
        lockRequestStr));
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/client/locking/EntityLock.java`
#### Snippet
```java
      worker.start();
    } else {
      LOG.info("Lock already queued : " + toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java`
#### Snippet
```java
        activeMaster = MasterAddressTracker.getMasterAddress(this.watcher);
      } catch (IOException e) {
        LOG.warn("Failed get of master address: " + e.toString());
      }
      if (activeMaster != null && activeMaster.equals(this.sn)) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
      if (existingServer != null && (existingServer.getStartcode() > serverName.getStartcode())) {
        LOG.info("Server serverName=" + serverName + " rejected; we already have "
          + existingServer.toString() + " registered with same hostname and port");
        return false;
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java`
#### Snippet
```java
          sb.append(key);
        }
        LOG.info("Waiting on regionserver(s) " + sb.toString());
        previousLogTime = EnvironmentEdgeManager.currentTime();
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java`
#### Snippet
```java
      if (!this.fs.exists(p)) {
        if (!this.fs.mkdirs(p, HiddenDirPerms)) {
          throw new IOException("Failed to create staging directory " + p.toString());
        }
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java`
#### Snippet
```java

    } catch (IOException e) {
      LOG.error("Failed to create or set permission on staging directory " + p.toString());
      throw new IOException(
        "Failed to create or set permission on staging directory " + p.toString(), e);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java`
#### Snippet
```java
      LOG.error("Failed to create or set permission on staging directory " + p.toString());
      throw new IOException(
        "Failed to create or set permission on staging directory " + p.toString(), e);
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java`
#### Snippet
```java
      // check whether the permission match
      LOG.warn("Found HBase directory permissions NOT matching expected permissions for "
        + p.toString() + " permissions=" + fs.getFileStatus(p).getPermission() + ", expecting "
        + dirPerms + ". Automatically setting the permissions. "
        + "You can change the permissions by setting \"" + dirPermsConfName
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
    if (unlock.get() || hasHeartbeatExpired()) {
      locked.set(false);
      LOG.debug((unlock.get() ? "UNLOCKED " : "TIMED OUT ") + toString());
      return null;
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
        return new NamespaceExclusiveLock();
      case SHARED:
        LOG.error("Shared lock on namespace not supported for " + toString());
        throw new IllegalArgumentException("Shared lock on namespace not supported");
      default:
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
        throw new IllegalArgumentException("Shared lock on namespace not supported");
      default:
        LOG.error("Unexpected lock type " + toString());
        throw new IllegalArgumentException("Wrong lock type: " + type.toString());
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
        return new RegionExclusiveLock();
      default:
        LOG.error("Only exclusive lock supported on regions for " + toString());
        throw new IllegalArgumentException("Only exclusive lock supported on regions.");
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
    if (ret) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("LOCKED " + toString());
      }
      lastHeartBeat.set(EnvironmentEdgeManager.currentTime());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
      return LockState.LOCK_ACQUIRED;
    }
    LOG.warn("Failed acquire LOCK " + toString() + "; YIELDING");
    return LockState.LOCK_EVENT_WAIT;
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
        return new TableSharedLock();
      default:
        LOG.error("Unexpected lock type " + toString());
        throw new IllegalArgumentException("Wrong lock type:" + type.toString());
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
    lastHeartBeat.set(EnvironmentEdgeManager.currentTime());
    if (LOG.isDebugEnabled()) {
      LOG.debug("Heartbeat " + toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java`
#### Snippet
```java
      return setupTableLock();
    } else {
      LOG.error("Unknown level specified in " + toString());
      throw new IllegalArgumentException("no namespace/table/region provided");
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
          }
        }
        LOG.info("Created version file at " + rootdir.toString() + " with version=" + version);
        return;
      } catch (IOException e) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
      } catch (IOException e) {
        if (retries > 0) {
          LOG.debug("Unable to create version file at " + rootdir.toString() + ", retrying", e);
          fs.delete(versionFile, false);
          try {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    if (null == desiredTable) {
      queryPath =
        new Path(new Path(rootPath, HConstants.BASE_NAMESPACE_DIR).toString() + "/*/*/*/");
    } else {
      queryPath = new Path(
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
    } else {
      queryPath = new Path(
        CommonFSUtils.getTableDir(rootPath, TableName.valueOf(desiredTable)).toString() + "/*/");
    }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    // Now we can do the move
    RegionPlan rp = new RegionPlan(hri, regionState.getServerName(), dest);
    assert rp.getDestination() != null : rp.toString() + " " + dest;

    try {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
      }
      if (e instanceof DoNotRetryIOException) {
        LOG.warn("{} tells us DoNotRetry due to {}, try={}, give up", serverName, e.toString(),
          numberOfAttemptsSoFar);
        return false;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
      if (e instanceof CallQueueTooBigException && numberOfAttemptsSoFar == 0) {
        LOG.warn("request to {} failed due to {}, try={}, this usually because"
          + " server is overloaded, give up", serverName, e.toString(), numberOfAttemptsSoFar);
        return false;
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/executor/ExecutorType.java`
#### Snippet
```java
  /** Returns Conflation of the executor type and the passed {@code serverName}. */
  String getExecutorName(String serverName) {
    return this.toString() + "-" + serverName.replace("%", "%%");
  }
}
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessChecker.java`
#### Snippet
```java
        } catch (IOException ioe) {
          throw new IllegalArgumentException(
            "Illegal principal name " + this.name + ": " + ioe.toString(), ioe);
        }
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    }
    if (LOG.isDebugEnabled()) {
      LOG.debug("Removed permission " + userPerm.toString());
    }
  }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java`
#### Snippet
```java
        LOG.info("Scheduled ServerCrashProcedure pid={} for {} (carryingMeta={}){}.", pid,
          serverName, carryingMeta,
          serverNode == null ? "" : " " + serverNode.toString() + ", oldState=" + oldState);
      }
    } finally {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
        String tableName =
          ((RegionCoprocessorEnvironment) env).getRegionInfo().getTable().getNameAsString();
        LOG.error("Removing coprocessor '" + env.toString() + "' from table '" + tableName + "'",
          e);
      } else {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
          e);
      } else {
        LOG.error("Removing coprocessor '" + env.toString() + "' from " + "environment", e);
      }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
        shutdown(env);
      } catch (Exception x) {
        LOG.error("Uncaught exception when shutting down coprocessor '" + env.toString() + "'", x);
      }
      throw new DoNotRetryIOException("Coprocessor: '" + env.toString() + "' threw: '" + e
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
        LOG.error("Uncaught exception when shutting down coprocessor '" + env.toString() + "'", x);
      }
      throw new DoNotRetryIOException("Coprocessor: '" + env.toString() + "' threw: '" + e
        + "' and has been removed from the active " + "coprocessor set.", e);
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
        : true;
      if (alive && time < timeout) {
        LOG.trace("Skipping the resubmit of " + task.toString() + "  because the server "
          + task.cur_worker_name + " is not marked as dead, we waited for " + time
          + " while the timeout is " + timeout);
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      heartbeat(path, version, slt.getServerName());
    } else if (slt.isResigned()) {
      LOG.info("Task " + path + " entered state=" + slt.toString());
      resubmitOrFail(path, FORCE);
    } else if (slt.isDone()) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      resubmitOrFail(path, FORCE);
    } else if (slt.isDone()) {
      LOG.info("Task " + path + " entered state=" + slt.toString());
      if (taskFinisher != null && !ZKSplitLog.isRescanNode(watcher, path)) {
        if (taskFinisher.finish(slt.getServerName(), ZKSplitLog.getFileName(path)) == Status.DONE) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
      }
    } else if (slt.isErr()) {
      LOG.info("Task " + path + " entered state=" + slt.toString());
      resubmitOrFail(path, CHECK);
    } else {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
    } else {
      LOG.error(HBaseMarkers.FATAL,
        "logic error - unexpected zk state for path = " + path + " data = " + slt.toString());
      setDone(path, FAILURE);
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.java`
#### Snippet
```java
          ) {
            LOG.info("task " + taskpath + " preempted from " + serverName
              + ", current task state and owner=" + slt.toString());
            worker.stopTask();
          }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
    Matcher m = REF_NAME_PATTERN.matcher(p.getName());
    if (m == null || !m.matches()) {
      LOG.warn("Failed match of store file name {}", p.toString());
      throw new IllegalArgumentException("Failed match of store file name " + p.toString());
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
    if (m == null || !m.matches()) {
      LOG.warn("Failed match of store file name {}", p.toString());
      throw new IllegalArgumentException("Failed match of store file name " + p.toString());
    }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
        if (AUDITLOG.isTraceEnabled()) {
          // audit log should store permission changes in addition to auth results
          AUDITLOG.trace("Granted permission " + perm.toString());
        }
      } else {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
        if (AUDITLOG.isTraceEnabled()) {
          // audit log should record all permission changes
          AUDITLOG.trace("Revoked permission " + perm.toString());
        }
      } else {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    // An ACL on a delete is useless, we shouldn't allow it
    if (delete.getAttribute(AccessControlConstants.OP_ATTRIBUTE_ACL) != null) {
      throw new DoNotRetryIOException("ACL on checkAndDelete has no effect: " + delete.toString());
    }
    // Require READ and WRITE permissions on the table, CF, and the KV covered
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    // An ACL on a delete is useless, we shouldn't allow it
    if (delete.getAttribute(AccessControlConstants.OP_ATTRIBUTE_ACL) != null) {
      throw new DoNotRetryIOException("ACL on delete has no effect: " + delete.toString());
    }
    // Require WRITE permissions on all cells covered by the delete. Unlike
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
        Path path =
          new Path(splitDir, HFileLink.createHFileLinkName(linkedTable, linkedRegion, hfileName));
        LOG.info("Created linkFile:" + path.toString() + " for child: " + hri.getEncodedName()
          + ", parent: " + regionInfoForFs.getEncodedName());
        return path;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
      byte[] lastKey = CellUtil.cloneRow(lk.get());
      if (!this.getRegionInfo().containsRange(firstKey.get(), lastKey)) {
        throw new WrongRegionException("Recovered hfile " + path.toString()
          + " does not fit inside region " + this.getRegionInfo().getRegionNameAsString());
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java

      if (!this.getRegionInfo().containsRange(firstKey.get(), lastKey)) {
        throw new WrongRegionException("Bulk load file " + srcPath.toString()
          + " does not fit inside region " + this.getRegionInfo().getRegionNameAsString());
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
          prevCell = cell;
        } while (scanner.next());
        LOG.info("Full verification complete for bulk load hfile: " + srcPath.toString() + " took "
          + (EnvironmentEdgeManager.currentTime() - verificationStartTime) + " ms");
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
      } else {
        LOG.info("Flush of region " + regionToFlush + " due to global heap pressure. "
          + "Flush type=" + flushType.toString() + ", Total Memstore Heap size="
          + TraditionalBinaryPrefix
            .long2String(server.getRegionServerAccounting().getGlobalMemStoreHeapSize(), "", 1)
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
              this.updatesBlockedMsHighWater.add(totalTime);
            }
            LOG.info("Unblocking updates for server " + server.toString());
          }
        }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
            if (requester.requestDelayedFlush(r, delay)) {
              LOG.info("{} requesting flush of {} because {} after random delay {} ms", getName(),
                r.getRegionInfo().getRegionNameAsString(), whyFlush.toString(), delay);
            }
          }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/WALSplitterHandler.java`
#### Snippet
```java
          if (server != null && server.isStopped()) {
            LOG.info("task execution interrupted because worker is exiting "
              + splitTaskDetails.toString());
          }
          coordination.endTask(new SplitLogTask.Resigned(this.serverName),
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
    if (!ourFiles.accept(fileName)) {
      throw new IllegalArgumentException(
        "The log file " + fileName + " doesn't belong to this WAL. (" + toString() + ")");
    }
    final String fileNameString = fileName.toString();
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
        "Moved " + files.length + " WAL file(s) to " + CommonFSUtils.getPath(this.walArchiveDir));
    }
    LOG.info("Closed WAL: " + toString());
  }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
          families.append(Bytes.toString(r.getValue().get(i)));
        }
        listForPrint.add(Bytes.toStringBinary(r.getKey()) + "[" + families.toString() + "]");
      }
      LOG.info("Too many WALs; count=" + logCount + ", max=" + this.maxLogs
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/RestoreTablesClient.java`
#### Snippet
```java
    if (manifest.getType() == BackupType.FULL) {
      LOG.info("Restoring '" + sTable + "' to '" + tTable + "' from full" + " backup image "
        + tableBackupPath.toString());
      conf.set(JOB_NAME_CONF_KEY, "Full_Restore-" + backupId + "-" + tTable);
      restoreTool.fullRestoreTable(conn, tableBackupPath, sTable, tTable, truncateIfExists,
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
          if (outputFs.delete(targetDirPath, true)) {
            LOG.debug(
              "Cleaning up uncompleted backup data at " + targetDirPath.toString() + " done.");
          } else {
            LOG.debug("No data has been copied to " + targetDirPath.toString() + ".");
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
              "Cleaning up uncompleted backup data at " + targetDirPath.toString() + " done.");
          } else {
            LOG.debug("No data has been copied to " + targetDirPath.toString() + ".");
          }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManager.java`
#### Snippet
```java
            "Current backup has an incremental backup ancestor, "
              + "touching its image manifest in {}" + " to construct the dependency.",
            logBackupPath.toString());
          BackupManifest lastIncrImgManifest = new BackupManifest(conf, logBackupPath);
          BackupImage lastIncrImage = lastIncrImgManifest.getBackupImage();
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
        backupInfo.getBackupId(), table));
      if (outputFs.delete(targetDirPath, true)) {
        LOG.info("Cleaning up backup data at " + targetDirPath.toString() + " done.");
      } else {
        LOG.info("No data has been found in " + targetDirPath.toString() + ".");
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
        LOG.info("Cleaning up backup data at " + targetDirPath.toString() + " done.");
      } else {
        LOG.info("No data has been found in " + targetDirPath.toString() + ".");
      }
    } catch (IOException e1) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
          getTableBackupDir(backupInfo.getBackupRootDir(), backupInfo.getBackupId(), table));
        if (outputFs.delete(targetDirPath, true)) {
          LOG.info("Cleaning up backup data at " + targetDirPath.toString() + " done.");
        } else {
          LOG.info("No data has been found in " + targetDirPath.toString() + ".");
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/BackupUtils.java`
#### Snippet
```java
          LOG.info("Cleaning up backup data at " + targetDirPath.toString() + " done.");
        } else {
          LOG.info("No data has been found in " + targetDirPath.toString() + ".");
        }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
    Path tableArchivePath = new Path(archivePath, tableName.getQualifierAsString());
    if (!fs.exists(tableArchivePath) || !fs.getFileStatus(tableArchivePath).isDirectory()) {
      LOG.debug("Folder tableArchivePath: " + tableArchivePath.toString() + " does not exists");
      tableArchivePath = null; // empty table has no archive
    }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java`
#### Snippet
```java
        "tableDescriptor.getNameAsString() = " + tableDescriptor.getTableName().getNameAsString());
      throw new FileNotFoundException("couldn't find Table Desc for table: " + tableName
        + " under tableInfoPath: " + tableInfoPath.toString());
    }
    return tableDescriptor;
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseZKTestingUtility.java`
#### Snippet
```java
    // Using randomUUID ensures that multiple clusters can be launched by
    // a same test, if it stops & starts them
    Path testDir = getDataTestDir("cluster_" + getRandomUUID().toString());
    clusterTestDir = new File(testDir.toString()).getAbsoluteFile();
    // Have it cleaned up on exit
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
    HRegionServer server = getRegionServer(getRegionServerIndex(serverName));
    if (server instanceof MiniHBaseClusterRegionServer) {
      LOG.info("Killing " + server.toString());
      ((MiniHBaseClusterRegionServer) server).kill();
    } else {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/testing/TestingHBaseClusterImpl.java`
#### Snippet
```java
      Configuration conf = util.getConfiguration();
      conf.set(HConstants.ZOOKEEPER_QUORUM, externalZkConnectString);
      conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, "/" + UUID.randomUUID().toString());
    }
    if (externalDfsUri == null) {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      if (s.timeOfOldestEdit() < now - modifiedFlushCheckInterval) {
        // we have an old enough edit in the memstore, flush
        whyFlush.append(s.toString() + " has an old edit so flush to free WALs");
        return true;
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // problem when validating
        LOG.warn("There was a recoverable bulk load failure likely due to a split. These (family,"
          + " HFile) pairs were not loaded: {}, in region {}", list.toString(), this);
        return null;
      }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
              store.close();
            } catch (IOException e) {
              LOG.warn("close store {} failed in region {}", store.toString(), this, e);
            }
          }
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        }

        status.markComplete("Flush successful " + fs.toString());
        return fs;
      } finally {
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    // caller as possible. (memStoreSizing might be a negative value already -- freeing memory)
    if (memStoreDataSize < 0) {
      LOG.error("Asked to modify this region's (" + this.toString()
        + ") memStoreSizing to a negative value which is incorrect. Current memStoreSizing="
        + (memStoreDataSize - delta) + ", delta=" + delta, new Exception());
```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    if (LOG.isDebugEnabled()) {
      // Write out region name, its encoded name and storeHotnessProtector as string.
      LOG.debug("Instantiated " + this + "; " + storeHotnessProtector.toString());
    }

```

### UnnecessaryToStringCall
Unnecessary `toString()` call
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  private void setupDataTestDirOnTestFS() throws IOException {
    if (dataTestDirOnTestFS != null) {
      LOG.warn("Data test on test fs dir already setup in " + dataTestDirOnTestFS.toString());
      return;
    }
```

## RuleId[id=SetReplaceableByEnumSet]
### SetReplaceableByEnumSet
`HashSet<>` can be replaced with 'EnumSet'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/TableOperationSpanBuilder.java`
#### Snippet
```java

  private static Set<Operation> unpackRowOperations(final CheckAndMutate cam) {
    final Set<Operation> ops = new HashSet<>();
    final Operation op = valueFrom(cam.getAction());
    switch (op) {
```

### SetReplaceableByEnumSet
`HashSet<>` can be replaced with 'EnumSet'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/trace/TableOperationSpanBuilder.java`
#### Snippet
```java

  private static Set<Operation> unpackRowOperations(final Row row) {
    final Set<Operation> ops = new HashSet<>();
    if (row instanceof CheckAndMutate) {
      final CheckAndMutate cam = (CheckAndMutate) row;
```

### SetReplaceableByEnumSet
`HashSet<>` can be replaced with 'EnumSet'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/quotas/QuotaFilter.java`
#### Snippet
```java
@InterfaceAudience.Public
public class QuotaFilter {
  private Set<QuotaType> types = new HashSet<>();
  private boolean hasFilters = false;
  private String namespaceRegex;
```

### SetReplaceableByEnumSet
`TreeSet` can be replaced with 'EnumSet'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java
    }

    Set<Permission.Action> actionSet = new TreeSet<Permission.Action>();
    if (mergeExistingPermissions) {
      List<UserPermission> perms = getUserPermissions(conf, rowKey, null, null, null, false);
```

### SetReplaceableByEnumSet
`HashSet<>` can be replaced with 'EnumSet'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java`
#### Snippet
```java

  public static Permission.Action[] permissionActionsFromString(String permission_actions) {
    Set<Permission.Action> actions = new HashSet<>();
    for (char c : permission_actions.toCharArray()) {
      switch (c) {
```

### SetReplaceableByEnumSet
`HashSet<>` can be replaced with 'EnumSet'
in `hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java`
#### Snippet
```java

  public static Permission.Action[] permissionActionsFromString(String permission_actions) {
    Set<Permission.Action> actions = new HashSet<>();
    for (char c : permission_actions.toCharArray()) {
      switch (c) {
```

## RuleId[id=InnerClassMayBeStatic]
### InnerClassMayBeStatic
Inner class `ArrayValueCollection` may be 'static'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayValueCollection<K, V> implements Collection<V> {

    private final ArrayHolder<K, V> holder;
```

### InnerClassMayBeStatic
Inner class `ArrayEntrySet` may be 'static'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  }

  private final class ArrayEntrySet<K, V> implements Set<Map.Entry<K, V>> {
    private final ArrayHolder<K, V> holder;

```

### InnerClassMayBeStatic
Inner class `IntegrityFixSuggester` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java`
#### Snippet
```java
  }

  class IntegrityFixSuggester extends TableIntegrityErrorHandlerImpl {
    HbckErrorReporter errors;

```

### InnerClassMayBeStatic
Inner class `DeleteVersionsNode` may be 'static'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker.java`
#### Snippet
```java
   * the previous node's mvcc. A node means there is a version deletion at the mvcc and ts.
   */
  protected class DeleteVersionsNode {
    public long ts;
    public long mvcc;
```

## RuleId[id=RegExpDuplicateAlternationBranch]
### RegExpDuplicateAlternationBranch
Duplicate branch in alternation
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.java`
#### Snippet
```java
  private static final Pattern prefetchPathExclude =
    Pattern.compile("(" + Path.SEPARATOR_CHAR + HConstants.HBASE_TEMP_DIRECTORY.replace(".", "\\.")
      + Path.SEPARATOR_CHAR + ")|(" + Path.SEPARATOR_CHAR
      + HConstants.HREGION_COMPACTIONDIR_NAME.replace(".", "\\.") + Path.SEPARATOR_CHAR + ")");

  public static void request(Path path, Runnable runnable) {
```

## RuleId[id=NestedSynchronizedStatement]
### NestedSynchronizedStatement
Nested `synchronized` statement
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ChoreService.java`
#### Snippet
```java
    // always lock chore first to prevent dead lock
    synchronized (chore) {
      synchronized (this) {
        try {
          // Chores should only ever be scheduled with a single ChoreService. If the choreService
```

### NestedSynchronizedStatement
Nested `synchronized` statement
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java

      // synchronized on walsById to avoid race with cleanOldLogs
      synchronized (this.walsById) {
        // Update walsById map
        for (Map.Entry<String, Map<String, NavigableSet<String>>> entry : this.walsById
```

### NestedSynchronizedStatement
Nested `synchronized` statement
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
          task.status = status;
          if (task.batch != null) {
            synchronized (task.batch) {
              if (status == SUCCESS) {
                task.batch.done++;
```

### NestedSynchronizedStatement
Nested `synchronized` statement
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java`
#### Snippet
```java

      if (nextReadValue > 0) {
        synchronized (readWaiters) {
          readPoint.set(nextReadValue);
          readWaiters.notifyAll();
```

### NestedSynchronizedStatement
Nested `synchronized` statement
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        // C. Finally notify anyone waiting on memstore to clear:
        // e.g. checkResources().
        synchronized (this) {
          notifyAll(); // FindBugs NN_NAKED_NOTIFY
        }
```

## RuleId[id=StringEqualsEmptyString]
### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeTokenIdentifier.java`
#### Snippet
```java
  @Override
  public UserGroupInformation getUser() {
    if (username == null || "".equals(username)) {
      return null;
    }
```

### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenIdentifier.java`
#### Snippet
```java
  @Override
  public UserGroupInformation getUser() {
    if (username == null || "".equals(username)) {
      return null;
    }
```

### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java`
#### Snippet
```java
    this.tableResource = tableResource;

    if (columnsStr != null && !columnsStr.equals("")) {
      this.columns = columnsStr.split(",");
    }
```

### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
    // null scan input is represented by ""
    String printScan = "";
    if (!scan.equals("")) {
      try {
        // get the real scan here in toString, not the Base64 string
```

### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/TableBackupClient.java`
#### Snippet
```java
  protected String getMessage(Exception e) {
    String msg = e.getMessage();
    if (msg == null || msg.equals("")) {
      msg = e.getClass().getName();
    }
```

### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      if (!plugins.contains(replicationCoprocessorClass)) {
        conf.set(CoprocessorHost.REGION_COPROCESSOR_CONF_KEY,
          (plugins.equals("") ? "" : (plugins + ",")) + replicationCoprocessorClass);
      }
    }
```

### StringEqualsEmptyString
`equals("")` can be replaced with 'isEmpty()'
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    }
    String formatted = "";
    if (message != null && !"".equals(message)) {
      formatted = message + " ";
    }
```

## RuleId[id=UnnecessaryBoxing]
### UnnecessaryBoxing
Unnecessary boxing
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    if (isNumericZero(src)) {
      src.get();
      return Long.valueOf(0);
    }
    return decodeNumericValue(src).longValue();
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/OrderedBytes.java`
#### Snippet
```java
    if (isNumericZero(src)) {
      src.get();
      return Double.valueOf(0.0);
    }

```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java`
#### Snippet
```java

  /** long constant for zero */
  public static final Long ZERO_L = Long.valueOf(0L);
  public static final String NINES = "99999999999999";
  public static final String ZEROES = "00000000000000";
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
      }
      StringBuilder sb = Strings.appendKeyValue(new StringBuilder(), "requestsPerSecond",
        Double.valueOf(getRequestCountPerSecond()));
      Strings.appendKeyValue(sb, "numberOfOnlineRegions",
        Integer.valueOf(getRegionMetrics().size()));
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
        Double.valueOf(getRequestCountPerSecond()));
      Strings.appendKeyValue(sb, "numberOfOnlineRegions",
        Integer.valueOf(getRegionMetrics().size()));
      Strings.appendKeyValue(sb, "usedHeapMB", getUsedHeapSize());
      Strings.appendKeyValue(sb, "maxHeapMB", getMaxHeapSize());
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ServerMetricsBuilder.java`
#### Snippet
```java
      float compactionProgressPct = Float.NaN;
      if (compactingCellCount > 0) {
        compactionProgressPct = Float.valueOf((float) compactedCellCount / compactingCellCount);
      }
      Strings.appendKeyValue(sb, "compactionProgressPct", compactionProgressPct);
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
   */
  public Scan setScanMetricsEnabled(final boolean enabled) {
    setAttribute(Scan.SCAN_ATTRIBUTES_METRICS_ENABLE, Bytes.toBytes(Boolean.valueOf(enabled)));
    return this;
  }
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    public int getNormalizerTargetRegionCount() {
      return getOrDefault(NORMALIZER_TARGET_REGION_COUNT_KEY, Integer::valueOf,
        Integer.valueOf(-1));
    }

```

### UnnecessaryBoxing
Redundant boxing, `Boolean.parseBoolean()` call can be used instead
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
        String value = Bytes.toString(entry.getValue().get());
        if (key.equalsIgnoreCase(IS_META)) {
          if (Boolean.valueOf(value) == false) {
            continue;
          }
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
    public long getNormalizerTargetRegionSize() {
      long target_region_size =
        getOrDefault(NORMALIZER_TARGET_REGION_SIZE_MB_KEY, Long::valueOf, Long.valueOf(-1));
      return target_region_size == Long.valueOf(-1)
        ? getOrDefault(NORMALIZER_TARGET_REGION_SIZE_KEY, Long::valueOf, Long.valueOf(-1))
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
      long target_region_size =
        getOrDefault(NORMALIZER_TARGET_REGION_SIZE_MB_KEY, Long::valueOf, Long.valueOf(-1));
      return target_region_size == Long.valueOf(-1)
        ? getOrDefault(NORMALIZER_TARGET_REGION_SIZE_KEY, Long::valueOf, Long.valueOf(-1))
        : target_region_size;
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
        getOrDefault(NORMALIZER_TARGET_REGION_SIZE_MB_KEY, Long::valueOf, Long.valueOf(-1));
      return target_region_size == Long.valueOf(-1)
        ? getOrDefault(NORMALIZER_TARGET_REGION_SIZE_KEY, Long::valueOf, Long.valueOf(-1))
        : target_region_size;
    }
```

### UnnecessaryBoxing
Redundant boxing, `Boolean.parseBoolean()` call can be used instead
in `hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java`
#### Snippet
```java
    this.check = check;
    if (returnResult != null) {
      this.returnResult = Boolean.valueOf(returnResult);
    }
  }
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        Integer regionCounter = primaryRSToRegionCounterMap.get(primaryRS);
        if (regionCounter == null) {
          regionCounter = Integer.valueOf(0);
        }
        regionCounter = regionCounter.intValue() + 1;
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        Integer hostRegionCounter = serverToHostingRegionCounterMap.get(currentRS);
        if (hostRegionCounter == null) {
          hostRegionCounter = Integer.valueOf(0);
        }
        hostRegionCounter = hostRegionCounter.intValue() + 1;
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
        Integer regionCounter = primaryRSToRegionCounterMap.get(primaryRS);
        if (regionCounter == null) {
          regionCounter = Integer.valueOf(0);
        }
        regionCounter = regionCounter.intValue() + 1;
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerClusterState.java`
#### Snippet
```java

    for (int i = 0; i < numTables; i++) {
      meanRegionsPerTable[i] = Double.valueOf(numRegionsPerTable[i]) / numServers;
    }

```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java
      // for actual overlaps.
      if (overlappedRegions > 1) {
        Integer key = Integer.valueOf(overlappedRegions);
        List<R> ranges = overlapRangeMap.get(key);
        if (ranges == null) {
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/SaslServerAuthenticationProviders.java`
#### Snippet
```java
   */
  public SaslServerAuthenticationProvider selectProvider(byte authByte) {
    return providers.get(Byte.valueOf(authByte));
  }

```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java`
#### Snippet
```java
        }
      }
      if (familyVersionStamps.contains(Long.valueOf(timestamp))) {
        if (visibilityTagsDeleteFamilyVersion != null) {
          if (!visibilityTagsDeleteFamilyVersion.isEmpty()) {
```

### UnnecessaryBoxing
Redundant boxing, `Boolean.parseBoolean()` call can be used instead
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.java`
#### Snippet
```java
    return tableDescriptor == null
      ? false
      : Boolean.valueOf(tableDescriptor.getValue(ACL_SYNC_TO_HDFS_ENABLE));
  }

```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
    this.watcher.getRecoverableZooKeeper().getZooKeeper().create(ZKSplitLog.getRescanNode(watcher),
      slt.toByteArray(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL,
      new CreateRescanAsyncCallback(), Long.valueOf(retries));
  }

```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
    // A negative retry count will lead to ignoring all error processing.
    this.watcher.getRecoverableZooKeeper().getZooKeeper().getData(path, this.watcher,
      new GetDataAsyncCallback(), Long.valueOf(-1) /* retry count */);
    SplitLogCounters.tot_mgr_get_data_queued.increment();
  }
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
  void update(byte[] encodedRegionName, Set<byte[]> families, long sequenceid,
    final boolean lowest) {
    Long l = Long.valueOf(sequenceid);
    this.highestSequenceIds.put(encodedRegionName, l);
    if (lowest) {
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.java`
#### Snippet
```java
    // And if we do not flush all the stores, then the maxFlushedSeqId is calculated by
    // lowestUnflushedSeqId - 1, so here let's plus the 1 back.
    Long wrappedSeqId = Long.valueOf(maxFlushedSeqId + 1);
    synchronized (tieLock) {
      this.flushingSequenceIds.remove(encodedRegionName);
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.java`
#### Snippet
```java
    }

    if (familyVersionStamps.contains(Long.valueOf(timestamp))) {
      return DeleteResult.FAMILY_VERSION_DELETED;
    }
```

### UnnecessaryBoxing
Unnecessary boxing
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java`
#### Snippet
```java
    restoreImages.put(backupImage.startTs, backupImage);
    for (BackupImage image : backupImage.getAncestors()) {
      restoreImages.put(Long.valueOf(image.startTs), image);
    }
    return new ArrayList<>(
```

## RuleId[id=SynchronizeOnNonFinalField]
### SynchronizeOnNonFinalField
Synchronization on a non-final field `outer`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java
    @Override
    public boolean nextKeyValue() throws IOException, InterruptedException {
      synchronized (outer) {
        if (!outer.nextKeyValue()) {
          return false;
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `outer`
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java`
#### Snippet
```java
    @Override
    public void write(K2 key, V2 value) throws IOException, InterruptedException {
      synchronized (outer) {
        outer.write(key, value);
      }
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `authTokenSecretMgr`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServer.java`
#### Snippet
```java
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
      // LeaderElector start. See HBASE-25875
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `authTokenSecretMgr`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/NettyRpcServer.java`
#### Snippet
```java
      // Start AuthenticationTokenSecretManager in synchronized way to avoid race conditions in
      // LeaderElector start. See HBASE-25875
      synchronized (authTokenSecretMgr) {
        setSecretManager(authTokenSecretMgr);
        authTokenSecretMgr.start();
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `fetchInProgress`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java

  private void waitForFetchToFinish() throws InterruptedException {
    synchronized (fetchInProgress) {
      while (fetchInProgress.get()) {
        // We don't want the fetches to block forever, for example if there are bugs
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `fetchInProgress`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/CachedClusterId.java`
#### Snippet
```java
      } finally {
        Preconditions.checkState(fetchInProgress.compareAndSet(true, false));
        synchronized (fetchInProgress) {
          fetchInProgress.notifyAll();
        }
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `this.balancer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
    }

    synchronized (this.balancer) {
      // Only allow one balance run at at time.
      if (this.assignmentManager.hasRegionsInTransition()) {
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `this.balancer`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
   */
  public BalanceResponse balanceOrUpdateMetrics() throws IOException {
    synchronized (this.balancer) {
      BalanceResponse response = balance();
      if (!response.isBalancerRan()) {
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `this.snapshotCleanerChore`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
  void switchSnapshotCleanup(final boolean on, final boolean synchronous) throws IOException {
    if (synchronous) {
      synchronized (this.snapshotCleanerChore) {
        switchSnapshotCleanup(on);
      }
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `coprocessorNames`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java

  public static Set<String> getLoadedCoprocessors() {
    synchronized (coprocessorNames) {
      return new HashSet(coprocessorNames);
    }
```

### SynchronizeOnNonFinalField
Synchronization on a non-final field `task.batch`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.java`
#### Snippet
```java
          task.status = status;
          if (task.batch != null) {
            synchronized (task.batch) {
              if (status == SUCCESS) {
                task.batch.done++;
```

## RuleId[id=MissingDeprecatedAnnotation]
### MissingDeprecatedAnnotation
Missing '@Deprecated' annotation
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   *             instead.
   */
  default CompletableFuture<Boolean> balance(boolean forcible) {
    return balance(BalanceRequest.newBuilder().setIgnoreRegionsInTransition(forcible).build())
      .thenApply(BalanceResponse::isBalancerRan);
```

## RuleId[id=ConditionCoveredByFurtherCondition]
### ConditionCoveredByFurtherCondition
Condition 'obj == null' covered by subsequent condition '!(obj instanceof NonceKey)'
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/NonceKey.java`
#### Snippet
```java
  @Override
  public boolean equals(Object obj) {
    if (obj == null || !(obj instanceof NonceKey)) {
      return false;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'controller != null' covered by subsequent condition 'controller instanceof HBaseRpcController'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
      // TODO: Ideally we should not use an RpcController other than HBaseRpcController at client
      // side. And now we may use ServerRpcController.
      if (controller != null && controller instanceof HBaseRpcController) {
        hrc = (HBaseRpcController) controller;
        if (!hrc.hasCallTimeout()) {
```

### ConditionCoveredByFurtherCondition
Condition 'controller != null' covered by subsequent condition 'controller instanceof HBaseRpcController'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
    int channelOperationTimeout) {
    HBaseRpcController hrc;
    if (controller != null && controller instanceof HBaseRpcController) {
      hrc = (HBaseRpcController) controller;
      if (!hrc.hasCallTimeout()) {
```

### ConditionCoveredByFurtherCondition
Condition 'obj == null' covered by subsequent condition '!(obj instanceof BigDecimalComparator)'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BigDecimalComparator.java`
#### Snippet
```java
  @Override
  public boolean equals(Object obj) {
    if (obj == null || !(obj instanceof BigDecimalComparator)) {
      return false;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'exception == null' covered by subsequent condition '!(exception instanceof Throwable)'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
   */
  public static Throwable findException(Object exception) {
    if (exception == null || !(exception instanceof Throwable)) {
      return null;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'cur instanceof RegionMovedException' covered by subsequent condition 'cur instanceof NotServingRegionException'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java

  public static boolean isSpecialException(Throwable cur) {
    return (cur instanceof RegionMovedException || cur instanceof RegionOpeningException
      || cur instanceof RegionTooBusyException || cur instanceof RpcThrottlingException
      || cur instanceof MultiActionResultTooLarge || cur instanceof RetryImmediatelyException
```

### ConditionCoveredByFurtherCondition
Condition 'cur instanceof RegionOpeningException' covered by subsequent condition 'cur instanceof NotServingRegionException'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java

  public static boolean isSpecialException(Throwable cur) {
    return (cur instanceof RegionMovedException || cur instanceof RegionOpeningException
      || cur instanceof RegionTooBusyException || cur instanceof RpcThrottlingException
      || cur instanceof MultiActionResultTooLarge || cur instanceof RetryImmediatelyException
```

### ConditionCoveredByFurtherCondition
Condition 'cur instanceof MultiActionResultTooLarge' covered by subsequent condition 'cur instanceof RetryImmediatelyException'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
    return (cur instanceof RegionMovedException || cur instanceof RegionOpeningException
      || cur instanceof RegionTooBusyException || cur instanceof RpcThrottlingException
      || cur instanceof MultiActionResultTooLarge || cur instanceof RetryImmediatelyException
      || cur instanceof CallQueueTooBigException || cur instanceof CallDroppedException
      || cur instanceof NotServingRegionException || cur instanceof RequestTooBigException);
```

### ConditionCoveredByFurtherCondition
Condition 'cur instanceof CallQueueTooBigException' covered by subsequent condition 'cur instanceof CallDroppedException'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
      || cur instanceof RegionTooBusyException || cur instanceof RpcThrottlingException
      || cur instanceof MultiActionResultTooLarge || cur instanceof RetryImmediatelyException
      || cur instanceof CallQueueTooBigException || cur instanceof CallDroppedException
      || cur instanceof NotServingRegionException || cur instanceof RequestTooBigException);
  }
```

### ConditionCoveredByFurtherCondition
Condition 'cur instanceof RegionMovedException' covered by subsequent condition 'cur instanceof NotServingRegionException'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/ClientExceptionsUtil.java`
#### Snippet
```java
      return true;
    }
    return !isSpecialException(cur) || (cur instanceof RegionMovedException)
      || cur instanceof NotServingRegionException;
  }
```

### ConditionCoveredByFurtherCondition
Condition 'cause != null' covered by subsequent condition 'cause instanceof IOException'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java`
#### Snippet
```java

    Throwable cause = se.getCause();
    if (cause != null && cause instanceof IOException) {
      throw (IOException) cause;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'o == null' covered by subsequent condition '!(o instanceof TableSplit)'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableSplit.java`
#### Snippet
```java
  @Override
  public boolean equals(Object o) {
    if (o == null || !(o instanceof TableSplit)) {
      return false;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'split == null' covered by subsequent condition '!(split instanceof TableSplit)'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
  protected List<InputSplit> createNInputSplitsUniform(InputSplit split, int n)
    throws IllegalArgumentIOException {
    if (split == null || !(split instanceof TableSplit)) {
      throw new IllegalArgumentIOException(
        "InputSplit for CreateNSplitsPerRegion can not be null + "
```

### ConditionCoveredByFurtherCondition
Condition 'qop != null' covered by subsequent condition '"auth-conf".equalsIgnoreCase(...)'
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
    private boolean isNegotiatedQopPrivacy() {
      String qop = getNegotiatedQop();
      return qop != null && "auth-conf".equalsIgnoreCase(qop);
    }

```

### ConditionCoveredByFurtherCondition
Condition 'o == null' covered by subsequent condition '!(o instanceof TableSplit)'
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
  @Override
  public boolean equals(Object o) {
    if (o == null || !(o instanceof TableSplit)) {
      return false;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'lastGcInfo != null' covered by subsequent condition 'lastGcInfo instanceof CompositeData'
in `hbase-http/src/main/java/org/apache/hadoop/hbase/util/JSONMetricUtil.java`
#### Snippet
```java
    long lastGcDuration = 0;
    Object lastGcInfo = getValueFromMBean(gcCollector, "LastGcInfo");
    if (lastGcInfo != null && lastGcInfo instanceof CompositeData) {
      CompositeData cds = (CompositeData) lastGcInfo;
      lastGcDuration = (long) cds.get("duration");
```

### ConditionCoveredByFurtherCondition
Condition 'that == null' covered by subsequent condition '!(that instanceof BlockBucket)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    @Override
    public boolean equals(Object that) {
      if (that == null || !(that instanceof BlockBucket)) {
        return false;
      }
```

### ConditionCoveredByFurtherCondition
Condition 'e instanceof RegionServerAbortedException' covered by subsequent condition 'e instanceof RegionServerStoppedException'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
        return false;
      }
      if (e instanceof RegionServerAbortedException || e instanceof RegionServerStoppedException) {
        // A better way is to return true here to let the upper layer quit, and then schedule a
        // background task to check whether the region server is dead. And if it is dead, call
```

### ConditionCoveredByFurtherCondition
Condition 'parentProcedure != null' covered by subsequent condition 'parentProcedure instanceof ServerCrashProcedure'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java`
#### Snippet
```java
    Procedure parentProcedure =
      env.getMasterServices().getMasterProcedureExecutor().getProcedure(parentId);
    if (parentProcedure != null && parentProcedure instanceof ServerCrashProcedure) {
      ((ServerCrashProcedure) parentProcedure).updateProgress(false);
    }
```

### ConditionCoveredByFurtherCondition
Condition 'obj == null' covered by subsequent condition '!(obj instanceof AuthenticationKey)'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationKey.java`
#### Snippet
```java
  @Override
  public boolean equals(Object obj) {
    if (obj == null || !(obj instanceof AuthenticationKey)) {
      return false;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'master != null' covered by subsequent condition 'master instanceof HMaster'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
      if (MapUtils.isNotEmpty(params)) {
        Object master = params.get(HMaster.MASTER);
        if (master != null && master instanceof HMaster) {
          zkw = ((HMaster) master).getZooKeeper();
          shareZK = true;
```

### ConditionCoveredByFurtherCondition
Condition 'filter != null' covered by subsequent condition 'filter instanceof AccessControlFilter'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java`
#### Snippet
```java
    Filter filter = query.getFilter();
    // Don't wrap an AccessControlFilter
    if (filter != null && filter instanceof AccessControlFilter) {
      return;
    }
```

### ConditionCoveredByFurtherCondition
Condition 'nextWriter != null' covered by subsequent condition 'nextWriter instanceof ProtobufLogWriter'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java`
#### Snippet
```java
      logRollAndSetupWalProps(oldPath, newPath, oldFileLen);
      this.writer = nextWriter;
      if (nextWriter != null && nextWriter instanceof ProtobufLogWriter) {
        this.hdfs_out = ((ProtobufLogWriter) nextWriter).getStream();
      } else {
```

### ConditionCoveredByFurtherCondition
Condition 'isolationLevel != null' covered by subsequent condition 'isolationLevel == IsolationLevel.READ_UNCOMMITTED'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  /** Returns readpoint considering given IsolationLevel. Pass {@code null} for default */
  public long getReadPoint(IsolationLevel isolationLevel) {
    if (isolationLevel != null && isolationLevel == IsolationLevel.READ_UNCOMMITTED) {
      // This scan can read even uncommitted transactions
      return Long.MAX_VALUE;
```

## RuleId[id=UnnecessaryFullyQualifiedName]
### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellScanner.java`
#### Snippet
```java
 * </pre>
 * <p>
 * Often used reading {@link org.apache.hadoop.hbase.Cell}s written by
 * {@link org.apache.hadoop.hbase.io.CellOutputStream}.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/ByteBufferTag.java`
#### Snippet
```java

/**
 * This is a {@link Tag} implementation in which value is backed by {@link java.nio.ByteBuffer}
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java`
#### Snippet
```java
  byte[] getValueArray();

  /** Returns The {@link java.nio.ByteBuffer} containing the value bytes. */
  ByteBuffer getValueByteBuffer();

```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.concurrent` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/JitterScheduledThreadPoolExecutorImpl.java`
#### Snippet
```java

  @Override
  protected <V> java.util.concurrent.RunnableScheduledFuture<V> decorateTask(Runnable runnable,
    java.util.concurrent.RunnableScheduledFuture<V> task) {
    return new JitteredRunnableScheduledFuture<>(task);
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.concurrent` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/JitterScheduledThreadPoolExecutorImpl.java`
#### Snippet
```java
  @Override
  protected <V> java.util.concurrent.RunnableScheduledFuture<V> decorateTask(Runnable runnable,
    java.util.concurrent.RunnableScheduledFuture<V> task) {
    return new JitteredRunnableScheduledFuture<>(task);
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.concurrent` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/JitterScheduledThreadPoolExecutorImpl.java`
#### Snippet
```java

  @Override
  protected <V> java.util.concurrent.RunnableScheduledFuture<V> decorateTask(Callable<V> callable,
    java.util.concurrent.RunnableScheduledFuture<V> task) {
    return new JitteredRunnableScheduledFuture<>(task);
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.concurrent` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/JitterScheduledThreadPoolExecutorImpl.java`
#### Snippet
```java
  @Override
  protected <V> java.util.concurrent.RunnableScheduledFuture<V> decorateTask(Callable<V> callable,
    java.util.concurrent.RunnableScheduledFuture<V> task) {
    return new JitteredRunnableScheduledFuture<>(task);
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellBuilderFactory.java`
#### Snippet
```java
 * Create a CellBuilder instance. Currently, we have two kinds of Cell Builder.
 * {@link CellBuilderType#DEEP_COPY} All bytes array passed into builder will be copied to build an
 * new Cell. The cell impl is {@link org.apache.hadoop.hbase.KeyValue}
 * {@link CellBuilderType#SHALLOW_COPY} Just copy the references of passed bytes array to build an
 * new Cell The cell impl is {@link org.apache.hadoop.hbase.IndividualBytesFieldCell} NOTE: The cell
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/CellBuilderFactory.java`
#### Snippet
```java
 * new Cell. The cell impl is {@link org.apache.hadoop.hbase.KeyValue}
 * {@link CellBuilderType#SHALLOW_COPY} Just copy the references of passed bytes array to build an
 * new Cell The cell impl is {@link org.apache.hadoop.hbase.IndividualBytesFieldCell} NOTE: The cell
 * impl may be changed in the future. The user application SHOULD NOT depend on any concrete cell
 * impl.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.io` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java`
#### Snippet
```java
/**
 * A byte sequence that is usable as a key or value. Based on
 * {@link org.apache.hadoop.io.BytesWritable} only this class is NOT resizable and DOES NOT
 * distinguish between the size of the sequence and the current capacity as
 * {@link org.apache.hadoop.io.BytesWritable} does. Hence its comparatively 'immutable'. When
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.io` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java`
#### Snippet
```java
 * {@link org.apache.hadoop.io.BytesWritable} only this class is NOT resizable and DOES NOT
 * distinguish between the size of the sequence and the current capacity as
 * {@link org.apache.hadoop.io.BytesWritable} does. Hence its comparatively 'immutable'. When
 * creating a new instance of this class, the underlying byte [] is not copied, just referenced. The
 * backing buffer is accessed when we go to serialize.
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESEncryptor.java`
#### Snippet
```java
      init();
    }
    return new javax.crypto.CipherOutputStream(out, cipher);
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESEncryptor.java`
#### Snippet
```java
  private boolean initialized = false;

  public AESEncryptor(javax.crypto.Cipher cipher, SecureRandom rng) {
    this.cipher = cipher;
    this.rng = rng;
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESEncryptor.java`
#### Snippet
```java
  }

  javax.crypto.Cipher getCipher() {
    return cipher;
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESEncryptor.java`
#### Snippet
```java
        rng.nextBytes(iv);
      }
      cipher.init(javax.crypto.Cipher.ENCRYPT_MODE, key, new IvParameterSpec(iv));
    } catch (InvalidKeyException e) {
      throw new RuntimeException(e);
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESEncryptor.java`
#### Snippet
```java
public class AESEncryptor implements Encryptor {

  private javax.crypto.Cipher cipher;
  private SecureRandom rng;
  private Key key;
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java`
#### Snippet
```java
  }

  javax.crypto.Cipher getCipher() {
    return cipher;
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java`
#### Snippet
```java
      init();
    }
    return new javax.crypto.CipherInputStream(in, cipher);
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java`
#### Snippet
```java
public class AESDecryptor implements Decryptor {

  private javax.crypto.Cipher cipher;
  private Key key;
  private byte[] iv;
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java`
#### Snippet
```java
    Preconditions.checkState(iv != null, "IV is null");
    try {
      cipher.init(javax.crypto.Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));
    } catch (InvalidKeyException e) {
      throw new RuntimeException(e);
```

### UnnecessaryFullyQualifiedName
Qualifier `javax.crypto` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java`
#### Snippet
```java
  private boolean initialized = false;

  public AESDecryptor(javax.crypto.Cipher cipher) {
    this.cipher = cipher;
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
  public static void main(String[] args) throws Exception {
    Configuration conf = HBaseConfiguration.create();
    java.util.Map<String, CompressionCodec> implMap = new java.util.HashMap<>();
    for (Algorithm algo : Algorithm.class.getEnumConstants()) {
      try {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary, and can be replaced with an import
in `hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/Compression.java`
#### Snippet
```java
  public static void main(String[] args) throws Exception {
    Configuration conf = HBaseConfiguration.create();
    java.util.Map<String, CompressionCodec> implMap = new java.util.HashMap<>();
    for (Algorithm algo : Algorithm.class.getEnumConstants()) {
      try {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/PrettyPrinter.java`
#### Snippet
```java
  /**
   * Convert a human readable string to its value.
   * @see org.apache.hadoop.hbase.util.PrettyPrinter#format(String, Unit)
   * @return the value corresponding to the human readable string
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ChecksumType.java`
#### Snippet
```java
  public abstract String getName();

  /** Function to get corresponding {@link org.apache.hadoop.util.DataChecksum.Type}. */
  public abstract DataChecksum.Type getDataChecksumType();

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/WeakObjectPool.java`
#### Snippet
```java
 * A {@code WeakReference} based shared object pool. The objects are kept in weak references and
 * associated with keys which are identified by the {@code equals} method. The objects are created
 * by {@link org.apache.hadoop.hbase.util.ObjectPool.ObjectFactory} on demand. The object creation
 * is expected to be lightweight, and the objects may be excessively created and discarded. Thread
 * safe.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java`
#### Snippet
```java
 * control over the tests so that they behave the same when run in any system. (Refer:
 * <a href="https://issues.apache.org/jira/browse/HBASE-2578">HBASE-2578</a> - The issue which added
 * the {@link org.apache.hadoop.hbase.util.EnvironmentEdgeManager}). The idea is to have a central
 * place where time can be assigned in HBase. That makes it easier to inject different
 * implementations of time. The default environment edge is the Java Current Time in millis. The
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java`
#### Snippet
```java
 * implementations of time. The default environment edge is the Java Current Time in millis. The
 * environment edge manager class is designed to be able to plug in a new implementation of time by
 * simply injecting an implementation of {@link org.apache.hadoop.hbase.util.EnvironmentEdge}
 * interface to {@link org.apache.hadoop.hbase.util.EnvironmentEdgeManager}
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java`
#### Snippet
```java
 * environment edge manager class is designed to be able to plug in a new implementation of time by
 * simply injecting an implementation of {@link org.apache.hadoop.hbase.util.EnvironmentEdge}
 * interface to {@link org.apache.hadoop.hbase.util.EnvironmentEdgeManager}
 * <p>
 * <b>Problems with Environment Edge:</b><br>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java`
#### Snippet
```java
 * unexpected failures due to timeout or unintended flow of execution.<br>
 * </p>
 * Because of the above issues, only {@link org.apache.hadoop.hbase.util.DefaultEnvironmentEdge} is
 * being used, whose implementation of time returns the {@link System#currentTimeMillis()}. It is
 * advised not to inject any other {@link org.apache.hadoop.hbase.util.EnvironmentEdge}.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java`
#### Snippet
```java
 * Because of the above issues, only {@link org.apache.hadoop.hbase.util.DefaultEnvironmentEdge} is
 * being used, whose implementation of time returns the {@link System#currentTimeMillis()}. It is
 * advised not to inject any other {@link org.apache.hadoop.hbase.util.EnvironmentEdge}.
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang.management` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java`
#### Snippet
```java
  /**
   * Get the system load average
   * @see java.lang.management.OperatingSystemMXBean#getSystemLoadAverage
   */
  public double getSystemLoadAverage() {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java

  /**
   * Similar to {@link WritableUtils#readVLong(java.io.DataInput)} but reads from a
   * {@link ByteBuff}.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java`
#### Snippet
```java

  /**
   * Similar to {@link WritableUtils#writeVLong(java.io.DataOutput, long)}, but writes to a
   * {@link ByteBuffer}.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.security` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
 * This class provides a common interface for interacting with user and group information across
 * changing APIs in different versions of Hadoop. It only provides access to the common set of
 * functionality in {@link org.apache.hadoop.security.UserGroupInformation} currently needed by
 * HBase, but can be extended as needs change.
 * </p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
    /**
     * Create a user for testing.
     * @see User#createUserForTesting(org.apache.hadoop.conf.Configuration, String, String[])
     */
    public static User createUserForTesting(Configuration conf, String name, String[] groups) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
     * Obtain credentials for the current process using the configured Kerberos keytab file and
     * principal.
     * @see User#login(org.apache.hadoop.conf.Configuration, String, String, String)
     * @param conf             the Configuration to use
     * @param fileConfKey      Configuration property key used to store the path to the keytab file
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.security` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/User.java`
#### Snippet
```java
  /**
   * Bridges {@code User} invocations to underlying calls to
   * {@link org.apache.hadoop.security.UserGroupInformation} for secure Hadoop 0.20 and versions
   * 0.21 and above.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.thrift` is unnecessary and can be removed
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/thrift/MetricsThriftServerSourceImpl.java`
#### Snippet
```java

/**
 * Hadoop 2 version of {@link org.apache.hadoop.hbase.thrift.MetricsThriftServerSource} Implements
 * BaseSource through BaseSourceImpl, following the pattern
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java

  @Override
  public org.apache.hadoop.hbase.regionserver.MetricsUserSource createUser(String shortUserName) {
    return new org.apache.hadoop.hbase.regionserver.MetricsUserSourceImpl(shortUserName,
      getUserAggregate());
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceFactoryImpl.java`
#### Snippet
```java
  @Override
  public org.apache.hadoop.hbase.regionserver.MetricsUserSource createUser(String shortUserName) {
    return new org.apache.hadoop.hbase.regionserver.MetricsUserSourceImpl(shortUserName,
      getUserAggregate());
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.yetus.audience` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/AuthUtil.java`
#### Snippet
```java
   * @return a ScheduledChore for renewals, if needed, and null otherwise.
   * @deprecated Deprecated since 2.2.0, this method will be
   *             {@link org.apache.yetus.audience.InterfaceAudience.Private} use only after 4.0.0.
   * @see <a href="https://issues.apache.org/jira/browse/HBASE-20886">HBASE-20886</a>
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.yetus.audience` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/AuthUtil.java`
#### Snippet
```java
 * for Secure Operation</a>
 * @deprecated since 2.2.0, to be marked as
 *             {@link org.apache.yetus.audience.InterfaceAudience.Private} in 4.0.0.
 * @see <a href="https://issues.apache.org/jira/browse/HBASE-20886">HBASE-20886</a>
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver.wal` is unnecessary and can be removed
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/wal/MetricsWALSourceImpl.java`
#### Snippet
```java
 * Class that transitions metrics from MetricsWAL into the metrics subsystem. Implements BaseSource
 * through BaseSourceImpl, following the pattern.
 * @see org.apache.hadoop.hbase.regionserver.wal.MetricsWALSource
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/ExportEndpointExample.java`
#### Snippet
```java

/**
 * A simple example on how to use {@link org.apache.hadoop.hbase.coprocessor.Export}.
 * <p>
 * For the protocol buffer definition of the ExportService, see the source file located under
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary, and can be replaced with an import
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/thrift2/DemoClient.java`
#### Snippet
```java
      port = Integer.parseInt(args[1]);
    }
    org.apache.hadoop.conf.Configuration conf = HBaseConfiguration.create();
    String principal = conf.get("hbase.thrift.kerberos.principal");
    if (principal != null) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ZooKeeperScanPolicyObserver.java`
#### Snippet
```java
 * This is an example showing how a RegionObserver could configured via ZooKeeper in order to
 * control a Region compaction, flush, and scan policy. This also demonstrated the use of shared
 * {@link org.apache.hadoop.hbase.coprocessor.RegionObserver} state. See
 * {@link RegionCoprocessorEnvironment#getSharedData()}.
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.fs.Path} object representing the region directory under
   * path rootdir
   * @param rootdir    qualified path of HBase root directory
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
   * @param tableName  name of table
   * @param regionName The encoded region name
   * @return {@link org.apache.hadoop.fs.Path} for region
   */
  public static Path getRegionDir(Path rootdir, TableName tableName, String regionName) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.fs.Path} object representing the namespace directory under
   * path rootdir
   * @param rootdir   qualified path of HBase root directory
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
   * @param rootdir   qualified path of HBase root directory
   * @param namespace namespace name
   * @return {@link org.apache.hadoop.fs.Path} for table
   */
  public static Path getNamespaceDir(Path rootdir, final String namespace) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.fs.Path} object representing the table directory under
   * path rootdir
   * @param rootdir   qualified path of HBase root directory
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
   * @param rootdir   qualified path of HBase root directory
   * @param tableName name of table
   * @return {@link org.apache.hadoop.fs.Path} for table
   */
  public static Path getTableDir(Path rootdir, final TableName tableName) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.hbase.TableName} object representing the table directory
   * under path rootdir
   * @param tablePath path of table
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java`
#### Snippet
```java
   * under path rootdir
   * @param tablePath path of table
   * @return {@link org.apache.hadoop.fs.Path} for table
   */
  public static TableName getTableName(Path tablePath) {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterId.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#toString()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionLocation.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#equals(java.lang.Object)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionLocation.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#equals(java.lang.Object)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionLocation.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#toString()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/HRegionLocation.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#hashCode()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/DoNotRetryIOException.java`
#### Snippet
```java
/**
 * Subclass if exception is not meant to be retried: e.g.
 * {@link org.apache.hadoop.hbase.UnknownScannerException}
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RemoteWithExtrasException.java`
#### Snippet
```java
/**
 * A {@link RemoteException} with some extra information. If source exception was a
 * {@link org.apache.hadoop.hbase.DoNotRetryIOException}, {@link #isDoNotRetry()} will return true.
 * <p>
 * A {@link RemoteException} hosts exceptions we got from the server.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/CatalogFamilyFormat.java`
#### Snippet
```java
/**
 * Helper class for generating/parsing
 * {@value org.apache.hadoop.hbase.HConstants#CATALOG_FAMILY_STR} family cells in meta table.
 * <p/>
 * The cells in catalog family are:
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
      return;
    }
    if (controller instanceof org.apache.hadoop.hbase.ipc.ServerRpcController) {
      ((ServerRpcController) controller).setFailedOn(ioe);
    } else {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary, and can be replaced with an import
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
    final Message request, final byte[] row) {
    return CoprocessorServiceCall.newBuilder()
      .setRow(org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.unsafeWrap(row))
      .setServiceName(CoprocessorRpcUtils.getServiceName(method.getService()))
      .setMethodName(method.getName())
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary, and can be replaced with an import
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
      // TODO!!!!! Come back here after!!!!! This is a double copy of the request if I read
      // it right copying from non-shaded to shaded version!!!!!! FIXXXXX!!!!!
      .setRequest(org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations
        .unsafeWrap(request.toByteArray()))
      .build();
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorRpcUtils.java`
#### Snippet
```java
    // TODO: UGLY COPY IN HERE!!!!
    builder.setValue(builder.getValueBuilder().setName(result.getClass().getName()).setValue(
      org.apache.hbase.thirdparty.com.google.protobuf.ByteString.copyFrom(result.toByteArray())));
    return builder.build();
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcClientFactory.java`
#### Snippet
```java

/**
 * Factory to create a {@link org.apache.hadoop.hbase.ipc.RpcClient}
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
  /**
   * Return the pool type specified in the configuration, which must be set to either
   * {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#RoundRobin} or
   * {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#ThreadLocal}, otherwise default to the
   * former. For applications with many user threads, use a small round-robin pool. For applications
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
   * Return the pool type specified in the configuration, which must be set to either
   * {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#RoundRobin} or
   * {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#ThreadLocal}, otherwise default to the
   * former. For applications with many user threads, use a small round-robin pool. For applications
   * with few user threads, you may want to try using a thread-local pool. In any case, the number
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
   * former. For applications with many user threads, use a small round-robin pool. For applications
   * with few user threads, you may want to try using a thread-local pool. In any case, the number
   * of {@link org.apache.hadoop.hbase.ipc.RpcClient} instances should not exceed the operating
   * system's hard limit on the number of connections.
   * @param config configuration
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
   * system's hard limit on the number of connections.
   * @param config configuration
   * @return either a {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#RoundRobin} or
   *         {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#ThreadLocal}
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
   * @param config configuration
   * @return either a {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#RoundRobin} or
   *         {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#ThreadLocal}
   */
  private static PoolMap.PoolType getPoolType(Configuration config) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
  /**
   * Return the pool size specified in the configuration, which is applicable only if the pool type
   * is {@link org.apache.hadoop.hbase.util.PoolMap.PoolType#RoundRobin}.
   * @param config configuration
   * @return the maximum pool size
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCoprocessorRpcChannelImpl.java`
#### Snippet
```java
      CoprocessorRpcUtils.getCoprocessorServiceRequest(method, request);
    stub.execRegionServerService(controller, csr,
      new org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<
        CoprocessorServiceResponse>() {

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SyncCoprocessorRpcChannel.java`
#### Snippet
```java
 * Base class which provides clients with an RPC connection to call coprocessor endpoint
 * {@link com.google.protobuf.Service}s. Note that clients should not use this class directly,
 * except through {@link org.apache.hadoop.hbase.client.Table#coprocessorService(byte[])}.
 * @deprecated Please stop using this class again, as it is too low level, which is part of the rpc
 *             framework for HBase. Will be deleted in 4.0.0.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MasterRegistry.java`
#### Snippet
```java
/**
 * Master based registry implementation. Makes RPCs to the configured master addresses from config
 * {@value org.apache.hadoop.hbase.HConstants#MASTER_ADDRS_KEY}.
 * <p/>
 * It supports hedged reads, set the fan out of the requests batch by
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncConnection.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.conf.Configuration} object used by this instance.
   * <p>
   * The reference returned is not a copy, so any change made to it will affect this instance.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java`
#### Snippet
```java

/**
 * This subclass of {@link org.apache.hadoop.hbase.client.RetriesExhaustedException} is thrown when
 * we have more information about which rows were causing which exceptions on what servers. You can
 * call {@link #mayHaveClusterIssues()} and if the result is false, you have input error problems,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * <p/>
   * The given
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Callback#update(byte[],byte[],Object)}
   * method will be called with the return value from each region's invocation.
   * @param methodDescriptor  the descriptor for the protobuf service method to call.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * Creates an instance of the given {@link Service} subclass for each table region spanning the
   * range from the {@code startKey} row to {@code endKey} row (inclusive), and invokes the passed
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method with each
   * {@link Service} instance.
   * <p/>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * <p/>
   * The given
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Callback#update(byte[],byte[],Object)}
   * method will be called with the return value from each region's
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} invocation.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Callback#update(byte[],byte[],Object)}
   * method will be called with the return value from each region's
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} invocation.
   * @param service  the protocol buffer {@code Service} implementation to call
   * @param startKey start region selection with region containing this row. If {@code null}, the
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   *                 {@code null}, selection will continue through the last table region.
   * @param callable this instance's
   *                 {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method will
   *                 be invoked once per table region, using the {@link Service} instance connected
   *                 to that region.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * @param <T>      the {@link Service} subclass to connect to
   * @param <R>      Return type for the {@code callable} parameter's
   *                 {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
   * @deprecated since 3.0.0, will removed in 4.0.0. The batch call here references the blocking
   *             interface for of a protobuf stub, so it is not possible to do it in an asynchronous
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * Creates an instance of the given {@link Service} subclass for each table region spanning the
   * range from the {@code startKey} row to {@code endKey} row (inclusive), and invokes the passed
   * {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method with each
   * {@link Service} instance.
   * @param service  the protocol buffer {@code Service} implementation to call
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   *                 {@code null}, selection will continue through the last table region.
   * @param callable this instance's
   *                 {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method will
   *                 be invoked once per table region, using the {@link Service} instance connected
   *                 to that region.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * @param <T>      the {@link Service} subclass to connect to
   * @param <R>      Return type for the {@code callable} parameter's
   *                 {@link org.apache.hadoop.hbase.client.coprocessor.Batch.Call#call} method
   * @return a map of result values keyed by region name
   * @deprecated since 3.0.0, will removed in 4.0.0. The batch call here references the blocking
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.conf.Configuration} object used by this instance.
   * <p>
   * The reference returned is not a copy, so any change made to it will affect this instance.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java

  /**
   * Gets the {@link org.apache.hadoop.hbase.client.TableDescriptor table descriptor} for this
   * table.
   * @throws java.io.IOException if a remote or network exception occurs.
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * Gets the {@link org.apache.hadoop.hbase.client.TableDescriptor table descriptor} for this
   * table.
   * @throws java.io.IOException if a remote or network exception occurs.
   */
  TableDescriptor getDescriptor() throws IOException;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Table.java`
#### Snippet
```java
   * @return The data coming from the specified rows, if it exists. If the row specified doesn't
   *         exist, the {@link Result} instance returned won't contain any
   *         {@link org.apache.hadoop.hbase.Cell}s, as indicated by {@link Result#isEmpty()}. If
   *         there are any failures even after retries, there will be a <code>null</code> in the
   *         results' array for those Gets, AND an exception will be thrown. The ordering of the
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.exceptions` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/ColumnFamilyDescriptorBuilder.java`
#### Snippet
```java
     * @param timeToLive Time-to-live of cell contents, in seconds.
     * @return this (for chained invocation)
     * @throws org.apache.hadoop.hbase.exceptions.HBaseException exception
     */
    public ModifyableColumnFamilyDescriptor setTimeToLive(String timeToLive) throws HBaseException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
 * {@link #setBatch(int) setBatch}.
 * <p>
 * To add a filter, call {@link #setFilter(org.apache.hadoop.hbase.filter.Filter) setFilter}.
 * <p>
 * For small scan, it is deprecated in 2.0.0. Now we have a {@link #setLimit(int)} method in Scan
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
     * @param obj The object to compare
     * @return true if the contents of the the two descriptors exactly match
     * @see java.lang.Object#equals(java.lang.Object)
     */
    @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
     * @param obj The object to compare
     * @return true if the contents of the the two descriptors exactly match
     * @see java.lang.Object#equals(java.lang.Object)
     */
    @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.exceptions` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/TableDescriptorBuilder.java`
#### Snippet
```java
   * @param pbBytes A pb serialized TableDescriptor instance with pb magic prefix
   * @return This instance serialized with pb with pb magic prefix
   * @throws org.apache.hadoop.hbase.exceptions.DeserializationException if an error occurred
   */
  public static TableDescriptor parseFrom(byte[] pbBytes) throws DeserializationException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutator.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.conf.Configuration} object used by this instance.
   * <p>
   * The reference returned is not a copy, so any change made to it will affect this instance.
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Put.java`
#### Snippet
```java
   * immutable and its backing array will not be modified for the duration of this Put.
   * @param cell individual cell
   * @throws java.io.IOException e
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestControllerFactory.java`
#### Snippet
```java

  /**
   * Constructs a {@link org.apache.hadoop.hbase.client.RequestController}.
   * @param conf The {@link Configuration} to use.
   * @return A RequestController which is built according to the configuration.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestControllerFactory.java`
#### Snippet
```java

/**
 * A factory class that constructs an {@link org.apache.hadoop.hbase.client.RequestController}.
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestController.java`
#### Snippet
```java
   *                        depends on the implementation.
   * @param trigger         The object to call periodically.
   * @throws java.io.InterruptedIOException If the waiting is interrupted
   */
  void waitForFreeSlot(long id, int periodToTrigger, Consumer<Long> trigger)
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RequestController.java`
#### Snippet
```java
   *                        depends on the implementation.
   * @param trigger         The object to call periodically.
   * @throws java.io.InterruptedIOException If the waiting is interrupted
   */
  void waitForMaximumCurrentTasks(long max, long id, int periodToTrigger, Consumer<Long> trigger)
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java`
#### Snippet
```java
  /**
   * Construct the Delete with user defined data. NOTED: 1) all cells in the familyMap must have the
   * delete type. see {@link org.apache.hadoop.hbase.Cell.Type} 2) the row of each cell must be same
   * with passed row.
   * @param row       row. CAN'T be null
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncBufferedMutator.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.conf.Configuration} object used by this instance.
   * <p>
   * The reference returned is not a copy, so any change made to it will affect this instance.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncTable.java`
#### Snippet
```java

  /**
   * Returns the {@link org.apache.hadoop.conf.Configuration} object used by this instance.
   * <p>
   * The reference returned is not a copy, so any change made to it will affect this instance.
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java`
#### Snippet
```java
   * Add the specified KeyValue to this operation.
   * @param cell individual Cell
   * @throws java.io.IOException e
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/MasterCoprocessorRpcChannelImpl.java`
#### Snippet
```java
      CoprocessorRpcUtils.getCoprocessorServiceRequest(method, request);
    stub.execMasterService(controller, csr,
      new org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<
        CoprocessorServiceResponse>() {

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Batch.java`
#### Snippet
```java
   * region's coprocessor {@link org.apache.hbase.thirdparty.com.google.protobuf.Service}
   * implementation.
   * @see org.apache.hadoop.hbase.client.coprocessor.Batch
   * @see org.apache.hadoop.hbase.client.Table#coprocessorService(byte[])
   * @see org.apache.hadoop.hbase.client.Table#coprocessorService(Class, byte[], byte[], Batch.Call)
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client.coprocessor` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Batch.java`
#### Snippet
```java
   * @param <R> the return type from the associated {@link Batch.Call#call(Object)}
   * @see org.apache.hadoop.hbase.client.Table#coprocessorService(Class, byte[], byte[],
   *      org.apache.hadoop.hbase.client.coprocessor.Batch.Call)
   */
  @InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * {@link org.apache.hadoop.hbase.snapshot.SnapshotCreationException} indicating the duplicate
   * naming. Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}.
   * @param snapshotName name of the snapshot to be created
   * @param tableName    name of the table for which snapshot is created
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * @param columnFamily column family within a table. If not present, major compact the table's all
   *                     column families.
   * @param compactType  {@link org.apache.hadoop.hbase.client.CompactType}
   */
  CompletableFuture<Void> majorCompact(TableName tableName, byte[] columnFamily,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * type.
   * @param tableName   table to compact
   * @param compactType {@link org.apache.hadoop.hbase.client.CompactType}
   */
  CompletableFuture<Void> compact(TableName tableName, CompactType compactType);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * compaction, both, or none.
   * @param tableName   table to examine
   * @param compactType {@link org.apache.hadoop.hbase.client.CompactType}
   * @return the current compaction state wrapped by a {@link CompletableFuture}
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * @param tableName    table to compact
   * @param columnFamily column family within a table
   * @param compactType  {@link org.apache.hadoop.hbase.client.CompactType}
   */
  CompletableFuture<Void> compact(TableName tableName, byte[] columnFamily,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * {@link org.apache.hadoop.hbase.snapshot.SnapshotCreationException} indicating the duplicate
   * naming. Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}. You should
   * probably use {@link #snapshot(String, org.apache.hadoop.hbase.TableName)} unless you are sure
   * about the type of snapshot that you want to take.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * naming. Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}. You should
   * probably use {@link #snapshot(String, org.apache.hadoop.hbase.TableName)} unless you are sure
   * about the type of snapshot that you want to take.
   * @param snapshot snapshot to take
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * type.
   * @param tableName   table to major compact
   * @param compactType {@link org.apache.hadoop.hbase.client.CompactType}
   */
  CompletableFuture<Void> majorCompact(TableName tableName, CompactType compactType);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java`
#### Snippet
```java
   * parameters) will fail with a {@link org.apache.hadoop.hbase.snapshot.SnapshotCreationException}
   * indicating the duplicate naming. Snapshot names follow the same naming constraints as tables in
   * HBase. See {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}.
   * @param snapshotName name to give the snapshot on the filesystem. Must be unique from all other
   *                     snapshots stored on the cluster
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  protected byte[] columnQualifier;
  protected CompareOperator op;
  protected org.apache.hadoop.hbase.filter.ByteArrayComparable comparator;
  protected boolean foundColumn = false;
  protected boolean matchedColumn = false;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
  public SingleColumnValueFilter(final byte[] family, final byte[] qualifier,
    final CompareOperator op, final byte[] value) {
    this(family, qualifier, op, new org.apache.hadoop.hbase.filter.BinaryComparator(value));
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
   */
  protected SingleColumnValueFilter(final byte[] family, final byte[] qualifier,
    final CompareOperator op, org.apache.hadoop.hbase.filter.ByteArrayComparable comparator,
    final boolean filterIfMissing, final boolean latestVersionOnly) {
    this(family, qualifier, op, comparator);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java

  /** Returns the comparator */
  public org.apache.hadoop.hbase.filter.ByteArrayComparable getComparator() {
    return comparator;
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java

    final CompareOperator compareOp = CompareOperator.valueOf(proto.getCompareOp().name());
    final org.apache.hadoop.hbase.filter.ByteArrayComparable comparator;
    try {
      comparator = ProtobufUtil.toComparator(proto.getComparator());
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
   */
  public SingleColumnValueFilter(final byte[] family, final byte[] qualifier,
    final CompareOperator op, final org.apache.hadoop.hbase.filter.ByteArrayComparable comparator) {
    this.columnFamily = family;
    this.columnQualifier = qualifier;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java`
#### Snippet
```java
    byte[] qualifier = ParseFilter.removeQuotesFromByteArray(filterArguments.get(1));
    CompareOperator op = ParseFilter.createCompareOperator(filterArguments.get(2));
    org.apache.hadoop.hbase.filter.ByteArrayComparable comparator =
      ParseFilter.createComparator(ParseFilter.removeQuotesFromByteArray(filterArguments.get(3)));

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ColumnValueFilter.java`
#### Snippet
```java
 * <p>
 * This filter is used to filter cells based on column and value. It takes a
 * {@link org.apache.hadoop.hbase.CompareOperator} operator (<, <=, =, !=, >, >=), and and a
 * {@link ByteArrayComparable} comparator.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/Filter.java`
#### Snippet
```java
   * @see org.apache.hadoop.hbase.KeyValue#shallowCopy() The transformed KeyValue is what is
   *      eventually returned to the client. Most filters will return the passed KeyValue unchanged.
   * @see org.apache.hadoop.hbase.filter.KeyOnlyFilter#transformCell(Cell) for an example of a
   *      transformation. Concrete implementers can signal a failure condition in their code by
   *      throwing an {@link IOException}.
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.regex` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java`
#### Snippet
```java
 * </pre>
 * <p>
 * Supports {@link java.util.regex.Pattern} flags as well:
 * <p>
 *
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.regex` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java`
#### Snippet
```java
 * </pre>
 *
 * @see java.util.regex.Pattern
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.replication` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationPeerConfigUtil.java`
#### Snippet
```java
   * Helper method to add/removev base peer configs from Configuration to ReplicationPeerConfig This
   * merges the user supplied peer configuration
   * {@link org.apache.hadoop.hbase.replication.ReplicationPeerConfig} with peer configs provided as
   * property hbase.replication.peer.base.configs in hbase configuration. Expected format for this
   * hbase configuration is "k1=v1;k2=v2,v2_1;k3=""". If value is empty, it will remove the existing
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java`
#### Snippet
```java
 * A wrapper filter that returns true from {@link #filterAllRemaining()} as soon as the wrapped
 * filters {@link Filter#filterRowKey(Cell)},
 * {@link Filter#filterCell(org.apache.hadoop.hbase.Cell)},
 * {@link org.apache.hadoop.hbase.filter.Filter#filterRow()} or
 * {@link org.apache.hadoop.hbase.filter.Filter#filterAllRemaining()} methods returns true.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java`
#### Snippet
```java
 * filters {@link Filter#filterRowKey(Cell)},
 * {@link Filter#filterCell(org.apache.hadoop.hbase.Cell)},
 * {@link org.apache.hadoop.hbase.filter.Filter#filterRow()} or
 * {@link org.apache.hadoop.hbase.filter.Filter#filterAllRemaining()} methods returns true.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java`
#### Snippet
```java
 * {@link Filter#filterCell(org.apache.hadoop.hbase.Cell)},
 * {@link org.apache.hadoop.hbase.filter.Filter#filterRow()} or
 * {@link org.apache.hadoop.hbase.filter.Filter#filterAllRemaining()} methods returns true.
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java
 * </p>
 * <p>
 * This filter can be wrapped with {@link org.apache.hadoop.hbase.filter.WhileMatchFilter} and
 * {@link org.apache.hadoop.hbase.filter.SkipFilter} to add more control.
 * </p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java
 * <p>
 * This filter can be wrapped with {@link org.apache.hadoop.hbase.filter.WhileMatchFilter} and
 * {@link org.apache.hadoop.hbase.filter.SkipFilter} to add more control.
 * </p>
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java`
#### Snippet
```java
 * </p>
 * <p>
 * Multiple filters can be combined using {@link org.apache.hadoop.hbase.filter.FilterList}.
 * </p>
 * If an already known column family is looked for, use
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.util` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryComparator.java`
#### Snippet
```java
/**
 * A binary comparator which lexicographically compares against the specified byte array using
 * {@link org.apache.hadoop.hbase.util.Bytes#compareTo(byte[], byte[])}.
 * @since 2.0.0
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/BinaryComparator.java`
#### Snippet
```java
@InterfaceAudience.Public
@SuppressWarnings("ComparableType") // Should this move to Comparator usage?
public class BinaryComparator extends org.apache.hadoop.hbase.filter.ByteArrayComparable {
  /**
   * Constructor
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.filter` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FilterListWithOR.java`
#### Snippet
```java
   * next family for RegionScanner, INCLUDE_AND_NEXT_ROW is the same. so we should pass current cell
   * to the filter, if row mismatch or row match but column family mismatch. (HBASE-18368)
   * @see org.apache.hadoop.hbase.filter.Filter.ReturnCode
   * @param subFilter   which sub-filter to calculate the return code by using previous cell and
   *                    previous return code.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.security` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/SaslUtil.java`
#### Snippet
```java

  /**
   * Returns {@link org.apache.hadoop.hbase.security.SaslUtil.QualityOfProtection} corresponding to
   * the given {@code stringQop} value.
   * @throws IllegalArgumentException If stringQop doesn't match any QOP.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/ShadedAccessControlUtil.java`
#### Snippet
```java
      AccessControlProtos.NamespacePermission.Builder builder =
        AccessControlProtos.NamespacePermission.newBuilder();
      builder.setNamespaceName(org.apache.hbase.thirdparty.com.google.protobuf.ByteString
        .copyFromUtf8(nsPerm.getNamespace()));
      Permission.Action[] actions = perm.getActions();
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/ShadedAccessControlUtil.java`
#### Snippet
```java
  }

  public static org.apache.hadoop.hbase.TableName toTableName(HBaseProtos.TableName tableNamePB) {
    return org.apache.hadoop.hbase.TableName.valueOf(
      tableNamePB.getNamespace().asReadOnlyByteBuffer(),
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/ShadedAccessControlUtil.java`
#### Snippet
```java

  public static org.apache.hadoop.hbase.TableName toTableName(HBaseProtos.TableName tableNamePB) {
    return org.apache.hadoop.hbase.TableName.valueOf(
      tableNamePB.getNamespace().asReadOnlyByteBuffer(),
      tableNamePB.getQualifier().asReadOnlyByteBuffer());
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
  /**
   * Creates a protocol buffer IsSplitOrMergeEnabledRequest
   * @param switchType see {@link org.apache.hadoop.hbase.client.MasterSwitchType}
   * @return a IsSplitOrMergeEnabledRequest
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
   * @param enabled     switch is enabled or not
   * @param synchronous set switch sync?
   * @param switchTypes see {@link org.apache.hadoop.hbase.client.MasterSwitchType}, it is a list.
   * @return a SetSplitOrMergeEnabledRequest
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
        RegionCoprocessorServiceExec exec = (RegionCoprocessorServiceExec) row;
        // DUMB COPY!!! FIX!!! Done to copy from c.g.p.ByteString to shaded ByteString.
        org.apache.hbase.thirdparty.com.google.protobuf.ByteString value =
          org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations
            .unsafeWrap(exec.getRequest().toByteArray());
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/RequestConverter.java`
#### Snippet
```java
        // DUMB COPY!!! FIX!!! Done to copy from c.g.p.ByteString to shaded ByteString.
        org.apache.hbase.thirdparty.com.google.protobuf.ByteString value =
          org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations
            .unsafeWrap(exec.getRequest().toByteArray());
        if (cpBuilder == null) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary, and can be replaced with an import
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/FailedSanityCheckException.java`
#### Snippet
```java
 */
@InterfaceAudience.Public
public class FailedSanityCheckException extends org.apache.hadoop.hbase.DoNotRetryIOException {

  private static final long serialVersionUID = 1788783640409186240L;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary, and can be replaced with an import
in `hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/UnknownProtocolException.java`
#### Snippet
```java
@SuppressWarnings("serial")
@InterfaceAudience.Public
public class UnknownProtocolException extends org.apache.hadoop.hbase.DoNotRetryIOException {
  private Class<?> protocol;

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java

  static String getEffectiveLevel(String loggerName) {
    org.apache.logging.log4j.Logger logger =
      org.apache.logging.log4j.LogManager.getLogger(loggerName);
    return logger.getLevel().name();
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
  static String getEffectiveLevel(String loggerName) {
    org.apache.logging.log4j.Logger logger =
      org.apache.logging.log4j.LogManager.getLogger(loggerName);
    return logger.getLevel().name();
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
  static Set<File> getActiveLogFiles() throws IOException {
    Set<File> ret = new HashSet<>();
    org.apache.logging.log4j.Logger logger = org.apache.logging.log4j.LogManager.getRootLogger();
    if (!(logger instanceof org.apache.logging.log4j.core.Logger)) {
      return ret;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
  static Set<File> getActiveLogFiles() throws IOException {
    Set<File> ret = new HashSet<>();
    org.apache.logging.log4j.Logger logger = org.apache.logging.log4j.LogManager.getRootLogger();
    if (!(logger instanceof org.apache.logging.log4j.core.Logger)) {
      return ret;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
    Set<File> ret = new HashSet<>();
    org.apache.logging.log4j.Logger logger = org.apache.logging.log4j.LogManager.getRootLogger();
    if (!(logger instanceof org.apache.logging.log4j.core.Logger)) {
      return ret;
    }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
      return ret;
    }
    org.apache.logging.log4j.core.Logger coreLogger = (org.apache.logging.log4j.core.Logger) logger;
    for (org.apache.logging.log4j.core.Appender appender : coreLogger.getAppenders().values()) {
      if (appender instanceof org.apache.logging.log4j.core.appender.FileAppender) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
      return ret;
    }
    org.apache.logging.log4j.core.Logger coreLogger = (org.apache.logging.log4j.core.Logger) logger;
    for (org.apache.logging.log4j.core.Appender appender : coreLogger.getAppenders().values()) {
      if (appender instanceof org.apache.logging.log4j.core.appender.FileAppender) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
    }
    org.apache.logging.log4j.core.Logger coreLogger = (org.apache.logging.log4j.core.Logger) logger;
    for (org.apache.logging.log4j.core.Appender appender : coreLogger.getAppenders().values()) {
      if (appender instanceof org.apache.logging.log4j.core.appender.FileAppender) {
        String fileName =
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
    org.apache.logging.log4j.core.Logger coreLogger = (org.apache.logging.log4j.core.Logger) logger;
    for (org.apache.logging.log4j.core.Appender appender : coreLogger.getAppenders().values()) {
      if (appender instanceof org.apache.logging.log4j.core.appender.FileAppender) {
        String fileName =
          ((org.apache.logging.log4j.core.appender.FileAppender) appender).getFileName();
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
      if (appender instanceof org.apache.logging.log4j.core.appender.FileAppender) {
        String fileName =
          ((org.apache.logging.log4j.core.appender.FileAppender) appender).getFileName();
        ret.add(new File(fileName));
      } else if (appender instanceof org.apache.logging.log4j.core.appender.AbstractFileAppender) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
          ((org.apache.logging.log4j.core.appender.FileAppender) appender).getFileName();
        ret.add(new File(fileName));
      } else if (appender instanceof org.apache.logging.log4j.core.appender.AbstractFileAppender) {
        String fileName =
          ((org.apache.logging.log4j.core.appender.AbstractFileAppender<?>) appender).getFileName();
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
      } else if (appender instanceof org.apache.logging.log4j.core.appender.AbstractFileAppender) {
        String fileName =
          ((org.apache.logging.log4j.core.appender.AbstractFileAppender<?>) appender).getFileName();
        ret.add(new File(fileName));
      } else if (appender instanceof org.apache.logging.log4j.core.appender.RollingFileAppender) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
          ((org.apache.logging.log4j.core.appender.AbstractFileAppender<?>) appender).getFileName();
        ret.add(new File(fileName));
      } else if (appender instanceof org.apache.logging.log4j.core.appender.RollingFileAppender) {
        String fileName =
          ((org.apache.logging.log4j.core.appender.RollingFileAppender) appender).getFileName();
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
      } else if (appender instanceof org.apache.logging.log4j.core.appender.RollingFileAppender) {
        String fileName =
          ((org.apache.logging.log4j.core.appender.RollingFileAppender) appender).getFileName();
        ret.add(new File(fileName));
      } else
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
        ret.add(new File(fileName));
      } else
        if (appender instanceof org.apache.logging.log4j.core.appender.RandomAccessFileAppender) {
          String fileName =
            ((org.apache.logging.log4j.core.appender.RandomAccessFileAppender) appender)
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
        if (appender instanceof org.apache.logging.log4j.core.appender.RandomAccessFileAppender) {
          String fileName =
            ((org.apache.logging.log4j.core.appender.RandomAccessFileAppender) appender)
              .getFileName();
          ret.add(new File(fileName));
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
          ret.add(new File(fileName));
        } else
          if (appender instanceof org.apache.logging.log4j.core.appender.MemoryMappedFileAppender) {
            String fileName =
              ((org.apache.logging.log4j.core.appender.MemoryMappedFileAppender) appender)
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.appender` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
          if (appender instanceof org.apache.logging.log4j.core.appender.MemoryMappedFileAppender) {
            String fileName =
              ((org.apache.logging.log4j.core.appender.MemoryMappedFileAppender) appender)
                .getFileName();
            ret.add(new File(fileName));
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java

  static void setLogLevel(String loggerName, String levelName) {
    org.apache.logging.log4j.Level level =
      org.apache.logging.log4j.Level.toLevel(levelName.toUpperCase());
    if (!level.toString().equalsIgnoreCase(levelName)) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
  static void setLogLevel(String loggerName, String levelName) {
    org.apache.logging.log4j.Level level =
      org.apache.logging.log4j.Level.toLevel(levelName.toUpperCase());
    if (!level.toString().equalsIgnoreCase(levelName)) {
      throw new IllegalArgumentException("Unsupported log level " + levelName);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.logging.log4j.core.config` is unnecessary, and can be replaced with an import
in `hbase-logging/src/main/java/org/apache/hadoop/hbase/logging/InternalLog4jUtils.java`
#### Snippet
```java
      throw new IllegalArgumentException("Unsupported log level " + levelName);
    }
    org.apache.logging.log4j.core.config.Configurator.setLevel(loggerName, level);
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.metrics` is unnecessary and can be removed
in `hbase-metrics/src/main/java/org/apache/hadoop/hbase/metrics/impl/CounterImpl.java`
#### Snippet
```java

/**
 * Custom implementation of {@link org.apache.hadoop.hbase.metrics.Counter} using LongAdder.
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary, and can be replaced with an import
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/ClusterStatusTracker.java`
#### Snippet
```java
  static byte[] toByteArray() {
    ZooKeeperProtos.ClusterUp.Builder builder = ZooKeeperProtos.ClusterUp.newBuilder();
    builder.setStartDate(new java.util.Date().toString());
    return ProtobufUtil.prependPBMagic(builder.build().toByteArray());
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.zookeeper` is unnecessary and can be removed
in `hbase-zookeeper/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java`
#### Snippet
```java

  /**
   * Convert Iterable of {@link org.apache.zookeeper.Op} we got into the ZooKeeper.Op instances to
   * actually pass to multi (need to do this in order to appendMetaData).
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * Get the current compaction state of a table. It could be in a compaction, or none.
   * @param tableName   table to examine
   * @param compactType {@link org.apache.hadoop.hbase.client.CompactType}
   * @return the current compaction state
   * @throws IOException if a remote or network exception occurs
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * parameters) will fail with a {@link SnapshotCreationException} indicating the duplicate naming.
   * Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}. Snapshot can
   * live with ttl seconds.
   * @param snapshotName  name to give the snapshot on the filesystem. Must be unique from all other
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * @param tableName    table to compact
   * @param columnFamily column family within a table
   * @param compactType  {@link org.apache.hadoop.hbase.client.CompactType}
   * @throws IOException if not a mob column family or if a remote or network exception occurs
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * @param name name of namespace descriptor
   * @return A descriptor
   * @throws org.apache.hadoop.hbase.NamespaceNotFoundException if the namespace was not found
   * @throws IOException                                        if a remote or network exception
   *                                                            occurs
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
  /**
   * Disable table and wait on completion. May timeout eventually. Use
   * {@link #disableTableAsync(org.apache.hadoop.hbase.TableName)} and
   * {@link #isTableDisabled(org.apache.hadoop.hbase.TableName)} instead. The table has to be in
   * enabled state for it to be disabled.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * Disable table and wait on completion. May timeout eventually. Use
   * {@link #disableTableAsync(org.apache.hadoop.hbase.TableName)} and
   * {@link #isTableDisabled(org.apache.hadoop.hbase.TableName)} instead. The table has to be in
   * enabled state for it to be disabled.
   * @throws IOException There could be couple types of IOException TableNotFoundException means the
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.snapshot` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * The cluster only knows about the most recent snapshot. Therefore, if another snapshot has been
   * run/started since the snapshot you are checking, you will receive an
   * {@link org.apache.hadoop.hbase.snapshot.UnknownSnapshotException}.
   * @param snapshot description of the snapshot to check
   * @return <tt>true</tt> if the snapshot is completed, <tt>false</tt> if the snapshot is still
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.snapshot` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   *         running
   * @throws IOException                                               if we have a network issue
   * @throws org.apache.hadoop.hbase.snapshot.HBaseSnapshotException   if the snapshot failed
   * @throws org.apache.hadoop.hbase.snapshot.UnknownSnapshotException if the requested snapshot is
   *                                                                   unknown
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.snapshot` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * @throws IOException                                               if we have a network issue
   * @throws org.apache.hadoop.hbase.snapshot.HBaseSnapshotException   if the snapshot failed
   * @throws org.apache.hadoop.hbase.snapshot.UnknownSnapshotException if the requested snapshot is
   *                                                                   unknown
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * parameters) will fail with a {@link SnapshotCreationException} indicating the duplicate naming.
   * Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}. Snapshot can
   * live with ttl seconds.
   * @param snapshotName  name to give the snapshot on the filesystem. Must be unique from all other
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * while).
   * @param tableName   table to compact
   * @param compactType {@link org.apache.hadoop.hbase.client.CompactType}
   * @throws IOException if a remote or network exception occurs
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java

  /**
   * Enable a table. May timeout. Use {@link #enableTableAsync(org.apache.hadoop.hbase.TableName)}
   * and {@link #isTableEnabled(org.apache.hadoop.hbase.TableName)} instead. The table has to be in
   * disabled state for it to be enabled.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
  /**
   * Enable a table. May timeout. Use {@link #enableTableAsync(org.apache.hadoop.hbase.TableName)}
   * and {@link #isTableEnabled(org.apache.hadoop.hbase.TableName)} instead. The table has to be in
   * disabled state for it to be enabled.
   * @param tableName name of the table
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   *                     table doesn't exist. TableNotDisabledException means the table isn't in
   *                     disabled state.
   * @see #isTableEnabled(org.apache.hadoop.hbase.TableName)
   * @see #disableTable(org.apache.hadoop.hbase.TableName)
   * @see #enableTableAsync(org.apache.hadoop.hbase.TableName)
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   *                     disabled state.
   * @see #isTableEnabled(org.apache.hadoop.hbase.TableName)
   * @see #disableTable(org.apache.hadoop.hbase.TableName)
   * @see #enableTableAsync(org.apache.hadoop.hbase.TableName)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * @see #isTableEnabled(org.apache.hadoop.hbase.TableName)
   * @see #disableTable(org.apache.hadoop.hbase.TableName)
   * @see #enableTableAsync(org.apache.hadoop.hbase.TableName)
   */
  default void enableTable(TableName tableName) throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * parameters) will fail with a {@link SnapshotCreationException} indicating the duplicate naming.
   * Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}.
   * @param snapshotName name to give the snapshot on the filesystem. Must be unique from all other
   *                     snapshots stored on the cluster
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * then it returns. It does not wait on the completion of Compaction (it can take a while).
   * @param tableName   table to compact
   * @param compactType {@link org.apache.hadoop.hbase.client.CompactType}
   * @throws IOException if a remote or network exception occurs
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * Helper that delegates to getClusterMetrics().getMasterCoprocessorNames().
   * @return an array of master coprocessors
   * @see org.apache.hadoop.hbase.ClusterMetrics#getMasterCoprocessorNames()
   */
  default List<String> getMasterCoprocessorNames() throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * {@link SnapshotCreationException} indicating the duplicate naming. Snapshot names follow the
   * same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}. You should
   * probably use {@link #snapshot(String, org.apache.hadoop.hbase.TableName)} unless you are sure
   * about the type of snapshot that you want to take.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}. You should
   * probably use {@link #snapshot(String, org.apache.hadoop.hbase.TableName)} unless you are sure
   * about the type of snapshot that you want to take.
   * @param snapshot snapshot to take
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.snapshot` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * <b>the name of the snapshot</b>. Attempts to take a snapshot with the same name (even a
   * different type or with different parameters) will fail with a
   * {@link org.apache.hadoop.hbase.snapshot.SnapshotCreationException} indicating the duplicate
   * naming. Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * {@link org.apache.hadoop.hbase.snapshot.SnapshotCreationException} indicating the duplicate
   * naming. Snapshot names follow the same naming constraints as tables in HBase. See
   * {@link org.apache.hadoop.hbase.TableName#isLegalFullyQualifiedTableName(byte[])}.
   * @param snapshotName name of the snapshot to be created
   * @param tableName    name of the table for which snapshot is created
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.snapshot` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * @throws IOException                                                if a remote or network
   *                                                                    exception occurs
   * @throws org.apache.hadoop.hbase.snapshot.SnapshotCreationException if snapshot creation failed
   * @throws IllegalArgumentException                                   if the snapshot request is
   *                                                                    formatted incorrectly
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java`
#### Snippet
```java
   * @param tableName    table to compact
   * @param columnFamily column family within a table
   * @param compactType  {@link org.apache.hadoop.hbase.client.CompactType}
   * @throws IOException if not a mob column family or if a remote or network exception occurs
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `com.google.protobuf` is unnecessary, and can be replaced with an import
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java

    // These are the protobuf classes coming from Hadoop. Not the one from hbase-shaded-protobuf
    protobufMessageLiteClass = com.google.protobuf.MessageLite.class;
    protobufMessageLiteBuilderClass = com.google.protobuf.MessageLite.Builder.class;

```

### UnnecessaryFullyQualifiedName
Qualifier `com.google.protobuf` is unnecessary, and can be replaced with an import
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/ProtobufDecoder.java`
#### Snippet
```java
    // These are the protobuf classes coming from Hadoop. Not the one from hbase-shaded-protobuf
    protobufMessageLiteClass = com.google.protobuf.MessageLite.class;
    protobufMessageLiteBuilderClass = com.google.protobuf.MessageLite.Builder.class;

    try {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapred` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReader.java`
#### Snippet
```java

  /**
   * @see org.apache.hadoop.mapred.RecordReader#createValue()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapred` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReader.java`
#### Snippet
```java

  /**
   * @see org.apache.hadoop.mapred.RecordReader#createKey()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapred` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java`
#### Snippet
```java
   * @param numSplits a hint to calculate the number of splits (mapred.map.tasks).
   * @return the input splits
   * @see InputFormat#getSplits(org.apache.hadoop.mapred.JobConf, int)
   */
  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java`
#### Snippet
```java
   * Configure conf to read from snapshotScans, with snapshots restored to a subdirectory of
   * restoreDir. Sets:
   * {@link org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormatImpl#RESTORE_DIRS_KEY},
   * {@link org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormatImpl#SNAPSHOT_TO_SCANS_KEY}
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java`
#### Snippet
```java
   * restoreDir. Sets:
   * {@link org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormatImpl#RESTORE_DIRS_KEY},
   * {@link org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormatImpl#SNAPSHOT_TO_SCANS_KEY}
   */
  public static void setInput(Configuration conf, Map<String, Collection<Scan>> snapshotScans,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapred` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapred/MultiTableSnapshotInputFormat.java`
#### Snippet
```java
/**
 * MultiTableSnapshotInputFormat generalizes
 * {@link org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat} allowing a MapReduce job to run
 * over one or more table snapshots, with one or more scans configured for each. Internally, the
 * input format delegates to {@link org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat} and
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ExportUtils.java`
#### Snippet
```java

/**
 * Some helper methods are used by {@link org.apache.hadoop.hbase.mapreduce.Export} and
 * org.apache.hadoop.hbase.coprocessor.Export (in hbase-endpooint).
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java`
#### Snippet
```java

  @Override
  protected void reduce(ImmutableBytesWritable rowKey, java.lang.Iterable<Text> lines,
    Reducer<ImmutableBytesWritable, Text, ImmutableBytesWritable, KeyValue>.Context context)
    throws java.io.IOException, InterruptedException {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java`
#### Snippet
```java
  protected void reduce(ImmutableBytesWritable rowKey, java.lang.Iterable<Text> lines,
    Reducer<ImmutableBytesWritable, Text, ImmutableBytesWritable, KeyValue>.Context context)
    throws java.io.IOException, InterruptedException {
    // although reduce() is called per-row, handle pathological case
    long threshold = context.getConfiguration().getLong("reducer.row.threshold", 1L * (1 << 30));
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableSnapshotInputFormat.java`
#### Snippet
```java
 * Internally, this input format restores each snapshot into a subdirectory of the given tmp
 * directory. Input splits and record readers are created as described in
 * {@link org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat} (one per region). See
 * {@link TableSnapshotInputFormat} for more notes on permissioning; the same caveats apply here.
 * @see TableSnapshotInputFormat
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @return The list of input splits.
   * @throws IOException When creating the list of splits fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits( org.apache.hadoop.mapreduce.JobContext)
   */
  public List<InputSplit> calculateAutoBalancedSplits(List<InputSplit> splits,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @return The list of input splits.
   * @throws IOException When creating the list of splits fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits( org.apache.hadoop.mapreduce.JobContext)
   */
  public List<InputSplit> calculateAutoBalancedSplits(List<InputSplit> splits,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @return The list of input splits.
   * @throws IOException When creating the list of splits fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits( org.apache.hadoop.mapreduce.JobContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @return The list of input splits.
   * @throws IOException When creating the list of splits fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits( org.apache.hadoop.mapreduce.JobContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @return The newly created record reader.
   * @throws IOException When creating the reader fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(
   *      org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @throws IOException When creating the reader fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(
   *      org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java`
#### Snippet
```java
   * @throws IOException When creating the reader fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(
   *      org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `com.google.protobuf` is unnecessary, and can be replaced with an import
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java

        // Try the unrelocated ByteString
        Class<?> byteStringClass = com.google.protobuf.ByteString.class;
        try {
          // See if it can load the relocated ByteString, which comes from hadoop-thirdparty.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.common.base` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java`
#### Snippet
```java
      }
      TableMapReduceUtil.addDependencyJarsForClasses(job.getConfiguration(),
        org.apache.hbase.thirdparty.com.google.common.base.Preconditions.class);
    } else {
      // output to live cluster
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java`
#### Snippet
```java
   * Returns the current configuration.
   * @return The current configuration.
   * @see org.apache.hadoop.conf.Configurable#getConf()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java`
#### Snippet
```java
   * Sets the configuration. This is used to set the details for the tables to be scanned.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java`
#### Snippet
```java
   * Sets the configuration. This is used to set the details for the tables to be scanned.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.wal` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/WALInputFormat.java`
#### Snippet
```java

/**
 * Simple {@link InputFormat} for {@link org.apache.hadoop.hbase.wal.WAL} files.
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java`
#### Snippet
```java
   * @return The list of input splits.
   * @throws IOException When creating the list of splits fails.
   * @see InputFormat#getSplits(org.apache.hadoop.mapreduce.JobContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java
   * Sets the configuration. This is used to determine the start keys for the given table.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java
   * Sets the configuration. This is used to determine the start keys for the given table.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java
   * Returns the current configuration.
   * @return The current configuration.
   * @see org.apache.hadoop.conf.Configurable#getConf()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java
   * @param numPartitions The total number of partitions.
   * @return The partition number for the <code>key</code>.
   * @see org.apache.hadoop.mapreduce.Partitioner#getPartition( java.lang.Object, java.lang.Object,
   *      int)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java
   * @param numPartitions The total number of partitions.
   * @return The partition number for the <code>key</code>.
   * @see org.apache.hadoop.mapreduce.Partitioner#getPartition( java.lang.Object, java.lang.Object,
   *      int)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java`
#### Snippet
```java
   * @param numPartitions The total number of partitions.
   * @return The partition number for the <code>key</code>.
   * @see org.apache.hadoop.mapreduce.Partitioner#getPartition( java.lang.Object, java.lang.Object,
   *      int)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapper.java`
#### Snippet
```java
 * @param <KEYOUT>   The type of the key.
 * @param <VALUEOUT> The type of the value.
 * @see org.apache.hadoop.mapreduce.Mapper
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java`
#### Snippet
```java
   * Sets the configuration. This is used to set up the grouping details.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java`
#### Snippet
```java
   * Sets the configuration. This is used to set up the grouping details.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java`
#### Snippet
```java
   * Returns the current configuration.
   * @return The current configuration.
   * @see org.apache.hadoop.conf.Configurable#getConf()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableReducer.java`
#### Snippet
```java
 * @param <VALUEIN> The type of the input value.
 * @param <KEYOUT>  The type of the output key.
 * @see org.apache.hadoop.mapreduce.Reducer
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableHFileOutputFormat.java`
#### Snippet
```java
   * function will configure the requisite number of reducers to write HFiles for multple tables
   * simultaneously
   * @param job                   See {@link org.apache.hadoop.mapreduce.Job}
   * @param multiTableDescriptors Table descriptor and region locator pairs
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.common.base` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java`
#### Snippet
```java
        TableMapReduceUtil.addDependencyJars(job);
        TableMapReduceUtil.addDependencyJarsForClasses(job.getConfiguration(),
          org.apache.hbase.thirdparty.com.google.common.base.Function.class /*
                                                                             * Guava used by
                                                                             * TsvParser
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java`
#### Snippet
```java
    }

    public class Reader implements java.io.Closeable {
      private final Configuration conf;

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java`
#### Snippet
```java
   * Returns the current configuration.
   * @return The current configuration.
   * @see org.apache.hadoop.conf.Configurable#getConf()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java`
#### Snippet
```java
   * @return The list of input splits.
   * @throws IOException When creating the list of splits fails.
   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits( org.apache.hadoop.mapreduce.JobContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java`
#### Snippet
```java
   * Sets the configuration. This is used to set the details for the table to be scanned.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java`
#### Snippet
```java
   * Sets the configuration. This is used to set the details for the table to be scanned.
   * @param configuration The configuration to set.
   * @see org.apache.hadoop.conf.Configurable#setConf( org.apache.hadoop.conf.Configuration)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableSnapshotInputFormatImpl.java`
#### Snippet
```java
/**
 * Shared implementation of mapreduce code over multiple table snapshots. Utilized by both mapreduce
 * {@link org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormat} and mapred
 * {@link org.apache.hadoop.hbase.mapred.MultiTableSnapshotInputFormat} implementations.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * The current progress of the record reader through its data.
   * @return A number between 0.0 and 1.0, the fraction of the data read.
   * @see org.apache.hadoop.mapreduce.RecordReader#getProgress()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * @return The current key.
   * @throws InterruptedException When the job is aborted.
   * @see org.apache.hadoop.mapreduce.RecordReader#getCurrentKey()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * @throws IOException          When setting up the reader fails.
   * @throws InterruptedException When the job is aborted.
   * @see org.apache.hadoop.mapreduce.RecordReader#initialize(
   *      org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * @throws InterruptedException When the job is aborted.
   * @see org.apache.hadoop.mapreduce.RecordReader#initialize(
   *      org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * @throws InterruptedException When the job is aborted.
   * @see org.apache.hadoop.mapreduce.RecordReader#initialize(
   *      org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
  /**
   * Closes the split.
   * @see org.apache.hadoop.mapreduce.RecordReader#close()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * @throws IOException          When reading the record failed.
   * @throws InterruptedException When the job was aborted.
   * @see org.apache.hadoop.mapreduce.RecordReader#nextKeyValue()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java`
#### Snippet
```java
   * @throws IOException          When the value is faulty.
   * @throws InterruptedException When the job is aborted.
   * @see org.apache.hadoop.mapreduce.RecordReader#getCurrentValue()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java`
#### Snippet
```java

  @Override
  protected void reduce(ImmutableBytesWritable row, java.lang.Iterable<Put> puts,
    Reducer<ImmutableBytesWritable, Put, ImmutableBytesWritable, KeyValue>.Context context)
    throws java.io.IOException, InterruptedException {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java`
#### Snippet
```java
  protected void reduce(ImmutableBytesWritable row, java.lang.Iterable<Put> puts,
    Reducer<ImmutableBytesWritable, Put, ImmutableBytesWritable, KeyValue>.Context context)
    throws java.io.IOException, InterruptedException {
    // although reduce() is called per-row, handle pathological case
    long threshold =
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/CellSortReducer.java`
#### Snippet
```java
  protected void reduce(ImmutableBytesWritable row, Iterable<Cell> kvs,
    Reducer<ImmutableBytesWritable, Cell, ImmutableBytesWritable, Cell>.Context context)
    throws java.io.IOException, InterruptedException {
    TreeSet<Cell> map = new TreeSet<>(CellComparator.getInstance());
    for (Cell kv : kvs) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
   * Returns the length of the split.
   * @return The length of the split.
   * @see org.apache.hadoop.mapreduce.InputSplit#getLength()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
   * @param split The split to compare to.
   * @return The result of the comparison.
   * @see java.lang.Comparable#compareTo(java.lang.Object)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
   * @param split The split to compare to.
   * @return The result of the comparison.
   * @see java.lang.Comparable#compareTo(java.lang.Object)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
   * Returns the details about this instance as a string.
   * @return The values of this instance as a string.
   * @see java.lang.Object#toString()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java`
#### Snippet
```java
   * Returns the region's location as an array.
   * @return The array containing the region location.
   * @see org.apache.hadoop.mapreduce.InputSplit#getLocations()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.common.base` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
        job.setNumReduceTasks(regionLocator.getStartKeys().length);
        TableMapReduceUtil.addDependencyJarsForClasses(job.getConfiguration(),
          org.apache.hbase.thirdparty.com.google.common.base.Preconditions.class);
      }
    } else if (hfileOutPath != null) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.common.base` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
        HFileOutputFormat2.configureIncrementalLoad(job, table.getDescriptor(), regionLocator);
        TableMapReduceUtil.addDependencyJarsForClasses(job.getConfiguration(),
          org.apache.hbase.thirdparty.com.google.common.base.Preconditions.class);
      }
    } else {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java`
#### Snippet
```java
    protected void reduce(CellWritableComparable row, Iterable<Cell> kvs,
      Reducer<CellWritableComparable, Cell, ImmutableBytesWritable, Cell>.Context context)
      throws java.io.IOException, InterruptedException {
      int index = 0;
      for (Cell kv : kvs) {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java`
#### Snippet
```java
   * @param args The command line parameters.
   * @return The newly created job.
   * @throws java.io.IOException When setting up the job fails.
   */
  public Job createSubmittableJob(Configuration conf, String[] args) throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.http` is unnecessary and can be removed
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/InfoServer.java`
#### Snippet
```java
    final Configuration c) throws IOException {
    HttpConfig httpConfig = new HttpConfig(c);
    HttpServer.Builder builder = new org.apache.hadoop.hbase.http.HttpServer.Builder();

    builder.setName(name)
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.http` is unnecessary and can be removed
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/InfoServer.java`
#### Snippet
```java
public class InfoServer {
  private static final String HBASE_APP_DIR = "hbase-webapps";
  private final org.apache.hadoop.hbase.http.HttpServer httpServer;

  /**
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
    addDependencyJarsForClasses(conf,
      // explicitly pull a class from each module
      org.apache.hadoop.hbase.HConstants.class, // hbase-common
      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded
      org.apache.hadoop.hbase.client.Put.class, // hbase-client
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.HConstants.class, // hbase-common
      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded
      org.apache.hadoop.hbase.client.Put.class, // hbase-client
      org.apache.hadoop.hbase.ipc.RpcServer.class, // hbase-server
      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded
      org.apache.hadoop.hbase.client.Put.class, // hbase-client
      org.apache.hadoop.hbase.ipc.RpcServer.class, // hbase-server
      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat
      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.client.Put.class, // hbase-client
      org.apache.hadoop.hbase.ipc.RpcServer.class, // hbase-server
      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat
      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat
      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.ipc.RpcServer.class, // hbase-server
      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat
      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat
      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce
      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.mapreduce` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat
      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat
      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce
      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics
      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.metrics.impl` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat
      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce
      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics
      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api
      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.metrics` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce
      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics
      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api
      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication
      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.replication` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics
      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api
      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication
      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http
      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.http` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api
      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication
      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http
      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure
      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.procedure2` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication
      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http
      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure
      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper
      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.zookeeper` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http
      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure
      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper
      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous
      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.common.collect` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure
      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper
      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous
      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson
      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.gson` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper
      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous
      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson
      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf
      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous
      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson
      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf
      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty
      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.io.netty.channel` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson
      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf
      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty
      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe
      org.apache.zookeeper.ZooKeeper.class, // zookeeper
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.unsafe` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf
      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty
      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe
      org.apache.zookeeper.ZooKeeper.class, // zookeeper
      com.codahale.metrics.MetricRegistry.class, // metrics-core
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.zookeeper` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty
      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe
      org.apache.zookeeper.ZooKeeper.class, // zookeeper
      com.codahale.metrics.MetricRegistry.class, // metrics-core
      org.apache.commons.lang3.ArrayUtils.class, // commons-lang
```

### UnnecessaryFullyQualifiedName
Qualifier `com.codahale.metrics` is unnecessary and can be removed
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe
      org.apache.zookeeper.ZooKeeper.class, // zookeeper
      com.codahale.metrics.MetricRegistry.class, // metrics-core
      org.apache.commons.lang3.ArrayUtils.class, // commons-lang
      io.opentelemetry.api.trace.Span.class, // opentelemetry-api
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.commons.lang3` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.zookeeper.ZooKeeper.class, // zookeeper
      com.codahale.metrics.MetricRegistry.class, // metrics-core
      org.apache.commons.lang3.ArrayUtils.class, // commons-lang
      io.opentelemetry.api.trace.Span.class, // opentelemetry-api
      io.opentelemetry.semconv.trace.attributes.SemanticAttributes.class, // opentelemetry-semconv
```

### UnnecessaryFullyQualifiedName
Qualifier `io.opentelemetry.api.trace` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      com.codahale.metrics.MetricRegistry.class, // metrics-core
      org.apache.commons.lang3.ArrayUtils.class, // commons-lang
      io.opentelemetry.api.trace.Span.class, // opentelemetry-api
      io.opentelemetry.semconv.trace.attributes.SemanticAttributes.class, // opentelemetry-semconv
      io.opentelemetry.context.Context.class); // opentelemetry-context
```

### UnnecessaryFullyQualifiedName
Qualifier `io.opentelemetry.semconv.trace.attributes` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      org.apache.commons.lang3.ArrayUtils.class, // commons-lang
      io.opentelemetry.api.trace.Span.class, // opentelemetry-api
      io.opentelemetry.semconv.trace.attributes.SemanticAttributes.class, // opentelemetry-semconv
      io.opentelemetry.context.Context.class); // opentelemetry-context
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `io.opentelemetry.context` is unnecessary, and can be replaced with an import
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java`
#### Snippet
```java
      io.opentelemetry.api.trace.Span.class, // opentelemetry-api
      io.opentelemetry.semconv.trace.attributes.SemanticAttributes.class, // opentelemetry-semconv
      io.opentelemetry.context.Context.class); // opentelemetry-context
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.http` is unnecessary, and can be replaced with an import
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java`
#### Snippet
```java
    ctx.getServletContext().setAttribute(CONF_CONTEXT_ATTRIBUTE, conf);
    // for org.apache.hadoop.metrics.MetricsServlet
    ctx.getServletContext().setAttribute(org.apache.hadoop.http.HttpServer2.CONF_CONTEXT_ATTRIBUTE,
      conf);
    ctx.getServletContext().setAttribute(ADMINS_ACL, adminsAcl);
```

### UnnecessaryFullyQualifiedName
Qualifier `java.text` is unnecessary and can be removed
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/AssignmentVerificationReport.java`
#### Snippet
```java
      System.err.println("[Error] Region assignment verification report" + "hasn't been filled up");
    }
    DecimalFormat df = new java.text.DecimalFormat("#.##");

    // Print some basic information
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.util` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/HealthChecker.java`
#### Snippet
```java
      LOG.warn("Caught exception : " + e);
      status = HealthCheckerExitStatus.FAILED_WITH_EXCEPTION;
      exceptionStackTrace = org.apache.hadoop.util.StringUtils.stringifyException(e);
    } finally {
      if (shexec.isTimedOut()) {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/Reference.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#toString()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java`
#### Snippet
```java

      @Override
      public org.apache.hadoop.hbase.io.hfile.HFile.Reader getReader() {
        return this.delegate.getReader();
      }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilter.java`
#### Snippet
```java
 * A Bloom filter implementation built on top of
 * {@link org.apache.hadoop.hbase.util.BloomFilterChunk}, encapsulating a set of fixed-size Bloom
 * filters written out at the time of {@link org.apache.hadoop.hbase.io.hfile.HFile} generation into
 * the data block stream, and loaded on demand at query time. This class only provides reading
 * capabilities.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java`
#### Snippet
```java
     * the two metadata formats do not have to be consistent. This does have to be consistent with
     * how
     * {@link CompoundBloomFilter#CompoundBloomFilter(DataInput, org.apache.hadoop.hbase.io.hfile.HFile.Reader, BloomFilterMetrics)}
     * reads fields.
     */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java`
#### Snippet
```java
/**
 * Adds methods required for writing a compound Bloom filter to the data section of an
 * {@link org.apache.hadoop.hbase.io.hfile.HFile} to the {@link CompoundBloomFilter} class.
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java`
#### Snippet
```java
 * Provides functionality to write ({@link BlockIndexWriter}) and read BlockIndexReader single-level
 * and multi-level block indexes. Examples of how to use the block index writer can be found in
 * {@link org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter} and {@link HFileWriterImpl}.
 * Examples of how to use the reader can be found in {@link HFileReaderImpl} and
 * org.apache.hadoop.hbase.io.hfile.TestHFileBlockIndex.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/SharedMemHFileBlock.java`
#### Snippet
```java
 * say, an exclusive memory HFileBlock would must be an heap block and a shared memory HFileBlock
 * would must be an off-heap block.
 * @see org.apache.hadoop.hbase.io.hfile.ExclusiveMemHFileBlock
 **/
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileScanner.java`
#### Snippet
```java
  ByteBuffer getValue();

  /** Returns Instance of {@link org.apache.hadoop.hbase.Cell}. */
  Cell getCell();

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.java`
#### Snippet
```java
   * unless caching is disabled for the request. Otherwise, only use heap if caching is enabled and
   * the expected block type is not DATA (which goes to off-heap L2 in combined cache).
   * @see org.apache.hadoop.hbase.io.hfile.HFileBlock.FSReader#readBlockData(long, long, boolean,
   *      boolean, boolean)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheKey.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class BlockCacheKey implements HeapSize, java.io.Serializable {
  private static final long serialVersionUID = -5199992013113130534L;
  private final String hfileName;
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java

  @Override
  public Set<java.util.Map.Entry<byte[], byte[]>> entrySet() {
    return map.entrySet();
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java`
#### Snippet
```java
   * @param pathName     indicate that the data is read from which file.
   * @return a flag indicate the checksum match or mismatch.
   * @see org.apache.hadoop.util.DataChecksum#verifyChunkedSums(ByteBuffer, ByteBuffer, String,
   *      long)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.hfile` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ExclusiveMemHFileBlock.java`
#### Snippet
```java
 * do nothing for the de-allocating.
 * <p>
 * @see org.apache.hadoop.hbase.io.hfile.SharedMemHFileBlock
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java`
#### Snippet
```java
  /**
   * Call {@link ByteBuff#release()} to decrease the reference count, if no other reference, it will
   * return back the {@link ByteBuffer} to {@link org.apache.hadoop.hbase.io.ByteBuffAllocator}
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio.channels` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.java`
#### Snippet
```java
      @Override
      public ByteBuffer allocate(long size) throws IOException {
        ByteBuffer buffer = fileChannel.map(java.nio.channels.FileChannel.MapMode.READ_WRITE,
          pos.getAndIncrement() * size, size);
        return buffer;
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.java`
#### Snippet
```java
      fileChannel = raf.getChannel();
      LOG.info("Allocating " + StringUtils.byteDesc(fileSize) + ", on the path:" + filePath);
    } catch (java.io.FileNotFoundException fex) {
      LOG.error("Can't create bucket cache file " + filePath, fex);
      throw fex;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.java`
#### Snippet
```java
  /**
   * Process the response for this call. You need to have the lock on
   * {@link org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection#responseWriteLock}
   * @return true if we proceed the call fully, false otherwise.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio.channels` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java

  /**
   * Helper for {@link #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)}.
   * Only one of readCh or writeCh should be non-null.
   * @param readCh  read channel
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java

  /**
   * Helper for {@link #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)}.
   * Only one of readCh or writeCh should be non-null.
   * @param readCh  read channel
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @param buf     buffer to read or write into/out of
   * @return bytes written
   * @throws java.io.IOException e
   * @see #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio.channels` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @return bytes written
   * @throws java.io.IOException e
   * @see #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)
   */
  private static int channelIO(ReadableByteChannel readCh, WritableByteChannel writeCh,
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @return bytes written
   * @throws java.io.IOException e
   * @see #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)
   */
  private static int channelIO(ReadableByteChannel readCh, WritableByteChannel writeCh,
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio.channels` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
  /**
   * This is a wrapper around
   * {@link java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)}. If the amount of data
   * is large, it writes to channel in smaller chunks. This is to avoid jdk from creating many
   * direct buffers as the size of ByteBuffer increases. There should not be any performance
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
  /**
   * This is a wrapper around
   * {@link java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)}. If the amount of data
   * is large, it writes to channel in smaller chunks. This is to avoid jdk from creating many
   * direct buffers as the size of ByteBuffer increases. There should not be any performance
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @param buffer  buffer to write
   * @return number of bytes written
   * @throws java.io.IOException e
   * @see java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio.channels` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @return number of bytes written
   * @throws java.io.IOException e
   * @see java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)
   */
  protected int channelRead(ReadableByteChannel channel, ByteBuffer buffer) throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java`
#### Snippet
```java
   * @return number of bytes written
   * @throws java.io.IOException e
   * @see java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)
   */
  protected int channelRead(ReadableByteChannel channel, ByteBuffer buffer) throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.wal` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/AbstractFSWALProvider.java`
#### Snippet
```java
    int attempt = 0;
    Exception ee = null;
    org.apache.hadoop.hbase.wal.WAL.Reader reader = null;
    while (reader == null && attempt++ < maxAttempts) {
      try {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java`
#### Snippet
```java
      if (
        e.getCause() != null && (e.getCause() instanceof ParseException
          || e.getCause() instanceof org.apache.hadoop.fs.ChecksumException)
      ) {
        LOG.warn("Parse exception from {}; continuing", path, e);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver.compactions` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.java`
#### Snippet
```java
      @Override
      public StoreFileWriter createWriter(InternalScanner scanner,
        org.apache.hadoop.hbase.regionserver.compactions.Compactor.FileDetails fd,
        boolean shouldDropBehind, boolean major, Consumer<Path> writerCreationTracker)
        throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/TableDescriptorChecker.java`
#### Snippet
```java

/**
 * Only used for master to sanity check {@link org.apache.hadoop.hbase.client.TableDescriptor}.
 */
@InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.encoding` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
        conf.getInt(HConstants.HBASE_META_VERSIONS, HConstants.DEFAULT_HBASE_META_VERSIONS))
      .setInMemory(true).setBlocksize(8 * 1024).setScope(HConstants.REPLICATION_SCOPE_LOCAL)
      .setDataBlockEncoding(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.ROW_INDEX_V1)
      .setBloomFilterType(BloomType.ROWCOL).build();
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.encoding` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
          conf.getInt(HConstants.HBASE_META_BLOCK_SIZE, HConstants.DEFAULT_HBASE_META_BLOCK_SIZE))
        .setScope(HConstants.REPLICATION_SCOPE_LOCAL).setBloomFilterType(BloomType.ROWCOL)
        .setDataBlockEncoding(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.ROW_INDEX_V1)
        .build())
      .setColumnFamily(getTableFamilyDescForMeta(conf))
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.encoding` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
          conf.getInt(HConstants.HBASE_META_BLOCK_SIZE, HConstants.DEFAULT_HBASE_META_BLOCK_SIZE))
        .setScope(HConstants.REPLICATION_SCOPE_LOCAL)
        .setDataBlockEncoding(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.ROW_INDEX_V1)
        .setBloomFilterType(BloomType.ROWCOL).build())
      .setCoprocessor(
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.io.encoding` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
      .setMaxVersions(HConstants.ALL_VERSIONS).setInMemory(true)
      .setScope(HConstants.REPLICATION_SCOPE_LOCAL)
      .setDataBlockEncoding(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.ROW_INDEX_V1)
      .setBloomFilterType(BloomType.ROWCOL).build();
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
  /**
   * Find descriptors by namespace.
   * @see #get(org.apache.hadoop.hbase.TableName)
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.conf` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ConfigurationUtil.java`
#### Snippet
```java
/**
 * Utilities for storing more complex collection types in
 * {@link org.apache.hadoop.conf.Configuration} instances.
 */
@InterfaceAudience.Public
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java`
#### Snippet
```java

/**
 * Simple sorted list implementation that uses {@link java.util.ArrayList} as the underlying
 * collection so we can support RandomAccess. All mutations create a new copy of the
 * <code>ArrayList</code> instance, so can be expensive. This class is only intended for use on
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/SortedList.java`
#### Snippet
```java
 * reference to the internal list first using get().
 * <p>
 * If constructed with a {@link java.util.Comparator}, the list will be sorted using the comparator.
 * Adding or changing an element using an index will trigger a resort.
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `com.codahale.metrics` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/YammerHistogramUtils.java`
#### Snippet
```java

  /**
   * Create a new {@link com.codahale.metrics.Histogram} instance. These constructors are not public
   * in 2.2.0, so we use reflection to find them.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

    List<CheckRegionConsistencyWorkItem> workItems = new ArrayList<>(regionInfoMap.size());
    for (java.util.Map.Entry<String, HbckRegionInfo> e : regionInfoMap.entrySet()) {
      if (e.getValue().getReplicaId() == RegionInfo.DEFAULT_REPLICA_ID) {
        workItems.add(new CheckRegionConsistencyWorkItem(e.getKey(), e.getValue()));
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
    // deployed/undeployed replicas.
    List<CheckRegionConsistencyWorkItem> replicaWorkItems = new ArrayList<>(regionInfoMap.size());
    for (java.util.Map.Entry<String, HbckRegionInfo> e : regionInfoMap.entrySet()) {
      if (e.getValue().getReplicaId() != RegionInfo.DEFAULT_REPLICA_ID) {
        replicaWorkItems.add(new CheckRegionConsistencyWorkItem(e.getKey(), e.getValue()));
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.exceptions` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java`
#### Snippet
```java
   * @param pbBytes A pb serialized {@link FilterWrapper} instance
   * @return An instance of {@link FilterWrapper} made from <code>bytes</code>
   * @throws org.apache.hadoop.hbase.exceptions.DeserializationException
   * @see #toByteArray
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.master` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
  public static final Class<
    ? extends ClusterStatusPublisher.Publisher> DEFAULT_STATUS_PUBLISHER_CLASS =
      org.apache.hadoop.hbase.master.ClusterStatusPublisher.MulticastPublisher.class;

  /**
```

### UnnecessaryFullyQualifiedName
Qualifier `java.text` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
        }
      }
      DecimalFormat df = new java.text.DecimalFormat("#.##");
      for (int i = 0; i < deltaLocality.length; i++) {
        System.out.print("\t\t Baseline locality for ");
```

### UnnecessaryFullyQualifiedName
Qualifier `java.text` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
    List<Float> dispersion = report.getDispersionInformation();
    if (simplePrint) {
      DecimalFormat df = new java.text.DecimalFormat("#.##");
      System.out.println("\tAvg dispersion score: " + df.format(dispersion.get(0))
        + " hosts;\tMax dispersion score: " + df.format(dispersion.get(1))
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/http/RegionReplicaInfo.java`
#### Snippet
```java
  private final ServerName serverName;
  private final long seqNum;
  /** See {@link org.apache.hadoop.hbase.HConstants#SERVERNAME_QUALIFIER_STR}. */
  private final ServerName targetServerName;
  private final Map<String, RegionInfo> mergeRegionInfo;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.master` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BalancerChore.java`
#### Snippet
```java

/**
 * Chore that will call HMaster.balance{@link org.apache.hadoop.hbase.master.HMaster#balance()} when
 * needed.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.master` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java

  /**
   * @see org.apache.hadoop.hbase.master.HMasterCommandLine
   */
  public static void main(String[] args) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java`
#### Snippet
```java
   * Queries the state of the {@link SplitOrMergeStateStore}. If it is not initialized, false is
   * returned. If switchType is illegal, false will return.
   * @param switchType see {@link org.apache.hadoop.hbase.client.MasterSwitchType}
   * @return The state of the switch
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.procedure2` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java

  /**
   * Fetches {@link org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher.RemoteOperation}s
   * from the given {@code remoteProcedures} and groups them by class of the returned operation.
   * Then {@code resolver} is used to dispatch {@link RegionOpenOperation}s and
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.yetus.audience` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCMultipleMergedRegionsProcedure.java`
#### Snippet
```java
 * different serialization profile writing out more than just two regions.
 */
@org.apache.yetus.audience.InterfaceAudience.Private
public class GCMultipleMergedRegionsProcedure
  extends AbstractStateMachineTableProcedure<GCMergedRegionsState> {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/SnapshotQuotaObserverChore.java`
#### Snippet
```java
    Set<TableName> tablesToFetchSnapshotsFrom) throws IOException {
    Multimap<TableName, String> snapshotsToCompute = HashMultimap.create();
    for (org.apache.hadoop.hbase.client.SnapshotDescription sd : admin.listSnapshots()) {
      TableName tn = sd.getTableName();
      if (tablesToFetchSnapshotsFrom.contains(tn)) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java`
#### Snippet
```java

  /**
   * See {@link ClientTokenUtil#obtainToken(org.apache.hadoop.hbase.client.Connection)}.
   * @deprecated External users should not use this method. Please post on the HBase dev mailing
   *             list if you need this method. Internal HBase code should use
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java`
#### Snippet
```java

  /**
   * See {@link ClientTokenUtil#obtainToken(org.apache.hadoop.hbase.client.AsyncConnection)}.
   * @deprecated External users should not use this method. Please post on the HBase dev mailing
   *             list if you need this method. Internal HBase code should use
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.exceptions` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlFilter.java`
#### Snippet
```java
   * @param pbBytes A pb serialized {@link AccessControlFilter} instance
   * @return An instance of {@link AccessControlFilter} made from <code>bytes</code>
   * @throws org.apache.hadoop.hbase.exceptions.DeserializationException
   * @see #toByteArray()
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/PermissionStorage.java`
#### Snippet
```java

  /**
   * Writes a set of permissions as {@link org.apache.hadoop.io.Writable} instances and returns the
   * resulting byte array. Writes a set of permission [user: table permission]
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.master.balancer` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java

  /**
   * Used by reflection in {@link org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory}.
   */
  @InterfaceAudience.Private
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.rsgroup` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java`
#### Snippet
```java
   * Filter servers based on the online servers.
   * <p/>
   * servers is actually a TreeSet (see {@link org.apache.hadoop.hbase.rsgroup.RSGroupInfo}), having
   * its contains()'s time complexity as O(logn), which is good enough.
   * <p/>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.security.visibility` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelService.java`
#### Snippet
```java
   * are part of the cell created from the WALEdits that are prepared for replication while calling
   * {@link org.apache.hadoop.hbase.replication.ReplicationEndpoint} .replicate().
   * {@link org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint} calls this
   * API to provide an opportunity to modify the visibility tags before replicating. the visibility
   * tags associated with the cell the serialization format associated with the tag
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.procedure` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinationManager.java`
#### Snippet
```java

  /**
   * Method to retrieve {@link org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs}
   */
  ProcedureCoordinatorRpcs getProcedureCoordinatorRpcs(String procType, String coordNode);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.procedure` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinationManager.java`
#### Snippet
```java

  /**
   * Method to retrieve {@link org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs}
   */
  ProcedureMemberRpcs getProcedureMemberRpcs(String procType) throws KeeperException;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/ConstraintException.java`
#### Snippet
```java
 */
@InterfaceAudience.Private
public class ConstraintException extends org.apache.hadoop.hbase.DoNotRetryIOException {
  private static final long serialVersionUID = 1197446454511704140L;

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.constraint` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraint.java`
#### Snippet
```java
   * to fail.
   * @param p {@link Put} to check
   * @throws org.apache.hadoop.hbase.constraint.ConstraintException when the {@link Put} does not
   *                                                                match the constraint.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.constraint` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraint.java`
#### Snippet
```java
 * <p/>
 * If a {@link Put} is invalid, the Constraint should throw some sort of
 * {@link org.apache.hadoop.hbase.constraint.ConstraintException}, indicating that the {@link Put}
 * has failed. When this exception is thrown, not further retries of the {@link Put} are attempted
 * nor are any other {@link Constraint Constraints} attempted (the {@link Put} is clearly not
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.constraint` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraint.java`
#### Snippet
```java
 * <p/>
 * If a {@link Constraint} fails to fail the {@link Put} via a
 * {@link org.apache.hadoop.hbase.constraint.ConstraintException}, but instead throws a
 * {@link RuntimeException}, the entire constraint processing mechanism
 * ({@link ConstraintProcessor}) will be unloaded from the table. This ensures that the region
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.concurrent` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.java`
#### Snippet
```java
   * We use the FlushTableSubprocedurePool, a class specific thread pool instead of
   * {@link org.apache.hadoop.hbase.executor.ExecutorService}. It uses a
   * {@link java.util.concurrent.ExecutorCompletionService} which provides queuing of completed
   * tasks which lets us efficiently cancel pending tasks upon the earliest operation failures.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/EndpointObserver.java`
#### Snippet
```java
   * Called before an Endpoint service method is invoked. The request message can be altered by
   * returning a new instance. Throwing an exception will abort the invocation. Calling
   * {@link org.apache.hadoop.hbase.coprocessor.ObserverContext#bypass()} has no effect in this
   * hook.
   * @param ctx        the environment provided by the region server
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java`
#### Snippet
```java
          } else {
            // rows are split between regions, do not retry
            throw new org.apache.hadoop.hbase.DoNotRetryIOException(msg);
          }
        }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/WALObserver.java`
#### Snippet
```java
 * being written to the WAL. Note that implementers of WALObserver will not see WALEdits that report
 * themselves as empty via {@link WALEdit#isEmpty()}.
 * {@link org.apache.hadoop.hbase.coprocessor.RegionObserver} provides hooks for adding logic for
 * WALEdits in the region context during reconstruction. Defines coprocessor hooks for interacting
 * with operations on the {@link org.apache.hadoop.hbase.wal.WAL}. Since most implementations will
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.wal` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationMarkerChore.java`
#### Snippet
```java
/**
 * This chore is responsible to create replication marker rows with special WALEdit with family as
 * {@link org.apache.hadoop.hbase.wal.WALEdit#METAFAMILY} and column qualifier as
 * {@link WALEdit#REPLICATION_MARKER} and empty value. If config key
 * {@link #REPLICATION_MARKER_ENABLED_KEY} is set to true, then we will create 1 marker row every
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.replication.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationMarkerChore.java`
#### Snippet
```java
 * {@link #REPLICATION_MARKER_ENABLED_KEY} is set to true, then we will create 1 marker row every
 * {@link #REPLICATION_MARKER_CHORE_DURATION_KEY} ms
 * {@link org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader} will populate
 * the Replication Marker edit with region_server_name, wal_name and wal_offset encoded in
 * {@link org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos.ReplicationMarkerDescriptor}
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.replication.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationMarkerChore.java`
#### Snippet
```java
 * the Replication Marker edit with region_server_name, wal_name and wal_offset encoded in
 * {@link org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos.ReplicationMarkerDescriptor}
 * object. {@link org.apache.hadoop.hbase.replication.regionserver.Replication} will change the
 * REPLICATION_SCOPE for this edit to GLOBAL so that it can replicate. On the sink cluster,
 * {@link org.apache.hadoop.hbase.replication.regionserver.ReplicationSink} will convert the
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.replication.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationMarkerChore.java`
#### Snippet
```java
 * object. {@link org.apache.hadoop.hbase.replication.regionserver.Replication} will change the
 * REPLICATION_SCOPE for this edit to GLOBAL so that it can replicate. On the sink cluster,
 * {@link org.apache.hadoop.hbase.replication.regionserver.ReplicationSink} will convert the
 * ReplicationMarkerDescriptor into a Put mutation to REPLICATION_SINK_TRACKER_TABLE_NAME_STR table.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java`
#### Snippet
```java

  /**
   * @see java.lang.Object#toString()
   */
  @Override
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java
  }

  private java.util.UUID toUUID(final HBaseProtos.UUID uuid) {
    return new java.util.UUID(uuid.getMostSigBits(), uuid.getLeastSigBits());
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java`
#### Snippet
```java

  private java.util.UUID toUUID(final HBaseProtos.UUID uuid) {
    return new java.util.UUID(uuid.getMostSigBits(), uuid.getLeastSigBits());
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
   * @param priority  priority
   * @param conf      configuration
   * @throws java.io.IOException Exception
   */
  public E checkAndLoadInstance(Class<?> implClass, int priority, Configuration conf)
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
   * @param priority  chaining priority
   * @param conf      configuration for coprocessor
   * @throws java.io.IOException Exception
   */
  public E load(Path path, String className, int priority, Configuration conf) throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
   * @param conf                  configuration for coprocessor
   * @param includedClassPrefixes class name prefixes to include
   * @throws java.io.IOException Exception
   */
  public E load(Path path, String className, int priority, Configuration conf,
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java`
#### Snippet
```java
   * Get all online regions of a table in this RS.
   * @return List of Region
   * @throws java.io.IOException
   */
  List<? extends Region> getRegions(TableName tableName) throws IOException;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coordination/ZkCoordinatedStateManager.java`
#### Snippet
```java

/**
 * ZooKeeper-based implementation of {@link org.apache.hadoop.hbase.CoordinatedStateManager}.
 * @deprecated since 2.4.0 and in 3.0.0, to be removed in 4.0.0, replaced by procedure-based
 *             distributed WAL splitter (see SplitWALManager) which doesn't use this zk-based
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.java`
#### Snippet
```java
public interface RpcSchedulerFactory {
  /**
   * Constructs a {@link org.apache.hadoop.hbase.ipc.RpcScheduler}.
   */
  RpcScheduler create(Configuration conf, PriorityFunction priority, Abortable server);
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.java`
#### Snippet
```java

/**
 * A factory class that constructs an {@link org.apache.hadoop.hbase.ipc.RpcScheduler}.
 */
@InterfaceAudience.LimitedPrivate({ HBaseInterfaceAudience.COPROC, HBaseInterfaceAudience.PHOENIX })
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   * Called after a new cell has been created during an increment operation, but before it is
   * committed to the WAL or memstore. Calling
   * {@link org.apache.hadoop.hbase.coprocessor.ObserverContext#bypass()} has no effect in this
   * hook.
   * @param ctx      the environment provided by the region server
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  /**
   * Called before creation of Reader for a store file. Calling
   * {@link org.apache.hadoop.hbase.coprocessor.ObserverContext#bypass()} has no effect in this
   * hook.
   * @param ctx    the environment provided by the region server
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.coprocessor` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
  /**
   * Called before replaying WALs for this region. Calling
   * {@link org.apache.hadoop.hbase.coprocessor.ObserverContext#bypass()} has no effect in this
   * hook.
   * @param ctx   the environment provided by the region server
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java`
#### Snippet
```java
   * To override or modify the compaction process, implementing classes can wrap the provided
   * {@link InternalScanner} with a custom implementation that is returned from this method. The
   * custom scanner can then inspect {@link org.apache.hadoop.hbase.Cell}s from the wrapped scanner,
   * applying its own policy to what gets written.
   * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `java.nio.charset` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerIdGenerator.java`
#### Snippet
```java
  public ScannerIdGenerator(ServerName serverName) {
    long hash = Hashing.murmur3_32()
      .hashString(serverName.toString(), java.nio.charset.StandardCharsets.UTF_8).asInt();
    this.serverNameHash = hash << 32;
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java`
#### Snippet
```java
 * actually, the SFM here is not a general 'Manager', it is only designed to manage the in memory
 * 'stripes', so we can select different store files when scanning or compacting. The 'tracking' of
 * store files is actually done in {@link org.apache.hadoop.hbase.regionserver.HRegionFileSystem}
 * and {@link HStore} before we have SFT. And since SFM is designed to only holds in memory states,
 * we will hold write lock when updating it, the lock is also used to protect the normal read/write
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationService.java`
#### Snippet
```java
/**
 * Gateway to Cluster Replication. Used by
 * {@link org.apache.hadoop.hbase.regionserver.HRegionServer}. One such application is a
 * cross-datacenter replication service that can keep two hbase clusters in sync.
 */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.client` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java`
#### Snippet
```java
   * Commit a daughter region, moving it from the split temporary directory to the proper location
   * in the filesystem.
   * @param regionInfo daughter {@link org.apache.hadoop.hbase.client.RegionInfo}
   */
  public Path commitDaughterRegion(final RegionInfo regionInfo, List<Path> allRegionFiles,
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java`
#### Snippet
```java
   *         greater than or equal to 0.0, and any value greater than 1.0 means we enter the
   *         emergency state that some stores have too many store files.
   * @see org.apache.hadoop.hbase.regionserver.Store#getCompactionPressure()
   */
  double getCompactionPressure();
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
/**
 * ScannerContext instances encapsulate limit tracking AND progress towards those limits during
 * invocations of {@link InternalScanner#next(java.util.List)} and
 * {@link RegionScanner#next(java.util.List)}.
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
 * ScannerContext instances encapsulate limit tracking AND progress towards those limits during
 * invocations of {@link InternalScanner#next(java.util.List)} and
 * {@link RegionScanner#next(java.util.List)}.
 * <p>
 * A ScannerContext instance should be updated periodically throughout execution whenever progress
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
 * <p>
 * Once a limit has been reached, the scan will stop. The invoker of
 * {@link InternalScanner#next(java.util.List)} or {@link RegionScanner#next(java.util.List)} can
 * use the appropriate check*Limit methods to see exactly which limits have been reached.
 * Alternatively, {@link #checkAnyLimitReached(LimitScope)} is provided to see if ANY limit was
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
 * <p>
 * Once a limit has been reached, the scan will stop. The invoker of
 * {@link InternalScanner#next(java.util.List)} or {@link RegionScanner#next(java.util.List)} can
 * use the appropriate check*Limit methods to see exactly which limits have been reached.
 * Alternatively, {@link #checkAnyLimitReached(LimitScope)} is provided to see if ANY limit was
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  /**
   * @return true if the progress tracked so far in this instance will be considered during an
   *         invocation of {@link InternalScanner#next(java.util.List)} or
   *         {@link RegionScanner#next(java.util.List)}. false when the progress tracked so far
   *         should not be considered and should instead be wiped away via {@link #clearProgress()}.
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
   * @return true if the progress tracked so far in this instance will be considered during an
   *         invocation of {@link InternalScanner#next(java.util.List)} or
   *         {@link RegionScanner#next(java.util.List)}. false when the progress tracked so far
   *         should not be considered and should instead be wiped away via {@link #clearProgress()}.
   *         This only applies to per-row progress, like batch and data/heap size. Block size is
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java

  /**
   * Used as an indication to invocations of {@link InternalScanner#next(java.util.List)} and
   * {@link RegionScanner#next(java.util.List)} that, if true, the progress tracked within this
   * {@link ScannerContext} instance should be considered while evaluating the limits. Useful for
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  /**
   * Used as an indication to invocations of {@link InternalScanner#next(java.util.List)} and
   * {@link RegionScanner#next(java.util.List)} that, if true, the progress tracked within this
   * {@link ScannerContext} instance should be considered while evaluating the limits. Useful for
   * enforcing a set of limits across multiple calls (i.e. the limit may not be reached in a single
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
   * <p>
   * Defaulting this value to false means that, by default, any tracked progress will be wiped clean
   * on invocations to {@link InternalScanner#next(java.util.List)} and
   * {@link RegionScanner#next(java.util.List)} and the call will be treated as though no progress
   * has been made towards the limits so far.
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
   * Defaulting this value to false means that, by default, any tracked progress will be wiped clean
   * on invocations to {@link InternalScanner#next(java.util.List)} and
   * {@link RegionScanner#next(java.util.List)} and the call will be treated as though no progress
   * has been made towards the limits so far.
   * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  /**
   * The different fields that can be used as limits in calls to
   * {@link InternalScanner#next(java.util.List)} and {@link RegionScanner#next(java.util.List)}
   */
  private static class LimitFields {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  /**
   * The different fields that can be used as limits in calls to
   * {@link InternalScanner#next(java.util.List)} and {@link RegionScanner#next(java.util.List)}
   */
  private static class LimitFields {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java

  /**
   * The state of the scanner after the invocation of {@link InternalScanner#next(java.util.List)}
   * or {@link RegionScanner#next(java.util.List)}.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java`
#### Snippet
```java
  /**
   * The state of the scanner after the invocation of {@link InternalScanner#next(java.util.List)}
   * or {@link RegionScanner#next(java.util.List)}.
   */
  NextState scannerState;
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util` is unnecessary, and can be replaced with an import
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
    queueList.append("Flush Queue Queue dump:\n");
    queueList.append("  Flush Queue:\n");
    java.util.Iterator<FlushQueueEntry> it = flushQueue.iterator();

    while (it.hasNext()) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.protobuf` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
      // TODO: COPIES!!!!!!
      builder.setValue(builder.getValueBuilder().setName(result.getClass().getName()).setValue(
        org.apache.hbase.thirdparty.com.google.protobuf.ByteString.copyFrom(result.toByteArray())));
      return builder.build();
    } catch (IOException ie) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.commons.lang3` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
  private boolean isHealthCheckerConfigured() {
    String healthScriptLocation = this.conf.get(HConstants.HEALTH_SCRIPT_LOC);
    return org.apache.commons.lang3.StringUtils.isNotBlank(healthScriptLocation);
  }

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java

  /**
   * @see org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine
   */
  public static void main(String[] args) {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.lang` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
      ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
      clazz = Class.forName(classname, true, classLoader).asSubclass(xface);
    } catch (java.lang.ClassNotFoundException nfe) {
      throw new IOException("Could not find class for " + classname);
    }
```

### UnnecessaryFullyQualifiedName
Qualifier `java.util.concurrent` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java`
#### Snippet
```java
   * We use the SnapshotSubprocedurePool, a class specific thread pool instead of
   * {@link org.apache.hadoop.hbase.executor.ExecutorService}. It uses a
   * {@link java.util.concurrent.ExecutorCompletionService} which provides queuing of completed
   * tasks which lets us efficiently cancel pending tasks upon the earliest operation failures.
   * HBase's ExecutorService (different from {@link java.util.concurrent.ExecutorService}) isn't
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java`
#### Snippet
```java

  /**
   * See {@link org.apache.hadoop.hbase.regionserver.StoreScanner#trySkipToNextRow(Cell)}
   * @param cell current cell
   * @return true means skip to next column, false means not
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver.querymatcher` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ColumnTracker.java`
#### Snippet
```java
 * <p>
 * These two methods returns a
 * {@link org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher.MatchCode} to define
 * what action should be taken.
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver.querymatcher` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/ExplicitColumnTracker.java`
#### Snippet
```java
 * <p>
 * These two methods returns a
 * {@link org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher.MatchCode} to define
 * what action should be taken.
 * <p>
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
 * <p>
 * To read an WAL, call
 * {@link WALFactory#createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)}. *
 * <h2>Failure Semantic</h2> If an exception on append or sync, roll the WAL because the current WAL
 * is now a lame duck; any more appends or syncs will fail also with the same original exception. If
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.fs` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
 * <p>
 * To read an WAL, call
 * {@link WALFactory#createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path)}. *
 * <h2>Failure Semantic</h2> If an exception on append or sync, roll the WAL because the current WAL
 * is now a lame duck; any more appends or syncs will fail also with the same original exception. If
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hbase.thirdparty.com.google.common.base` is unnecessary, and can be replaced with an import
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceHFileSplitterJob.java`
#### Snippet
```java

      TableMapReduceUtil.addDependencyJars(job.getConfiguration(),
        org.apache.hbase.thirdparty.com.google.common.base.Preconditions.class);
    } else {
      throw new IOException("No bulk output directory specified");
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.tools` is unnecessary and can be removed
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/mapreduce/MapReduceBackupCopyJob.java`
#### Snippet
```java

      // reflection preparation for private methods and fields
      Class<?> classDistCp = org.apache.hadoop.tools.DistCp.class;
      Method methodCleanup = classDistCp.getDeclaredMethod("cleanup");

```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.shaded.protobuf` is unnecessary, and can be replaced with an import
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
      BackupProtos.TableServerTimestamp.newBuilder();
    tstBuilder
      .setTableName(org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.toProtoTableName(table));

    for (Entry<String, Long> entry : map.entrySet()) {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.shaded.protobuf` is unnecessary, and can be replaced with an import
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
    for (BackupProtos.ServerTimestamp st : list) {
      ServerName sn =
        org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.toServerName(st.getServerName());
      map.put(sn.getHostname() + ":" + sn.getPort(), st.getTimestamp());
    }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseCommonTestingUtility.java`
#### Snippet
```java
/**
 * Common helpers for testing HBase that do not depend on specific server/etc. things.
 * @see org.apache.hadoop.hbase.HBaseCommonTestingUtil
 * @deprecated since 3.0.0, will be removed in 4.0.0. Use
 *             {@link org.apache.hadoop.hbase.testing.TestingHBaseCluster} instead.
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        HStore store = getStore(familyName);
        if (store == null) {
          ioException = new org.apache.hadoop.hbase.DoNotRetryIOException(
            "No such column family " + Bytes.toStringBinary(familyName));
        } else {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.ipc` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
    }
    IOException exception =
      org.apache.hadoop.hbase.ipc.CoprocessorRpcUtils.getControllerException(controller);
    if (exception != null) {
      throw exception;
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        && !(mutation instanceof Increment) && !(mutation instanceof Append)
    ) {
      throw new org.apache.hadoop.hbase.DoNotRetryIOException(
        "Action must be Put or Delete or Increment or Delete");
    }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  private void checkRow(final Row action, final byte[] row) throws DoNotRetryIOException {
    if (!Bytes.equals(row, action.getRow())) {
      throw new org.apache.hadoop.hbase.DoNotRetryIOException("Action's getRow must match");
    }
  }
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java

  /**
   * Replace any cell timestamps set to {@link org.apache.hadoop.hbase.HConstants#LATEST_TIMESTAMP}
   * provided current timestamp.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.hadoop.hbase.regionserver` is unnecessary and can be removed
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
  /**
   * Interrupt any region options that have acquired the region lock via
   * {@link #startRegionOperation(org.apache.hadoop.hbase.regionserver.Region.Operation)}, or
   * {@link #startBulkRegionOperation(boolean)}.
   */
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  /**
   * Abruptly Shutdown HBase mini cluster. Does not shutdown zk or dfs if running.
   * @throws java.io.IOException throws in case command is unsuccessful
   */
  public void killMiniHBaseCluster() throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `java.io` is unnecessary and can be removed
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
  /**
   * Shutdown HBase mini cluster.Does not shutdown zk or dfs if running.
   * @throws java.io.IOException in case command is unsuccessful
   */
  public void shutdownMiniHBaseCluster() throws IOException {
```

### UnnecessaryFullyQualifiedName
Qualifier `org.apache.zookeeper` is unnecessary, and can be replaced with an import
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/HBaseTestingUtility.java`
#### Snippet
```java
    // we receive all the events, so don't have to capture the event, just
    // closing the connection should be enough.
    ZooKeeper monitor = new ZooKeeper(quorumServers, 1000, new org.apache.zookeeper.Watcher() {
      @Override
      public void process(WatchedEvent watchedEvent) {
```

## RuleId[id=RegExpSingleCharAlternation]
### RegExpSingleCharAlternation
Single character alternation in RegExp
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFile.java`
#### Snippet
```java
  private static final char TRACK_FILE_SEPARATOR = '.';

  static final Pattern TRACK_FILE_PATTERN = Pattern.compile("^f(1|2)\\.\\d+$");

  // 16 MB, which is big enough for a tracker file
```

## RuleId[id=Anonymous2MethodRef]
### Anonymous2MethodRef
Anonymous new Function() can be replaced with method reference
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
  @Deprecated
  public static List<KeyValue> ensureKeyValues(List<Cell> cells) {
    List<KeyValue> lazyList = Lists.transform(cells, new Function<Cell, KeyValue>() {
      @Override
      public KeyValue apply(Cell arg0) {
```

### Anonymous2MethodRef
Anonymous new Runnable() can be replaced with method reference
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java`
#### Snippet
```java

    // Every few mins clean the JMX cache.
    executor.getExecutor().scheduleWithFixedDelay(new Runnable() {
      @Override
      public void run() {
```

### Anonymous2MethodRef
Anonymous new Gauge() can be replaced with method reference
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/ExampleMasterObserverWithMetrics.java`
#### Snippet
```java
      // Register a custom gauge. The Gauge object will be registered in the metrics registry and
      // periodically the getValue() is invoked to obtain the snapshot.
      registry.register("totalMemory", new Gauge<Long>() {
        @Override
        public Long getValue() {
```

### Anonymous2MethodRef
Anonymous new Runnable() can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
    this.connections = new PoolMap<>(getPoolType(conf), getPoolSize(conf));

    this.cleanupIdleConnectionTask = IDLE_CONN_SWEEPER.scheduleAtFixedRate(new Runnable() {

      @Override
```

### Anonymous2MethodRef
Anonymous new PrivilegedExceptionAction() can be replaced with method reference
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/NettyHBaseSaslRpcClientHandler.java`
#### Snippet
```java
    NettyFutureUtils.addListener(ctx.channel().closeFuture(), f -> saslRpcClient.dispose());
    try {
      byte[] initialResponse = ugi.doAs(new PrivilegedExceptionAction<byte[]>() {

        @Override
```

### Anonymous2MethodRef
Anonymous new Principal() can be replaced with method reference
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
            @Override
            public Principal getUserPrincipal() {
              return new Principal() {
                @Override
                public String getName() {
```

### Anonymous2MethodRef
Anonymous new PathFilter() can be replaced with method reference
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterWalManager.java`
#### Snippet
```java
   * Filter *in* WAL files that are for the hbase:meta Region.
   */
  final static PathFilter META_FILTER = new PathFilter() {
    @Override
    public boolean accept(Path p) {
```

## RuleId[id=Java8MapApi]
### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Get.java`
#### Snippet
```java
  public Get addColumn(byte[] family, byte[] qualifier) {
    NavigableSet<byte[]> set = familyMap.get(family);
    if (set == null) {
      set = new TreeSet<>(Bytes.BYTES_COMPARATOR);
      familyMap.put(family, set);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java`
#### Snippet
```java
  public Scan addColumn(byte[] family, byte[] qualifier) {
    NavigableSet<byte[]> set = familyMap.get(family);
    if (set == null) {
      set = new TreeSet<>(Bytes.BYTES_COMPARATOR);
      familyMap.put(family, set);
```

### Java8MapApi
Can be replaced with single 'Map.getOrDefault' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
    public ReturnCode canTakeOperation(HRegionLocation loc, long heapSizeOfRow) {
      // Is it ok for limit of request size?
      long currentRequestSize = serverRequestSizes.containsKey(loc.getServerName())
        ? serverRequestSizes.get(loc.getServerName())
        : 0L;
```

### Java8MapApi
Can be replaced with single 'Map.getOrDefault' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
      if (code == ReturnCode.INCLUDE) {
        long currentRows =
          serverRows.containsKey(loc.getServerName()) ? serverRows.get(loc.getServerName()) : 0L;
        serverRows.put(loc.getServerName(), currentRows + 1);
      }
```

### Java8MapApi
Can be replaced with single 'Map.getOrDefault' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
    public ReturnCode canTakeOperation(HRegionLocation loc, long heapSizeOfRow) {
      long currentRows =
        serverRows.containsKey(loc.getServerName()) ? serverRows.get(loc.getServerName()) : 0L;
      // accept at least one row
      if (currentRows == 0 || currentRows < maxRowsPerRequest) {
```

### Java8MapApi
Can be replaced with single 'Map.getOrDefault' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/SimpleRequestController.java`
#### Snippet
```java
    public void notifyFinal(ReturnCode code, HRegionLocation loc, long heapSizeOfRow) {
      if (code == ReturnCode.INCLUDE) {
        long currentRequestSize = serverRequestSizes.containsKey(loc.getServerName())
          ? serverRequestSizes.get(loc.getServerName())
          : 0L;
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
  List<Cell> getCellList(byte[] family) {
    List<Cell> list = getFamilyCellMap().get(family);
    if (list == null) {
      list = new ArrayList<>();
      getFamilyCellMap().put(family, list);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      byte[] family = CellUtil.cloneFamily(kv);
      NavigableMap<byte[], NavigableMap<Long, byte[]>> columnMap = familyMap.get(family);
      if (columnMap == null) {
        columnMap = new TreeMap<>(Bytes.BYTES_COMPARATOR);
        familyMap.put(family, columnMap);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      byte[] qualifier = CellUtil.cloneQualifier(kv);
      NavigableMap<Long, byte[]> versionMap = columnMap.get(qualifier);
      if (versionMap == null) {
        versionMap = new TreeMap<>(new Comparator<Long>() {
          @Override
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java`
#### Snippet
```java

      List<Scan> scanList = tableMaps.get(tableName);
      if (scanList == null) {
        scanList = new ArrayList<>();
        tableMaps.put(tableName, scanList);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableSnapshotInputFormatImpl.java`
#### Snippet
```java

      Collection<Scan> snapshotScans = rtn.get(snapshotName);
      if (snapshotScans == null) {
        snapshotScans = Lists.newArrayList();
        rtn.put(snapshotName, snapshotScans);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProxyUserAuthenticationFilter.java`
#### Snippet
```java
      final String key = StringUtils.toLowerCase(entry.getKey());
      List<String> strings = m.get(key);
      if (strings == null) {
        strings = new ArrayList<String>();
        m.put(key, strings);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
    for (int i = 0; i < regionsToReturn.size(); i++) {
      List<Integer> pos = returnMap.get(regionsToReturn.get(i).getDestination());
      if (pos == null) {
        pos = new ArrayList<>();
        returnMap.put(regionsToReturn.get(i).getDestination(), pos);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/FavoredStochasticBalancer.java`
#### Snippet
```java
    RegionInfo region, ServerName host) {
    List<RegionInfo> regionsOnServer = assignmentMapForFavoredNodes.get(host);
    if (regionsOnServer == null) {
      regionsOnServer = Lists.newArrayList();
      assignmentMapForFavoredNodes.put(host, regionsOnServer);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeLoadBalancer.java`
#### Snippet
```java
    RegionInfo region, ServerName host) {
    List<RegionInfo> regionsOnServer = assignmentMapForFavoredNodes.get(host);
    if (regionsOnServer == null) {
      regionsOnServer = new ArrayList<>();
      assignmentMapForFavoredNodes.put(host, regionsOnServer);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
      if (assignmentMap != null) {
        List<RegionInfo> regionsForServer = assignmentMap.get(currentServer);
        if (regionsForServer == null) {
          regionsForServer = new ArrayList<>();
          assignmentMap.put(currentServer, regionsForServer);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java

      Set<ServerName> serversInRack = rackToFNMapping.get(rack);
      if (serversInRack == null) {
        serversInRack = Sets.newHashSet();
        rackToFNMapping.put(rack, serversInRack);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java`
#### Snippet
```java
        Integer key = Integer.valueOf(overlappedRegions);
        List<R> ranges = overlapRangeMap.get(key);
        if (ranges == null) {
          ranges = new ArrayList<>();
          overlapRangeMap.put(key, ranges);
```

### Java8MapApi
Can be replaced with single 'Map.getOrDefault' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.java`
#### Snippet
```java
          map.put(first, value + 1);

          value = map.containsKey(last) ? map.get(last) : 0;
          map.put(last, value - 1);
        }
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java`
#### Snippet
```java
          String rack = rackManager.getRack(servers.get(j / slotsPerServer));
          Map<RegionInfo, Float> rackLocality = rackRegionLocality.get(rack);
          if (rackLocality == null) {
            rackLocality = new HashMap<>();
            rackRegionLocality.put(rack, rackLocality);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java`
#### Snippet
```java

    List<RegionInfo> serverRegions = result.get(serverName);
    if (serverRegions == null) {
      serverRegions = new ArrayList<RegionInfo>();
      result.put(serverName, serverRegions);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityNewVersionBehaivorTracker.java`
#### Snippet
```java
    public void addVersionDelete(Cell cell) {
      SortedMap<Long, TagInfo> set = deletesMap.get(cell.getTimestamp());
      if (set == null) {
        set = new TreeMap<>();
        deletesMap.put(cell.getTimestamp(), set);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.java`
#### Snippet
```java
            cell.getQualifierLength());
          List<Integer> auths = userAuths.get(user);
          if (auths == null) {
            auths = new ArrayList<>();
            userAuths.put(user, auths);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java`
#### Snippet
```java
        String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(wal);
        NavigableSet<String> wals = walsByGroup.get(walPrefix);
        if (wals == null) {
          wals = new TreeSet<>();
          walsByGroup.put(walPrefix, wals);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java`
#### Snippet
```java
      } else {
        ArrayList<HStoreFile> stripe = candidateStripes.get(endRow);
        if (stripe == null) {
          stripe = new ArrayList<>();
          candidateStripes.put(endRow, stripe);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker.java`
#### Snippet
```java
    public void addVersionDelete(Cell cell) {
      SortedSet<Long> set = deletesMap.get(cell.getTimestamp());
      if (set == null) {
        set = new TreeSet<>();
        deletesMap.put(cell.getTimestamp(), set);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupManifest.java`
#### Snippet
```java
        TableName tn = ProtobufUtil.toTableName(tst.getTableName());
        Map<String, Long> map = incrTimeRanges.get(tn);
        if (map == null) {
          map = new HashMap<>();
          incrTimeRanges.put(tn, map);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupAdminImpl.java`
#### Snippet
```java
            String rootDir = info.getBackupRootDir();
            HashSet<TableName> allTables = allTablesMap.get(rootDir);
            if (allTables == null) {
              allTables = new HashSet<>();
              allTablesMap.put(rootDir, allTables);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
            tblMap = map.get(tTable);
          }
          if (tblMap.get(region) == null) {
            tblMap.put(region, new HashMap<>());
          }
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
          }
          Map<String, List<Pair<String, Boolean>>> famMap = tblMap.get(region);
          if (famMap.get(fam) == null) {
            famMap.put(fam, new ArrayList<>());
          }
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/impl/BackupSystemTable.java`
#### Snippet
```java
        if (set.contains(tableName)) {
          ArrayList<BackupInfo> list = tableHistoryMap.get(tableName);
          if (list == null) {
            list = new ArrayList<>();
            tableHistoryMap.put(tableName, list);
```

### Java8MapApi
Can be replaced with single 'Map.computeIfAbsent' method call
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            Pair<byte[], Collection<HStoreFile>> storeFiles = future.get();
            List<HStoreFile> familyFiles = result.get(storeFiles.getFirst());
            if (familyFiles == null) {
              familyFiles = new ArrayList<>();
              result.put(storeFiles.getFirst(), familyFiles);
```

## RuleId[id=Convert2Lambda]
### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/NamespaceDescriptor.java`
#### Snippet
```java

  public static final Comparator<NamespaceDescriptor> NAMESPACE_DESCRIPTOR_COMPARATOR =
    new Comparator<NamespaceDescriptor>() {
      @Override
      public int compare(NamespaceDescriptor namespaceDescriptor,
```

### Convert2Lambda
Anonymous new Function() can be replaced with lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java`
#### Snippet
```java
  @Deprecated
  public static List<KeyValue> ensureKeyValues(List<Cell> cells) {
    List<KeyValue> lazyList = Lists.transform(cells, new Function<Cell, KeyValue>() {
      @Override
      public KeyValue apply(Cell arg0) {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/security/UserProvider.java`
#### Snippet
```java
          throws Exception {

          return executor.submit(new Callable<String[]>() {
            @Override
            public String[] call() throws Exception {
```

### Convert2Lambda
Anonymous new Comparator\>() can be replaced with lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java
  public CopyOnWriteArrayMap(final Comparator<? super K> keyComparator) {
    this.keyComparator = keyComparator;
    this.holder = new ArrayHolder<>(keyComparator, new Comparator<Entry<K, V>>() {
      @Override
      public int compare(Entry<K, V> o1, Entry<K, V> o2) {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-common/src/main/java/org/apache/hadoop/hbase/types/CopyOnWriteArrayMap.java`
#### Snippet
```java

  public CopyOnWriteArrayMap() {
    this(new Comparator<K>() {
      @Override
      public int compare(K o1, K o2) {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java`
#### Snippet
```java

    // Every few mins clean the JMX cache.
    executor.getExecutor().scheduleWithFixedDelay(new Runnable() {
      @Override
      public void run() {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/BufferedMutatorExample.java`
#### Snippet
```java

      for (int i = 0; i < TASK_COUNT; i++) {
        futures.add(workerPool.submit(new Callable<Void>() {
          @Override
          public Void call() throws Exception {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/AbstractRpcClient.java`
#### Snippet
```java
    this.connections = new PoolMap<>(getPoolType(conf), getPoolSize(conf));

    this.cleanupIdleConnectionTask = IDLE_CONN_SWEEPER.scheduleAtFixedRate(new Runnable() {

      @Override
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseHbck.java`
#### Snippet
```java
  public List<Boolean> bypassProcedure(List<Long> pids, long waitTime, boolean override,
    boolean recursive) throws IOException {
    BypassProcedureResponse response = ProtobufUtil.call(new Callable<BypassProcedureResponse>() {
      @Override
      public BypassProcedureResponse call() throws Exception {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java`
#### Snippet
```java
      NavigableMap<Long, byte[]> versionMap = columnMap.get(qualifier);
      if (versionMap == null) {
        versionMap = new TreeMap<>(new Comparator<Long>() {
          @Override
          public int compare(Long l1, Long l2) {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java`
#### Snippet
```java

  public TreeSet<byte[]> createTreeSet() {
    return new TreeSet<>(new Comparator<Object>() {
      @Override
      public int compare(Object o1, Object o2) {
```

### Convert2Lambda
Anonymous new Comparator\>\>() can be replaced with lambda
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java`
#### Snippet
```java
    RowTracker() {
      nextRows = new PriorityQueue<>(fuzzyKeysData.size(),
        new Comparator<Pair<byte[], Pair<byte[], byte[]>>>() {
          @Override
          public int compare(Pair<byte[], Pair<byte[], byte[]>> o1,
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java`
#### Snippet
```java

  private static final Comparator<FileStatus> FILE_STATUS_ID_COMPARATOR =
    new Comparator<FileStatus>() {
      @Override
      public int compare(FileStatus a, FileStatus b) {
```

### Convert2Lambda
Anonymous new Comparator\>() can be replaced with lambda
in `hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java`
#### Snippet
```java
    getBalancedSplits(final List<Pair<SnapshotFileInfo, Long>> files, final int ngroups) {
    // Sort files by size, from small to big
    Collections.sort(files, new Comparator<Pair<SnapshotFileInfo, Long>>() {
      public int compare(Pair<SnapshotFileInfo, Long> a, Pair<SnapshotFileInfo, Long> b) {
        long r = a.getSecond() - b.getSecond();
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/RegionHDFSBlockLocationFinder.java`
#### Snippet
```java
      public ListenableFuture<HDFSBlocksDistribution> reload(final RegionInfo hri,
        HDFSBlocksDistribution oldValue) throws Exception {
        return executor.submit(new Callable<HDFSBlocksDistribution>() {
          @Override
          public HDFSBlocksDistribution call() throws Exception {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java`
#### Snippet
```java
    // We only need to assign the regionsToMove to
    // the first n = regionsToMove.size() RS that has least load.
    Collections.sort(serverLoadList, new Comparator<ServerAndLoad>() {
      @Override
      public int compare(ServerAndLoad s1, ServerAndLoad s2) {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePreadReader.java`
#### Snippet
```java
    // Prefetch file blocks upon open if requested
    if (cacheConf.shouldPrefetchOnOpen() && cacheIfCompactionsOff()) {
      PrefetchExecutor.request(path, new Runnable() {
        @Override
        public void run() {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.java`
#### Snippet
```java
   */
  public Set<Integer> getLeastFilledBuckets(Set<Integer> excludedBuckets, int bucketCount) {
    Queue<Integer> queue = MinMaxPriorityQueue.<Integer> orderedBy(new Comparator<Integer>() {
      @Override
      public int compare(Integer left, Integer right) {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
        CountDownLatch latch = new CountDownLatch(tableDirs.size());
        for (Path dir : tableDirs) {
          executor.submit(new Runnable() {
            @Override
            public void run() {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java`
#### Snippet
```java
   */
  static final Comparator<FileStatus> TABLEINFO_FILESTATUS_COMPARATOR =
    new Comparator<FileStatus>() {
      @Override
      public int compare(FileStatus left, FileStatus right) {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
    List<RegionInfo> regionInfos = new ArrayList<>();
    for (final RegionInfo newRegion : newRegions) {
      completionService.submit(new Callable<RegionInfo>() {
        @Override
        public RegionInfo call() throws IOException {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java`
#### Snippet
```java
    final ExecutorCompletionService<Void> completionService = new ExecutorCompletionService<>(exec);
    for (final RegionInfo hri : regions) {
      completionService.submit(new Callable<Void>() {
        @Override
        public Void call() throws IOException {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java`
#### Snippet
```java
  }

  final static Comparator<HbckRegionInfo> COMPARATOR = new Comparator<HbckRegionInfo>() {
    @Override
    public int compare(HbckRegionInfo l, HbckRegionInfo r) {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java
          }

          futures.add(executor.submit(new Runnable() {
            @Override
            public void run() {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java`
#### Snippet
```java

      // comparator to sort KeyValues with latest modtime
      final Comparator<Cell> comp = new Comparator<Cell>() {
        @Override
        public int compare(Cell k1, Cell k2) {
```

### Convert2Lambda
Anonymous new Function() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java`
#### Snippet
```java
  private static final int DEFAULT_RETRIES_NUMBER = 3;

  private static final Function<File, Path> FUNC_FILE_TO_PATH = new Function<File, Path>() {
    @Override
    public Path apply(File file) {
```

### Convert2Lambda
Anonymous new Comparator\>() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/ClusterStatusPublisher.java`
#### Snippet
```java
    // We're sending the new deads first.
    List<Map.Entry<ServerName, Integer>> entries = new ArrayList<>(lastSent.entrySet());
    Collections.sort(entries, new Comparator<Map.Entry<ServerName, Integer>>() {
      @Override
      public int compare(Map.Entry<ServerName, Integer> o1, Map.Entry<ServerName, Integer> o2) {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
        }

        Runnable getRegionStoreFileMapCall = new Runnable() {
          @Override
          public void run() {
```

### Convert2Lambda
Anonymous new UncaughtExceptionHandler() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.java`
#### Snippet
```java
  @Override
  protected UncaughtExceptionHandler getUncaughtExceptionHandler() {
    return new UncaughtExceptionHandler() {

      @Override
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java`
#### Snippet
```java
  }

  private static final Comparator<HFileDeleteTask> COMPARATOR = new Comparator<HFileDeleteTask>() {

    @Override
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
    waitUntilStarted();
    if (aclZNode.equals(ZKUtil.getParent(path))) {
      asyncProcessNodeUpdate(new Runnable() {
        @Override
        public void run() {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
    waitUntilStarted();
    if (aclZNode.equals(ZKUtil.getParent(path))) {
      asyncProcessNodeUpdate(new Runnable() {
        @Override
        public void run() {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
      if (ZKUtil.watchAndCheckExists(watcher, aclZNode)) {
        try {
          executor.submit(new Callable<Void>() {
            @Override
            public Void call() throws KeeperException {
```

### Convert2Lambda
Anonymous new Runnable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java`
#### Snippet
```java
    waitUntilStarted();
    if (path.equals(aclZNode)) {
      asyncProcessNodeUpdate(new Runnable() {
        @Override
        public void run() {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.java`
#### Snippet
```java
      new ExecutorCompletionService<>(executor);
    for (final FileStatus st : manifestFiles) {
      completionService.submit(new Callable<SnapshotRegionManifest>() {
        @Override
        public SnapshotRegionManifest call() throws IOException {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.java`
#### Snippet
```java
      new ExecutorCompletionService<>(executor);
    for (final FileStatus region : regions) {
      completionService.submit(new Callable<SnapshotRegionManifest>() {
        @Override
        public SnapshotRegionManifest call() throws IOException {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java`
#### Snippet
```java

    for (final SnapshotRegionManifest regionManifest : regionManifests) {
      completionService.submit(new Callable<Void>() {
        @Override
        public Void call() throws IOException {
```

### Convert2Lambda
Anonymous new Comparator() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java`
#### Snippet
```java
  }

  private static final Comparator<Constraint> constraintComparator = new Comparator<Constraint>() {
    @Override
    public int compare(Constraint c1, Constraint c2) {
```

### Convert2Lambda
Anonymous new Predicate() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ScopeWALEntryFilter.java`
#### Snippet
```java
    byte[] family = CellUtil.cloneFamily(cell);
    if (CellUtil.matchingColumn(cell, WALEdit.METAFAMILY, WALEdit.BULK_LOAD)) {
      return bulkLoadFilter.filterCell(cell, new Predicate<byte[]>() {
        @Override
        public boolean apply(byte[] family) {
```

### Convert2Lambda
Anonymous new Predicate() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationHFileCleaner.java`
#### Snippet
```java
      return Collections.emptyList();
    }
    return Iterables.filter(files, new Predicate<FileStatus>() {
      @Override
      public boolean apply(FileStatus file) {
```

### Convert2Lambda
Anonymous new Predicate() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java`
#### Snippet
```java
      return Collections.emptyList();
    }
    return Iterables.filter(files, new Predicate<FileStatus>() {
      @Override
      public boolean apply(FileStatus file) {
```

### Convert2Lambda
Anonymous new Comparator\>() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
   */
  private static final Set<Class<? extends Coprocessor>> legacyWarning =
    new ConcurrentSkipListSet<>(new Comparator<Class<? extends Coprocessor>>() {
      @Override
      public int compare(Class<? extends Coprocessor> c1, Class<? extends Coprocessor> c2) {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java`
#### Snippet
```java
        new ExecutorCompletionService<>(storeFileCloserThreadPool);
      for (HStoreFile f : result) {
        completionService.submit(new Callable<Void>() {
          @Override
          public Void call() throws IOException {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.java`
#### Snippet
```java
      new ThreadFactoryBuilder().setDaemon(true).setNameFormat("WAL-Shutdown-%d").build());

    Future<Void> future = shutdownExecutor.submit(new Callable<Void>() {
      @Override
      public Void call() throws Exception {
```

### Convert2Lambda
Anonymous new Callable() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
      for (final ColumnFamilyDescriptor family : htableDescriptor.getColumnFamilies()) {
        status.setStatus("Instantiating store for column family " + family);
        completionService.submit(new Callable<HStore>() {
          @Override
          public HStore call() throws IOException {
```

### Convert2Lambda
Anonymous new Callable\>\>() can be replaced with lambda
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
            }
          }
          completionService.submit(new Callable<Pair<byte[], Collection<HStoreFile>>>() {
            @Override
            public Pair<byte[], Collection<HStoreFile>> call() throws IOException {
```

## RuleId[id=UnnecessaryContinue]
### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeSaslClientAuthenticationProvider.java`
#### Snippet
```java
      for (Callback callback : callbacks) {
        if (callback instanceof RealmChoiceCallback) {
          continue;
        } else if (callback instanceof NameCallback) {
          nc = (NameCallback) callback;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-examples/src/main/java/org/apache/hadoop/hbase/security/provider/example/ShadeSaslServerAuthenticationProvider.java`
#### Snippet
```java
          pc = (PasswordCallback) callback;
        } else if (callback instanceof RealmCallback) {
          continue; // realm is ignored
        } else {
          throw new UnsupportedCallbackException(callback, "Unrecognized SASL PLAIN Callback");
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegistryEndpointsRefresher.java`
#### Snippet
```java
            LOG.warn("Interrupted during wait", e);
            Thread.currentThread().interrupt();
            continue;
          }
        }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      ) {
        i++;
        continue;
      }
    }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java`
#### Snippet
```java
      ) {
        // WHITESPACE or TAB found
        continue;
      } else if (checkForOr(filterStringAsByteArray, i)) {
        // OR found
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-client/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslClientAuthenticationProvider.java`
#### Snippet
```java
      for (Callback callback : callbacks) {
        if (callback instanceof RealmChoiceCallback) {
          continue;
        } else if (callback instanceof NameCallback) {
          nc = (NameCallback) callback;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutputSaslHelper.java`
#### Snippet
```java
      for (Callback callback : callbacks) {
        if (callback instanceof RealmChoiceCallback) {
          continue;
        } else if (callback instanceof NameCallback) {
          nc = (NameCallback) callback;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java`
#### Snippet
```java
        ServerName server = servers.get((j + serverIdx) % numServers);
        if (cluster.wouldLowerAvailability(region, server)) {
          continue;
        } else {
          assignments.computeIfAbsent(server, k -> new ArrayList<>()).add(region);
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
        LOG.warn("Cannot place the favored nodes for region " + regionInfo.getRegionNameAsString()
          + " because " + e, e);
        continue;
      }
    }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-balancer/src/main/java/org/apache/hadoop/hbase/favored/FavoredNodeAssignmentHelper.java`
#### Snippet
```java
        LOG.warn("Cannot place the favored nodes for region " + regionInfo.getRegionNameAsString()
          + " because " + e, e);
        continue;
      }
    }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/mob/RSMobFileCleanerChore.java`
#### Snippet
```java
                currentPath, e);
              regionMobs.clear();
              continue;
            } catch (IOException e) {
              LOG.error("Failed to clean the obsolete mob files for table={}",
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/hbck/HbckChore.java`
#### Snippet
```java
        if (hri == null && !mergedParentRegions.contains(encodedRegionName)) {
          report.getOrphanRegionsOnFS().put(encodedRegionName, regionDir);
          continue;
        }
      }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java`
#### Snippet
```java
            throw (InterruptedIOException) new InterruptedIOException().initCause(e);
          }
          continue;
        } else {
          throw cause;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/security/provider/DigestSaslServerAuthenticationProvider.java`
#### Snippet
```java
          pc = (PasswordCallback) callback;
        } else if (callback instanceof RealmCallback) {
          continue; // realm is ignored
        } else {
          throw new UnsupportedCallbackException(callback, "Unrecognized SASL DIGEST-MD5 Callback");
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSource.java`
#### Snippet
```java
        Path newPath = getReplSyncUpPath(path);
        newPaths.add(newPath);
        continue;
      } else {
        // See if Path exists in the dead RS folder (there could be a chain of failures
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SegmentScanner.java`
#### Snippet
```java
        keepSeeking = true;
        key = firstKeyOnPreviousRow;
        continue;
      } else {
        keepSeeking = false;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
          }
        } catch (InterruptedException ex) {
          continue;
        } catch (ConcurrentModificationException ex) {
          continue;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java`
#### Snippet
```java
          continue;
        } catch (ConcurrentModificationException ex) {
          continue;
        } catch (Exception ex) {
          LOG.error("Cache flusher failed for entry " + fqe, ex);
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java`
#### Snippet
```java
            keepSeeking = true;
            key = firstKeyOfPreviousRow;
            continue;
          } else {
            keepSeeking = false;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java`
#### Snippet
```java
            if (bulkLoadEvent != null) {
              hRegion.replayWALBulkLoadEventMarker(bulkLoadEvent);
              continue;
            }
          }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/BrokenStoreFileCleaner.java`
#### Snippet
```java
          } catch (IOException e) {
            LOG.warn("Failed to list files in {}, cleanup is skipped there", storePath);
            continue;
          }
        }
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java`
#### Snippet
```java
      long maxTtl = storeConfigInfo.getStoreFileTtl();
      if (maxTtl == Long.MAX_VALUE || (currentTime - maxTtl < maxTs)) {
        continue;
      } else {
        return true;
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.java`
#### Snippet
```java
      long maxTtl = storeConfigInfo.getStoreFileTtl();
      if (maxTtl == Long.MAX_VALUE || (currentTime - maxTtl < maxTs)) {
        continue;
      } else if (filesCompacting == null || !filesCompacting.contains(sf)) {
        expiredStores.add(sf);
```

### UnnecessaryContinue
`continue` is unnecessary as the last statement in a loop
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          } catch (IOException e) {
            handleException(fs.getFileSystem(), filePath, e);
            continue;
          }
        }
```

## RuleId[id=RegExpUnnecessaryNonCapturingGroup]
### RegExpUnnecessaryNonCapturingGroup
Unnecessary non-capturing group `(?:_SeqId_[0-9]+_)`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java

  /** Regex that will work for hfiles */
  private static final Pattern HFILE_NAME_PATTERN = Pattern.compile("^(" + HFILE_NAME_REGEX + ")");

  /**
```

### RegExpUnnecessaryNonCapturingGroup
Unnecessary non-capturing group `(?:_del)`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java

  /** Regex that will work for hfiles */
  private static final Pattern HFILE_NAME_PATTERN = Pattern.compile("^(" + HFILE_NAME_REGEX + ")");

  /**
```

### RegExpUnnecessaryNonCapturingGroup
Unnecessary non-capturing group `(?:_SeqId_[0-9]+_)`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
   * Bulk loaded hfiles has (_SeqId_[0-9]+_) has suffix. The mob del file has (_del) as suffix.
   */
  public static final String HFILE_NAME_REGEX = "[0-9a-f]+(?:(?:_SeqId_[0-9]+_)|(?:_del))?";

  /** Regex that will work for hfiles */
```

### RegExpUnnecessaryNonCapturingGroup
Unnecessary non-capturing group `(?:_del)`
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java`
#### Snippet
```java
   * Bulk loaded hfiles has (_SeqId_[0-9]+_) has suffix. The mob del file has (_del) as suffix.
   */
  public static final String HFILE_NAME_REGEX = "[0-9a-f]+(?:(?:_SeqId_[0-9]+_)|(?:_del))?";

  /** Regex that will work for hfiles */
```

## RuleId[id=CopyConstructorMissesField]
### CopyConstructorMissesField
Copy constructor does not copy field 'durability'
in `hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java`
#### Snippet
```java
  }

  protected Mutation(Mutation clone) {
    super(clone);
    this.row = clone.getRow();
```

## RuleId[id=EqualsWhichDoesntCheckParameterClass]
### EqualsWhichDoesntCheckParameterClass
`equals()` should check the class of its parameter
in `hbase-client/src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java`
#### Snippet
```java
  @Override
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value = "EQ_UNUSUAL", justification = "")
  public boolean equals(Object obj) {
    return obj == null;
  }
```

### EqualsWhichDoesntCheckParameterClass
`equals()` should check the class of its parameter
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java`
#### Snippet
```java

  @Override
  public boolean equals(Object o) {
    return map.equals(o);
  }
```

## RuleId[id=ThreadStartInConstruction]
### ThreadStartInConstruction
Call to `start()` during object construction
in `hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/BlockingRpcConnection.java`
#### Snippet
```java
    if (this.rpcClient.conf.getBoolean(BlockingRpcClient.SPECIFIC_WRITE_THREAD, false)) {
      callSender = new CallSender(threadName, this.rpcClient.conf);
      callSender.start();
    } else {
      callSender = null;
```

### ThreadStartInConstruction
Call to `start()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/MetaRegionLocationCache.java`
#### Snippet
```java
      SLEEP_INTERVAL_MS_BETWEEN_RETRIES, SLEEP_INTERVAL_MS_MAX);
    threadFactory.newThread(() -> loadMetaLocationsFromZk(retryFactory.create(), ZNodeOpType.INIT))
      .start();
  }

```

### ThreadStartInConstruction
Call to `start()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java`
#### Snippet
```java
    if (evictionThread) {
      this.evictionThread = new EvictionThread(this);
      this.evictionThread.start(); // FindBugs SC_START_IN_CTOR
    } else {
      this.evictionThread = null;
```

### ThreadStartInConstruction
Call to `start()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.java`
#### Snippet
```java
    if (evictionThread) {
      this.evictionThread = new EvictionThread(this);
      this.evictionThread.start(); // FindBugs SC_START_IN_CTOR
    } else {
      this.evictionThread = null;
```

### ThreadStartInConstruction
Call to `start()` during object construction
in `hbase-server/src/main/java/org/apache/hadoop/hbase/master/region/MasterRegionFlusherAndCompactor.java`
#### Snippet
```java
    flushThread = new Thread(this::flushLoop, region.getRegionInfo().getTable() + "-Flusher");
    flushThread.setDaemon(true);
    flushThread.start();
    compactExecutor = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()
      .setNameFormat(region.getRegionInfo().getTable() + "-Store-Compactor").setDaemon(true)
```

## RuleId[id=BigDecimalMethodWithoutRoundingCalled]
### BigDecimalMethodWithoutRoundingCalled
'BigDecimal.divide()' called without a rounding mode argument
in `hbase-client/src/main/java/org/apache/hadoop/hbase/Size.java`
#### Snippet
```java
    BigDecimal rval = BigDecimal.valueOf(value);
    for (int i = 0; i != Math.abs(diff); ++i) {
      rval = diff > 0 ? rval.multiply(SCALE_BASE) : rval.divide(SCALE_BASE);
    }
    return rval.doubleValue();
```

## RuleId[id=CastCanBeRemovedNarrowingVariableType]
### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'cl' to 'CoprocessorClassLoader'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java`
#### Snippet
```java
        CoprocessorClassLoader.getClassLoader(path, getClass().getClassLoader(), pathPrefix, conf);
      try {
        implClass = ((CoprocessorClassLoader) cl).loadClass(className, includedClassPrefixes);
      } catch (ClassNotFoundException e) {
        throw new IOException("Cannot load external coprocessor class " + className, e);
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'r' to 'HRegion'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java`
#### Snippet
```java
        // stop serving reads
        if (isRegionStale(encodedName, time)) {
          ((HRegion) r).setReadsEnabled(false); // stop serving reads
        }
        continue;
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'r' to 'HRegion'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.java`
#### Snippet
```java
      }
      lastRefreshTimes.put(encodedName, time);
      ((HRegion) r).setReadsEnabled(true); // restart serving reads
    }

```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'r' to 'HRegion'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java`
#### Snippet
```java
        }

        HRegion hr = (HRegion) r;
        for (HStore s : hr.stores.values()) {
          try {
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'region' to 'HRegion'
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
      for (Region region : hrs.getOnlineRegionsLocalContext()) {
        if (region.getTableDescriptor().getTableName().equals(tableName)) {
          ret.add((HRegion) region);
        }
      }
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'region' to 'HRegion'
in `hbase-testing-util/src/main/java/org/apache/hadoop/hbase/MiniHBaseCluster.java`
#### Snippet
```java
      for (Region region : hrs.getRegions(tableName)) {
        if (region.getTableDescriptor().getTableName().equals(tableName)) {
          ret.add((HRegion) region);
        }
      }
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'fs' to 'FlushResultImpl'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        FlushResult fs = flushcache(true, false, FlushLifeCycleTracker.DUMMY);
        if (fs.isFlushSucceeded()) {
          seqId = ((FlushResultImpl) fs).flushSequenceId;
        } else if (fs.getResult() == FlushResult.Result.CANNOT_FLUSH_MEMSTORE_EMPTY) {
          seqId = ((FlushResultImpl) fs).flushSequenceId;
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'fs' to 'FlushResultImpl'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
          seqId = ((FlushResultImpl) fs).flushSequenceId;
        } else if (fs.getResult() == FlushResult.Result.CANNOT_FLUSH_MEMSTORE_EMPTY) {
          seqId = ((FlushResultImpl) fs).flushSequenceId;
        } else if (fs.getResult() == FlushResult.Result.CANNOT_FLUSH) {
          // CANNOT_FLUSH may mean that a flush is already on-going
```

### CastCanBeRemovedNarrowingVariableType
Cast may be removed by changing the type of 'fs' to 'FlushResultImpl'
in `hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java`
#### Snippet
```java
        } else {
          throw new IOException("Could not bulk load with an assigned sequential ID because the "
            + "flush didn't run. Reason for not flushing: " + ((FlushResultImpl) fs).failureReason);
        }
      }
```

